{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8abd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM config\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"Qwen/QwQ-32B\",\n",
    "#     base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "#     api_key=getenv(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "smol = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-8b:free\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-8b:free\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Loading \n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from scraping.metadata import extract_metadata\n",
    "import pprint\n",
    "\n",
    "directory_name = \"data\"\n",
    "file_name = \"2505.00312 aware-net_adaptive_weighted_averaging_for_robust_ensemble_network_in_deepfake_detection.pdf\"\n",
    "\n",
    "#TODO: generalize this to automatically split all files from a directory\n",
    "# from langchain_community.document_loaders import FileSystemBlobLoader\n",
    "# from langchain_community.document_loaders.generic import GenericLoader\n",
    "# from langchain_community.document_loaders.parsers import PyMuPDFParser\n",
    "\n",
    "# loader = GenericLoader(\n",
    "#     blob_loader=FileSystemBlobLoader(\n",
    "#         path=directory_name,\n",
    "#         glob=\"*.pdf\",\n",
    "#     ),\n",
    "#     blob_parser=PyMuPDFParser(),\n",
    "# )\n",
    "# documents = loader.load()\n",
    "# print(documents[0].page_content)\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path=f\"{directory_name}/{file_name}\",\n",
    "    extract_tables=\"markdown\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "#TODO: clean up the documents byremoving references etc.\n",
    "\n",
    "def clean_arxiv_content(text):\n",
    "\n",
    "    import re\n",
    "    # Remove references, bibliography or works cited sections regardless of case or extra formatting\n",
    "    text = re.sub(\n",
    "        r'\\n\\s*(References|Bibliography|Works Cited)\\s*:?\\s*\\n.*', \n",
    "        '', \n",
    "        text, \n",
    "        flags=re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # Remove citation patterns [1], [2-5], (Author, 2023)\n",
    "    text = re.sub(r'\\[\\d+(?:[-,]\\s*\\d+)*\\]', '', text)\n",
    "    text = re.sub(r'\\([A-Za-z\\s]+,?\\s*\\d{4}[a-z]?\\)', '', text)\n",
    "    \n",
    "    # Clean LaTeX artifacts\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', text)  # \\textbf{}, \\cite{}, etc.\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)  # \\section, \\subsection\n",
    "    \n",
    "    # # Remove figure/table references\n",
    "    # text = re.sub(r'Figure\\s+\\d+', 'Figure', text, flags=re.IGNORECASE)\n",
    "    # text = re.sub(r'Table\\s+\\d+', 'Table', text, flags=re.IGNORECASE)\n",
    "    # text = re.sub(r'Equation\\s+\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# print(\"\\nBefore cleaning:\")\n",
    "# print(documents[6].page_content)\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_arxiv_content(doc.page_content)\n",
    "\n",
    "# print(\"\\nAfter cleaning:\")\n",
    "# print(documents[6].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 150,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# replace metadata of every chunk with the metadata of the original document\n",
    "metadata = extract_metadata(file_name)\n",
    "for i, text in enumerate(texts):\n",
    "    texts[i].metadata = metadata\n",
    "\n",
    "\n",
    "print(f\"Total number of chunks: {len(texts)}\")\n",
    "# print(texts[0].metadata)\n",
    "print(f\"First chunk content: {texts[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecc286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "collection_name = \"arxiv-abstracts\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "   model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"size\": 768, # Size of the embedding vector\n",
    "        \"distance\": \"Cosine\"\n",
    "    }\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1b8010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 2505.00312}, page_content=\"deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. while multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. in response, we propose AWARE-NET - a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: xception, res2net101, and efficientnet-b7. our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. our experiments achieved state-of-the-art intra-dataset performance with auc scores of 99.22% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.06% (ff++) and 99.94% (celebdf-v2) without augmentation. with augmentation, we achieve auc scores of 99.47% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.43% (ff++) and 99.95% (celebdf-v2). the framework demonstrates robust cross-dataset generalization, achieving auc scores of 88.20% and 72.52%, and f1 scores of 93.16% and 80.62% in cross-dataset evaluations.\"),\n",
       " Document(metadata={'id': 2505.00337}, page_content='text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. to fill this gap, we introduce \\\\textbf{t2vphysbench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including newtonian mechanics, conservation principles, and phenomenological effects. our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. the results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.'),\n",
       " Document(metadata={'id': 2505.00426}, page_content='3d part assembly aims to understand part relationships and predict their 6-dof poses to construct realistic 3d shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. however, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. in this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an iterative closest point (icp) process. then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. to verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. the code has been released on https://github.com/ruiyuan-zhang/zero-shot-assembly.'),\n",
       " Document(metadata={'id': 2505.00452}, page_content='the problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. however, its practical applicability remains limited, particularly in real-world outdoor scenarios. these environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3d lines, and varying lighting conditions, making the task notoriously difficult. furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. in this study, we present a small dataset named \"clearlines\", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3d line detection algorithms.'),\n",
       " Document(metadata={'id': 2505.00482}, page_content='we present jointdit, a diffusion transformer that models the joint distribution of rgb and depth. by leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, jointdit not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. this solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. with these techniques, we train our model across all noise levels for each modality, enabling jointdit to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. jointdit demonstrates outstanding joint generation performance. furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. the project page is available at https://byungki-k.github.io/jointdit/.'),\n",
       " Document(metadata={'id': 2505.00584}, page_content='detecting and tracking objects is a crucial component of any autonomous navigation method. for the past decades, object detection has yielded promising results using neural networks on various datasets. while many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. in this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar autonomous vehicle (av) datasets. our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. we also present our results of a baseline lightweight noise recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\\\\% on 11 categories across 10086 images and 2145 radar point-clouds.'),\n",
       " Document(metadata={'id': 2505.00681}, page_content='multimodal llms are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. this makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. to remedy this, we provide a new video reasoning dataset called minerva for modern multimodal models. each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. we perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. we use this to explore both human and llm-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. the dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\\\#minerva.'),\n",
       " Document(metadata={'id': 2505.00703}, page_content='recent advancements in large language models have demonstrated how chain-of-thought (cot) and reinforcement learning (rl) can improve performance. however, applying such reasoning strategies to the visual generation domain remains largely unexplored. in this paper, we present t2i-r1, a novel reasoning-enhanced text-to-image generation model, powered by rl with a bi-level cot reasoning process. specifically, we identify two levels of cot that can be utilized to enhance different stages of generation: (1) the semantic-level cot for high-level planning of the prompt and (2) the token-level cot for low-level pixel processing during patch-by-patch generation. to better coordinate these two levels of cot, we introduce bicot-grpo with an ensemble of generation rewards, which seamlessly optimizes both generation cots within the same training step. by applying our reasoning strategies to the baseline model, janus-pro, we achieve superior performance with 13% improvement on t2i-compbench and 19% improvement on the wise benchmark, even surpassing the state-of-the-art model flux.1. code is available at: https://github.com/caraj7/t2i-r1'),\n",
       " Document(metadata={'id': 2505.00986}, page_content='continual test-time adaptation (ctta) continuously adapts the deployed model on every incoming batch of data. while achieving optimal accuracy, existing ctta approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. in this work, we first introduce a novel paradigm -- on-demand tta -- which triggers adaptation only when a significant domain shift is detected. then, we present od-tta, an on-demand tta framework for accurate and efficient adaptation on edge devices. od-tta comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate tta only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled batch normalization (bn) update scheme to enable memory-efficient adaptation with small batch sizes. extensive experiments show that od-tta achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making tta a practical reality.'),\n",
       " Document(metadata={'id': 2505.00998}, page_content='human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. recent score-based generative models (sgms) have demonstrated impressive results on this task. however, their training process involves complex curvature trajectories, leading to unstable training process. in this paper, we propose a deterministic-to-stochastic diverse latent feature mapping (dsdfm) method for human motion synthesis. dsdfm consists of two stages. the first human motion reconstruction stage aims to learn the latent space distribution of human motions. the second diverse motion generation stage aims to build connections between the gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. this stage is achieved by the designed deterministic feature mapping procedure with derode and stochastic diverse output generation procedure with divsde.dsdfm is easy to train compared to previous sgms-based methods and can enhance diversity without introducing additional training parameters.through qualitative and quantitative experiments, dsdfm achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.'),\n",
       " Document(metadata={'id': 2505.01007}, page_content='this paper proves a new watermarking method to embed the ownership information into a deep neural network (dnn), which is robust to fine-tuning. specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised fourier transform to extract frequency components from the convolutional filter. additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. in this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. preliminary experiments demonstrate the effectiveness of our method.'),\n",
       " Document(metadata={'id': 2505.01016}, page_content='the success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. while fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. the critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. we adapt a standard yolov8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original coco validation set. our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\\\% absolute map50) on the fine-grained fruit task compared to only training the head. strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\\\\% absolute map difference) on the coco benchmark across all tested freeze levels. we conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.'),\n",
       " Document(metadata={'id': 2505.01263}, page_content='movie dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. to address these issues, we propose a large language model (llm) based flow matching architecture for dubbing, named flowdubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. first, we introduce qwen2.5 as the backbone of llm to learn the in-context sequence from movie scripts and reference audio. then, the proposed semantic-aware learning focuses on capturing llm semantic knowledge at the phoneme level. next, dual contrastive aligning (dca) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. finally, the proposed flow-based voice enhancing (fve) improves acoustic quality in two aspects, which introduces an llm-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. the demos are available at {\\\\href{https://galaxycong.github.io/llm-flow-dubber/}{\\\\textcolor{red}{https://galaxycong.github.io/llm-flow-dubber/}}}.'),\n",
       " Document(metadata={'id': 2505.01313}, page_content='this paper proposes a neural architecture search space using resnet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. in addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. the experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the mnist, fashion-mnist and cifar100 datasets.'),\n",
       " Document(metadata={'id': 2505.01406}, page_content=\"the rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. in this work, we introduce vidstamp, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. by fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, vidstamp learns to embed high-capacity, flexible watermarks with minimal perceptual impact. leveraging architectural components such as 3d convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. vidstamp embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log p-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. code: code: \\\\url{https://github.com/spin-umass/vidstamp}\"),\n",
       " Document(metadata={'id': 2505.01583}, page_content='understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. we propose tempura (temporal event masked prediction and understanding for reasoning in action), a two-stage training framework that enhances video temporal understanding. tempura first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. tempura then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. we train tempura on ver, a large-scale dataset curated by us that comprises 1m training instances and 500k videos with temporally aligned event descriptions and structured reasoning steps. experiments on temporal grounding and highlight detection benchmarks demonstrate that tempura outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.'),\n",
       " Document(metadata={'id': 2505.01657}, page_content=\"personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. although effective, existing methods face two main issues. first, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. to address these issues, we propose retrieval augment personalized image generation guided by recommendation (ragar). our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. extensive experiments and human evaluations on three real-world datasets demonstrate that ragar achieves significant improvements in both personalization and semantic metrics compared to five baselines.\"),\n",
       " Document(metadata={'id': 2505.01713}, page_content='long-term action anticipation (lta) aims to predict future actions over an extended period. previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. recent researches leverage large language models (llms) by utilizing text-based inputs which suffer severe information loss. to tackle these limitations single-modality methods face, we propose a novel intention-conditioned vision-language (icvl) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of llms. considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (vlm) to infer behavioral intentions as comprehensive textual features directly from video inputs. the inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. these enhanced visual representations, along with textual prompts, are fed into llm for future action anticipation. furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. extensive experiments with state-of-the-art performance on ego4d, epic-kitchens-55, and egtea gaze+ datasets fully demonstrate the effectiveness and superiority of the proposed method.'),\n",
       " Document(metadata={'id': 2505.01958}, page_content='large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.'),\n",
       " Document(metadata={'id': 2505.01996}, page_content='we highlight a curious empirical result within modern vision transformers (vits). specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. this is in contrast to other elements of a vit that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\\\\eg, cnns) exhibiting good performance in their absence. in this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. additionally, we propose token graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. we validate our approach in both supervised and self-supervised training methods.'),\n",
       " Document(metadata={'id': 2505.02018}, page_content='reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. in this paper, we introduce a graduate-level, multi-disciplinary, englishchinese benchmark, dubbed as reasoning bench (r-bench), for assessing the reasoning capability of both language and multimodal models. rbench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both english and chinese. these questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an olympiad-level multi-disciplinary benchmark. we evaluate widely used models, including openai o1, gpt-4o, deepseek-r1, etc. experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. even the top-performing model openai o1 achieves only 53.2% accuracy on our multimodal evaluation. data and code are made publicly available at here.'),\n",
       " Document(metadata={'id': 2505.02048}, page_content='the acquisition of information-rich images within a limited time budget is crucial in medical imaging. medical image translation (mit) can help enhance and supplement existing datasets by generating synthetic images from acquired data. while generative adversarial nets (gans) and diffusion models (dms) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. in fact, the imitation of acquisition noise or content hallucination hinder clinical utility. here, we introduce yoda (you only denoise once - or average), a novel 2.5d diffusion-based framework for volumetric mit. yoda unites diffusion and regression paradigms to produce realistic or noise-free outputs. furthermore, we propose expectation-approximation (expa) dm sampling, which draws inspiration from mri signal averaging. expa-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain mri and pelvic mri-ct - we show that diffusion and regression sampling yield similar results in practice. as such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. building on these insights, we demonstrate that yoda outperforms several state-of-the-art gan and dm methods. notably, yoda-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. our findings challenge the presumed advantages of dms in mit and pave the way for the practical application of mit in medical imaging.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "df = pd.read_csv('final_paper_list.csv')\n",
    "\n",
    "abstr = df['abstract']\n",
    "ids = df['id']\n",
    "df = pd.DataFrame({\n",
    "    \"page_content\": abstr,\n",
    "    \"metadata\": [id for id in ids]\n",
    "})\n",
    "df = df.rename(columns={\"page_content\": \"content\", \"metadata\": \"metadata\"})\n",
    "\n",
    "\n",
    "abstracts = [Document(page_content=row['content'], metadata={\"id\": row['metadata']}) for _, row in df.iterrows()]\n",
    "\n",
    "abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "610083cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f46e2114e96e4da2955c1438f001faa3',\n",
       " '7d99f515539c413c84f7e30902162a4a',\n",
       " 'c66e978a766f46c09a0403ef42cb486f',\n",
       " 'dd04fdd2e53b43fd8353615c60fdbffa',\n",
       " '331abeeb596b49af96afa80b4dd6dcaf',\n",
       " '06fdc245d08c4d718eec22da2e5a5ffd',\n",
       " '8490a6ddf92e4680a4a303a21d209dab',\n",
       " '29a44c22348e48bab29d875139e65817',\n",
       " '5780e2cd1aba460980640a1e50e28cf0',\n",
       " '4df79840fc8f4e259560384877875bca',\n",
       " '6ea0fe3d16c340cfb60ccdd655206f92',\n",
       " '23f458a244fb40b78ee572beea62c2cf',\n",
       " '439b9c39e8f8484dae7e385536cc01d3',\n",
       " '7682fae2fecb459aa17039b88b0b53bd',\n",
       " '7debd3796e084a2db5b04101d5e23acf',\n",
       " 'ddd8502c98b34bbfbb7411ceee5ad83b',\n",
       " '7de6bfc70023425cae440530413ded21',\n",
       " '0b63516cd0bd4dd6855a59273bdba141',\n",
       " '4acf38a6e34f4c27b39acba334bac300',\n",
       " 'dec60f78ed8243bb85128f0b7fe5ffa0',\n",
       " 'f08e415888544e72ab8d43dc96098d04',\n",
       " '3754a06cf6de423183ab2d19427ca15f']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667047e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def evaluate_similarity_search(directory_path=\"final_datasets\"):\n",
    "    # Get all JSON files from the directory\n",
    "    json_files = list(Path(directory_path).glob(\"*.jsonl\"))\n",
    "    print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        paper_id = file_path.stem  # Get filename without extension\n",
    "        print(f\"Processing file: {file_path} with paper ID: {paper_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Read JSON file\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle both single object and list of objects\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            # Process each question in the file\n",
    "            for item in data:\n",
    "                if 'question' in item:\n",
    "                    total += 1\n",
    "                    question = item['question']\n",
    "                    \n",
    "                    # Perform similarity search\n",
    "                    results = vector_store.similarity_search(\n",
    "                        question,\n",
    "                        k=1,\n",
    "                        score_threshold=0.4\n",
    "                    )\n",
    "                    \n",
    "                    if results:\n",
    "                        top_result_id = results[0].metadata['id']\n",
    "                        \n",
    "                        # Compare paper_id with the top result's ID\n",
    "                        if str(top_result_id) == paper_id:\n",
    "                            matches += 1\n",
    "                            \n",
    "                    print(f\"Question: {question}\")\n",
    "                    print(f\"Expected ID: {paper_id}\")\n",
    "                    print(f\"Retrieved ID: {top_result_id if results else 'No results'}\")\n",
    "                    print(\"Match: \", str(top_result_id) == paper_id if results else False)\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing file {file_path}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (matches / total) * 100 if total > 0 else 0\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Total questions processed: {total}\")\n",
    "    print(f\"Correct matches: {matches}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_similarity_search(\"/Users/adithyankrishnan/Downloads/arxiv-rag/final_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"What new benchmarks are introduced in the paper by Liqiang Jing et al. for evaluating hallucination\"\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "   sq, \n",
    "    k=5, \n",
    "    score_threshold=0.4\n",
    ")\n",
    "results = [result.metadata for result in results]\n",
    "print(\"Search Results:\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import random\n",
    "\n",
    "def get_rewritten_queries(question: str, llm) -> List[str]:\n",
    "    \"\"\"Generate multiple versions of the input question using an LLM.\"\"\"\n",
    "    multi_query_template = PromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide ONLY the alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "    \n",
    "    multi_query_chain = multi_query_template | llm\n",
    "    queries = multi_query_chain.invoke({\"question\": question}).content.split('\\n')\n",
    "    \n",
    "    # Clean up queries and add original question\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "    queries.append(question)\n",
    "    return queries\n",
    "\n",
    "def get_top_paper_id_with_rank_aggregation(queries: List[str], vector_store, k: int = 3) -> str:\n",
    "    \"\"\"Get the paper ID with the lowest summed rank across multiple rewritten queries.\"\"\"\n",
    "    rank_scores = {}\n",
    "\n",
    "    for query in queries:\n",
    "        results = vector_store.similarity_search(query, k=k, score_threshold=0.4)\n",
    "        \n",
    "        for rank, doc in enumerate(results, start=1):  # rank starts at 1\n",
    "            paper_id = doc.metadata['id']\n",
    "            rank_scores[paper_id] = rank_scores.get(paper_id, 0) + rank\n",
    "    \n",
    "    if rank_scores:\n",
    "        # Return the paper with the lowest total rank sum\n",
    "        return min(rank_scores.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_top_paper_id(queries: List[str], vector_store) -> str:\n",
    "    \"\"\"Get the most frequent paper ID from multiple queries.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        results = vector_store.similarity_search(\n",
    "            query,\n",
    "            k=1,\n",
    "            score_threshold=0.4\n",
    "        )\n",
    "        if results:\n",
    "            all_results.append(results[0].metadata['id'])\n",
    "    \n",
    "    # Return most common paper ID if we have results, else None\n",
    "    if all_results:\n",
    "        return Counter(all_results).most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def evaluate_similarity_search_with_rewriting(directory_path: str = \"final_datasets\"):\n",
    "    \"\"\"Evaluate similarity search with query rewriting.\"\"\"\n",
    "    json_files = list(Path(directory_path).glob(\"*.jsonl\"))\n",
    "    print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "    \n",
    "    # Tracking metrics\n",
    "    total = 0\n",
    "    matches = 0\n",
    "    results_log: List[Dict] = []\n",
    "\n",
    "    index = random.randint(0, 50)\n",
    "    \n",
    "    for file_path in json_files[:7]:\n",
    "        paper_id = file_path.stem\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            for item in data[index:index+1]:  # Process only one item for brevity\n",
    "                if 'question' in item:\n",
    "                    total += 1\n",
    "                    question = item['question']\n",
    "                    \n",
    "                    # Get multiple versions of the question\n",
    "                    print(f\"\\nOriginal Question: {question}\")\n",
    "                    queries = get_rewritten_queries(question, smol)\n",
    "                    print(\"Rewritten queries:\")\n",
    "                    for i, q in enumerate(queries, 1):\n",
    "                        print(f\"{i}. {q}\")\n",
    "                    \n",
    "                    # Get most frequent paper ID from all queries\n",
    "                    predicted_paper_id = get_top_paper_id(queries, vector_store)\n",
    "                    print(f\"Predicted Paper ID for re-written queries: {predicted_paper_id}\")\n",
    "                    \n",
    "                    # Check if it's a match\n",
    "                    is_match = str(predicted_paper_id) == paper_id if predicted_paper_id else False\n",
    "                    if is_match:\n",
    "                        matches += 1\n",
    "                    \n",
    "                    # Log results\n",
    "                    result = {\n",
    "                        'original_question': question,\n",
    "                        'rewritten_queries': queries,\n",
    "                        'expected_id': paper_id,\n",
    "                        'predicted_id': predicted_paper_id,\n",
    "                        'is_match': is_match\n",
    "                    }\n",
    "                    results_log.append(result)\n",
    "                    \n",
    "                    print(f\"Expected ID: {paper_id}\")\n",
    "                    print(f\"Predicted ID: {predicted_paper_id}\")\n",
    "                    print(f\"Match: {is_match}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    accuracy = (matches / total) * 100 if total > 0 else 0\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Total questions processed: {total}\")\n",
    "    print(f\"Correct matches: {matches}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return results_log\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluate_similarity_search_with_rewriting(\"/Users/adithyankrishnan/Downloads/arxiv-rag/final_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17286209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "search_query = \"Explain the training implementation of the aware-net model in the paper in detail.\"\n",
    "\n",
    "# Multi-query generation\n",
    "\n",
    "multi_query_template = PromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide ONLY the alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "\n",
    "multi_query_chain = (\n",
    "    multi_query_template | smol\n",
    ")\n",
    "\n",
    "def split_queries(queries):\n",
    "    return queries.split(\"\\n\")\n",
    "\n",
    "multiple_queries = split_queries(multi_query_chain.invoke({\"question\": search_query}).content)\n",
    "\n",
    "print(\"Generated Queries:\")\n",
    "for i, query in enumerate(multiple_queries):\n",
    "    print(f\"{i+1}. {query}\")\n",
    "\n",
    "# TODO: set a threshold for similarity search score\n",
    "# TODO: decide a vaue of k OR make it configurable\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    search_query, \n",
    "    k=5, \n",
    "    score_threshold=0.4\n",
    ")\n",
    "\n",
    "# # pprint.pprint(results)\n",
    "\n",
    "# for i in results:\n",
    "#     print(i.page_content)\n",
    "#     print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# TODO: improve prompt to include more persona\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are an expert in CVPR topics and help students to learn by answering questions solely based on the provided context which are taken from research papers in arxiv.\n",
    "\n",
    "Focus on explaining concepts in detail and substantiate answers with relevant context from the given information.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Identify Key Concepts**: Upon receiving a question, pinpoint the core topics within CVPR relevant to the inquiry.\n",
    "2. **Contextual Analysis**: Thoroughly review the provided context to gather accurate and pertinent information specific to the question.\n",
    "3. **Detailed Explanation**: Craft a comprehensive explanation, incorporating key details and any relevant examples that illuminate the concept.\n",
    "4. **Clarification and Depth**: Ensure the response is clear, well-substantiated, and sufficiently detailed to aid student understanding.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Provide a paragraph elaborating the concept or answering the inquiry.\n",
    "- Ensure clarity and depth, utilizing examples if applicable.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always derive the response solely from the given context.\n",
    "- Ensure terminologies and technical details are accurately explained within the framework of the provided context.\n",
    "                                      \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer the question based on the context provided above. If the context is not sufficient, say \"I don't know\" or \"I don't have enough information to answer this question.\" Do not make up answers or provide information not present in the context.                                      \n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    prompt | llm\n",
    ")\n",
    "\n",
    "response1 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(results)})\n",
    "print(\"Response with context: \", response1.content)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "response2 = rag_chain.invoke({\"question\": search_query, \"context\": \"\"})\n",
    "print(\"Response without context: \", response2.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916722a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

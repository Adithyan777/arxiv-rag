{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8abd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM config\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "    api_key=getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Loading \n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from scraping.metadata import extract_metadata\n",
    "import pprint\n",
    "\n",
    "directory_name = \"data\"\n",
    "file_name = \"2505.00312 aware-net_adaptive_weighted_averaging_for_robust_ensemble_network_in_deepfake_detection.pdf\"\n",
    "\n",
    "#TODO: generalize this to automatically split all files from a directory\n",
    "# from langchain_community.document_loaders import FileSystemBlobLoader\n",
    "# from langchain_community.document_loaders.generic import GenericLoader\n",
    "# from langchain_community.document_loaders.parsers import PyMuPDFParser\n",
    "\n",
    "# loader = GenericLoader(\n",
    "#     blob_loader=FileSystemBlobLoader(\n",
    "#         path=directory_name,\n",
    "#         glob=\"*.pdf\",\n",
    "#     ),\n",
    "#     blob_parser=PyMuPDFParser(),\n",
    "# )\n",
    "# documents = loader.load()\n",
    "# print(documents[0].page_content)\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path=f\"{directory_name}/{file_name}\",\n",
    "    extract_tables=\"markdown\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "#TODO: clean up the documents byremoving references etc.\n",
    "\n",
    "def clean_arxiv_content(text):\n",
    "\n",
    "    import re\n",
    "    # Remove references, bibliography or works cited sections regardless of case or extra formatting\n",
    "    text = re.sub(\n",
    "        r'\\n\\s*(References|Bibliography|Works Cited)\\s*:?\\s*\\n.*', \n",
    "        '', \n",
    "        text, \n",
    "        flags=re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # Remove citation patterns [1], [2-5], (Author, 2023)\n",
    "    text = re.sub(r'\\[\\d+(?:[-,]\\s*\\d+)*\\]', '', text)\n",
    "    text = re.sub(r'\\([A-Za-z\\s]+,?\\s*\\d{4}[a-z]?\\)', '', text)\n",
    "    \n",
    "    # Clean LaTeX artifacts\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', text)  # \\textbf{}, \\cite{}, etc.\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)  # \\section, \\subsection\n",
    "    \n",
    "    # # Remove figure/table references\n",
    "    # text = re.sub(r'Figure\\s+\\d+', 'Figure', text, flags=re.IGNORECASE)\n",
    "    # text = re.sub(r'Table\\s+\\d+', 'Table', text, flags=re.IGNORECASE)\n",
    "    # text = re.sub(r'Equation\\s+\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # # Clean extra whitespace\n",
    "    # text = re.sub(r'\\s+', ' ', text)\n",
    "    # text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "print(\"\\nBefore cleaning:\")\n",
    "print(documents[6].page_content)\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_arxiv_content(doc.page_content)\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(documents[6].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "from langchain_text_splitters\n",
    "\n",
    "# replace metadata of every chunk with the metadata of the original document\n",
    "metadata = extract_metadata(file_name)\n",
    "for i, text in enumerate(texts):\n",
    "    texts[i].metadata = metadata\n",
    "\n",
    "\n",
    "print(f\"Total number of chunks: {len(texts)}\")\n",
    "# print(texts[0].metadata)\n",
    "print(f\"First chunk content: {texts[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "collection_name = \"arxiv-reader\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "   model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"size\": 768, # Size of the embedding vector\n",
    "        \"distance\": \"Cosine\"\n",
    "    }\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610083cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17286209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval\n",
    "\n",
    "search_query = \"Explain me about the implementation of aware-net in the paper in detail.\"\n",
    "\n",
    "# TODO: set a threshold for similarity search score\n",
    "# TODO: decide a vaue of k OR make it configurable\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    search_query, \n",
    "    k=3, \n",
    "    score_threshold=0.5\n",
    ")\n",
    "pprint.pprint(len(results))\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# TODO: improve prompt to include more persona\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    prompt | llm\n",
    ")\n",
    "\n",
    "response1 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(results)})\n",
    "print(\"Response with context: \", response1.content)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "response2 = rag_chain.invoke({\"question\": search_query, \"context\": \"\"})\n",
    "print(\"Response without context: \", response2.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916722a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Abstract

Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problem-solving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, English-Chinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. R-Bench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and cross-linguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI 01, GPT-40, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI 01 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available.

# Introduction

Setting goals is the first step in turning the invisible into the visible. — Tony Robbins

Reasoning, the systematic process of synthesizing knowledge to solve novel problems, lies at the heart of intelligence. Yet, as foundation models grow increasingly sophisticated, existing benchmarks fail to comprehensively assess their complex reasoning capabilities. Before equipping foundation models with reasoning skills, we must first define goals for them by establishing a reliable evaluation to assess their reasoning capabilities.

Realizing system-I (quick and intuitive thinking) and system-II (slow and deliberate reasoning) raises distinct requirements on foundation models. Evaluating system-I involves knowledge and memory, requiring various daily conversations and knowledge-based questions. Evaluating system-II requires complex reasoning skills, necessitating a diverse range of analytical and deductive questions, which are more challenging to collect and filter.

This paper focuses on building a reliable complex reasoning benchmark for both large language models (LLMs) and multimodal large language models (MLLMs). We believe four properties are critical for an ideal assessment:

- **Comprehensiveness**: A comprehensive evaluation is essential, akin to evaluating human intelligence.
- **Difficulty**: The evaluation should effectively discriminate between model performances and provide valuable guidance.
- **Multimodality**: The benchmark should assess both LLMs and MLLMs, reflecting our multimodal world.
- **Multilingualism**: Robust reasoning skills should transfer across languages, and assessment should reveal whether models genuinely reason or overfit to a specific language.

While there have been attempts to create such benchmarks, none incorporate all four properties simultaneously. For example, MMLU is comprehensive but close to saturation and lacks multimodality and multilingualism. MMMU is holistic for multimodal reasoning but also close to saturation and cannot evaluate language models or multilingual performance. Math-focused benchmarks like Frontiermath, Omni-Math, and AIME are challenging for mathematical reasoning but lack comprehensiveness and multilingual testing.

Our goal is to build R-Bench, aligning with the four proposed properties. We follow over 100 college courses from 19 departments at Tsinghua University and collect challenging problems from exams, textbooks, quizzes, and homework. After rigorous screening by experts and models, we select 1,094 questions spanning 108 subjects for language models and 665 questions covering 83 subjects for multimodal models.

We test the reasoning capabilities of various powerful proprietary models (e.g., OpenAI 01, GPT-40, Gemini, Claude, Llama 3, Qwen 2.5) and open-source models. Our findings:

- Existing multidisciplinary evaluations have nearly reached saturation; R-Bench poses a greater challenge.
- R-Bench is more complex and requires higher reasoning than existing benchmarks.
- Multimodal reasoning remains challenging; models lag behind text-based reasoning.
- Chain of Thought (CoT) can enhance reasoning in most chat models but not in reasoning-specialized models.
- Models maintain high consistency in answering Chinese and English questions of equal difficulty.
- Foundation models perform differently across disciplines.

# 2. R-Bench

This section introduces the construction process of R-Bench, involving data collection, filtering, and improvement. The overall pipeline is illustrated below.

**Figure 2:** Pipeline of building R-Bench. The process is divided into six steps. The funnel represents screening. Blue balls are filtered out; brown ones are preserved. KQ and RQ denote knowledge-based and reasoning-based questions, respectively. AQ and CQ represent ambiguous and clear questions. -T indicates text-only testing for LLMs; -M means multimodal testing; zh represents the Chinese version.

## 2.1. Data Collection

We investigated curriculum systems across 19 departments at Tsinghua University, covering over 100 courses. Senior undergraduates and graduate students from different departments acted as experts, providing reasoning question-answer pairs. A total of 51 experts, with at least two from each department, helped collect and filter questions. Key aspects controlled:

1. Questions must align with the provided collection list.
2. Experts filter out knowledge-based (memory) questions, retaining reasoning-based questions with sufficient difficulty.
3. All questions must have verifiable answers; proof-based questions are excluded.

## 2.2. Data Digitization

Collected questions were in messy formats (pictures, screenshots, text, etc.). A data annotation team of about 20 people organized, digitized, checked, and compiled all questions into Excel sheets. For language models: Department - Subject - Question text - Answer text. For multimodal models: Department - Subject - Question text - Answer text - Question Images. Tools like GPT-40 and Mathpix were used for OCR, followed by manual proofreading.

**Figure 3:** Some examples in R-Bench. These examples show R-Bench is multidisciplinary, multimodal, and multilingual. Problems are complex and require deep reasoning, not quick thinking.

## 2.3. Data Filtering

Three rounds of data filtering:

- **Expert Screening**: Experts filter out knowledge-based questions, retaining reasoning-based ones.
- **Model Screening**: Using OpenAI 01, questions with less than 2,000 reasoning tokens are filtered out to ensure high reasoning difficulty.
- **Manual Review**: Checks for completeness, repetition, ambiguity, and subject balance. Duplication detection tools are used, and the number of questions per subject is capped at 50.

## 2.4. Options and Translations

To enable automatic evaluation, all questions are converted to single-choice format. GPT-40 constructs 5 options per question, with an additional “All other answers are incorrect” option, totaling 6 choices. Options are checked and adjusted for numerical gaps. English-Chinese translations are manually constructed and reviewed by three bilingual experts for correctness and clarity.

## 2.5. Overview of R-Bench

R-Bench is a graduate-level, multi-discipline, multilingual benchmark for evaluating complex reasoning in both language and multimodal models.

- **R-Bench-T**: Text-only questions in English for LLMs.
- **R-Bench-Tzh**: Text-only questions in Chinese for LLMs.
- **R-Bench-M**: Multimodal questions in English for MLLMs.
- **R-Bench-Mzh**: Multimodal questions in Chinese for MLLMs.

**Figure 4:** R-Bench spans 19 departments, including mathematics, physics, biology, computer science, and chemistry, covering over 100 subjects. 1,094 questions for language models and 665 for multimodal models. All questions are available in both English and Chinese.

# 3. Experiments

After developing R-Bench, we use it to assess the complex reasoning capabilities of various LLMs and MLLMs, including both open-source and closed-source models.

## 3.1. Reasoning Comparison with Other Benchmarks

To demonstrate R-Bench’s focus on reasoning, we use:

- **Expert Scoring**: 30 questions each from R-Bench-T and MMLU (and R-Bench-M and MMMU) are compared by experts for reasoning requirements.
- **Model Scoring**: OpenAI 01 is used to compare reasoning requirements based on reasoning tokens and direct comparison.

**Table 2:** Comparison of reasoning requirements for R-Bench-T and MMLU via expert and model voting.

|           | R-Bench-T win | MMLU win | Tie  |
|-----------|---------------|----------|------|
| Expert    | 85.94         | 10.62    | 3.44 |
| Model     | 76.67         | 20.00    | 3.33 |

**Table 3:** Comparison for R-Bench-M and MMMU.

|           | R-Bench-M win | MMMU win | Tie  |
|-----------|---------------|----------|------|
| Expert    | 76.88         | 15.94    | 7.19 |
| Model     | 83.33         | 13.33    | 3.33 |

**Table 4:** Average thinking time of OpenAI 01 on 30 samples from different benchmarks (TT = thinking time).

| Benchmark   | TT (seconds) |
|-------------|--------------|
| MMLU        | 13.5         |
| R-Bench-T   | 98.2         |
| MMMU        | 20.3         |
| R-Bench-M   | 91.7         |

Results indicate R-Bench requires significantly higher reasoning ability than MMLU and MMMU.

## 3.2. Evaluating Reasoning Capability of Different Models

R-Bench-T is used to assess LLMs (OpenAI 01, GPT-40, DeepSeek-R1, Gemini, Claude, Qwen2.5, Llama3, etc.) in both English and Chinese. API calls use default parameters; open-source models are run locally with temperature set to 0. CoT prompting is used by default.

**Table 5:** Performance comparison of models on R-Bench-T (Top-1 accuracy, %).

| Model Name            | R-Bench-T | R-Bench-Tzh |
|-----------------------|-----------|-------------|
| 01-20241217           | 69.0      | 70.1        |
| Gemini-2.0-flash      | 68.4      | 67.5        |
| Claude3.5-sonnet1022  | 39.7      | 38.3        |
| Doubao1.5pro-20250121 | 62.0      | 63.4        |
| GPT-40-20241120       | 53.6      | 51.6        |
| DeepSeek-R1           | 61.2      | 59.3        |
| Qwen3-235B-A22B       | 58.0      | 58.4        |
| Llama-3.3-70B-Instruct| 49.5      | 47.6        |

R-Bench-M is used to evaluate MLLMs (OpenAI 01, GPT-40, Claude, Qwen2.5-VL, InternVL 2.5, etc.) in both languages.

**Table 6:** Performance comparison of models on R-Bench-M (Top-1 accuracy, %).

| Model Name            | R-Bench-M | R-Bench-Mzh |
|-----------------------|-----------|-------------|
| 01-20241217           | 53.2      | 55.0        |
| GPT-40-20241120       | 33.4      | 33.2        |
| Qwen2-VL-72B          | 25.1      | 25.7        |
| LLaVA-OneVision-7B    | 23.8      | 23.5        |
| InternVL-2.5-8B       | 15.9      | 17.1        |

**Table 7:** Assessing the performance impact of CoT across different models.

| Model Name            | w/ CoT | w/o CoT |
|-----------------------|--------|---------|
| 01-mini20240912       | 64.0   | 64.0    |
| GPT-40-20241120       | 53.6   | 51.5    |
| Qwen2.5-32B-Instruct  | 50.8   | 44.6    |
| Llama-3.3-70B-Instruct| 49.5   | 47.4    |

## 3.3. Observations and Findings

- Multimodal reasoning remains challenging for current models. For example, OpenAI 01 achieves 69.0% on R-Bench-T but only 53.2% on R-Bench-M.
- Models designed for reasoning tasks outperform chat models in complex reasoning.
- The gap between open-source and closed-source models is more pronounced in multimodal reasoning.
- CoT improves performance in most chat models but not in reasoning-specialized models.
- Models maintain high consistency in answering equally difficult Chinese and English questions.
- Performance varies significantly across disciplines.

# 4. R-Bench Statistics

R-Bench covers a wide range of disciplines and subjects. Below is a sample of the distribution from the full tables:

| Discipline                  | Specific Subject                       | Count |
|-----------------------------|----------------------------------------|-------|
| Materials                   | Analysis and Characterization          | 3     |
| Vehicle Engineering         | Principles of Automotive Power System  | 11    |
| Computer Science            | Discrete Mathematics                   | 12    |
| Statistics                  | Probability Theory                     | 42    |
| Environment                 | Water Treatment Engineering            | 12    |
| Mechanical Engineering      | Theoretical Mechanics                  | 16    |
| Chemistry                   | Physical Chemistry                     | 12    |
| Physics                     | Analytical Mechanics                   | 25    |
| Mathematics                 | Complex Analysis                       | 8     |
| Biology                     | Genetics                               | 13    |
| Economics                   | Game Theory and Mechanism Design       | 8     |

**Figure 1:** Top-1 accuracy comparison of different models on MMLU, MMMU, and R-Bench. R-Bench poses a greater challenge to current models.

**Figure 4:** Statistics of R-Bench: the benchmark spans 19 departments, including mathematics, physics, biology, computer science, and chemistry, covering over 100 subjects such as Inorganic Chemistry, Chemical Reaction Kinetics, and Electromagnetism. It features 1,094 questions for language models and 665 for multimodal models.

# Conclusion

R-Bench is a graduate-level, multi-disciplinary, multilingual benchmark designed to rigorously evaluate complex reasoning capabilities in both language and multimodal models. It addresses the limitations of existing benchmarks by incorporating comprehensiveness, difficulty, multimodality, and multilingualism. Experimental results show that even advanced models struggle with complex reasoning, especially in multimodal contexts, highlighting the need for continued research and development in this area.

# Acknowledgments

The authors thank the experts and data annotation team for their contributions to data collection, digitization, and review, as well as the model developers and the broader research community for their support and feedback.
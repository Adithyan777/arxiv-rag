# Abstract

Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called Minerva for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available.

# Introduction

Video understanding has long been a holy grail for the field of computer vision. Video benchmarks have evolved from testing action recognition on short clips to more recent benchmarks testing advanced capabilities over longer time horizons, and are steadily approaching human performance on these recent benchmarks. While these datasets aim to measure complex video understanding capabilities, they do not provide further insight into how or why models succeed or fail beyond the correctness of their final answers.

One way to inspect model failures is by exposure to step-by-step rationales generated by the model. Attempts to improve these rationales have led to a paradigm shift towards models, particularly in the text domain, that spend more time "thinking"—i.e., producing intermediate thoughts towards a final answer, often via additional compute at inference time.

Reasoning in the video domain, however, looks very different to reasoning in the text or coding domain. The high dimensionality and multi-modality of video means that solving a video task often requires temporal localization, perceptually recognizing key objects, events and actions from multiple modalities such as speech or frames, and applying logical reasoning to tie these intermediates all together. Complex queries in video question answering (videoQA) hence naturally require multi-step processes to solve, where each step may require a different skill and/or access to a different modality input. We refer to this multi-step process as a reasoning trace for videoQA.

Yet, existing video benchmarks only evaluate final answers—they only check the outcome and not the reasoning. It is not clear, however, if a model arrives at a correct answer due to a successful execution of key steps, pure chance, linguistic bias, or the process of elimination of answer choices. Conversely, if it fails to produce the correct answer, was it completely unable to solve the problem, or did it get close but ultimately make a mistake that changed the final answer? Using final answers alone makes it difficult to design datasets that will remain challenging but still provide signal—all the more pertinent as new models improve at breakneck speed.

**Figure 1:** Examples from Minerva. We introduce Minerva, a complex video question-answering dataset. Unlike existing video datasets, the answer to each question is accompanied by a detailed reasoning trace, which outlines the steps required to come to the answer. Videos cover multiple domains such as sports, cooking, short films and science lectures. Reasoning traces are detailed, including timestamps and key actions. We show a single frame from each video.

In light of this, we release Minerva—a benchmark for video reasoning which consists of questions, answers, and reasoning traces. Minerva is challenging and complex: every question requires multiple steps to solve, and even the best-performing frontier model achieves only 66.2% accuracy, while humans are able to achieve 92.5%. The dataset is hence well positioned for hill-climbing and future assessment of multimodal models.

Every question in Minerva requires complex reasoning using two or more skills (e.g., numerical reasoning, temporal reasoning, spatial navigation). Videos also span multiple domains, with various video lengths from 2 minutes to over 1.5 hours, making the dataset diverse. For each question we also provide the hand-crafted, detailed reasoning trace, with the steps that are required to come to the correct answer.

We benchmark both open-source and proprietary models on our dataset, and manually compare some of their reasoning traces to the ground truth reasoning annotations to identify failure modes. We use these failure modes to build a taxonomy of errors—a rubric for video reasoning. This rubric is specific to the video domain, and highlights the following broad categories of errors: Perceptual Correctness, Temporal Localization, Logical Reasoning, and Completeness.

We use this rubric to assess how both humans and LLMs grade reasoning outputs along these criteria when given the ground truth reasoning, illuminating where automatic graders correlate to humans and where they may not, highlighting directions for potential improvement for reference-based metrics. In addition, we also find in some cases that LLM performance on video reasoning improves simply by being prompted with the rubric.

To summarize, we make the following contributions:

- We introduce Minerva, a challenging video reasoning benchmark for LMMs consisting of 1,515 hand-crafted questions. For each question, we provide 5 answer choices, as well as detailed, manually-annotated reasoning traces.
- We evaluate multiple frontier models on Minerva, both open-source and proprietary, and perform ablations to show low textual bias and the importance of visual information.
- We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We find that failure modes are primarily related to temporal localization and visual perception. Preliminary analysis of LLM-based metrics show areas for future work.


# Related Work

## Recent VideoQA Datasets

A number of valuable video datasets have been proposed recently. Some include a variety of tasks, such as video QA, captioning, and grounding, but use relatively short videos. Others focus on short-form content. These benchmarks, along with others, rely on semi-automatic pipelines using LLMs for annotation. In contrast, our dataset is entirely manually annotated. Some benchmarks specifically address the challenge of temporally difficult answer candidates in VideoQA, emphasizing the importance of visual information. Other datasets bridge the gap between short and long-form content, and some extend to even longer videos. In contrast to these existing benchmarks, our work provides not only the final outputs but also human-annotated reasoning traces, enabling future evaluations to assess the models' reasoning process in addition to its accuracy.

## Reasoning Traces for Video

Few datasets provide auxiliary dataset beyond final answers. Some recent works have aimed to develop automatic labeling pipelines to instead generate noisy auxiliary information at scale. Some provide bounding box pseudolabels generated by off-the-shelf models for image VQA, while others construct a pipeline connecting different frozen models together to label bounding boxes for sparse key frames. Some works aim to generate text-based chain of thought for videos; however, their automated rationales tend to contain substantial information about the video that does not relate to the particular query, rather than providing specific reasoning for the given question.

## Reasoning Analysis

With the growth of reasoning models, substantial work has been conducted into analyzing reasoning in text-only settings and remains an open problem. Previous works demonstrate that correctness of final answers alone often does not indicate correct reasoning, commonly due to hallucination of reasoning steps or answers. These works fall into reference-based (which use additional human annotation as reference) and reference-free evaluation (without such additional information). Multiple works find that reference-based evaluation proves consistently more reliable, and that language models are poor evaluators of reasoning in isolation.

In this work, we primarily focus on contributing a high-quality dataset with reference annotations for video reasoning, which may spur the development of and provide a comparison for further research into both reference-based and, by comparison, reference-free metrics for video reasoning. In both text and multimodal contexts, well-designed, itemized evaluation criteria for rating different model responses give substantially more consistent, well-aligned LLM scores—we follow along these lines of work by providing a rubric for scoring video reasoning, and provide a preliminary analysis of its effectiveness.

# Minerva

We introduce Minerva, a videoQA dataset consisting of 1,515 challenging questions for videos of varying length. Each question has 5 answer options and a detailed reasoning trace accompanying the final answer. The key features of our dataset are:

- Multistep, complex questions. Each question requires multiple steps and multiple skills to solve.
- Multimodality, with both frames and ASR needed for some questions.
- High quality—the entire dataset is hand crafted by experienced annotators.
- Intermediate reasoning—detailed, hand-crafted reasoning traces.

These reasoning traces allow us to perform an analysis of where models are making errors, beyond just their final answer outputs. We do this via reference-based analysis of model outputs, which can be performed solely in the text domain and is therefore much cheaper than reference-free assessments.

**Figure 2:** Dataset statistics. Video lengths (left), lengths of answers and reasoning (middle), and domains (right). Videos cover a wide range of lengths, with some longer than 100 minutes. Every question comes with a reasoning trace which is long and detailed, mean number of words is 92 (middle). Domains are hand-selected to include videos that lend themselves well to complex reasoning questions.

## Dataset Construction

As multimodal models continue to improve, it becomes increasingly non-trivial to come up with challenging questions. Questions should be complex, requiring multiple steps to solve, and not solvable with only the speech or external knowledge.

Our dataset construction pipeline consists of the following steps:

1. **Video Selection**: We begin by selecting video domains from YouTube that lend themselves well to questions fulfilling the desiderata above.
2. **Manual Annotation**: Raters propose questions, answers and reasoning traces.
3. **Quality Review**: Questions are reviewed by other raters.
4. **Adversarial Filtering**: We attempt to mitigate textual bias using consensus from multiple frontier text-only models.

### Video Selection

Video selection is non-trivial, as many videos online contain simple storylines, few shots, or are dominated by talking heads. To avoid this, we identify a non-exhaustive set of domains that lend themselves well to reasoning. These are described below and shown in Figure 2.

- **Short Films**: Complex, multi-event videos that typically have a story line.
- **Sports and Board Games**: Sports such as tennis, basketball, motorsports, hockey, cricket, and board games such as chess, scrabble and risk.
- **Educational**: STEM lectures online, though this is a small portion of the dataset.
- **Lifestyle**: VLOG-style lifestyle videos, including cooking, general how-to videos, travel vlogs and pet videos.


### Annotation and Quality Review

Once videos are identified, raters then propose complex questions, answers, decoys, reasoning traces and label question types. Raters are instructed to attempt to propose questions such that each question requires at least two of the following skills: Temporal Reasoning, Counting, Cause and Effect, Goal Reasoning, Situational Awareness, Event Occurrence, State Changes, Reading (OCR), Listening (identifying a detail in the audio track), Spatial Perception, Numerical Reasoning, Object Recognition, Counterfactual Reasoning.

Each annotation is verified by at least one other rater. The annotations are also periodically reviewed by the paper's authors, who provide detailed feedback to the raters to ensure annotations are being provided in-line with guidelines.

### Filtering for Quality and Biases

We take several steps to address quality and potential for biases in the final dataset. We begin by filtering examples where the groundtruth annotated reasoning traces are of low quality or too short. We address potential text-biases both in terms of QAD-only and ASR-only with adversarial filtering. Our filtering process consists of taking the consensus agreement in order to avoid discarding difficult questions that models may have answered correctly by chance across a diverse range of open- and closed- source text-only baselines.

# Benchmarking

## Models

We benchmark a number of open-source and proprietary models, described below.

- **Blind Baselines**: We first evaluate models using a text-only prompt in two settings: (i) the model is given only the question, answer and decoys (QAD baseline); (ii) the model is additionally given an ASR transcript of the video (QAD+ASR baseline).
- **Video Models**: We experiment with 3 of the best-performing open source VideoQA models, as well as proprietary models from Google DeepMind, OpenAI, and Anthropic.


### Prompts

We conduct an ablation on the impact of prompting styles on Minerva with our best model. We try out 3 styles of prompting: (i) asking the model to answer the question directly; (ii) asking the model to reason step by step; and (iii) additionally providing the model with the Minerva rubric for video reasoning.

### Implementation Details

For all models, we sample video frames uniformly from the videos and construct prompts by interleaving them with timestamps. Since many questions ask about specific times in the speech and the video, we also interleave the ASR with timestamps at five second intervals. We prompt models to provide their answer choice in a machine-readable format for parsing.

## MCQ Performance

### Modality Ablations

QAD-only and QAD+ASR-only baselines with no visual information are provided in the table below.


| Method | wo ASR | w ASR |
| :-- | :-- | :-- |
| Random | 20.00 | 20.00 |
| GPT-4o | 19.60 | 29.17 |
| Gemini 2.0 Flash | 23.04 | 28.25 |
| Qwen-2.5VL | 18.88 | 23.17 |
| DeepSeek | 21.45 | 22.51 |

All QAD-only baselines get close to chance performance, indicating that the decoy answer choices do not offer cues to the correct answer. ASR-only baselines using powerful language models obtain extremely low performance compared to those with frames, showcasing the necessity of visual information for Minerva.

### Scoring Final Answers (MCQ)

| Method | Frames | ASR | MCQ-Acc. |
| :-- | :-- | :-- | :-- |
| Random | - | - | 20.00 |
| Qwen2.5-VL | 768 |  | 35.05 |
| VideoLLaMA3 | 180 |  | 35.91 |
| InternVideo2.5 | 256 |  | 35.18 |
| Claude3.5 Sonnet v2 | 64 |  | 31.28 |
| OpenAI o1 | 64 |  | 43.48 |
| GPT-4o | 250 |  | 45.54 |
| GPT-4.1 | 256 |  | 53.99 |
| Gemini 2.0 Flash | 256 |  | 53.47 |
| Gemini 2.5 Flash Thinking | 256 |  | 57.30 |
| Gemini 2.5 Pro Thinking | 1024 |  | 66.20 |
| Human performance | all |  | 92.54 |

Contrary to prior work, the gap between open-source and proprietary models has narrowed, with Qwen2.5-VL and InternVideo2.5 outperforming Claude Sonnet. Gemini 2.5 Pro Thinking sets the state-of-the-art in the dataset at 66.2%. With peak performance still far from human performance, Minerva will be a challenging benchmark to measure progress on video understanding.

### Prompt Ablations

| Prompting Method | MCQ Accuracy | MiRA |
| :-- | :-- | :-- |
| Direct Answer | 46.47 |  |
| Reasoning | 51.22 | 0.65 |
| Minerva Rubric | 53.47 | 0.75 |

Asking the model to perform step-by-step reasoning rather than directly producing an answer results in a significant boost to MCQ accuracy. Explicitly providing the rubric in the prompt improves the final score even further.

## Human Performance

A human study by asking raters a disjoint set to answer the questions in the dataset shows a big gap between model and human performance, with the best model still almost 30% behind human performance.

# Analysis of Reasoning Traces

## Assessing Methods for Video Reasoning

Besides increased performance, prompting models to provide reasoning steps also greatly improves interpretability, but objectively studying their correctness and helpfulness irrespective of the final answer is still an open problem. While human assessments are usually the gold standard, they are expensive and time-consuming, so we provide a preliminary exploration of LLM-based assessments as well.

### Taxonomy of Video Reasoning Errors

We propose a simple taxonomy of reasoning errors for video models applied to complex questions:

1. **Perceptual Correctness**: Errors in perception, including identifying objects, actions, events, or correctly parsing the ASR or performing OCR.
2. **Temporal Localization**: Errors in identifying the correct temporal ranges of the video to solve the question.
3. **Logical Reasoning**: Errors in reasoning given the perceptual elements.
4. **Completeness**: A reasoning trace is not complete if it lacks required steps to produce the correct answer.

We use these four axes, along with a 3-point Likert score to create a general rubric for evaluating reasoning traces provided by video models, which we call the Minerva rubric.

### Human Assessments

We provide model reasoning traces to human raters, along with QADs and ground truth reasoning traces, and ask them to score each reasoning trace with the Minerva rubric.

### MiRA LLM-based Assessments

We ask an LLM to score reasoning traces according to the Minerva rubric—the same score the human raters provided. We experiment with both reference-based and reference-free prompts. All scores are normalized to be between 0 and 1.

### Results

Aggregate human scores for the four axes of the reasoning rubric show temporal grounding has the lowest score, followed by perceptual correctness. Scores for logical reasoning and completeness are high, suggesting models are failing largely in the video domain, and not text/logical domain.

Providing a reference results in substantially stronger correlations along the Temporal, Perceptual, and Completeness axes particularly for Temporal. It also matches the relative difficulty of each category to human judgment.

Scoring reasoning using MiRA on the full set of questions shows that scores are lowest for temporal localization and perceptual correctness, with the largest variation among models in the former. This mirrors the results from the human study, suggesting that when video models with heavy, well-trained LLMs are applied to video reasoning, they tend to provide plausible sounding detailed reasoning traces, however they still lack some key perceptual insights and temporal grounding in the video.

# Conclusion

We present Minerva, a videoQA dataset with complex questions, answers and ground truth reasoning traces. Our dataset is challenging for multiple frontier multimodal models, and is useful for providing insights into the reasoning failures of these models. Our analysis of using LLMs to judge model-generated reasoning traces shows promise and points out opportunities for future work in this direction.

# Acknowledgments

We are grateful to Antoine Yang, Anelia Angelova, Mario Lucic, Karel Lenc and Boyu Wang from the Gemini team for their support.
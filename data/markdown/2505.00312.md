# Abstract

Deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. While multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. In response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures Xception, Res2Net101, and EfficientNet-B7. Our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. Unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architectures influence based on their detection reliability. Our experiments achieved state-of-the-art intra-dataset performance with AUC scores of 99.22 FF and 100.00 CelebDF-v2, and F1 scores of 98.06 FF and 99.94 CelebDF-v2 without augmentation. With augmentation, we achieve AUC scores of 99.47 FF and 100.00 CelebDF-v2, and F1 scores of 98.43 FF and 99.95 CelebDF-v2. The framework demonstrates robust cross-dataset generalization, achieving AUC scores of 88.20 and 72.52, and F1 scores of 93.16 and 80.62 in cross-dataset evaluations.

# Introduction

The computer vision field has seen huge advancements in recent years, and this led to the creation of a new field of Generative AI where a user prompts a computer to make an image from text and it does that. It has become mainstream, WhatsApp, Snapchat has integrated Meta AI and My AI respectively, which can easily generate images on the fly, and it doesnt take too long also. This has led to a rise in privacy concerns and people are becoming aware of the darker side of this advancement in which it may be used to cause harm.

Advancements in the field of AI, specifically in the field of Machine Learning and Deep Learning have become instrumental in accelerating the development of technology, contributing to the spread of fake media throughout society. One such darker side is the widespread misuse of deepfake content generated using. Deepfake content creation is one of the widely misused applications of the field. Deepfake finds its root from the word deep learning and fake content. The content can be anything that we consume in this digital word ranging from videos, images, audios or even news. The majority of the deepfake content today are deepfake videos which present a complex duality of advantages and disadvantages. whichre causing mental harm, defamation, blackmailing, public opinion manipulation and posing a threat to national security.

On the one hand, it offers significant benefits specifically in marketing and entertainment, where it can lower production costs and enhance creative expressions. For instance, deepfakes enable brands to produce customized advertising campaigns using the existing footages of actors without a need of reshoot saving potential resources. Deepfake technology is being widely used by the filmmakers and animation specialists for aging and de-aging actors with VFX and CGI techniques and bring back the aged or dead actors to life with actor doubles.

Political prisoners are also spotlighting the potential use of AI and deepfake technology. One such use was made by the former Prime Minister of Pakistan Imran Khan, who has been imprisoned since August 2023, during a campaign rally held online urging his supporters to vote his party in large numbers. This method allowed him to deliver victory speech despite his incarceration, illustrating how deepfake technology can be used to navigate political limitations and influence public perception even from the prison.

However, these advantages come with serious drawbacks. The same capabilities that enable creative applications also enable malicious use, such as spreading disinformation and manipulating public opinion. Earlier incarnations of deepfake media were primitive and often associated with static imagery that was of low quality and could easily be detected. In recent years, this has changed due to improvements in DL models and the sharing of open-source algorithms for content generation. GANs have been the backbone in the rapid progress of the field, starting from GAN in 2014 to StyleGAN in 2018 weve come a long way in generating hyper-realistic images from user prompt.

Our work makes several significant contributions to the field of deepfake detection and are summarized as follows:

We propose AWARE-NET, a unique ensemble framework that combines three state-of-the-art architectures through a two-tier fusion strategy, leveraging multiple instances per architecture and introducing learnable weighting mechanism for optimal architecture fusion.

Unlike traditional ensemble with fixed weights, we introduce a novel adaptive weighting mechanism that automatically learns the optimal contribution of each architecture during training, enabling model to dynamically adapt to each architectures strength.

We implement a systematic approach to model diversity by maintaining three independent instances of each architecture with different initializations, combined with averaging mechanism to reduce variance and enhance prediction stability.

We develop a fully differentiable end-to-end framework that jointly optimizes model parameters and architectural weights while providing interpretable insights into architecture contributions through learned weights.

The rest of the paper is organized as follows Section 2 provides a comprehensive review of related work, discussing existing approaches and their limitations in the context of Deepfake detection and ensemble counterparts. Section 3 outlines the methodology and experimental setup, detailing the architecture used, data preprocessing steps, and evaluation metrics. Sections 4, 5, and 6 present the implementation details, including model training and optimization strategies, followed by the experimental results and a thorough analysis of the findings. Finally, Section 6 concludes the paper with a summary of key insights and potential directions for future research.

# Related Work

Ensemble learning techniques have become essential in the domain of deepfake detection due to their ability to combine the strengths of multiple models, improving robustness and generalization. Atas and Karakose proposed a hybrid approach combining deep convolutional neural networks D-CNN with statistical models such as Support Vector Machine SVM, Random Forest, and Logistic Regression. While this approach demonstrated the advantages of combining multiple learning strategies, its reliance on fixed model architectures limits adaptation to evolving deepfake techniques. Addressing this limitation, Manju and Kalarani introduced a more flexible solution utilizing DenseNet and XGBoost, though it still faces challenges in generalizing across different types of deepfake manipulations.

The complex spatiotemporal nature of video deepfakes presents unique challenges in detection. Bakliwal et al. proposed combining 2D and 3D convolutional neural networks CNNs to capture both spatial features and temporal dynamics. While effective, this methods high computational costs limit its real-time applications. Minhas et al. addressed this limitation by leveraging EfficientNetB0, providing a more computationally efficient solution though with limited temporal feature analysis. Khan and Dang-Nguyen further improved upon these approaches by introducing a hybrid transformer model that integrates CNNs for spatial feature extraction with transformers for global temporal dependencies, though generalization across newer deepfake techniques remains challenging.

As deepfake generation techniques evolve, model adaptability has become crucial. Gao et al. presented an incremental learning approach using adapter-based modules to prevent catastrophic forgetting, enabling dynamic adaptation to new data. However, this approach faces challenges with model complexity and computational demands. Han et al. proposed SIGMA-DF, a meta-learning framework that optimizes intra-class and inter-class distances for improved generalization, though class imbalance issues persist, affecting model sensitivity across different deepfake types.

Addressing adversarial attacks has emerged as a critical focus in deepfake detection. Guan et al. developed ensemble techniques to defend against adversarial perturbations, highlighting the vulnerability of current detection models. While this improved attack resilience, Rana and Sung took a different approach with DeepfakeStack, combining multiple state-of-the-art models to enhance detection accuracy, though without explicit adversarial attack protection. These developments underscore the ongoing challenge of maintaining model robustness in increasingly hostile environments.

Generalization across various deepfake types and datasets remains an ongoing challenge. Ha et al. addressed this by integrating Vision Transformers ViT with CNN models, improving performance on low-quality and side-face manipulations. Cozzolino et al. focused on identity-aware learning, utilizing 3D Morphable Models 3DMM for facial motion analysis, though limitations persist in detecting subtle manipulations beyond facial motion. These challenges highlight the need for more dynamic and adaptable detection methods capable of handling emerging deepfake variations while maintaining robust performance across different quality levels and manipulation types.

While these approaches show progress, they lack optimal model combination strategies. We propose AWARE-NET, a two-tier framework that combines intra-architecture averaging for stability and inter-architecture learnable weights for optimal fusion, automatically discovering each architectures importance during training.

# Methodology

Our proposed methodology introduces a novel two-tier hierarchical ensemble learning framework for deepfake detection that leverages both model-level and architectural-level fusion strategies. The framework comprises three distinct deep learning architectures Xception, Res2Net101 and EfficientNet-B7 with each architecture having three independent instances to enhance the ability to capture diverse feature representations and reduce model bias. Rather than treating these networks as black boxes and averaging their outputs, we carefully analyze their feature spaces. This two-tier approach enables both intra-architecture averaging and inter-architecture adaptive weighting through a learnable mechanism that automatically discovers optimal architecture contributions during training.

## Model Architecture Design

The foundation of our framework lies in the careful selection and implementation of three complementary deep learning architectures. Each architecture brings unique strengths to the ensemble.

Xception leverages depthwise separable convolutions that efficiently process cross-channel correlations while significantly reducing computational complexity, as in equation 1. The architecture first applies channel-wise spatial convolutions followed by pointwise convolutions, making it effective at capturing fine-grained spatial features crucial for detecting manipulation artifacts.

S = DSConv(x) ∈

where DSConv represents the depthwise separable convolution operation.

Res2Net101 implements a multi-scale feature extraction approach through its hierarchical residual-like connections. The architecture processes feature at multiple granularities within a single residual block, enabling the capture of both fine and coarse manipulation patterns at various scales. The deep 101-layer architecture provides substantial model capacity for learning complex feature hierarchies represented in equation 2.

S_m = f(∑_i w_i H_i(x)) ∈

where H_i represents hierarchical feature maps at different scales, and w are scale-specific weights.

EfficientNet-B7 utilizes compound scaling to optimally balance network depth, width and resolution, as evident through equation 3. This architecture achieves state-of-the-art performance through its balanced scaling of all dimensions of the network, providing efficient feature extraction at multiple levels of abstraction.

S_y = f(CompoundScale(x)) ∈

where φ represents the compound scaling coefficients.

**Figure 2:** AWARE-NETs two-tier architecture 1 intra-architecture ensemble combining Xception, Res2Net101, and EfficientNet-B7 through mean pooling, and 2 adaptive weighting mechanism that optimizes architecture-specific fusion weights through backpropagation.

## First-Tier Intra-Architecture Ensemble

At the first level of our ensemble, we employ model-level fusion within each architecture family. For each architecture A ∈ {Xception, Res2Net101, EfficientNetB7}, we maintain three independent instances {M_1, M_2, M_3} with different initializations while maintaining the same architectural blueprints, leading to different optimization trajectories during training. Averaging multiple instances helps reduce model variance and provides more stable predictions.

Given an input image x, each model produces a sigmoid-normalized prediction, as in equation 4.

σ(x) = 1/(1+e^(-M(x))) ∈

The architecture-specific prediction p_A(x) is then computed as the arithmetic means of its instance outputs, shown in equation 5.

p_A(x) = (1/3)∑σ_i(x), i ∈ {1,2,3}

## Second-Tier Learnable Inter-Architecture Fusion

The second tier implements an adaptive weighting mechanism between architectures using a learnable parameter vector w = {W_1,W_2,W_3}. This approach differs from traditional fixed-weight ensembles by allowing the model to automatically discover the optimal architecture contributions through end-to-end training.

Let p(x) = {p_Xception(x), p_Res2Net(x), p_EfficientNet(x)} be the vector of architecture-specific predictions. The weights undergo softmax normalization to ensure interpretability and proper scaling, using equation 6.

softmax(w) = {e^W_1/Z, e^W_2/Z, e^W_3/Z}

where Z = ∑_i e^W_i is the normalization factor.

This softmax normalization ensures:

- Non-negativity: α_i ≥ 0 for all i
- Sum-to-one constraint: ∑α_i = 1
- Interpretability: Each α_i represents the relative importance of architecture i

The final ensemble prediction y(x) is computed as the weighted sum, as in equation 7:

y(x) = ∑α_i p_i(x), i ∈ {1, 2, 3}

**Figure 3:** Internal structure of CNN backbones Xception local artifacts, Res2Net101 multi-scale features, and EfficientNet-B7 global patterns with their respective processing flows.

During training, the model learns both the individual model parameters and the optimal architecture weights w through end-to-end backpropagation. The gradient flow through the softmax normalization allows the model to automatically discover the relative importance of each architecture, as in equation 8.

∂L/∂w_i = ∂L/∂y · ∂y_i/∂α_i · ∂α_i/∂w_i

where ∂y_i/∂α_i = p_i(x) and ∂α_i/∂w_i = α_i(1 - α_i) for i = j, -α_iα_j for i ≠ j

This adaptive weighting mechanism enables the ensemble to learn the optimal contribution of each architecture based on their effectiveness for deepfake detection. The architectural diversity in our ensemble is purposefully chosen: Xception leverages depthwise separable convolutions, Res2Net101 enables multi-scale feature extractions, and EfficientNet-B7 provides compound-scaled feature processing. The combination of multiple instances per architecture and learned inter-architecture weights allows our model to leverage both model diversity and architectural strengths in an optimal manner.

## Datasets

In this study, we adopted FaceForensics and CelebDF-v2 for training and evaluation of our approach. FaceForensics is an extensive manipulative videos database containing both real and manipulated videos. Real videos are sourced from paid actors and youtube. The dataset contains 1363 real videos from youtube, and paid actors and 8045 Manipulated videos are generated using several techniques like FaceSwap, NeuralTextures, FaceShifter, Deepfakes, DeepFakeDetection and Face2Face. FaceForensics comes with several compression versions like raw, c23 and c40 the most compressed one. In this study, we adopted the c23 version for training and evaluating our approach.

The other dataset used in our study is CelebDF-v2. It consists of 5639 fake videos and 890 real videos sourced from paid actors and youtube as well. The videos in CelebDF-v2 are diverse, containing various lightning conditions, backgrounds along with different facial expressions making it a valuable resource in the deepfake detection domain. These two datasets are frequently used in the deepfake detection domain with FF being attributed to the most challenging one for the models and enhancing the generalizing capabilities.

## Dataset Preprocessing

We first start our preprocessing pipeline by extracting frames from the video dataset and using dlib for face detection and processing the facial landmarks. 32 frames from real videos and 16 from fake videos were extracted and the dynamic frame sampling rate was adopted. We create 30 augmented images for training by applying various augmentations to the dataset, including 15-degree rotations, 10-degree shears, flips, skewing, and jittering on the cropped face images. Weve used augmenter for image augmentation pipeline.

# Implementation Details

Our implementation follows a two-phase training strategy using PyTorch and timm library for model architectures. In Phase 1, we independently train each base model Xception, Res2Net101, EfficientNet-B7 using AdamW optimizer lr=1e-4, weight_decay=1e-5 and cosine annealing scheduler with warm restarts T_0=3 epochs. In Phase 2, we freeze the pre-trained base models and construct our ensemble with weights initialized uniformly w={1/3,1/3,1/3}. Inputs are processed through frozen models and combines their predictions using softmax-normalized learnable weights.

We employ mixed-precision training with gradient accumulation steps=2 and a batch size of 32. Early stopping monitors validation loss with a patience of 7 epochs and a minimum delta of 0.001, requiring at least 10 epochs before stopping. Training uses cross-entropy loss and maintains the best model based on validation performance. The framework processes 224×224 RGB images and includes standard normalization mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]. We utilized 8 CPU workers for data loading and pin memory enabled for faster GPU transfer. All hyperparameters are managed through a centralized configuration file, enabling easy experimentation.

# Results Discussion

Our experimental results demonstrate the effectiveness of AWARE-NET across both intra-dataset and cross-dataset evaluations. We present a comprehensive analysis of our frameworks performance and discuss the key factors contributing to its success.

Comprehensive performance matrix in Figure 4 showing scores across three key metrics AUC, Accuracy, F1 for all architectures on both datasets, with and without augmentation. The color gradient from dark blue to yellow represents scores from lower to higher values. The matrix demonstrates consistent performance improvements with augmentation and AWARE-NETs superior performance across all conditions.

## Intra-Dataset Performance

AWARE-NET consistently outperforms individual architectures across all metrics in intra-dataset evaluations. Without augmentation, our framework achieves superior AUC scores of 99.22 on FF and 100.00 on CelebDF-v2, surpassing the best individual model performances Xception 98.90 and 100.00 respectively.

**Figure 5:** Model Performance Comparison. Radar plot comparing all five-evaluation metrics AUC, Accuracy, Precision, Recall, F1 across architectures on the FF dataset.

The plot in Figure 5 reveals the balanced performance of AWARE-NET across all metrics, with the filled areas representing each models performance envelope. The range is set from 95-100 to better highlight the subtle differences between models. This improvement can be attributed to several key factors:

**Two-Tier Fusion Strategy:** The combination of intra-architecture averaging and learnable inter-architecture weights effectively reduces model variance while optimizing architecture contributions. This is evidenced by the consistent improvement in F1 scores 98.06 for FF and 99.94 for CelebDF-v2.

**Complementary Feature Extraction:** Each architecture contributes uniquely to the ensemble:

- Xceptions efficient processing of local artifacts
- Res2Net101s multi-scale feature extraction
- EfficientNetB7s balanced feature processing

**Adaptive Weight Learning:** The learnable weights mechanism automatically discovers optimal architecture combinations, leading to more robust predictions than any single architecture.

With augmentation, AWARE-NETs performance further improves, achieving AUC scores of 99.47 FF and maintaining 100.00 CelebDF-v2. The augmentation benefits are particularly evident in the F1 score improvements 98.43 for FF and 99.95 for CelebDF-v2.

## Cross-Dataset Generalization

The most significant advantage of AWARE-NET becomes apparent in cross-dataset evaluations.

**Figure 7:** Cross-Dataset Performance Comparison. Bar chart showing AUC scores for each architecture in both transfer directions. The left group shows performance when models trained on FF are tested on CelebDF-v2, while the right group shows the reverse scenario.

Without augmentation, our framework achieves remarkable improvements in generalization:

**FF to CelebDF-v2:** AWARE-NET achieves an AUC of 88.20 and F1 score of 93.16, substantially outperforming individual architectures best individual AUC 30.31 by EfficientNetB7.

**CelebDF-v2 to FF:** The framework maintains competitive performance with an AUC of 72.52 and F1 score of 80.62.

**Figure 8:** Cross-Dataset Generalization Performance. Visualization of performance degradation in cross-dataset scenarios. The lines show how each models performance changes when tested on a different dataset.

Line plot in Figure 6 illustrates the effect of data augmentation on AUC scores for the FF dataset. Each line represents a different architecture, demonstrating how performance improves with augmentation. The steeper slope of AWARE-NETs line indicates it benefits more effectively from augmentation compared to individual models.

In Figure 8 AWARE-NET purple maintains the highest performance and shows the most robust generalization, with a notably smaller performance drop compared to individual models, especially in CelebDF-v2 FF transfer. The dramatic improvement in cross-dataset performance can be attributed to:

**Figure 10:** Cross-Dataset Performance Simplified. Simplified visualization of cross-dataset performance comparing FF blue and CelebDF-v2 orange transfer performance across architectures.

**Diverse Feature Learning:** Multiple instances of each architecture capture different aspects of deepfake artifacts

**Adaptive Fusion:** The learnable weights mechanism adjusts architecture contributions based on their reliability for different types of manipulations

Figure 9 shows relative performance improvement of AWARE-NET compared to the best individual model across different metrics. Blue bars show improvements on FF dataset, while green bars show improvements on CelebDF-v2. Notable improvements include 0.32 AUC increase on FF and consistent positive gains across most metrics on CelebDF-v2.

The stark difference in bar heights in Figure 10 highlights the challenging nature of cross-dataset generalization and AWARE-NETs significant improvement in handling this challenge.

However, we observe that augmentation in cross-dataset scenarios doesnt consistently improve performance. As shown in Table 4, the AUC scores slightly decrease with augmentation in cross-dataset evaluation FF to CelebDF-v2 69.66 vs. 72.52 without augmentation. This suggests that domain-specific augmentation strategies might be needed for better cross-dataset generalization.

These results demonstrate that AWARE-NETs novel two-tier approach with learnable weights effectively addresses the challenges of deepfake detection, particularly in cross-dataset scenarios where traditional approaches often struggle. The frameworks ability to maintain high performance across different datasets and its robust generalization capabilities make it a promising solution for real-world deepfake detection applications.

The performance improvements are particularly noteworthy in challenging cross-dataset scenarios, where AWARE-NET achieves up to 57.89 improvement in AUC scores compared to the best individual model CelebDF-v2 to FF transfer. This significant enhancement in generalization capability, combined with consistent intra-dataset performance improvements, validates the effectiveness of our adaptive weighting mechanism and two-tier fusion strategy.

## Performance Tables

**Table 1: Intra Dataset Evaluation Without Augmentation**


| Architectures | AUC | Accuracy | Precision | Recall | F1 | AUC | Accuracy | Precision | Recall | F1 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  |  | FF |  |  |  |  | CelebDF-v2 |  |  |
| Xception | 98.90 | 96.58 | 96.80 | 99.02 | 97.90 | 100.00 | 99.85 | 99.86 | 99.89 | 99.88 |
| Res2Net101 | 98.72 | 96.14 | 96.68 | 98.58 | 97.62 | 99.99 | 99.60 | 99.46 | 99.91 | 99.69 |
| EfficientNetB7 | 98.64 | 97.13 | 97.57 | 98.89 | 98.23 | 100.00 | 99.80 | 99.73 | 99.95 | 99.84 |
| AWARE-NET | 99.22 | 96.86 | 97.24 | 98.90 | 98.06 | 100.00 | 99.92 | 99.91 | 99.97 | 99.94 |

Comparison of detection performance between individual architectures and AWARE-NET on FF and CelebDF-v2 datasets. AWARE-NET achieves superior performance with AUC improvements of 0.32 and F1 score improvements of 0.06 on FF compared to the best individual model, demonstrating the effectiveness of our two-tier fusion strategy.

**Table 2: Intra Dataset Evaluation With Augmentation**


| Architectures | AUC | Accuracy | Precision | Recall | F1 | AUC | Accuracy | Precision | Recall | F1 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  |  | FF |  |  |  |  | CelebDF-v2 |  |  |
| Xception | 99.27 | 97.04 | 97.74 | 98.60 | 98.17 | 100.00 | 99.90 | 99.91 | 99.94 | 99.92 |
| Res2Net101 | 99.25 | 96.94 | 97.39 | 98.84 | 98.11 | 100.00 | 99.67 | 99.57 | 99.88 | 99.74 |
| EfficientNetB7 | 99.06 | 97.01 | 97.54 | 98.09 | 98.14 | 100.00 | 99.90 | 99.91 | 99.91 | 99.91 |
| AWARE-NET | 99.47 | 97.45 | 97.54 | 99.33 | 98.43 | 100.00 | 99.94 | 99.91 | 100.00 | 99.95 |

Performance evaluation with augmented training data showing AWARE-NETs enhanced robustness, achieving peak AUC scores of 99.47 FF and 100.00 CelebDF-v2. The learnable weight mechanism effectively leverages the augmented features for improved detection accuracy.

**Table 3: Cross Dataset Evaluation - Without Augmentation**


| Architectures | AUC | Accuracy | Precision | Recall | F1 | AUC | Accuracy | Precision | Recall | F1 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  |  | FF → CelebDF-v2 |  |  |  |  | CelebDF-v2 → FF |  |  |
| Xception | 71.76 | 62.40 | 69.93 | 97.29 | 81.38 | 24.37 | 52.02 | 91.51 | 6.53 | 12.19 |
| Res2Net101 | 67.05 | 57.91 | 67.67 | 91.99 | 77.98 | 24.61 | 52.11 | 91.39 | 6.87 | 12.78 |
| EfficientNetB7 | 71.64 | 64.09 | 71.40 | 92.21 | 80.48 | 30.31 | 55.99 | 96.98 | 13.74 | 24.08 |
| AWARE-NET | 72.52 | 66.06 | 72.92 | 90.14 | 80.62 | 88.20 | 70.14 | 87.30 | 99.85 | 93.16 |

Cross-dataset evaluation demonstrating AWARE-NETs superior generalization capabilities. Notable improvement in FF to CelebDF-v2 transfer with AUC of 88.20 compared to best individual model 30.31, highlighting the effectiveness of our adaptive weighting mechanism.

**Table 4: Cross Dataset Evaluation With Augmentation**


| Architectures | AUC | Accuracy | Precision | Recall | F1 | AUC | Accuracy | Precision | Recall | F1 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  |  | FF → CelebDF-v2 |  |  |  |  | CelebDF-v2 → FF |  |  |
| Xception | 68.88 | 59.11 | 68.16 | 95.55 | 79.57 | 22.96 | 51.84 | 96.50 | 4.32 | 8.28 |
| Res2Net101 | 69.75 | 60.36 | 68.89 | 95.37 | 79.99 | 25.26 | 52.87 | 94.69 | 7.46 | 13.83 |
| EfficientNetB7 | 70.53 | 61.10 | 69.25 | 96.27 | 80.56 | 23.55 | 52.26 | 97.57 | 5.03 | 9.57 |
| AWARE-NET | 69.66 | 59.20 | 68.08 | 98.20 | 80.41 | 22.43 | 51.61 | 97.46 | 3.60 | 6.95 |

# Conclusion

We presented AWARE-NET, a novel two-tier ensemble framework for deepfake detection that addresses key limitations in existing approaches through innovative architectural design and adaptive weighting mechanisms. Our framework demonstrates significant improvements in both intra-dataset performance and cross-dataset generalization capabilities.

The key contributions of our work include the introduction of a hierarchical ensemble strategy that combines intra-architecture averaging with learnable inter-architecture fusion, enabling automatic discovery of optimal architecture contributions during training. The systematic approach to model diversity through multiple instances per architecture, combined with adaptive weighting, provides enhanced robustness and generalization capabilities that surpass traditional fixed-weight ensemble methods.

Our experimental results validate the effectiveness of AWARE-NET, achieving state-of-the-art performance on standard benchmarks with AUC scores of 99.47 on FF and 100.00 on CelebDF-v2 datasets. Most notably, the framework demonstrates exceptional cross-dataset generalization with up to 57.89% improvement in AUC scores compared to individual architectures, addressing one of the most challenging aspects of deepfake detection.

The learnable weighting mechanism provides interpretable insights into architecture contributions while maintaining end-to-end differentiability, making the framework both effective and practical for real-world applications. Future work will focus on extending the framework to handle emerging deepfake generation techniques and exploring domain-specific augmentation strategies for improved cross-dataset performance.

# Acknowledgments

The authors would like to thank the anonymous reviewers for their valuable feedback and suggestions that helped improve this work. We also acknowledge the computational resources provided by our institutions that made this research possible.


# Abstract

Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested—many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce T2VPhysBench, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.

# Introduction

Text-to-video generative models have achieved remarkable success in recent years, driven by advances in the Transformer architecture and diffusion model techniques. By leveraging large-scale, cross-modal video-text data from the Internet, these models now produce videos with high fidelity and appealing aesthetics, transforming both digital art creation and user engagement on the Web. Modern systems such as Sora, WanX, and Kling have demonstrated the ability to follow complex human instructions with impressive accuracy, positioning text-to-video generation as a central feature of today’s web experiences.

Despite these gains, fundamental concerns remain about whether text-to-video models respect basic physical laws. Generated videos often violate constraints such as rigid-body collisions, fluid dynamics, or simple gravity, which can lead to unrealistic or even misleading content. Such errors become critical in applications like robotics and autonomous driving, where adherence to real-world physics is essential for safety and system reliability. It is therefore crucial to evaluate how well current models capture these core principles.

Recent years have seen a growing suite of benchmarks for text-to-video models, covering compositional property combinations, temporal dynamics, object counting, and storytelling. However, systematic evaluation of physical constraint adherence remains underexplored. Early benchmarking efforts on this topic have introduced physics-inspired prompts and provided valuable insights, but they typically rely on pixel-level or visual-matching metrics that do not fully align with human judgments. In addition, most existing tests use scenario-based designs rather than grounding tasks in first-principles laws (e.g., Newton’s laws or Bernoulli’s principle). To bridge these gaps, a human-centered, law-driven benchmark is needed to more faithfully reflect real-world physical understanding and to guide future improvements.

In this paper, we introduce a human-evaluated, first-principles benchmark, namely T2VPhysBench, designed to assess whether text-to-video models can follow 12 fundamental physical laws. We include both leading open-source models and state-of-the-art commercial systems, reflecting the latest advances in 2025. Our study exposes persistent challenges in modeling physical behavior and offers insights into why models fail.

Our contributions can be summarized as follows:

- We introduce a first-principled benchmark that systematically evaluates whether modern text-to-video generation models respect twelve fundamental physical laws, covering Newtonian mechanics, conservation principles, and phenomenological effects.
- Through a rigorous human evaluation protocol, we demonstrate that all state-of-the-art text-to-video models consistently fail to satisfy even basic physical constraints, with average compliance scores below 0.60 across every law category.
- By incorporating progressively more concrete hints, naming the law and adding detailed mechanistic descriptions, we show that prompt refinement alone cannot overcome the models’ inability to generate physically coherent videos.
- We challenge models with counterfactual prompts that explicitly request physically impossible scenarios and find that they often comply, producing rule-violating videos and revealing a reliance on surface patterns rather than true physical reasoning.

**Roadmap.** In Section 2, we review prior works. In Section 3, we show the details of our proposed benchmark. In Section 4, we present the main evaluation results. In Section 5, we provide insights to understand the failure of text-to-video models in following physical constraints. In Section 6, we draw a conclusion for this paper.

# Related Works

**Benchmarks on Text-to-Video Generation.** As text-to-video models have become fundamental game changers in online experiences, particularly in creative art creation, their evaluation has become a crucial area of focus. Existing benchmarks have covered video fidelity, compositional ability, temporal dynamics, and storytelling capabilities. Benchmarking how text-to-video generation models adhere to basic physical laws is another key area. For example, VideoPhy proposes a human-evaluated benchmark examining collisions between different materials, while the Physics-IQ benchmark evaluates models based on their ability to extend given video frames using automated metrics like MSE or IoU. While these works provide valuable early insights, they do not approach the problem from a first-principles physical law perspective, nor do they incorporate careful human evaluation, highlighting the need for our work.

**Text-to-Video Generative Models.** Early approaches to text-to-video can be traced back to VAEs and GANs conditioned on text, which were limited by weak generative abilities and weak video-text connections. Empowered by large-scale visual-text pretraining and the development of modern video diffusion models, recent text-to-video generative models have significantly improved the quality of generated videos and their ability to follow complex textual prompts. Despite strong video fidelity and instruction-following abilities, their fundamental capability to adhere to simple physical laws still exhibits significant gaps, which is one of the key motivations for this benchmark.

# The T2VPhysBench Benchmark

In this section, we first present the baseline video generation models, then introduce our benchmark prompts, and finally describe the evaluation protocol.

## 3.1 Baseline Models

We selected a diverse set of state-of-the-art video generation models released between 2023 and 2025 to ensure our evaluation reflects the latest advances and uncovers their limitations in following physical constraints. Our benchmark includes ten models, spanning both closed-source and open-source systems.


| Model Name | Year | Params | Organization | Open |
| :-- | :-- | :-- | :-- | :-- |
| Kling | 2024 | NA | Kuai | No |
| Wan 2.1 | 2025 | 14B | Alibaba | Yes |
| Sora | 2024 | NA | OpenAI | No |
| Mochi-1 | 2024 | 10B | Genmo | Yes |
| LTX Video | 2024 | 2B | Lightricks | Yes |
| Pika 2.2 | 2025 | NA | Pika Labs | No |
| Dreamina | 2024 | NA | ByteDance | No |
| Qingying | 2024 | 5B | Zhipu | Yes |
| SD Video | 2023 | 1.4B | Stability AI | Yes |
| Hailuo | 2025 | NA | MiniMax | No |

For generation, we use the lowest available resolution (typically 720p) to balance visual fidelity with physical accuracy. We fix a 16:9 aspect ratio and choose a short video duration (usually 4 seconds) to concentrate the evaluation on fundamental physical behaviors.

## 3.2 Benchmark Prompts

In this benchmark, we address the problem of enforcing physical constraints using a first-principles approach. Rather than relying on intuition or everyday contexts, our prompts are derived directly from fundamental laws of physics. We organize these laws into three categories: Newton’s laws, conservation laws, and phenomenological principles. In each category we select four specific laws for a total of twelve, and for each law we design seven prompts based on realistic scenarios. Consequently, each model is evaluated on 84 distinct prompts.

**Newton Principles:**

- Newton’s First Law (Inertia)
- Newton’s Second Law (Force-Acceleration)
- Newton’s Third Law (Action-Reaction)
- Law of Universal Gravitation

**Conservation Principles:**

- Conservation of Energy
- Conservation of Mass
- Conservation of Linear Momentum
- Conservation of Angular Momentum

**Phenomenon Principles:**

- Hooke’s Law
- Snell’s Law
- Law of Reflection
- Bernoulli’s Principle

**Figure 1:** All 12 physical laws evaluated in this benchmark, illustrated with video examples from various text-to-video models.

## 3.3 Evaluation Protocol

To align with human judgment and address the fidelity-only limitations of prior physical benchmarks, we adopt a fully manual evaluation protocol. Three annotators (undergraduate or graduate students) independently review every generated video and assign it one of four quality levels based on its adherence to the target physical law:

- Level 1 (score 0.0): The video fails to demonstrate the intended physical behavior.
- Level 2 (score 0.25): The video exhibits a clear violation of the law.
- Level 3 (score 0.5): The video is largely correct but contains minor inaccuracies.
- Level 4 (score 1.0): The video fully and accurately conforms to the law.

For each model, we average the scores across all prompts and annotators to produce a single physical-consistency score, which is then used to rank the models.

# Experiments

In this section, we show the main experiment results of our proposed benchmark.

## 4.1 Overall Physical Constraint Results

| Model | Newton Principles | Conservation Principles | Phenomenon Principles | Avg. Score |
| :-- | :-- | :-- | :-- | :-- |
| SD Video | 0.21 | 0.19 | 0.19 | 0.19 |
| Hailuo | 0.27 | 0.15 | 0.25 | 0.22 |
| Dreamina | 0.19 | 0.13 | 0.38 | 0.23 |
| Sora | 0.31 | 0.15 | 0.38 | 0.28 |
| LTX Video | 0.40 | 0.13 | 0.40 | 0.31 |
| Pika 2.2 | 0.38 | 0.19 | 0.40 | 0.32 |
| Mochi-1 | 0.40 | 0.23 | 0.40 | 0.34 |
| Kling | 0.52 | 0.17 | 0.38 | 0.35 |
| Qingying | 0.35 | 0.23 | 0.63 | 0.40 |
| Wan 2.1 | 0.56 | 0.29 | 0.42 | 0.42 |

**Observation 4.1:** Despite advances in video generation, all evaluated models score below 0.60 on basic Newtonian and conservation laws, highlighting a consistent failure to model fundamental physics.

**Observation 4.2:** The score variance between different types of laws is noticeable. Conservation principles are substantially harder for current models, whereas Newton’s laws and phenomenon principles yield consistently higher scores.

**Observation 4.3:** The difference between the highest and lowest average scores (0.42 vs. 0.19) reveals a substantial performance gap, motivating targeted improvements in physical reasoning capabilities.

## 4.2 Impact of Hint Levels

We explore whether providing progressively more concrete hints in the prompts helps models follow physical constraints. Three hint levels are considered:

- **Initial Prompt:** The original prompt without additional hints.
- **First-Level Hint:** The name of the relevant physical law is explicitly provided.
- **Second-Level Hint:** A fully concrete scenario with detailed physical interpretation is provided, alongside naming the law.

**Figure 2:** Prompt and Video Examples with Different Hint Levels.

**Observation 4.4:** Despite consistent improvements on a small number of physical laws, for most physical laws, increasing the hint level does not enhance the physical law-following scores, and in many cases, even leads to a negative impact at both hint levels.

## 4.3 Impact of Counterfactual Prompts

To assess whether the models truly understand physical laws rather than rely on superficial pattern matching, we design counterfactual prompts that explicitly describe impossible scenarios.


| Model | Newton Principles | Conservation Principles | Phenomenon Principles | Avg. Score |
| :-- | :-- | :-- | :-- | :-- |
| Kling | 0.19 | 0.31 | 0.00 | 0.17 |
| Dreamina | 0.31 | 0.25 | 0.06 | 0.21 |
| Mochi-1 | 0.25 | 0.31 | 0.31 | 0.29 |
| Wan 2.1 | 0.31 | 0.50 | 0.13 | 0.31 |
| SD Video | 0.31 | 0.31 | 0.31 | 0.31 |
| LTX Video | 0.44 | 0.25 | 0.31 | 0.33 |
| Qingying | 0.50 | 0.38 | 0.31 | 0.40 |
| Sora | 0.38 | 0.56 | 0.44 | 0.46 |
| Hailuo | 0.31 | 0.63 | 0.44 | 0.46 |
| Pika 2.2 | 0.63 | 0.56 | 0.25 | 0.48 |

**Observation 4.5:** Even when instructed to violate the laws, all models score poorly in all the physical law classes, demonstrating an inability to understand impossible physics.

**Observation 4.6:** Models that excel under standard prompts can be easily misled by counterfactuals, showing their compliance is rooted in memorized patterns rather than genuine physical reasoning.

# Discussion

Several open directions and possible solutions to the inherent limitations of text-to-video models in adhering to physical constraints are discussed:

- **World Foundation Models:** These refer to large neural networks that simulate physical environments and predict outcomes based on given inputs. When such a model is used as a backbone for video generation, the output naturally obeys learned physical rules.
- **Rule-based Machine Learning:** Another promising direction is the explicit integration of physical laws into the model training process via rules, constraints, or symbolic reasoning. This extends previous frameworks that merely match videos with text, without embedding the laws of mechanics into the model architecture or loss functions.


# Conclusion

T2VPhysBench provides a rigorous, human-centered evaluation of text-to-video models against first-principles physics. By systematically identifying where state-of-the-art text-to-video systems violate basic laws, our work highlights critical gaps that could affect downstream applications in robotics, autonomous vehicles, and scientific visualization—domains where physical realism is essential for safety and reliability. Researchers and practitioners can use our benchmark to flag unsafe or misleading outputs, guide the design of physics-aware architectures, and curate more robust training datasets. Ultimately, this contributes to building generative models that not only produce visually compelling content but also behave in a physically coherent manner.

# Appendix

## A Implementation Details

We show some extra details of the selected generators in this subsection.

**Kling:** Closed-source, developed by Kuai, released 2024. Four versions (Kling 1.0, 1.5, 1.6, 2.0). Supports 5 or 10 second videos, aspect ratios 16:9, 1:1, 9:16. Prompt dictionary, AI-generated prompt hints, negative prompts, seed selection, up to four videos in parallel.

**Wan 2.1:** Open-source, developed by Alibaba, released 2025. Two variants (Fast, Professional). Multiple aspect ratios, extended prompt input, Inspiration Mode, generates videos with sound.

**Sora:** Closed-source, developed by OpenAI, released 2024. Output in 480p, 720p, 1080p, aspect ratios 16:9, 1:1, 9:16. 30 FPS videos, durations 5–20 seconds. Style presets, four videos in parallel.

**Mochi-1:** Open-source, developed by Genmo, released 2024. Supports 480p, 16:9, 5 seconds at 24 FPS. Random prompt suggestions, seed function, two videos in parallel.

**LTX Video:** Open-source, developed by Lightricks, released 2024. Preset styles, 768×512 (512p), aspect ratios 16:9, 1:1, 9:16, 5 seconds at 24 FPS. Shot type, scene location, style presets, references, voiceover scripts.

**Pika 2.2:** Closed-source, developed by Pika Labs, introduced 2025. Features Pikaframes, Pikaaffects, Pikascenes, Pikaddition, Pikawaps. 720p or 1080p, aspect ratios 16:9, 9:16, 1:1, 4:5, 4:3, 5:2. 5 or 10 second clips, negative prompts, seed inputs, four videos at once.

**Dreamina:** Closed-source, developed by Bytedance, launched 2024. Four variants, aspect ratios 16:9, 21:9, 4:3, 1:1, 3:4, 9:16. 5 or 10 second videos, 24 FPS.

**Qingying:** Commercial edition of CogVideo family by Zhipu, released 2023–2024. Two modes (Fast, Quality), 5 second videos at 60 or 30 FPS, aspect ratios 16:9, 9:16, 1:1, 3:4, 4:3. Advanced settings: video style, emotional atmosphere, camera movement, AI-generated sound.

**Hailuo:** Closed-source, developed by MiniMax, introduced 2025. Features T2V-01-Director and T2V-01. 720p, 16:9, 6 seconds, 24 FPS.

**Stable Video Diffusion:** Open-source, developed by Stability AI, released 2023. Aspect ratios 16:9, 3:2, 1:1, 4:5, 9:16. 4 second videos.

## B Limitations

T2VPhysBench is entirely empirical—we document where and how models fail, but do not offer theoretical analyses or guarantees that explain why these architectures struggle with physical constraints. The reliance on manual annotation, essential to capture nuanced human judgments, limits scalability and rapid iteration. Extending the benchmark to larger model sets or more laws will require substantial annotation effort or the development of reliable automated proxies.

## C Impact Statement

T2VPhysBench addresses a foundational trustworthiness challenge in generative AI: ensuring that synthesized videos obey real-world physics. By systematically identifying where state-of-the-art text-to-video systems violate basic laws, our work highlights critical gaps that could affect downstream applications in robotics, autonomous vehicles, and scientific visualization. Researchers and practitioners can use our benchmark to flag unsafe or misleading outputs, guide the design of physics-aware architectures, and curate more robust training datasets. We do not foresee direct negative uses of this benchmark—exposing model failures to physical laws is unlikely to enable harmful behavior. If anything, understanding these limitations can prevent the misuse of generated videos in safety-critical systems.

## D Video Examples

In this section, we present a wide range of video samples generated using the prompts proposed in this benchmark, as illustrated in Figures 7–30. Each figure includes results from five distinct text-to-video models, with five key frames selected from the video samples to illustrate how they change over time. These selected video instances align with all the experiments discussed in Section 4.

**Figure 7:** Results of Generating Videos Following Newton’s First Law.

**Figure 8:** Results of Generating Videos Following Newton’s First Law.

**Figure 9:** Results of Generating Videos Following Newton’s Second Law.

**Figure 10:** Results of Generating Videos Following Newton’s Second Law.

**Figure 11:** Results of Generating Videos Following Newton’s Third Law.

**Figure 12:** Results of Generating Videos Following Newton’s Third Law.

**Figure 13:** Results of Generating Videos Following Law of Universal Gravitation.

**Figure 14:** Results of Generating Videos Following Law of Universal Gravitation.

**Figure 15:** Results of Generating Videos Following Conservation of Energy.

**Figure 16:** Results of Generating Videos Following Conservation of Energy.

**Figure 17:** Results of Generating Videos Following Conservation of Mass.

**Figure 18:** Results of Generating Videos Following Conservation of Mass.

**Figure 19:** Results of Generating Videos Following Conservation of Momentum.

**Figure 20:** Results of Generating Videos Following Conservation of Momentum.

**Figure 21:** Results of Generating Videos Following Conservation of Angular Momentum.

**Figure 22:** Results of Generating Videos Following Conservation of Angular Momentum.

**Figure 23:** Results of Generating Videos Following Hooke’s Law.

**Figure 24:** Results of Generating Videos Following Hooke’s Law.

**Figure 25:** Results of Generating Videos Following Snell’s Law.

**Figure 26:** Results of Generating Videos Following Snell’s Law.

**Figure 27:** Results of Generating Videos Following Law of Reflection.

**Figure 28:** Results of Generating Videos Following Law of Reflection.

**Figure 29:** Results of Generating Videos Following Bernoulli’s Principle.

**Figure 30:** Results of Generating Videos Following Bernoulli’s Principle.

# Abstract

Continual Test-time adaptation (CTTA) continuously adapts the deployed model on every incoming batch of data. While achieving optimal accuracy, existing CTTA approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. In this work, we first introduce a novel paradigm—on-demand TTA—which triggers adaptation only when a significant domain shift is detected. Then, we present OD-TTA, an on-demand TTA framework for accurate and efficient model adaptation on edge devices. OD-TTA comprises three innovative techniques: (1) a lightweight domain shift detection mechanism to activate TTA only when it is needed, drastically reducing the overall computation overhead; (2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy; (3) a decoupled Batch Normalization (BN) update scheme to enable memory-efficient adaptation with small batch sizes. Extensive experiments show that OD-TTA achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making TTA a practical reality.

# Introduction

Deep neural networks (DNNs) have achieved remarkable success in real-time edge tasks such as object detection, image recognition, and autonomous driving. However, as a data-driven technique, DNNs typically achieve optimal performance only when training and testing data share the same distribution. In real-world scenarios, testing data often experiences distribution variations, known as domain shifts, due to factors such as weather changes, sensor noise, or lighting conditions, which can result in significant performance degradation.

To address this challenge, previous works have developed continual test-time adaptation (CTTA), which enables the pre-trained DNN model to continuously adapt to unseen domains using only unlabeled test data in either a self-supervised or unsupervised manner. Self-supervised learning approaches generate pseudo-labels for test data and fine-tune the model. In contrast, unsupervised approaches, such as entropy minimization, are considered more efficient because they only update the model once per input batch using entropy loss.

Recently, several approaches have focused on improving the efficiency of TTA to enable more practical deployment on resource-constrained devices. However, existing efficient TTA approaches fail to fundamentally address the efficiency issue, as they still adhere to the CTTA paradigm which continuously executes resource-intensive backpropagation for each test batch. Additionally, considering that the domain shift between consecutive batches is usually minor in real-world scenarios, CTTA may not yield substantial accuracy improvements.

In this paper, we introduce a more practical and efficient paradigm, referred to as on-demand TTA. Unlike continual TTA, on-demand TTA triggers model adaptation only when a significant domain shift that leads to an unacceptable application-defined performance drop occurs. This paradigm introduces several key challenges: (1) on-demand TTA requires continuous monitoring of the data distribution for every incoming sample or batch for potential domain shift detection. However, efficiently quantifying the domain shift or performance drop without labels is challenging and remains under-explored in existing TTA literature; (2) differing from continual TTA, where the distribution of consecutive batches usually remains similar, on-demand TTA inherently deals with more severe shifts after a domain shift is detected; (3) a notable limitation of existing Batch Normalization (BN)-based TTA is its dependence on large batch sizes, which requires considerable memory.

To address these challenges, we propose OD-TTA, an end-to-end efficient On-Demand TTA framework designed for edge devices. We drew three key insights from our observations and experimental studies to guide the design of OD-TTA. First, we observed that entropy can be used not only for adaptation as in existing methods but also for detecting domain shifts. Based on this, we devised a novel lightweight domain shift detection mechanism using exponential moving average (EMA) entropy. Second, we found that adapting from different similar or non-similar source domains yields distinct post-adaptation performance. Therefore, instead of always adapting from the previous domain as in continual TTA, we propose a similar domain selection pipeline that constructs and selects the closest domain for adaptation, resulting in better performance and faster convergence. Third, inspired by the insight that updating BN statistics and BN parameters consumes different amounts of memory and shows different sensitivity to batch sizes, we designed a decoupled BN update scheme that adapts the BN statistics and BN parameters asynchronously with different batch sizes, enabling effective model adaptation within a constrained memory budget.

We compare our proposed OD-TTA with strong baselines on CIFAR10-C, ImageNet-C, and SHIFT. Our proposed method achieves the best accuracy and energy efficiency over all the baselines while maintaining minimal memory requirements. Specifically, OD-TTA achieves up to 9.7% higher accuracy within comparable memory, and up to 47% energy saving on CIFAR10-C. In particular, OD-TTA is the only effective method for BN-based models when operating with a batch size of 1.

Our contributions are summarized as follows:
- We introduced the concept of on-demand TTA and presented OD-TTA, a novel on-demand TTA framework for edge devices. OD-TTA comprises a lightweight domain shift detector, a source domain selection module, and a decoupled BN updating strategy.
- We implemented OD-TTA on Jetson Orin Nano and evaluated its performance across multiple datasets. Our results indicate that OD-TTA achieves superior performance with minimal system overhead.
- Finally, our proposed paradigm and framework open the door, for the first time, to making TTA a practical reality with high accuracy and minimal system overheads on resource-constrained edge devices.

# Related Work

## Continual Test-Time Adaptation

Existing CTTA methodologies can be categorized into self-supervised and unsupervised learning paradigms. Self-supervised CTTA initially generates pseudo labels for the testing data, then utilizes these labels to fine-tune the pretrained model in a supervised manner. Unsupervised CTTA addresses the domain shift by updating certain layers of the model through unsupervised loss functions. Benz et al. highlighted the critical role of batch normalization (BN) layers in adapting to domain shifts. Following this, TENT was proposed as an early TTA method, updating BN layers by simple entropy minimization. EATA improves upon TENT by filtering out redundant and unreliable data and re-weighting remaining data. SAR replaces the BN layer with a group normalization (GN) layer to make TTA work even when the batch size is one, but incurs high latency during adaptation. Recent works proposed memory-efficient adaptation, such as MECTA and Eco-TTA.

Our work differs from all existing research in that we propose a completely new on-demand TTA paradigm and devise a suite of techniques to ensure it outperforms existing CTTA methods.

## Domain Shift Detection

Domain shift detection is an essential part of OD-TTA, which monitors the distribution shift in the data stream to trigger the adaptation. Previous methods either use auxiliary neural networks or feature-based statistics, but these often require large batch sizes or are memory-intensive. Our detection approach is both lightweight and effective, offering a significant advantage by being adaptable to any batch-size configuration.

# On-demand Test-time Adaptation

## Problem Formulation

In practical edge computing scenarios, sensor data arrive sequentially as $$ S_{seq} = \{s_1, s_2, ..., s_t, ...\} $$, where $$ S_t $$ represents either a single sample or a small batch of samples arriving at time $$ t $$. Domain shifts occur unpredictably, resulting in accuracy drop. On-demand TTA aims to adapt the model $$ f_s $$ only when a substantial domain shift results in unacceptable performance degradation. On-demand TTA operates under the constraint that the source dataset is not accessible during adaptation. Moreover, the adaptation must be performed directly on-device in an unsupervised manner, making it suitable for resource-constrained edge environments.

## OD-TTA Overview

OD-TTA comprises two fundamental modules: domain shift detection and model adaptation. When a pre-trained model is deployed in real-world scenarios, it continuously performs inference on the incoming data stream while monitoring potential domain shifts using the proposed lightweight shift detection mechanism. Once a shift is detected, OD-TTA triggers an adaptation process involving two steps. First, OD-TTA selects the closest domain from a pool of candidates (pretrained or pre-adapted models), which can accelerate the subsequent adaptation process and enhance the adaptation performance by ensuring that adaptation starts from a more similar distribution. Second, OD-TTA adapts the model to align with the new domain data using a decoupled BN updating strategy, which effectively reduces the memory consumption while maintaining comparable accuracy.

## Domain Shift Detection

The first objective in on-demand TTA is to detect the occurrence of domain shifts in a lightweight manner, as this needs to be performed continuously on all incoming data. Since the ground truth labels of the test data are unavailable, monitoring accuracy drop caused by domain shift is challenging. Inspired by entropy minimization, which improves model performance by reducing the entropy of predictions during training, we draw the following insight:

> During inference, the model accuracy is inversely correlated with the entropy of the predictions.

Entropy measures the uncertainty in the model's predictions. When there is a domain shift, the model tends to produce more uncertain predictions (higher entropy), as it struggles to generalize to the new distribution.

### EMA Entropy Calculation

Certain samples may result in overly confident predictions, disrupting this inverse correlation. Since data arrives in a streaming manner during testing, sample-wise entropy cannot accurately characterize the model performance. Thus, we introduce an Exponential Moving Average (EMA) strategy to smooth the sample-wise entropy and incorporate historical entropy values, providing a more stable accuracy estimation.

The formula for calculating the EMA entropy is:
$$
E_t = E_{t-1}(1-m) + m x_t
$$
where $$ E_t $$ represents the EMA entropy at time $$ t $$, $$ m $$ denotes the momentum factor with a value between 0 and 1, and $$ x_t $$ is the entropy value of the current input sample at time $$ t $$.

After each adaptation process, OD-TTA records the EMA entropy over the next few samples (e.g., 100) as the entropy baseline, reflecting the current model's capability on the adapted domain data. If the EMA entropy of incoming samples exceeds a user-defined threshold, an adaptation is triggered.

## Source Domain Selection

CTTA always adapts the model from the previous domain, which may not be effective in on-demand TTA due to significant distribution shifts. The source domain can significantly impact the adaptation process, including convergence speed and post-adaptation accuracy. This motivates us to select the domain most similar to the new domain from a candidate pool before adaptation.

### Candidate Pool Construction

The process of creating a domain candidate pool involves generating multiple sets of data with diverse distributions. The training dataset is split into multiple subsets and the pre-trained model is adapted on each subset, updating only the BN layers in a supervised manner. This results in multiple domain candidates in the pool by saving the BN layers. Saving only the BN layers is highly storage-efficient.

### Similar Candidate Selection

To select the most similar domain, we cache $$ N $$ samples from the new domain, process them through the source model to obtain the test BN statistics, and compute the L2 distance between the BN means of the new domain and each candidate in the pool. The candidate with the smallest distance is selected for adaptation.

## Decoupled BN Update

Most existing CTTA approaches require large batch sizes, which consumes significant memory due to backpropagation. SAR replaces BN layers with GN layers to address the batch size issue, but GN is more computation-intensive and yields lower performance when batch size is large.

Adapting only the BN layers with a small amount of data can achieve good performance. Updating BN statistics requires only a forward pass, which is memory-efficient yet highly sensitive to batch size. Updating BN parameters is less sensitive to batch size but involves backpropagation, which is more memory-intensive.

### BN Statistics Update

We efficiently reuse cached samples to form a small dataset to adapt the BN layers. The samples are split into batches, and an EMA approach integrates the BN statistics of the source model and the batches of new domain data.

### BN Parameters Update

After updating the BN statistics, the BN parameters are fine-tuned through backpropagation with a small batch size. To achieve stable fine-tuning, we introduce a sample filter to remove unreliable samples and a contrastive loss as a regularization term to refine the entropy loss.

The overall loss for adaptation is:
$$
L_{total} = L_{entropy} + \lambda L_{contrastive}
$$
where $$ L_{entropy} $$ is the regular entropy loss and $$ L_{contrastive} $$ is a contrastive loss to regularize the adaptation process.

# Evaluation

## Experiment Details

### Datasets

- **CIFAR10-C and ImageNet-C**: Variants of the original datasets, constructed by applying 15 different types of common corruption.
- **SHIFT**: A domain shift dataset designed for autonomous driving systems, showcasing three domain shifts: daytime→night, clear→foggy, and clear→rainy.

### Baselines

OD-TTA is compared with continual TTA baselines: CoTTA, TENT, EATA, SAR, and MECTA.

### Adaptation Details

For OD-TTA, 128–512 samples are used for source domain selection and decoupled BN update on CIFAR10-C and ImageNet-C under batch size 1. For batch size 16–64, 512 samples are used for adaptation. In decoupled adaptation, BN statistics are updated in large batch size and BN parameters in small batch size.

### Implementation

OD-TTA is evaluated on Jetson Orin Nano, a widely used edge device. Classification tasks are evaluated with batch sizes of 1 and 16 on edge devices; batch size 64 and segmentation tasks are evaluated on a server.

## Main Results

### Accuracy vs. Memory

**Table 1: Comparison of accuracy on CIFAR-10-C and ImageNet-C using ResNet-50 along with memory consumption on Jetson Orin Nano.**

| Method   | Batch Size 1 | Batch Size 16 | Batch Size 64 |
|----------|--------------|---------------|---------------|
|          | Cifar10 | ImageNet | Mem (MB) | Cifar10 | ImageNet | Mem (MB) | Cifar10 | ImageNet | Mem (MB) |
| Source   | 59.5   | 26.9     | 242       | 59.5    | 26.9     | 358      | 59.5    | 26.9     | 809      |
| CoTTA    | 10.0   | 0.1      | 889       | 78.1    | 28.9     | 3519     | 81.1    | 34.2     | 12179    |
| Tent     | 10.1   | 0.1      | 434       | 78.0    | 26.0     | 1728     | 81.0    | 35.5     | 5779     |
| EATA     | 22.8   | 0.8      | 506       | 78.3    | 31.9     | 1728     | 81.0    | 38.4     | 5783     |
| SAR      | 68.3   | 35.3     | 429       | 70.4    | 35.2     | 1723     | 69.2    | 37.3     | 5780     |
| MECTA    | 65.0   | 10.0     | 378       | 81.3    | 33.2     | 1231     | 81.3    | 33.7     | 3836     |
| Ours     | 78.0   | 33.1     | 414       | 83.0    | 37.4     | 1677     | 84.9    | 40.4     | 5763     |

**Table 2: Adaptation mIoU on SHIFT along with memory consumption.**

| Method  | Day→Night | Clear→Foggy | Clear→Rainy | Avg.   | Memory |
|---------|-----------|-------------|-------------|--------|--------|
| Source  | 27.30     | 17.74       | 10.98       | 18.67  | 502    |
| CoTTA   | 24.43     | 20.96       | 15.88       | 20.42  | 2065   |
| Tent    | 24.72     | 22.12       | 17.71       | 21.52  | 1163   |
| EATA    | 24.58     | 21.53       | 16.54       | 20.88  | 1216   |
| SAR     | 23.61     | 7.63        | 4.22        | 11.82  | 1185   |
| MECTA   | 24.23     | 20.59       | 15.42       | 20.08  | 855    |
| Ours    | 31.08     | 25.17       | 19.05       | 25.10  | 1165   |

**Figure 1:** OD-TTA achieves a superior trade-off between memory, energy, and accuracy compared to state-of-the-art CTTA baselines. The radius of circles represents memory usage.

**Figure 2:** OD-TTA overview. The model performs regular inference while monitoring domain shifts. Once a shift is detected, OD-TTA selects the most similar BN candidate from a candidate pool and asynchronously adapts the BN statistics and affine parameters using a few new domain data.

### Energy Consumption

**Figure 4:** Energy consumption for processing domain data sequences of varying lengths under batch size (a) 1 and (b) 16.

OD-TTA achieves up to 47.1% energy savings compared to other adaptation methods in both batch size settings. Unlike CTTA methods which perform gradient-based updates for every batch of data, OD-TTA performs a one-time adaptation using only a few samples from the new domain.

## Detailed Results

### Analysis of Shift Detection

**Figure 5:** EMA entropy change along data stream on Cifar10-C. The red dotted lines are where domain shift is detected. Domains change after every 10,000 samples, as denoted by the changes in background color.

OD-TTA successfully detected 13 out of 15 domain shifts. The two undetected shifts did not result in accuracy drops.

### Analysis of Source Domain Selection

**Table 3: Evaluation of source model selection on CIFAR-10-C and ImageNet-C.**

| Adaptation from      | Batch Size 1 | Batch Size 16 | Batch Size 64 |
|----------------------|--------------|---------------|---------------|
|                      | Cifar10 | ImageNet | Cifar10 | ImageNet | Cifar10 | ImageNet |
| Prev-Domain          | 79.5    | 27.8     | 84.4    | 31.4     | 86.0    | 33.4     |
| Source-Model         | 80.6    | 32.8     | 84.6    | 35.4     | 85.9    | 37.5     |
| Selected-Domain      | 81.0    | 34.6     | 85.7    | 36.7     | 86.1    | 39.1     |

### Analysis of Decoupled BN Update

Increasing the number of adaptation samples enhances overall performance; the benefits become trivial when the number of samples exceeds 1024. Employing contrastive loss results in up to 2.6% accuracy improvement on ImageNet-C.

### Effectiveness on MobileNet

OD-TTA on MobileNetV2 on Cifar10-C consistently outperforms all the baselines in each batch size setting.

# Conclusion

This paper proposes a novel concept called on-demand TTA, which triggers adaptation only when a domain shift is detected. We introduce OD-TTA, a framework designed to realize on-demand TTA for edge devices. OD-TTA comprises three key components: domain shift detection to monitor distribution shifts on the fly, source domain selection to optimize the efficacy of the source model for adaptation, and decoupled BN adaptation to update the model efficiently under limited memory constraints. The experiment results show that OD-TTA significantly outperforms baselines while maintaining comparable memory overhead.

**Impact Statement:**  
OD-TTA enables efficient and selective adaptation, ensuring high performance while minimizing resource usage, making it a robust and scalable solution for real-world deployment. Our approach is specifically designed for widely-used BN-based models. Other architectures, such as Vision Transformers, are not in the scope of this paper.
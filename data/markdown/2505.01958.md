# Abstract

Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs—the large language model, the vision backbone, and the projector—to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.

# Introduction

Large Language Models (LLMs), such as GPT-3 and ChatGPT, have showcased remarkable proficiency in language tasks, yet they encounter significant challenges when it comes to processing multimodal inputs. This limitation has driven a shift in research towards Large Vision-Language Models (LVLMs), which integrate advanced LLMs with Vision Foundation Models (VFMs) to enhance multimodal understanding. LVLMs have demonstrated impressive capabilities across various tasks that require visual and textual integration, including Visual Question Answering, Image Captioning, and Visual Entailment.

Despite these advances, visual hallucination remains a persistent issue in LVLMs. This phenomenon occurs when models generate inaccurate or misleading information unrelated to the actual visual input, potentially leading to misinformation and raising concerns about safety and reliability in real-world applications. Visual object hallucination, including object existence, attribute, and relation, has garnered significant attention due to its widespread occurrence in images.

Current works on visual object hallucination mainly focus on evaluation and mitigation. For example, new polling-based query techniques and reward model augmentations have been proposed to probe and reduce hallucination. However, existing works lack a comprehensive component-level analysis of the model architecture to pinpoint where and how hallucinations occur.

In this work, we focus on visual object-related hallucination and LLaVA-like LVLMs, which typically consist of three modules: the large language model (LLM), the vision backbone, and the projector. Errors in any of these modules can lead to issues in the overall performance or functionality of the model. Therefore, we conduct an independent analysis of each component to identify potential sources of error and their impact.

Our main findings are:
1. The LLM in LVLM is able to generate faithful content when captions of images are provided as input.
2. Hallucinations exist in the perception process of the vision backbone.
3. The projector is able to preserve visual features but has trouble aligning between visual and textual spaces.

Based on our observations, we propose methods for the two problematic components to mitigate their hallucination issue. To improve the vision backbone, we propose to finetune CLIP with fine-grained data and fine-grained perception-based visual instruction tuning, and find that both of them can reduce hallucination caused by the vision backbone. For the projector, we propose a contrastive alignment objective with three variations, which can all be integrated into the original training pipeline with minimal additional costs.

To conduct a comprehensive hallucination evaluation, we develop a fine-grained hallucination benchmark named QA-VisualGenome, which is built upon the Visual Genome dataset. Unlike existing object-oriented hallucination benchmarks, QA-VisualGenome emphasizes the detailed attribute and relationship hallucinations. Furthermore, existing hallucination benchmarks primarily focus on perception-based hallucinations for general objects, neglecting cognition-based hallucinations such as the names of people and famous buildings. To address this gap, we construct a cognition-based hallucination benchmark named QA-FB15K, which is based on the FB-15K dataset, a multimodal knowledge graph with textual entities, image entities, and textual relations.

Our contributions can be summarized as follows:
- We analyze the hallucination caused by each component in LVLMs and provide component-wise takeaway messages.
- Based on our observation, we propose several methods to improve each hallucinated component.
- We construct a fine-grained hallucination benchmark based on Visual Genome and a cognition-based hallucination benchmark based on FB15k for evaluation.
- We extensively evaluate our proposed methods on various benchmarks, and provide in-depth analysis.

# 1. Hallucination Analysis

LVLMs consist of three components: language decoder (D), vision encoder (V), and projector (P). We first introduce the datasets for evaluation and then provide in-depth analysis for each component.

## 1.1 Settings

We select two benchmarks to benchmark the performance of each component:
- **POPE**: A benchmark designed for evaluating object existence hallucinations in LVLMs, incorporating three sampling methods for generating negative samples: random, popular, and adversarial.
- **QA-VisualGenome**: To further investigate the hallucination issue on relations and attributes of objects, we construct a new fine-grained evaluation benchmark based on the VisualGenome dataset, which collects dense annotations of attributes and relationships of objects for each image. Specifically, we design two types of Yes-or-No questions to evaluate models: attributes and relations.

## 1.2 Language Decoder

**Conjecture 1:** LLM in LVLM is able to generate faithful content when image captions are provided as input.

To validate this conjecture, we use the POPE dataset to evaluate the performance of LLMs. Instead of providing images to the LVLMs, we only input text descriptions of the images. This helped assess the model's ability to hallucinate when provided with accurate textual descriptions of the image. We also test the original Vicuna as a baseline.

**Table 1: Performance of LLMs across different datasets when visual information is provided in textual format.**

| Model               | POPE Random Acc | POPE Popular Acc | POPE Adversarial Acc | QA-VisualGenome Attribute Acc | QA-VisualGenome Relation Acc |
|---------------------|-----------------|------------------|----------------------|-------------------------------|------------------------------|
| LLaVA-7B            | 87.42           | 86.63            | 85.13                | 64.67                         | 67.57                        |
| Vicuna-7B           | 92.67           | 92.67            | 93.00                | 57.23                         | 79.50                        |
| Vicuna-7B LLaVA     | 100.00          | 100.00           | 99.67                | 68.29                         | 63.2                         |
| LLaVA-13B           | 91.33           | 88.33            | 84.33                | 55.99                         | 56.40                        |
| Vicuna-13B          | 87.90           | 95.00            | 90.00                | 87.90                         | 87.90                        |
| Vicuna-13B LLaVA    | 99.67           | 99.67            | 99.33                | 75.41                         | 84.30                        |

From the results, we found that the performance will be improved largely if we provide the correct visual information in a textual format. This indicates the current main reason for hallucination is caused by a vision encoder or projector.

## 1.3 Vision Encoder

**Conjecture 2:** There are hallucinations in the perception process of the vision encoder.

To verify this factor, we conducted experiments using CLIP on a text-image matching task.

**Table 2: Performance of CLIP in the text-image matching across different datasets measured by Accuracy.**

| POPE Random | POPE Popular | POPE Adversarial | QA-VisualGenome Attribute | QA-VisualGenome Relation |
|-------------|--------------|------------------|--------------------------|--------------------------|
| 83.33       | 87.30        | 86.00            | 61.57                    | 60.22                    |

Overall, we found that the performance of CLIP on the text-matching task is not good. For example, the performance of CLIP on the text-image matching task is 83.33% accuracy on the random setting of POPE, indicating the presence of hallucinations within the vision encoder's perception process.

## 1.4 Projector

We analyze the projector module from two perspectives: preserving visual information and aligning visual and textual spaces.

**Conjecture 3:** The projector should not result in significant visual information loss.

We formalize the hypothesis using the notion of V-information. We compare the V-information between pre- and post-projector representations and a target property (e.g., a classification label). The goal is to determine whether information loss occurs in the projection layer.

**Table 3: Performance of linear probing using pre- and post-projector image features on CIFAR10, CIFAR100 and ImageNet. Accuracy is used as the metric.**

| Dataset   | Perf pre | Perf post | Drop (%) |
|-----------|----------|-----------|----------|
| CIFAR10   | 96.27    | 96.15     | -0.12    |
| CIFAR100  | 81.78    | 81.02     | -0.93    |
| ImageNet  | 71.97    | 70.83     | -1.58    |

For the 13B LLaVA model, performance percentage drop of post-projection features is less than 2%, indicating that the visual features are well preserved by the projectors in both models.

**Conjecture 4:** The projector should align the visual and textual spaces.

To probe the alignment between two spaces, we collect caption data and compute the similarity between a projected image feature and the textual embedding of its caption.

**Table 4: Cosine similarity between projected image features and textual embedding of corresponding captions across different datasets.**

| Dataset         | Token Length | Cos. Sim. (7B) | Cos. Sim. (13B) |
|-----------------|-------------|----------------|-----------------|
| MSCOCO          | 15.16       | 0.03           | 0.04            |
| LLaVA Caption   | 15.09       | 0.03           | 0.04            |
| ALLaVA          | 222.83      | 0.05           | 0.06            |

The cosine similarities of the two features are fairly low, indicating nearly independent relationships. The projector in LLaVA models may not function as an alignment module as well as expected, which could be one of the causes of hallucination for the entire model.

# 2. Mitigating Object Hallucination Caused by Different Modules

Based on the analysis, we further devised different methods to mitigate the object hallucination in different components in LVLMs.

## 2.1 How to alleviate the hallucination caused by CLIP?

The vision backbone within LVLMs also contributes to hallucinations. The CLIP model, as the vision encoder of LLaVA, is trained on massive image-caption pairs from the internet with a contrastive loss objective. However, these captions are typically brief and noisy, and negative pairs often differ substantially from positive ones. Therefore, it is likely that the model can distinguish them without needing to capture the finer details in the images.

To address this issue, we propose two methods to reduce hallucination caused by the vision backbone:

- **Tuning CLIP with fine-grained data:** We leverage GPT-4 to generate negative examples, which are then used in a contrastive learning setup to improve the discriminative ability of CLIP. By exposing the model to these fine-grained differences, CLIP becomes better at understanding nuanced visual features.

- **Fine-grained perception-based visual instruction tuning:** We attempt to enable the LLM to perceive the fine-grained information within the CLIP vision encoder. We propose fine-grained perception-based visual instruction tuning, where we randomly select two bounding boxes from the image, and then use the object attributes corresponding to these bounding boxes and their relationships to generate the corresponding captions.

**Figure 2:** Tuning CLIP with fine-grained data (left) and fine-grained perception-based instruction tuning (right).

## 2.2 How to reduce hallucination caused by the projector?

Hallucination introduced by the projector may be due to the inability of aligning visual and textual spaces, manifested by the low cosine similarity of caption embeddings and projected image features. Therefore, a straightforward remedy would be to explicitly bridge the image and caption representation during LLaVA's alignment stage.

### 2.2.1 Loss Objectives

Besides autoregressive image-text generation loss, we introduce an in-batch contrastive alignment loss, where we maximize the similarity between a projected image feature and the corresponding text embedding for its caption. We design three settings that involve the contrastive loss in different fashions:

- **Integrated Alignment Loss:** The contrastive loss is integrated to the alignment stage with a learnable weight.
- **Integrated Alignment Loss (Frozen):** The weight is fixed.
- **Separate Contrastive Alignment Loss:** A contrastive alignment stage solely for the projector is prepended.

# 3. Results and Analysis

We introduce the benchmarks on which our methods are evaluated:

- **Object-based benchmarks:** POPE and POPE-NoCaps.
- **Attribute- and relation-based benchmark:** QA-VisualGenome.

All experiments are conducted on 4 A100 GPUs. For the alignment stage, we set per-GPU batch size to 64, which is also the batch size for contrastive alignment. We choose the well-known LLaVA-v1.5-7B model as our baseline.

## 3.1 Can our methods reduce hallucination caused by the vision encoder?

**Table 5: Performance of different methods across different benchmarks.**

| Method      | POPE Random Acc | POPE Popular Acc | POPE Adversarial Acc | QA-VisualGenome Attribute Acc | QA-VisualGenome Relation Acc |
|-------------|-----------------|------------------|----------------------|-------------------------------|------------------------------|
| LLaVA-7B    | 87.42           | 86.63            | 85.13                | 64.67                         | 67.57                        |
| w-ECLIP     | 87.80           | 87.30            | 85.87                | 67.67                         | 67.00                        |
| w-FineIns   | 87.77           | 86.80            | 85.53                | 69.01                         | 69.75                        |

From this table, several key observations can be drawn:
1. The proposed w-ECLIP method demonstrates superior performance compared to LLaVA-7B on perception-based benchmarks.
2. w-FineIns exhibits better performance than baseline on perception-based benchmarks.
3. Compared to w-FineIns, w-ECLIP demonstrates comparable or even better performance on perception-based benchmarks.

## 3.2 Can our methods reduce hallucination caused by the projector?

**Table 6: Performance of different projector alignment methods across different benchmarks.**

| Method               | POPE Random Acc | POPE Popular Acc | POPE Adversarial Acc | QA-VisualGenome Attribute Acc | QA-VisualGenome Relation Acc |
|----------------------|-----------------|------------------|----------------------|-------------------------------|------------------------------|
| LLaVA-7B             | 87.42           | 86.63            | 85.13                | 64.67                         | 67.57                        |
| Int. Align. (train)  | 88.21           | 86.70            | 84.27                | 60.95                         | 66.67                        |
| Int. Align. (frozen) | 88.04           | 86.67            | 84.50                | 63.84                         | 66.73                        |
| Sep. Ctrs. Align.    | 88.56           | 87.33            | 84.57                | 64.26                         | 69.60                        |

For object-oriented benchmarks POPE and POPE-NoCaps, the model trained with Separate Contrastive Alignment Loss outperforms others on most splits of benchmarks, though the improvement over baseline seems marginal.

## 3.3 Can our method influence other hallucinations?

To further investigate the influence of our method on other kinds of hallucination, we introduced the Cognition-based benchmark necessitating world knowledge in LVLMs for problem solving. We construct a cognition-based benchmark QA-FB15k based on the knowledge graph FB15K.

**Table 7: Performance of different methods on QA-FB15K.**

| Method      | Entity Acc | Entity F1 | Relation Acc | Relation F1 |
|-------------|------------|-----------|--------------|-------------|
| LLaVA-7B    | 78.39      | 73.14     | 56.79        | 48.79       |
| Int. Align. | 84.28      | 83.03     | 59.16        | 58.07       |
| Sep. Ctrs.  | 83.94      | 81.65     | 59.39        | 57.41       |
| w-ECLIP     | 77.60      | 71.47     | 56.79        | 45.58       |
| w-FineIns   | 76.47      | 69.86     | 55.45        | 49.10       |

Contrastive alignment objective is beneficial for cognition-based knowledge, as evidenced by the performance boost on QA-FB15K. Neither w-FineIns nor w-ECLIP shows any improvement on the cognition-based benchmark.

# 4. More Analysis

Additional experimental results on the hallucination benchmark and general benchmark, ablation study, and performance comparison with more baselines are provided in the appendix.

**Table 8: Illustration of potential hallucinations in the components of LVLMs, and the corresponding mitigation methods**

| Component       | Hallucination? | Mitigation         |
|-----------------|---------------|--------------------|
| Vision Backbone | ✓             | w-ECLIP, w-FineIns |
| Projector       | ✓             | Int. Align., Sep. Ctrs. Align. |
| LLM             | ×             | NA                 |

**Table 9: Performance on the Amber dataset across different model variants. Bold indicates best scores per row.**

| Method          | F1 Score |
|-----------------|----------|
| DoLa            | 80.2     |
| ITT             | 83.7     |
| VCD             | 83.2     |
| AGLA            | 84.6     |
| OPERA           | 85.2     |
| DOPRA           | 85.6     |
| HALC            | 83.9     |
| FastV           | 81.3     |
| Less is more    | 86.0     |
| CCA-LLAVA       | 85.5     |
| LRV             | 80.0     |
| Amber           | 81.6     |
| EAH             | 85.7     |
| w-ECLIP         | 85.9     |
| w-FineIns       | 85.5     |
| Int. Align.     | 85.5     |
| Sep. Ctrs. Align| 86.0     |

**Table 10: POPE F1 scores for baselines and proposed methods. Bold indicates the highest score.**

| Model           | Conv | Detail | Complex | Full |
|-----------------|------|--------|---------|------|
| LLaVA-7B        | 92   | 75     | 75      | 81   |
| w-ECLIP         | 93   | 84     | 87      | 88   |
| w-FineIns       | 94   | 86     | 86      | 89   |
| Int. Align.     | 95   | 87     | 83      | 89   |
| Sep. Ctrs. Align| 99   | 85     | 87      | 90   |

**Table 11: Model performance comparison on different categories and the full set on LLaVA-Bench.**

| Method    | POPE 2-3 Acc | POPE 2-3 F1 | POPE 5-6 Acc | POPE 5-6 F1 | POPE 8-9 Acc | POPE 8-9 F1 |
|-----------|--------------|-------------|--------------|-------------|--------------|-------------|
| w-ECLIP   | 87.80        | 86.38       | 86.93        | 85.84       | 85.62        | 83.97       |

**Table 12: Ablation study on the impact of loss function components across different POPE test subsets**

| Component | POPE Subset 1 | POPE Subset 2 | POPE Subset 3 |
|-----------|--------------|--------------|--------------|
| 1 only    | ...          | ...          | ...          |
| 2 only    | ...          | ...          | ...          |

# 5. Related Work

Our work is related to the large vision-language model and hallucination in large vision-language model.

## Large Vision-Language Model

The multimodal learning field has recently pivoted its focus towards Large Vision-Language Models (LVLMs). Current advanced LVLMs primarily comprise three essential components: a language encoder, a visual encoder, and a cross-modal alignment mechanism. Typically, the language encoder is implemented as a language model, such as LLaMA or Vicuna. In contrast, the visual encoder is usually based on VFMs like ViT. The role of the cross-modal alignment component is to integrate visual features with text representations, enabling the language encoder to effectively interpret visual semantics. LVLMs generally undergo a series of training stages, including initial alignment and finetuning with tailored language-image instruction-following datasets.

## Hallucinations in Large Vision-language Models

Since hallucination issues and mitigation techniques have been extensively explored in text generation, research on hallucinations in LVLMs is attracting more attention. Several researchers propose metrics and benchmarks to evaluate hallucination in LVLMs. Recently, various methods have been proposed to mitigate hallucinations in LVLMs, leveraging a range of techniques including decoding strategies, post-processing methods, the development of higher-quality datasets, and modality alignment. Despite the success of the existing works, there lacks a comprehensive study of what causes visual hallucinations in LVLMs.

# 6. Conclusion

In this paper, our study delves into the visual hallucination problem in LVLMs, identifying its sources within the model's components. By independently analyzing the LLM, vision backbone, and projector, we propose targeted mitigation strategies. We introduce fine-grained hallucination benchmarks, QA-VisualGenome and QA-FB15k, to comprehensively evaluate hallucinations. Our methods demonstrate effectiveness in reducing hallucinations, contributing to the reliability and accuracy of LVLMs.

**Limitations:** Our work primarily focuses on analyzing and improving hallucinations of general objects, such as tables and people, while neglecting the research topic of how to mitigate cognition-level hallucinations, such as the names of individuals and famous buildings.
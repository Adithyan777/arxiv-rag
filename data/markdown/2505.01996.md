# Abstract

We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance—albeit suboptimal—when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (e.g., CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying (TG)—a simple yet effective complement to skip connections that further improves the condition of input tokens. We validate our approach in both supervised and self-supervised training methods.

# Introduction

Vision Transformers (ViTs) have demonstrated impressive performance on various computer vision tasks, such as object detection, semantic segmentation, video understanding, visual-language learning, and many others. Much of this success is credited to the self-attention mechanism—or Self-Attention Block (SAB)—which enables ViTs to selectively focus on relevant parts of the input sequence while generating each element of the output sequence. Another critical element is the MLP—or Feedforward Networks (FFN)—which facilitates intra-token communication across channel dimensions. Finally, an identity mapping—commonly called a skip connection—is included in both the SAB and FFN to ensure good performance.

We argue that the Jacobian of the SAB is disproportionately ill-conditioned compared to other components, notably the FFN block. Poor Jacobian condition is fundamentally detrimental to gradient descent training, making convergence and stability challenging. An obvious strategy in this regard is to directly model the Jacobian of the entire network. This is problematic in practice due to the sheer size of modern transformer networks. So we make the following simplifying assumptions:

1. The condition of the network Jacobian is bound by the most ill-conditioned sub-block Jacobian.
2. A proxy for the condition of the sub-block Jacobian is the condition of the output embedding of that block.

Although we do not formally prove these assumptions, we indirectly validate them through our extensive empirical analysis. First, we show that better conditioning of a sub-block’s output embeddings leads to improved empirical performance across several ViT benchmarks. Second, we empirically demonstrate that the Jacobian condition of the sub-block is improved.

## Skip in Transformers

Skip connections have become an indispensable component in modern transformer neural networks and have been empirically proven to be a de facto component. We take one pre-trained regular ViT-Tiny model and measure the condition of the SAB and FFN outputs, both with and without skip connections. We observe that the SAB output embedding without skip connections is highly ill-conditioned, with a condition number around 1e6. In contrast, the other three configurations have relatively low condition numbers, around 1e3. In other words, the output embedding of SAB without skip connections has significantly worse condition than the other configurations.

Consequently, we train the models from scratch with three different configurations: the standard SAB and FFN with skip connections, SAB without skip connections, and FFN without skip connections. For a ViT-Tiny model trained upon the CIFAR-10 dataset, classification accuracy drops modestly when skip connections are removed in the FFN, but retained in the SAB. Surprisingly, however, the model performance becomes near catastrophic—with a 22% drop in accuracy—when skip connections are removed from the SAB while retained in the FFN. Further, as dataset size increases, models without skip connections in the SAB tend to degrade significantly in performance and at a larger scale (e.g., Tiny-ImageNet) are not trainable at all.

Additionally, the training loss of three configurations demonstrates that the SAB without skip connections—which is significantly ill-conditioned—converges much more slowly than the other two configurations and diverges after 30 epochs. Interestingly, this critical dependence on skip connections is unique to transformers.

## Skip in ConvMixer

To further highlight the poor conditioning of the SAB, we conducted a similar experiment on a modern CNN, ConvMixer. ConvMixer has a similar architecture and competitive performance to ViT with the exception that self-attention is replaced by a convolution block to spatially mix tokens. Unlike ViT, training ConvMixer without skip connections still achieves competitive performance. This contrast further suggests that the SAB introduces an extreme conditioning issue, making skip connections indispensable for stable training in ViTs.

**Contributions:**

- We present a proposition that characterizes why SAB output embedding without skip connection is fundamentally ill-conditioned, which challenges training convergence and stability.
- A theoretical analysis on the role of skip connection within the SAB is undertaken. We demonstrate that it significantly improves the condition of the block’s output embedding, enhancing stability and performance.
- Finally, we propose a novel approach—Token Graying (TG)—to better pre-condition the input tokens. This step, when used in conjunction with conventional skip connections, improves ViT in both supervised and self-supervised settings.

Our central contribution in this paper is not TG itself, but the insight that the SAB within modern ViTs is intrinsically ill-conditioned. Our hope is that this insight opens up brand new lines of inquiry for the vision community for more effective and efficient ViT design.

# Related Work

## Self-Attention

Self-attention has become central to transformer architectures because it captures long-range dependencies by dynamically weighting interactions between tokens. Building on this foundation, transformer-based large language models have significantly advanced language understanding. In computer vision, researchers introduced Vision Transformers (ViTs) for image classification, achieving superior results compared to traditional convolutional neural networks. Moreover, researchers have trained Diffusion Transformers, replacing the commonly used U-Net backbone with a transformer that operates on latent patches and inherits the excellent scaling properties of the transformer model class. Similarly, self-attention mechanisms have been successfully applied in speech recognition and physics-informed neural networks, further demonstrating their versatility across diverse applications.

## Skip Connections

Skip connections were introduced in ResNet to make deep networks easier to optimize and to enhance accuracy by enabling significantly increased network depth. Since their introduction, skip connections have been extensively studied and applied across various Convolutional Neural Network (CNN) architectures. Building on these ideas, residual networks can be viewed as a collection of multiple paths of varying lengths, which contributes to the network’s flexibility and resilience. Furthermore, gradient stability in deep ResNets has been addressed, noting that while skip connections alleviate the issue of vanishing gradients, they may also introduce gradient explosion at extreme depths. To counter this, scaling the skip connections according to the layer index has been proposed, thereby stabilizing gradients and ensuring network expressivity even in the limit of infinite depth.

Recently, it was reported that VGG—a model without skip connections—achieves comparable results to ResNet—a model with skip connections—on modern datasets. To the best of our knowledge, only one paper challenges skip connection in transformer architectures, attempting to train deep transformer networks without using skip connections and achieving performance comparable to standard models. However, this approach requires five times more iterations, and the reasons why skip connections are crucial for self-attention-based transformers remain unresolved.

# Preliminary

In this section, we outline the definitions of the self-attention blocks used in the Vision Transformer architecture and establish the mathematical notation for the quantities that will be utilized in the following sections.

A Vision Transformer architecture consists of L stacked self-attention blocks (SABs) and feedforward networks (FFNs). An identity mapping—commonly referred to as a skip connection—is applied to connect inputs and outputs of transformation in both SAB and FFN.

## Self-Attention Blocks in ViT

Given an input sequence $$X \in \mathbb{R}^{n \times d}$$, with n tokens of dimension d, a SAB is defined as

$$
X_{\text{out}}^S = SA(X_{\text{in}}) + X_{\text{in}}
$$

where the self-attention output embedding is

$$
SA(X_{\text{in}}) = QK^T V
$$

with

- $$Q = X_{\text{in}} W_Q$$
- $$K = X_{\text{in}} W_K$$
- $$V = X_{\text{in}} W_V$$

where $$W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$$ are learnable parameters. The activation function is typically softmax, but alternatives exist.

For general vision transformers, multiple heads $$SA(X_{\text{in}}^i)$$, where $$1 \leq i \leq h$$, are commonly used. Learnable matrices are divided into head numbers h, such that $$W_Q, W_K, W_V \in \mathbb{R}^{h \times d \times d_h}$$, where $$d_h = d / h$$. Then all outputs of each attention head are concatenated together before the skip connection.

## Feed Forward Networks in ViT

Given an input sequence $$X \in \mathbb{R}^{n \times d}$$, an FFN is defined as

$$
X_{\text{out}} = W_{\text{down}} g(W_{\text{up}} X) + X
$$

where $$W_{\text{up}} \in \mathbb{R}^{4d \times d}$$ is up projection, $$W_{\text{down}} \in \mathbb{R}^{d \times 4d}$$ is down projection, and $$g$$ is an activation function.

## Conditioning of Matrices

For a rectangular full rank matrix $$A \in \mathbb{R}^{n \times d}$$, the condition number is defined as

$$
\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
$$

where $$\sigma_{\max}$$ and $$\sigma_{\min}$$ are the maximal and minimal singular values of $$A$$, respectively. A lower condition number implies better condition.

# Theoretical Analysis

In this section, we provide a theoretical analysis showing that the output embedding of the SAB without skip connections is highly ill-conditioned. This matters as the condition of the output embedding is a proxy for the condition of the sub-block’s Jacobian. We then demonstrate that an essential function of the skip connection is to improve this conditioning.

## Self-Attention is Ill-Conditioned

We attempt to validate our claim for the simpler case of linear attention.

**Proposition 4.1:**  
Assume $$X \in \mathbb{R}^{n \times d}$$, $$W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$$ have entries that are independently drawn from a distribution with zero mean. The condition number of the SAB output embedding without skip-connection can be expressed as

$$
\kappa(SA(X)) \leq C \cdot \kappa(X)^3
$$

where $$C$$ is a fixed constant close to unity.

This shows the condition number of the output embedding without skip connection is bounded by the cube of the condition number of the input matrix $$X$$. Unless the condition of $$X$$ is unity, the output of the SAB without skip connection will therefore result in a poorly conditioned embedding. Applying this same process across multiple layers within a transformer results in a highly ill-conditioned outcome.

For FFN output embeddings without skip connection using linear activation $$g$$:

$$
\kappa(MLP(X)) \leq C_{\text{down}} C_{\text{up}} \kappa(X)
$$

Compared to the self-attention mechanism, the MLP transformation does not worsen conditioning as severely, since it has a much lower bound on the condition number.

## Skip Connection Improves Condition

We theoretically demonstrate that the skip connection improves the condition of the self-attention output embeddings.

**Proposition 4.2:**  
Let $$X \in \mathbb{R}^{n \times d}$$ and $$W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$$ have entries that are independently drawn from a distribution with zero mean. We define $$M = W_Q W_K^T X^T X W_V$$ and assume $$M$$ is positive semi-definite. We have the following bounds on the condition numbers:

$$
\kappa(X + M) \leq \kappa(X) + \kappa(M)
$$

This shows that during training, the condition number of the SAB output embedding has a much lower bound than that of the SAB output embedding without skip connections. Furthermore, with other conditions being equal, better condition of SAB can contribute to faster convergence and more stable gradient updates, reducing the risk of gradient explosion or vanishing. This ensures the signal can propagate well during the training process in the Vision Transformers.

# Methodology

In this section, we introduce Token Graying (TG), a simple yet effective method that employs Singular Value Decomposition and Discrete Cosine Transform techniques to reconstruct better-conditioned input tokens.

## Singular Value Decomposition (SVD)

Singular Value Decomposition of a matrix $$X \in \mathbb{R}^{n \times d}$$ is the factorization of $$X$$ into the product of three matrices:

$$
X = U \Sigma V^T
$$

where the columns of $$U \in \mathbb{R}^{n \times n}$$ and $$V \in \mathbb{R}^{d \times d}$$ are orthonormal and $$\Sigma \in \mathbb{R}^{n \times d}$$ is a rectangular diagonal matrix whose diagonal elements are non-negative in descending order and represent the singular values.

We can reduce the condition number by increasing the minimal singular value while keeping the maximal singular value unchanged. Hence, our goal is to reconstruct $$X = U \Sigma' V^T$$, where $$\Sigma'$$ is formed by amplifying non-maximal elements of $$\Sigma$$ while keeping the maximal element unchanged.

**Algorithm 1: SVD Token Graying**

- For each minibatch:
    - Convert image $$x_i$$ into token $$X$$
    - Compute SVD: $$X = U \Sigma V^T$$
    - Normalize elements to [1]
    - Amplify $$\Sigma$$
    - SVD reconstruct: $$X' = U \Sigma' V^T$$
    - Patch Embedding: $$Z = \text{PatchEmbed}(X')$$
    - Forward Pass, Compute Loss, Backward Pass, Update Parameters

Training a vision transformer is significantly slower when using SVD reconstruction.

| Methods         | ViT-B | ViT-B SVDTG | ViT-B DCTTG |
|-----------------|-------|-------------|-------------|
| Time (days)     | 0.723 | 4.552       | 0.732       |

## Discrete Cosine Transform (DCT)

A Discrete Cosine Transform (DCT) expresses a finite sequence of data points as a sum of cosine functions oscillating at different frequencies. It is real-valued and has an inverse (IDCT).

We choose DCT-II, the most commonly used form. For a real-valued sequence $$x$$ of length $$N$$:

$$
\text{DCT}(x)_k = \sum_{i=0}^{N-1} x_i \cos\left(\frac{\pi (i + 1/2) k}{N}\right)
$$

For the reconstruction process, the IDCT combines these frequency components to recover the original signal.

**Algorithm 2: DCT Token Graying**

- For each minibatch:
    - Convert image $$x_i$$ into token $$X$$
    - Compute DCT: $$X' = D X D^T$$
    - Normalize elements to [1]
    - Amplify $$X'$$
    - Inverse DCT: $$X'' = D^T X' D$$
    - Patch Embedding: $$Z = \text{PatchEmbed}(X'')$$
    - Forward Pass, Compute Loss, Backward Pass, Update Parameters

DCT avoids the expensive computational cost of SVD and has a much lower complexity.

# Experiments

Vision Transformers (ViTs) have emerged as powerful models in the field of computer vision, demonstrating remarkable performance across a variety of tasks. This section is dedicated to validating and analyzing our proposed algorithm for both supervised and self-supervised learning in ViTs. For all experiments, unless specified otherwise, we use amplification coefficient 0.95.

## Supervised Learning ViT

We validate our methods in supervised learning using different types and scales of ViTs. Cross-entropy loss is used.

- **ViT**: We train the ViT-Tiny (12 layers, 3 heads, head dimension 64, token dimension 192) on the Tiny-ImageNet dataset. We also train ViT-Base (12 layers, 12 heads, head dimension 64, token dimension 768) on the ImageNet-1K dataset.
- **Swin**: We train the Swin small model (patch size 4, window size 7) on the ImageNet-1K dataset.
- **CaiT**: We train CaiT small (24 layers) on the ImageNet-1K dataset.
- **PVT**: We train PVT V2 b3 on the ImageNet-1K dataset.

### Results

| Method                 | Top-1 Acc | Top-5 Acc |
|------------------------|-----------|-----------|
| 1kQK                   | 43.0      | 62.7      |
| 1kQK SVD               | 44.3      | 68.8      |
| 1kQK DCT               | 44.2      | 68.8      |
| softmax QK             | 43.0      | 67.3      |
| softmax QK SVD         | 44.7      | 69.7      |
| softmax QK DCT         | 44.8      | 69.7      |

**Table 2:** ViT-Tiny results on Tiny-ImageNet datasets.

| Method         | Top-1 Acc | in   | out  |
|----------------|-----------|------|------|
| ViT-Base       | 81.0      | 6.72 | 6.74 |
| ViT-Base SVDTG | 81.2      | 6.64 | 6.66 |
| ViT-Base SVDTG | 81.2      | 6.47 | 6.66 |
| ViT-Base SVDTG | 81.4      | 6.15 | 6.17 |
| ViT-Base SVDTG | 81.4      | 5.73 | 5.71 |
| ViT-Base SVDTG | 81.0      | 5.29 | 5.25 |
| ViT-Base DCTTG | 81.1      | 6.47 | 6.49 |
| ViT-Base DCTTG | 81.3      | 6.42 | 6.43 |
| ViT-Base DCTTG | 81.2      | 6.33 | 6.35 |
| ViT-Base DCTTG | 81.1      | 6.25 | 6.25 |
| ViT-Base DCTTG | 81.0      | 6.01 | 6.02 |

**Table 3:** ViT-Base results using SVD and DCT token graying methods with different amplification coefficients. "in" and "out" are the average token condition numbers in log scale before and after each SAB.

| Method             | Top-1 Acc | Top-5 Acc |
|--------------------|-----------|-----------|
| ViT-S              | 80.2      | 95.1      |
| ViT-S DCTTG        | 80.4      | 95.2      |
| ViT-B              | 81.0      | 95.3      |
| ViT-B DCTTG        | 81.3      | 95.4      |
| Swin-S             | 81.3      | 95.6      |
| Swin-S DCTTG       | 81.6      | 95.6      |
| CaiT-S             | 82.6      | 96.1      |
| CaiT-S DCTTG       | 82.7      | 96.3      |
| PVT V2 b3          | 82.9      | 96.0      |
| PVT V2 b3 DCTTG    | 83.0      | 96.1      |

**Table 4:** Top-1 and Top-5 classification accuracy on ImageNet-1K dataset using DCT token graying on different ViTs.

## Self-Supervised Learning ViT

We validate our methods on self-supervised learning models as well as on fine-tuning pretrained models. Masked Image Modeling (MIM) learns representations by reconstructing images that have been corrupted through masking.

### Pre-training

In the pre-training stage, we pre-train our model on the ImageNet-1k training set without using ground-truth labels in a self-supervised manner. We use a batch size of 2048, a base learning rate of 1.5e-4, and a weight decay of 0.05, and we train for 400 epochs.

### Finetuning and Results

In the finetuning stage, we finetune the pre-trained models on ImageNet-1k for classification tasks using cross-entropy loss. We use a batch size of 512, a base learning rate of 5e-4, a weight decay of 0.05, and train for 100 epochs.

| Method      | Top-1 Acc | Top-5 Acc |
|-------------|-----------|-----------|
| MAE         | 83.0      | 96.4      |
| MAE DCTTG   | 83.2      | 96.6      |

**Table 5:** MAE pretrained ViT-Base results finetuned on ImageNet dataset-1k.

# Limitation

Our work establishes a theoretical foundation demonstrating why skip connections act as essential conditioning regularizers within self-attention blocks. Additionally, we introduce a method designed to further enhance the output embeddings of self-attention blocks. However, certain limitations remain. For instance, while our regularizer is effective, training in low-precision settings may pose challenges due to the DCT operation, which involves many multiplications and summations that are sensitive to quantization errors. Additionally, for some variants of ViTs, the performance improvement might be marginal.

# Conclusion

This paper provides a theoretical demonstration that the output embeddings of linear self-attention blocks—when lacking skip connections—are inherently ill-conditioned, a finding that empirically extends to conventional softmax self-attention blocks as well. Furthermore, we show that skip connections serve as a powerful regularizer by significantly improving the conditioning of self-attention blocks’ output embeddings. In addition, we use the condition of the self-attention output embeddings as a proxy for that of its Jacobian, empirically demonstrating that input tokens with improved conditioning lead to a better-conditioned Jacobian. Building on this insight, we introduce SVDTG and DCTTG, simple yet effective methods designed to improve the condition of input tokens. We then validate their effectiveness on both supervised and unsupervised learning tasks. Ultimately, we hope these insights will guide the vision community in designing more effective and efficient ViT architectures for downstream applications.

# Supplementary Material

## A. Theoretical Framework

We provide the proofs for our main propositions about the conditioning of self-attention blocks with and without skip connections.

## B. ConvMixer

We demonstrate how removing skip connections in Convolutional Neural Networks (CNNs) impacts performance. ConvMixer, an extremely simple model inspired by Vision Transformers, consists of ConvMixer blocks. For the ConvDepthwise transformation, we analyze the condition number in the linear case and in the absence of batch normalization. Compared to the Self-Attention Mechanism, the ConvDepthwise transformation demonstrates better conditioning with a lower bound on condition numbers. Performance of the ConvMixer Tiny model does not degrade when skip connections are removed, contrasting with ViTs.

## C. Empirical Neural Tangent Kernel

The Neural Tangent Kernel (NTK) describes the evolution of deep neural networks during training by gradient descent. We measure the condition of the self-attention output embeddings as a proxy for its Jacobian condition, since analyzing the transformer model NTK is computationally expensive. Empirically, the self-attention exhibits a disproportionately ill-conditioned spectrum, and using our TG methods, we observe an improvement in this regard.

**Figure 1:** We demonstrate a curious phenomenon: removing skip connections from the Self-Attention Block (SAB) versus the Feedforward Network (FFN) in Vision Transformer (ViT) models results in different performance drop.  
**Figure 2:** Top-1 Accuracy using the ConvMixer-Tiny model on three different datasets with and without skip connections.  
**Figure 3:** Training loss for three different configurations of ViT-Tiny models trained on the Tiny-ImageNet dataset.  
**Figure 4:** Condition numbers across all layers of trained ViT-Tiny on Tiny-ImageNet datasets.  
**Figure 5:** We compute the condition number of the SAB blocks output embeddings during ViT-Tiny training using SVDTG and DCTTG with different amplification coefficients.  
**Figure 6:** Condition numbers of all layers in the trained ViT-Base model with and without the DCTTG method.  
**Figure 7:** Spectrum of Jacobian of one SAB using SVDTG. Lower represents better input token condition.  
**Figure 8:** Spectrum of Jacobian of one SAB using DCTTG. Lower represents better input token condition.
id,title,categories,abstract,doi,created,updated,authors
2505.00295,fine-grained spatial-temporal perception for gas leak segmentation,cs.cv cs.ai,"gas leaks pose significant risks to human health and the environment. despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. in this paper, we propose a fine-grained spatial-temporal perception (fgstp) algorithm for gas leak segmentation. fgstp captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. specifically, we first construct a correlation volume to capture motion information between consecutive frames. then, the fine-grained perception progressively refines the object-level features using previous outputs. finally, a decoder is employed to optimize boundary segmentation. because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, gasvid. experimental results on gasvid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (sota) models.",,2025-05-01,,"['xinlong zhao', 'shan du']"
2505.00312,aware-net: adaptive weighted averaging for robust ensemble network in   deepfake detection,cs.cv,"deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. while multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. in response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: xception, res2net101, and efficientnet-b7. our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. our experiments achieved state-of-the-art intra-dataset performance with auc scores of 99.22% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.06% (ff++) and 99.94% (celebdf-v2) without augmentation. with augmentation, we achieve auc scores of 99.47% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.43% (ff++) and 99.95% (celebdf-v2). the framework demonstrates robust cross-dataset generalization, achieving auc scores of 88.20% and 72.52%, and f1 scores of 93.16% and 80.62% in cross-dataset evaluations.",10.1049/icp.2025.1162,2025-05-01,,"['muhammad salman', 'iqra tariq', 'mishal zulfiqar', 'muqadas jalal', 'sami aujla', 'sumbal fatima']"
2505.00334,quaternion wavelet-conditioned diffusion models for image   super-resolution,cs.cv cs.lg,"image super-resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. the ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. while deep learning has significantly advanced sr, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. in this work, we introduce resqu a novel sr framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. furthermore, we also leverage the generative priors of foundation models such as stable diffusion. extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding sr results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. the code will be available after the revision process.",,2025-05-01,2025-05-05,"['luigi sigillo', 'christian bianchi', 'aurelio uncini', 'danilo comminiello']"
2505.00335,efficient neural video representation with temporally coherent   modulation,cs.cv cs.ai,"implicit neural representations (inr) has found successful applications across diverse domains. to employ inr in real-life, it is important to speed up training. in the field of inr for video applications, the state-of-the-art approach employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors. however, the grid usage, which does not consider the video's dynamic nature, leads to redundant use of trainable parameters. as a result, it has significantly lower parameter efficiency and higher bitrate compared to nerv-style methods that do not use a parametric encoding. to address the problem, we propose neural video representation with temporally coherent modulation (nvtm), a novel framework that can capture dynamic characteristics of video. by decomposing the spatio-temporal 3d video data into a set of 2d grids with flow information, nvtm enables learning video representation rapidly and uses parameter efficiently. our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the nerv-style method, with a speed increase of over 3 times. also, it remarks an average of 1.54db/0.019 improvements in psnr/lpips on uvg (dynamic) (even with 10% fewer parameters) and an average of 1.84db/0.013 improvements in psnr/lpips on mcl-jcv (dynamic), compared to previous grid-type works. by expanding this to compression tasks, we demonstrate comparable performance to video compression standards (h.264, hevc) and recent inr approaches for video compression. additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. project page is https://sujiikim.github.io/nvtm/.",,2025-05-01,,"['seungjun shin', 'suji kim', 'dokwan oh']"
2505.00337,t2vphysbench: a first-principles benchmark for physical consistency in   text-to-video generation,cs.lg cs.ai cs.cl cs.cv,"text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. to fill this gap, we introduce \textbf{t2vphysbench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including newtonian mechanics, conservation principles, and phenomenological effects. our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. the results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.",,2025-05-01,,"['xuyang guo', 'jiayan huo', 'zhenmei shi', 'zhao song', 'jiahao zhang', 'jiale zhao']"
2505.00369,automated segmenta-on of pediatric neuroblastoma on multi-modal mri:   results of the sppin challenge at miccai 2023,cs.cv,"surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. this requires careful planning, often via magnetic resonance imaging (mri)-based anatomical 3d models. however, creating these models is often time-consuming and user dependent. we organized the surgical planning in pediatric neuroblastoma (sppin) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model mri. the challenge started with a training phase, where teams received 78 sets of mri scans from 34 patients, consisting of both diagnostic and post-chemotherapy mri scans. the final test phase, consisting of 18 mri sets from 9 patients, determined the ranking of the teams. ranking was based on the dice similarity coefficient (dice score), the 95th percentile of the hausdorff distance (hd95) and the volumetric similarity (vs). the sppin challenge was hosted at miccai 2023. the final leaderboard consisted of 9 teams. the highest-ranking team achieved a median dice score 0.82, a median hd95 of 7.69 mm and a vs of 0.91, utilizing a large, pretrained network called stu-net. a significant difference for the segmentation results between diagnostic and post-chemotherapy mri scans was observed (dice = 0.89 vs dice = 0.59, p = 0.01) for the highest-ranking team. sppin is the first medical segmentation challenge in extracranial pediatric oncology. the highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.",,2025-05-01,,"['m. a. d. buser', 'd. c. simons', 'm. fitski', 'm. h. w. a. wijnen', 'a. s. littooij', 'a. h. ter brugge', 'i. n. vos', 'm. h. a. janse', 'm. de boer', 'r. ter maat', 'j. sato', 's. kido', 's. kondo', 's. kasai', 'm. wodzinski', 'h. muller', 'j. ye', 'j. he', 'y. kirchhoff', 'm. r. rokkus', 'g. haokai', 's. zitong', 'm. fernández-patón', 'd. veiga-canuto', 'd. g. ellis', 'm. r. aizenberg', 'b. h. m. van der velden', 'h. kuijf', 'a. de luca', 'a. f. w. van der steeg']"
2505.00374,towards lightweight hyperspectral image super-resolution with depthwise   separable dilated convolutional network,eess.iv cs.cv,"deep neural networks have demonstrated highly competitive performance in super-resolution (sr) for natural images by learning mappings from low-resolution (lr) to high-resolution (hr) images. however, hyperspectral super-resolution remains an ill-posed problem due to the high spectral dimensionality of the data and the scarcity of available training samples. moreover, existing methods often rely on large models with a high number of parameters or require the fusion with panchromatic or rgb images, both of which are often impractical in real-world scenarios. inspired by the mobilenet architecture, we introduce a lightweight depthwise separable dilated convolutional network (dsdcn) to address the aforementioned challenges. specifically, our model leverages multiple depthwise separable convolutions, similar to the mobilenet architecture, and further incorporates a dilated convolution fusion block to make the model more flexible for the extraction of both spatial and spectral features. in addition, we propose a custom loss function that combines mean squared error (mse), an l2 norm regularization-based constraint, and a spectral angle-based loss, ensuring the preservation of both spectral and spatial details. the proposed model achieves very competitive performance on two publicly available hyperspectral datasets, making it well-suited for hyperspectral image super-resolution tasks. the source codes are publicly available at: \href{https://github.com/usman1021/lightweight}{https://github.com/usman1021/lightweight}.",,2025-05-01,,"['usman muhammad', 'jorma laaksonen', 'lyudmila mihaylova']"
2505.00378,cues3d: unleashing the power of sole nerf for consistent and unique   instances in open-vocabulary 3d panoptic segmentation,cs.cv,"open-vocabulary 3d panoptic segmentation has recently emerged as a significant trend. top-performing methods currently integrate 2d segmentation with geometry-aware 3d primitives. however, the advantage would be lost without high-fidelity 3d point clouds, such as methods based on neural radiance field (nerf). these methods are limited by the insufficient capacity to maintain consistency across partial observations. to address this, recent works have utilized contrastive loss or cross-view association pre-processing for view consensus. in contrast to them, we present cues3d, a compact approach that relies solely on nerf instead of pre-associations. the core idea is that nerf's implicit 3d field inherently establishes a globally consistent geometry, enabling effective object distinction without explicit cross-view supervision. we propose a three-phase training framework for nerf, initialization-disambiguation-refinement, whereby the instance ids are corrected using the initially-learned knowledge. additionally, an instance disambiguation method is proposed to match nerf-rendered 3d masks and ensure globally unique 3d instance identities. with the aid of cues3d, we obtain highly consistent and unique 3d instance id for each object across views with a balanced version of nerf. our experiments are conducted on scannet v2, scannet200, scannet++, and replica datasets for 3d instance, panoptic, and semantic segmentation tasks. cues3d outperforms other 2d image-based methods and competes with the latest 2d-3d merging based methods, while even surpassing them when using additional 3d point clouds. the code link could be found in the appendix and will be released on \href{https://github.com/mrobotit/cues3d}{github}",,2025-05-01,,"['feng xue', 'wenzhuang xu', 'guofeng zhong', 'anlong minga', 'nicu sebe']"
2505.0038,the invisible threat: evaluating the vulnerability of cross-spectral   face recognition to presentation attacks,cs.cv,"cross-spectral face recognition systems are designed to enhance the performance of facial recognition systems by enabling cross-modal matching under challenging operational conditions. a particularly relevant application is the matching of near-infrared (nir) images to visible-spectrum (vis) images, enabling the verification of individuals by comparing nir facial captures acquired with vis reference images. the use of nir imaging offers several advantages, including greater robustness to illumination variations, better visibility through glasses and glare, and greater resistance to presentation attacks. despite these claimed benefits, the robustness of nir-based systems against presentation attacks has not been systematically studied in the literature. in this work, we conduct a comprehensive evaluation into the vulnerability of nir-vis cross-spectral face recognition systems to presentation attacks. our empirical findings indicate that, although these systems exhibit a certain degree of reliability, they remain vulnerable to specific attacks, emphasizing the need for further research in this area.",,2025-05-01,,"['anjith george', 'sebastien marcel']"
2505.00394,sota: spike-navigated optimal transport saliency region detection in   composite-bias videos,cs.cv,"existing saliency detection methods struggle in real-world scenarios due to motion blur and occlusions. in contrast, spike cameras, with their high temporal resolution, significantly enhance visual saliency maps. however, the composite noise inherent to spike camera imaging introduces discontinuities in saliency detection. low-quality samples further distort model predictions, leading to saliency bias. to address these challenges, we propose spike-navigated optimal transport saliency region detection (sota), a framework that leverages the strengths of spike cameras while mitigating biases in both spatial and temporal dimensions. our method introduces spike-based micro-debias (sm) to capture subtle frame-to-frame variations and preserve critical details, even under minimal scene or lighting changes. additionally, spike-based global-debias (sg) refines predictions by reducing inconsistencies across diverse conditions. extensive experiments on real and synthetic datasets demonstrate that sota outperforms existing methods by eliminating composite noise bias. our code and dataset will be released at https://github.com/lwxfight/sota.",,2025-05-01,,"['wenxuan liu', 'yao deng', 'kang chen', 'xian zhong', 'zhaofei yu', 'tiejun huang']"
2505.00421,real-time animatable 2dgs-avatars with detail enhancement from monocular   videos,cs.cv,"high-quality, animatable 3d human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. however, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. to address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2d gaussian splatting (2dgs). by leveraging 2dgs and global smpl pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. furthermore, we introduce a rotation compensation network (rcn) that learns rotation residuals by integrating local geometric features with global pose parameters. this network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.",,2025-05-01,,"['xia yuan', 'hai yuan', 'wenyi ge', 'ying fu', 'xi wu', 'guanyu xing']"
2505.00426,leveraging pretrained diffusion models for zero-shot part assembly,cs.cv,"3d part assembly aims to understand part relationships and predict their 6-dof poses to construct realistic 3d shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. however, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. in this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an iterative closest point (icp) process. then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. to verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. the code has been released on https://github.com/ruiyuan-zhang/zero-shot-assembly.",,2025-05-01,,"['ruiyuan zhang', 'qi wang', 'jiaxiang liu', 'yu zhang', 'yuchi huo', 'chao wu']"
2505.00452,clearlines - camera calibration from straight lines,cs.cv,"the problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. however, its practical applicability remains limited, particularly in real-world outdoor scenarios. these environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3d lines, and varying lighting conditions, making the task notoriously difficult. furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. in this study, we present a small dataset named ""clearlines"", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3d line detection algorithms.",,2025-05-01,,"['gregory schroeder', 'mohamed sabry', 'cristina olaverri-monreal']"
2505.00462,"corstitch - a free, open source software for stitching and   georeferencing underwater coral reef videos",eess.iv cs.cv,"corstitch is an open-source software developed to automate the creation of accurate georeferenced reef mosaics from video transects obtained through automated rapid reef assessment system surveys. we utilized a fourier-based image correlation algorithm to stitch sequential video frames, aligning them with synchronized gnss timestamps. the resulting compressed keyhole markup language files, compatible with geographic information systems such as google earth, enable detailed spatial analysis. validation through comparative analysis of mosaics from two temporally distinct surveys of the same reef demonstrated the software's consistent and reliable performance.",,2025-05-01,,"['julian christopher l. maypa', 'johnenn r. manalang', 'maricor n. soriano']"
2505.00482,jointdit: enhancing rgb-depth joint modeling with diffusion transformers,cs.cv cs.ai,"we present jointdit, a diffusion transformer that models the joint distribution of rgb and depth. by leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, jointdit not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. this solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. with these techniques, we train our model across all noise levels for each modality, enabling jointdit to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. jointdit demonstrates outstanding joint generation performance. furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. the project page is available at https://byungki-k.github.io/jointdit/.",,2025-05-01,,"['kwon byung-ki', 'qi dai', 'lee hyoseok', 'chong luo', 'tae-hyun oh']"
2505.00497,keysync: a robust approach for leakage-free lip synchronization in high   resolution,cs.cv,"lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. however, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. to address these shortcomings, we present keysync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. we show that keysync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to lipleak, our novel leakage metric. furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. code and model weights can be found at https://antonibigata.github.io/keysync.",,2025-05-01,,"['antoni bigata', 'rodrigo mira', 'stella bounareli', 'michał stypułkowski', 'konstantinos vougioukas', 'stavros petridis', 'maja pantic']"
2505.00502,towards scalable human-aligned benchmark for text-guided image editing,cs.cv,"a variety of text-guided image editing models have been proposed recently. however, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. to address this, we introduce a novel human-aligned benchmark for text-guided image editing (hatie). providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. also, hatie provides a fully-automated and omnidirectional evaluation pipeline. particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. we empirically verify that the evaluation of hatie is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.",,2025-05-01,,"['suho ryu', 'kihyun kim', 'eugene baek', 'dongsoo shin', 'joonseok lee']"
2505.00507,heal3d: heuristical-enhanced active learning for 3d object detection,cs.cv,"active learning has proved to be a relevant approach to perform sample selection for training models for autonomous driving. particularly, previous works on active learning for 3d object detection have shown that selection of samples in uncontrolled scenarios is challenging. furthermore, current approaches focus exclusively on the theoretical aspects of the sample selection problem but neglect the practical insights that can be obtained from the extensive literature and application of 3d detection models. in this paper, we introduce heal (heuristical-enhanced active learning for 3d object detection) which integrates those heuristical features together with localization and classification to deliver the most contributing samples to the model's training. in contrast to previous works, our approach integrates heuristical features such as object distance and point-quantity to estimate the uncertainty, which enhance the usefulness of selected samples to train detection models. our quantitative evaluation on kitti shows that heal presents competitive map with respect to the state-of-the-art, and achieves the same map as the full-supervised baseline with only 24% of the samples.",,2025-05-01,2025-05-05,"['esteban rivera', 'surya prabhakaran', 'markus lienkamp']"
2505.00511,inconsistency-based active learning for lidar object detection,cs.cv,"deep learning models for object detection in autonomous driving have recently achieved impressive performance gains and are already being deployed in vehicles worldwide. however, current models require increasingly large datasets for training. acquiring and labeling such data is costly, necessitating the development of new strategies to optimize this process. active learning is a promising approach that has been extensively researched in the image domain. in our work, we extend this concept to the lidar domain by developing several inconsistency-based sample selection strategies and evaluate their effectiveness in various settings. our results show that using a naive inconsistency approach based on the number of detected boxes, we achieve the same map as the random sampling strategy with 50% of the labeled data.",,2025-05-01,,"['esteban rivera', 'loic stratil', 'markus lienkamp']"
2505.00512,interloc: lidar-based intersection localization using road segmentation   with automated evaluation method,cs.cv cs.ro,"online localization of road intersections is beneficial for autonomous vehicle localization, mapping and motion planning. intersections offer strong landmarks to correct vehicle pose estimation in gnss dropouts and anchor new sensor data in up-to-date maps. they are also decisive routing nodes in road network graphs. despite that importance, intersection localization has not been widely studied, with existing methods either ignore the rich semantic information already computed onboard or depend on scarce, hand-labeled intersection datasets. to close that gap, this paper presents a lidar-based method for online vehicle-centric intersection localization. we fuse semantic road segmentation with vehicle local pose to detect intersection candidates in a bird's eye view (bev) representation. we then refine those candidates by analyzing branch topology and correcting the intersection point in a least squares formulation. to evaluate our method, we introduce an automated benchmarking pipeline that pairs localized intersection points with openstreetmap (osm) intersection nodes using precise gnss/ins ground-truth poses. experiments on semantickitti show that the method outperforms the latest learning-based baseline in accuracy and reliability. moreover, sensitivity tests demonstrate that our method is robust to challenging segmentation error levels, highlighting its applicability in the real world.",,2025-05-01,2025-05-02,"['nguyen hoang khoi tran', 'julie stephany berrio', 'mao shan', 'zhenxing ming', 'stewart worrall']"
2505.00525,a methodological and structural review of parkinsons disease detection   across diverse data modalities,eess.iv cs.cv cs.lg,"parkinsons disease (pd) is a progressive neurological disorder that primarily affects motor functions and can lead to mild cognitive impairment (mci) and dementia in its advanced stages. with approximately 10 million people diagnosed globally 1 to 1.8 per 1,000 individuals, according to reports by the japan times and the parkinson foundation early and accurate diagnosis of pd is crucial for improving patient outcomes. while numerous studies have utilized machine learning (ml) and deep learning (dl) techniques for pd recognition, existing surveys are limited in scope, often focusing on single data modalities and failing to capture the potential of multimodal approaches. to address these gaps, this study presents a comprehensive review of pd recognition systems across diverse data modalities, including magnetic resonance imaging (mri), gait-based pose analysis, gait sensory data, handwriting analysis, speech test data, electroencephalography (eeg), and multimodal fusion techniques. based on over 347 articles from leading scientific databases, this review examines key aspects such as data collection methods, settings, feature representations, and system performance, with a focus on recognition accuracy and robustness. this survey aims to serve as a comprehensive resource for researchers, providing actionable guidance for the development of next generation pd recognition systems. by leveraging diverse data modalities and cutting-edge machine learning paradigms, this work contributes to advancing the state of pd diagnostics and improving patient care through innovative, multimodal approaches.",,2025-05-01,,"['abu saleh musa miah', 'taro suzuki', 'jungpil shin']"
2505.00534,a robust deep networks based multi-object multicamera tracking system   for city scale traffic,cs.cv,"vision sensors are becoming more important in intelligent transportation systems (its) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. however, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. these challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. to address these issues, we propose an efficient and cost-effective deep learning-based framework for multi-object multi-camera tracking (mo-mct). the proposed framework utilizes mask r-cnn for object detection and employs non-maximum suppression (nms) to select target objects from overlapping detections. transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. the final solution identification module performs feature extraction using resnet-152 coupled with deep sort based vehicle tracking. the proposed framework is evaluated on the 5th ai city challenge dataset (track 3), comprising 46 camera feeds. among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. the proposed framework achieves competitive performance with an idf1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.",10.1007/s11042-023-16243-7,2025-05-01,,"['muhammad imran zaman', 'usama ijaz bajwa', 'gulshan saleem', 'rana hammad raza']"
2505.00564,x-ray illicit object detection using hybrid cnn-transformer neural   network architectures,cs.cv,"in the field of x-ray security applications, even the smallest details can significantly impact outcomes. objects that are heavily occluded or intentionally concealed pose a great challenge for detection, whether by human observation or through advanced technological applications. while certain deep learning (dl) architectures demonstrate strong performance in processing local information, such as convolutional neural networks (cnns), others excel in handling distant information, e.g., transformers. in x-ray security imaging the literature has been dominated by the use of cnn-based methods, while the integration of the two aforementioned leading architectures has not been sufficiently explored. in this paper, various hybrid cnn-transformer architectures are evaluated against a common cnn object detection baseline, namely yolov8. in particular, a cnn (hgnetv2) and a hybrid cnn-transformer (next-vit-s) backbone are combined with different cnn/transformer detection heads (yolov8 and rt-detr). the resulting architectures are comparatively evaluated on three challenging public x-ray inspection datasets, namely eds, hixray, and pidray. interestingly, while the yolov8 detector with its default backbone (csp-darknet53) is generally shown to be advantageous on the hixray and pidray datasets, when a domain distribution shift is incorporated in the x-ray images (as happens in the eds datasets), hybrid cnn-transformer architectures exhibit increased robustness. detailed comparative evaluation results, including object-level detection performance and object-size error analysis, demonstrate the strengths and weaknesses of each architectural combination and suggest guidelines for future research. the source code and network weights of the models employed in this study are available at https://github.com/jgenc/xray-comparative-evaluation.",,2025-05-01,,"['jorgen cani', 'christos diou', 'spyridon evangelatos', 'panagiotis radoglou-grammatikis', 'vasileios argyriou', 'panagiotis sarigiannidis', 'iraklis varlamis', 'georgios th. papadopoulos']"
2505.00568,multimodal masked autoencoder pre-training for 3d mri-based brain tumor   analysis with missing modalities,cs.cv cs.ai,"multimodal magnetic resonance imaging (mri) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. this behavior is especially valuable in medical imaging, where annotations are often scarce. however, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. in practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. therefore, we introduce bm-mae, a masked image modeling pre-training strategy tailored for multimodal mri data. the same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. this allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. code and trained models are available at: https://github.com/lucas-rbnt/bm-mae",,2025-05-01,2025-05-02,"['lucas robinet', 'ahmad berjaoui', 'elizabeth cohen-jonathan moyal']"
2505.00584,synthesizing and identifying noise levels in autonomous vehicle camera   radar datasets,cs.cv cs.ai eess.iv eess.sp,"detecting and tracking objects is a crucial component of any autonomous navigation method. for the past decades, object detection has yielded promising results using neural networks on various datasets. while many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. in this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar autonomous vehicle (av) datasets. our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. we also present our results of a baseline lightweight noise recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.",,2025-05-01,,"['mathis morales', 'golnaz habibi']"
2505.00592,uncertainty-aware multi-expert knowledge distillation for imbalanced   disease grading,cs.cv cs.lg,"automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. however, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. to address the problem, we propose a novel \textbf{u}ncertainty-aware \textbf{m}ulti-experts \textbf{k}nowledge \textbf{d}istillation (umkd) framework to transfer knowledge from multiple expert models to a single student model. specifically, to extract discriminative features, umkd decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. at the output space, an uncertainty-aware decoupled distillation (udd) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. additionally, umkd also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous kd approaches. extensive experiments on histology prostate grading (\textit{sicapv2}) and fundus image grading (\textit{aptos}) demonstrate that umkd achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.",,2025-05-01,,"['shuo tong', 'shangde gao', 'ke liu', 'zihang huang', 'hongxia xu', 'haochao ying', 'jian wu']"
2505.00599,visual trajectory prediction of vessels for inland navigation,cs.cv,"the future of inland navigation increasingly relies on autonomous systems and remote operations, emphasizing the need for accurate vessel trajectory prediction. this study addresses the challenges of video-based vessel tracking and prediction by integrating advanced object detection methods, kalman filters, and spline-based interpolation. however, existing detection systems often misclassify objects in inland waterways due to complex surroundings. a comparative evaluation of tracking algorithms, including bot-sort, deep oc-sort, and byetrack, highlights the robustness of the kalman filter in providing smoothed trajectories. experimental results from diverse scenarios demonstrate improved accuracy in predicting vessel movements, which is essential for collision avoidance and situational awareness. the findings underline the necessity of customized datasets and models for inland navigation. future work will expand the datasets and incorporate vessel classification to refine predictions, supporting both autonomous systems and human operators in complex environments.",,2025-05-01,,"['alexander puzicha', 'konstantin wüstefeld', 'kathrin wilms', 'frank weichert']"
2505.00606,dietary intake estimation via continuous 3d reconstruction of food,cs.cv cs.lg,"monitoring dietary habits is crucial for preventing health risks associated with overeating and undereating, including obesity, diabetes, and cardiovascular diseases. traditional methods for tracking food intake rely on self-reported data before or after the eating, which are prone to inaccuracies. this study proposes an approach to accurately monitor ingest behaviours by leveraging 3d food models constructed from monocular 2d video. using colmap and pose estimation algorithms, we generate detailed 3d representations of food, allowing us to observe changes in food volume as it is consumed. experiments with toy models and real food items demonstrate the approach's potential. meanwhile, we have proposed a new methodology for automated state recognition challenges to accurately detect state changes and maintain model fidelity. the 3d reconstruction approach shows promise in capturing comprehensive dietary behaviour insights, ultimately contributing to the development of automated and accurate dietary monitoring tools.",,2025-05-01,,"['wallace lee', 'yuhao chen']"
2505.00615,pixel3dmm: versatile screen-space priors for single-image 3d face   reconstruction,cs.cv cs.ai,"we address the 3d reconstruction of human faces from a single rgb image. to this end, we propose pixel3dmm, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3d morphable face model (3dmm). we exploit the latent features of the dino foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. we train our model by registering three high-quality 3d face datasets against the flame mesh topology, which results in a total of over 1,000 identities and 976k images. for 3d face reconstruction, we propose a flame fitting opitmization that solves for the 3dmm parameters from the uv-coordinate and normal estimates. to evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.",,2025-05-01,,"['simon giebenhain', 'tobias kirschstein', 'martin rünz', 'lourdes agapito', 'matthias nießner']"
2505.00619,diverse semantics-guided feature alignment and decoupling for   visible-infrared person re-identification,cs.cv,"visible-infrared person re-identification (vi-reid) is a challenging task due to the large modality discrepancy between visible and infrared images, which complicates the alignment of their features into a suitable common space. moreover, style noise, such as illumination and color contrast, reduces the identity discriminability and modality invariance of features. to address these challenges, we propose a novel diverse semantics-guided feature alignment and decoupling (dsfad) network to align identity-relevant features from different modalities into a textual embedding space and disentangle identity-irrelevant features within each modality. specifically, we develop a diverse semantics-guided feature alignment (dsfa) module, which generates pedestrian descriptions with diverse sentence structures to guide the cross-modality alignment of visual features. furthermore, to filter out style information, we propose a semantic margin-guided feature decoupling (smfd) module, which decomposes visual features into pedestrian-related and style-related components, and then constrains the similarity between the former and the textual embeddings to be at least a margin higher than that between the latter and the textual embeddings. additionally, to prevent the loss of pedestrian semantics during feature decoupling, we design a semantic consistency-guided feature restitution (scfr) module, which further excavates useful information for identification from the style-related features and restores it back into the pedestrian-related features, and then constrains the similarity between the features after restitution and the textual embeddings to be consistent with that between the features before decoupling and the textual embeddings. extensive experiments on three vi-reid datasets demonstrate the superiority of our dsfad.",,2025-05-01,,"['neng dong', 'shuanglin yan', 'liyan zhang', 'jinhui tang']"
2505.00627,brain foundation models with hypergraph dynamic adapter for brain   disease analysis,cs.cv,"brain diseases, such as alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. however, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. in this work, we propose sam-brain3d, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 mri sub-modalities, and hypergraph dynamic adapter (hyda), a lightweight adapter for efficient and effective downstream adaptation. sam-brain3d captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. hyda leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.",,2025-05-01,,"['zhongying deng', 'haoyu wang', 'ziyan huang', 'lipei zhang', 'angelica i. aviles-rivero', 'chaoyu liu', 'junjun he', 'zoe kourtzi', 'carola-bibiane schönlieb']"
2505.0063,"vision mamba in remote sensing: a comprehensive survey of techniques,   applications and outlook",cs.cv,"deep learning has profoundly transformed remote sensing, yet prevailing architectures like convolutional neural networks (cnns) and vision transformers (vits) remain constrained by critical trade-offs: cnns suffer from limited receptive fields, while vits grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. state space models (ssms), particularly the recently proposed mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. this survey presents a comprehensive review of mamba-based methodologies in remote sensing, systematically analyzing about 120 mamba-based remote sensing studies to construct a holistic taxonomy of innovations and applications. our contributions are structured across five dimensions: (i) foundational principles of vision mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid ssm formulations, (iii) macro-architectural integrations, including cnn-transformer-mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. by bridging the gap between ssm theory and remote sensing practice, this survey establishes mamba as a transformative framework for remote sensing analysis. to our knowledge, this paper is the first systematic review of mamba architectures in remote sensing. our work provides a structured foundation for advancing research in remote sensing systems through ssm-based methods. we curate an open-source repository (https://github.com/baobao0926/awesome-mamba-in-remote-sensing) to foster community-driven advancements.",,2025-05-01,2025-05-03,"['muyi bao', 'shuchang lyu', 'zhaoyang xu', 'huiyu zhou', 'jinchang ren', 'shiming xiang', 'xiangtai li', 'guangliang cheng']"
2505.00643,deep learning assisted outer volume removal for highly-accelerated   real-time dynamic mri,eess.iv cs.ai cs.cv physics.med-ph,"real-time (rt) dynamic mri plays a vital role in capturing rapid physiological processes, offering unique insights into organ motion and function. among these applications, rt cine mri is particularly important for functional assessment of the heart with high temporal resolution. rt imaging enables free-breathing, ungated imaging of cardiac motion, making it a crucial alternative for patients who cannot tolerate conventional breath-hold, ecg-gated acquisitions. however, achieving high acceleration rates in rt cine mri is challenging due to aliasing artifacts from extra-cardiac tissues, particularly at high undersampling factors. in this study, we propose a novel outer volume removal (ovr) method to address this challenge by eliminating aliasing contributions from non-cardiac regions in a post-processing framework. our approach estimates the outer volume signal for each timeframe using composite temporal images from time-interleaved undersampling patterns, which inherently contain pseudo-periodic ghosting artifacts. a deep learning (dl) model is trained to identify and remove these artifacts, producing a clean outer volume estimate that is subsequently subtracted from the corresponding k-space data. the final reconstruction is performed with a physics-driven dl (pd-dl) method trained using an ovr-specific loss function to restore high spatio-temporal resolution images. experimental results show that the proposed method at high accelerations achieves image quality that is visually comparable to clinical baseline images, while outperforming conventional reconstruction techniques, both qualitatively and quantitatively. the proposed approach provides a practical and effective solution for artifact reduction in rt cine mri without requiring acquisition modifications, offering a pathway to higher acceleration rates while preserving diagnostic quality.",,2025-05-01,,"['merve gülle', 'sebastian weingärtner', 'mehmet akçakaya']"
2505.00668,deep reinforcement learning for urban air quality management:   multi-objective optimization of pollution mitigation booth placement in   metropolitan environments,cs.cv cs.ai cs.lg,"urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like delhi, where exposure to harmful pollutants severely impacts public health. delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. this study presents a novel deep reinforcement learning (drl) framework to optimize the placement of air purification booths to improve the air quality index (aqi) in the city of delhi. we employ proximal policy optimization (ppo), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. our approach is benchmarked against conventional placement strategies, including random and greedy aqi-based methods, using multi-dimensional performance evaluation metrics such as aqi improvement, spatial coverage, population and traffic impact, and spatial entropy. experimental results demonstrate that the rl-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. notably, the drl framework achieves an optimal trade-off between aqi reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. the findings underscore the potential of ai-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.",,2025-05-01,,"['kirtan rajesh', 'suvidha rupesh kumar']"
2505.00681,minerva: evaluating complex video reasoning,cs.lg cs.cv,"multimodal llms are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. this makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. to remedy this, we provide a new video reasoning dataset called minerva for modern multimodal models. each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. we perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. we use this to explore both human and llm-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. the dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.",,2025-05-01,,"['arsha nagrani', 'sachit menon', 'ahmet iscen', 'shyamal buch', 'ramin mehran', 'nilpa jha', 'anja hauth', 'yukun zhu', 'carl vondrick', 'mikhail sirotenko', 'cordelia schmid', 'tobias weyand']"
2505.00684,visual test-time scaling for gui agent grounding,cs.cv cs.ai cs.lg,"we introduce regionfocus, a visual test-time scaling approach for vision language model agents. understanding webpages is challenging due to the visual complexity of gui images and the large number of interface elements, making accurate action selection difficult. our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. to support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. even with a simple region selection strategy, we observe significant performance gains of 28+\% on screenspot-pro and 24+\% on webvoyager benchmarks on top of two state-of-the-art open vision language model agents, ui-tars and qwen2.5-vl, highlighting the effectiveness of visual test-time scaling in interactive settings. we achieve a new state-of-the-art grounding performance of 61.6\% on the screenspot-pro benchmark by applying regionfocus to a qwen2.5-vl-72b model. our code will be released publicly at https://github.com/tiangeluo/regionfocus.",,2025-05-01,,"['tiange luo', 'lajanugen logeswaran', 'justin johnson', 'honglak lee']"
2505.00687,guidesr: rethinking guidance for one-step high-fidelity diffusion-based   super-resolution,eess.iv cs.cv,"in this paper, we propose guidesr, a novel single-step diffusion-based image super-resolution (sr) model specifically designed to enhance image fidelity. existing diffusion-based sr approaches typically adapt pre-trained generative models to image restoration tasks by adding extra conditioning on a vae-downsampled representation of the degraded input, which often compromises structural fidelity. guidesr addresses this limitation by introducing a dual-branch architecture comprising: (1) a guidance branch that preserves high-fidelity structures from the original-resolution degraded input, and (2) a diffusion branch, which a pre-trained latent diffusion model to enhance perceptual quality. unlike conventional conditioning mechanisms, our guidance branch features a tailored structure for image restoration tasks, combining full resolution blocks (frbs) with channel attention and an image guidance network (ign) with guided attention. by embedding detailed structural information directly into the restoration pipeline, guidesr produces sharper and more visually consistent results. extensive experiments on benchmark datasets demonstrate that guidesr achieves state-of-the-art performance while maintaining the low computational cost of single-step approaches, with up to 1.39db psnr gain on challenging real-world datasets. our approach consistently outperforms existing methods across various reference-based metrics including psnr, ssim, lpips, dists and fid, further representing a practical advancement for real-world image restoration.",,2025-05-01,,"['aditya arora', 'zhengzhong tu', 'yufei wang', 'ruizheng bai', 'jian wang', 'sizhuo ma']"
2505.0069,towards autonomous micromobility through scalable urban simulation,cs.cv cs.ai cs.ro,"micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. assisting humans with ai agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. in this work, we present a scalable urban simulation solution to advance autonomous micromobility. first, we build urban-sim - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. urban-sim contains three critical modules: hierarchical urban generation pipeline, interactive dynamics generation strategy, and asynchronous scene sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. then, we propose urban-bench - a suite of essential tasks and benchmarks to gauge various capabilities of the ai agents in achieving autonomous micromobility. urban-bench includes eight tasks based on three core skills of the agents: urban locomotion, urban navigation, and urban traverse. we evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.",,2025-05-01,,"['wayne wu', 'honglin he', 'chaoyuan zhang', 'jack he', 'seth z. zhao', 'ran gong', 'quanyi li', 'bolei zhou']"
2505.00693,robotic visual instruction,cs.ro cs.ai cs.cv,"recently, natural language has been the primary medium for human-robot interaction. however, its inherent lack of spatial precision introduces challenges for robotic task definition such as ambiguity and verbosity. moreover, in some public settings where quiet is required, such as libraries or hospitals, verbal communication with robots is inappropriate. to address these limitations, we introduce the robotic visual instruction (rovi), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. rovi effectively encodes spatial-temporal information into human-interpretable visual instructions through 2d sketches, utilizing arrows, circles, colors, and numbers to direct 3d robotic manipulation. to enable robots to understand rovi better and generate precise actions based on rovi, we present visual instruction embodied workflow (view), a pipeline formulated for rovi-conditioned policies. this approach leverages vision-language models (vlms) to interpret rovi inputs, decode spatial and temporal constraints from 2d pixel space via keypoint extraction, and then transform them into executable 3d action sequences. we additionally curate a specialized dataset of 15k instances to fine-tune small vlms for edge deployment,enabling them to effectively learn rovi capabilities. our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. notably, view achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. project website: https://robotic-visual-instruction.github.io/",,2025-05-01,2025-05-06,"['yanbang li', 'ziyang gong', 'haoyang li', 'xiaoqi huang', 'haolan kang', 'guangping bai', 'xianzheng ma']"
2505.00702,rayzer: a self-supervised large view synthesis model,cs.cv,"we present rayzer, a self-supervised multi-view 3d vision model trained without any 3d supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3d awareness. concretely, rayzer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. during training, rayzer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing rayzer to be trained with 2d image supervision. the emerging 3d awareness of rayzer is attributed to two key factors. first, we design a self-supervised framework, which achieves 3d-aware auto-encoding of input images by disentangling camera and scene representations. second, we design a transformer-based model in which the only 3d prior is the ray structure, connecting camera, pixel, and scene simultaneously. rayzer demonstrates comparable or even superior novel view synthesis performance than ``oracle'' methods that rely on pose annotations in both training and testing. project: https://hwjiang1510.github.io/rayzer/",,2025-05-01,,"['hanwen jiang', 'hao tan', 'peng wang', 'haian jin', 'yue zhao', 'sai bi', 'kai zhang', 'fujun luan', 'kalyan sunkavalli', 'qixing huang', 'georgios pavlakos']"
2505.00703,t2i-r1: reinforcing image generation with collaborative semantic-level   and token-level cot,cs.cv cs.ai cs.cl cs.lg,"recent advancements in large language models have demonstrated how chain-of-thought (cot) and reinforcement learning (rl) can improve performance. however, applying such reasoning strategies to the visual generation domain remains largely unexplored. in this paper, we present t2i-r1, a novel reasoning-enhanced text-to-image generation model, powered by rl with a bi-level cot reasoning process. specifically, we identify two levels of cot that can be utilized to enhance different stages of generation: (1) the semantic-level cot for high-level planning of the prompt and (2) the token-level cot for low-level pixel processing during patch-by-patch generation. to better coordinate these two levels of cot, we introduce bicot-grpo with an ensemble of generation rewards, which seamlessly optimizes both generation cots within the same training step. by applying our reasoning strategies to the baseline model, janus-pro, we achieve superior performance with 13% improvement on t2i-compbench and 19% improvement on the wise benchmark, even surpassing the state-of-the-art model flux.1. code is available at: https://github.com/caraj7/t2i-r1",,2025-05-01,,"['dongzhi jiang', 'ziyu guo', 'renrui zhang', 'zhuofan zong', 'hao li', 'le zhuo', 'shilin yan', 'pheng-ann heng', 'hongsheng li']"
2505.00704,controllable weather synthesis and removal with video diffusion models,cs.gr cs.cv,"generating realistic and controllable weather effects in videos is valuable for many applications. physics-based weather simulation requires precise reconstructions that are hard to scale to in-the-wild videos, while current video editing often lacks realism and control. in this work, we introduce weatherweaver, a video diffusion model that synthesizes diverse weather effects -- including rain, snow, fog, and clouds -- directly into any input video without the need for 3d modeling. our model provides precise control over weather effect intensity and supports blending various weather types, ensuring both realism and adaptability. to overcome the scarcity of paired training data, we propose a novel data strategy combining synthetic videos, generative image editing, and auto-labeled real-world videos. extensive evaluations show that our method outperforms state-of-the-art methods in weather simulation and removal, providing high-quality, physically plausible, and scene-identity-preserving results over various real-world videos.",,2025-05-01,,"['chih-hao lin', 'zian wang', 'ruofan liang', 'yuxuan zhang', 'sanja fidler', 'shenlong wang', 'zan gojcic']"
2505.00755,p2p-insole: human pose estimation using foot pressure distribution and   motion sensors,cs.cv cs.ai,"this work presents p2p-insole, a low-cost approach for estimating and visualizing 3d human skeletal data using insole-type sensors integrated with imus. each insole, fabricated with e-textile garment techniques, costs under usd 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. the system employs a transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. these facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. this work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.",,2025-05-01,,"['atsuya watanabe', 'ratna aisuwarya', 'lei jing']"
2505.00757,efficient on-chip implementation of 4d radar-based 3d object detection   on hailo-8l,cs.cv cs.ai,"4d radar has attracted attention in autonomous driving due to its ability to enable robust 3d object detection even under adverse weather conditions. to practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. addressing this, we present the first on-chip implementation of a 4d radar-based 3d object detection model on the hailo-8l ai accelerator. although conventional 3d convolutional neural network (cnn) architectures require 5d inputs, the hailo-8l only supports 4d tensors, posing a significant challenge. to overcome this limitation, we introduce a tensor transformation method that reshapes 5d inputs into 4d formats during the compilation process, enabling direct deployment without altering the model structure. the proposed system achieves 46.47% ap_3d and 52.75% ap_bev, maintaining comparable accuracy to gpu-based models while achieving an inference speed of 13.76 hz. these results demonstrate the applicability of 4d radar-based perception technologies to autonomous driving systems.",,2025-05-01,,"['woong-chan byun', 'dong-hee paek', 'seung-hyun song', 'seung-hyun kong']"
2505.00772,person detection and re-identification in open-world settings of retail   stores and public spaces,cs.cv,"practical applications of computer vision in smart cities usually assume system integration and operation in challenging open-world environments. in the case of person re-identification task the main goal is to retrieve information whether the specific person has appeared in another place at a different time instance of the same video, or over multiple camera feeds. this typically assumes collecting raw data from video surveillance cameras in different places and under varying illumination conditions. in the considered open-world setting it also requires detection and localization of the person inside the analyzed video frame before the main re-identification step. with multi-person and multi-camera setups the system complexity becomes higher, requiring sophisticated tracking solutions and re-identification models. in this work we will discuss existing challenges in system design architectures, consider possible solutions based on different computer vision techniques, and describe applications of such systems in retail stores and public spaces for improved marketing analytics. in order to analyse sensitivity of person re-identification task under different open-world environments, a performance of one close to real-time solution will be demonstrated over several video captures and live camera feeds. finally, based on conducted experiments we will indicate further research directions and possible system improvements.",,2025-05-01,,"['branko brkljač', 'milan brkljač']"
2505.00786,ai-ready snow radar echogram dataset (sred) for climate change   monitoring,cs.cv,"tracking internal layers in radar echograms with high accuracy is essential for understanding ice sheet dynamics and quantifying the impact of accelerated ice discharge in greenland and other polar regions due to contemporary global climate warming. deep learning algorithms have become the leading approach for automating this task, but the absence of a standardized and well-annotated echogram dataset has hindered the ability to test and compare algorithms reliably, limiting the advancement of state-of-the-art methods for the radar echogram layer tracking problem. this study introduces the first comprehensive ``deep learning ready'' radar echogram dataset derived from snow radar airborne data collected during the national aeronautics and space administration operation ice bridge (oib) mission in 2012. the dataset contains 13,717 labeled and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation, wet) with varying along-track resolutions. to demonstrate its utility, we evaluated the performance of five deep learning models on the dataset. our results show that while current computer vision segmentation algorithms can identify and track snow layer pixels in echogram images, advanced end-to-end models are needed to directly extract snow depth and annual accumulation from echograms, reducing or eliminating post-processing. the dataset and accompanying benchmarking framework provide a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation, advancing our understanding of polar ice sheets response to climate warming.",,2025-05-01,,"['oluwanisola ibikunle', 'hara talasila', 'debvrat varshney', 'jilu li', 'john paden', 'maryam rahnemoonfar']"
2505.00788,spatialllm: a compound 3d-informed design towards spatially-intelligent   large multimodal models,cs.cv,"humans naturally understand 3d spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. current large multimodal models (lmms), however, lack of this capability of 3d spatial reasoning. this limitation stems from the scarcity of 3d training data and the bias in current model designs toward 2d data. in this paper, we systematically study the impact of 3d-informed data, architecture, and training setups, introducing spatialllm, a large multi-modal model with advanced 3d spatial reasoning abilities. to address data limitations, we develop two types of 3d-informed training datasets: (1) 3d-informed probing data focused on object's 3d location and orientation, and (2) 3d-informed conversation data for complex spatial relationships. notably, we are the first to curate vqa data that incorporate 3d orientation relationships on real images. furthermore, we systematically integrate these two types of training data with the architectural and training designs of lmms, providing a roadmap for optimal design aimed at achieving superior 3d reasoning capabilities. our spatialllm advances machines toward highly capable 3d-informed reasoning, surpassing gpt-4o performance by 8.7%. our systematic empirical design and the resulting findings offer valuable insights for future research in this direction.",,2025-05-01,,"['wufei ma', 'luoxin ye', 'nessa mcweeney', 'celso m de melo', 'alan yuille', 'jieneng chen']"
2505.00805,advancing wheat crop analysis: a survey of deep learning approaches   using hyperspectral imaging,cs.cv eess.iv,"as one of the most widely cultivated and consumed crops, wheat is essential to global food security. however, wheat production is increasingly challenged by pests, diseases, climate change, and water scarcity, threatening yields. traditional crop monitoring methods are labor-intensive and often ineffective for early issue detection. hyperspectral imaging (hsi) has emerged as a non-destructive and efficient technology for remote crop health assessment. however, the high dimensionality of hsi data and limited availability of labeled samples present notable challenges. in recent years, deep learning has shown great promise in addressing these challenges due to its ability to extract and analysis complex structures. despite advancements in applying deep learning methods to hsi data for wheat crop analysis, no comprehensive survey currently exists in this field. this review addresses this gap by summarizing benchmark datasets, tracking advancements in deep learning methods, and analyzing key applications such as variety classification, disease detection, and yield estimation. it also highlights the strengths, limitations, and future opportunities in leveraging deep learning methods for hsi-based wheat crop analysis. we have listed the current state-of-the-art papers and will continue tracking updating them in the following https://github.com/fadi-07/awesome-wheat-hsi-deeplearning.",,2025-05-01,,"['fadi abdeladhim zidi', 'abdelkrim ouafi', 'fares bougourzi', 'cosimo distante', 'abdelmalik taleb-ahmed']"
2505.00836,the comparability of model fusion to measured data in confuser rejection,cs.cv,"data collection has always been a major issue in the modeling and training of large deep learning networks, as no dataset can account for every slight deviation we might see in live usage. collecting samples can be especially costly for synthetic aperture radar (sar), limiting the amount of unique targets and operating conditions we are able to observe from. to counter this lack of data, simulators have been developed utilizing the shooting and bouncing ray method to allow for the generation of synthetic sar data on 3d models. while effective, the synthetically generated data does not perfectly correlate to the measured data leading to issues when training models solely on synthetic data. we aim to use computational power as a substitution for this lack of quality measured data, by ensembling many models trained on synthetic data. synthetic data is also not complete, as we do not know what targets might be present in a live environment. therefore we need to have our ensembling techniques account for these unknown targets by applying confuser rejection in which our models will reject unknown targets it is presented with, and only classify those it has been trained on.",,2025-05-01,,"['conor flynn', 'christopher ebersole', 'edmund zelnio']"
2505.00866,are minimal radial distortion solvers really necessary for relative pose   estimation?,cs.cv,"estimating the relative pose between two cameras is a fundamental step in many applications such as structure-from-motion. the common approach to relative pose estimation is to apply a minimal solver inside a ransac loop. highly efficient solvers exist for pinhole cameras. yet, (nearly) all cameras exhibit radial distortion. not modeling radial distortion leads to (significantly) worse results. however, minimal radial distortion solvers are significantly more complex than pinhole solvers, both in terms of run-time and implementation efforts. this paper compares radial distortion solvers with two simple-to-implement approaches that do not use minimal radial distortion solvers: the first approach combines an efficient pinhole solver with sampled radial undistortion parameters, where the sampled parameters are used for undistortion prior to applying the pinhole solver. the second approach uses a state-of-the-art neural network to estimate the distortion parameters rather than sampling them from a set of potential values. extensive experiments on multiple datasets, and different camera setups, show that complex minimal radial distortion solvers are not necessary in practice. we discuss under which conditions a simple sampling of radial undistortion parameters is preferable over calibrating cameras using a learning-based prior approach. code and newly created benchmark for relative pose estimation under radial distortion are available at https://github.com/kocurvik/rdnet.",,2025-05-01,,"['viktor kocur', 'charalambos tzamos', 'yaqing ding', 'zuzana berger haladova', 'torsten sattler', 'zuzana kukelova']"
2505.00935,autonomous embodied agents: when robotics meets deep learning reasoning,cs.ro cs.ai cs.cv,"the increase in available computing power and the deep learning revolution have allowed the exploration of new topics and frontiers in artificial intelligence research. a new field called embodied artificial intelligence, which places at the intersection of computer vision, robotics, and decision making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. the recent availability of large collections of 3d models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. these intelligent agents are intended to perform a certain task in a possibly unknown environment. to this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. this dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. we aim to contribute to research in embodied ai and autonomous agents, in order to foster future work in this field. we present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.",,2025-05-01,,['roberto bigazzi']
2505.00938,cdformer: cross-domain few-shot object detection transformer against   feature confusion,cs.cv cs.ai,"cross-domain few-shot object detection (cd-fsod) aims to detect novel objects across different domains with limited class instances. feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. in this work, we introduce cdformer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. the method specifically tackles feature confusion through two key modules: object-background distinguishing (obd) and object-object distinguishing (ood). the obd module leverages a learnable background token to differentiate between objects and background, while the ood module enhances the distinction between objects of different classes. experimental results demonstrate that cdformer outperforms previous state-of-the-art approaches, achieving 12.9% map, 11.0% map, and 10.4% map improvements under the 1/5/10 shot settings, respectively, when fine-tuned.",,2025-05-01,,"['boyuan meng', 'xiaohan zhang', 'peilin li', 'zhe wu', 'yiming li', 'wenkai zhao', 'beinan yu', 'hui-liang shen']"
2505.00975,generating animated layouts as structured text representations,cs.cv,"despite the remarkable progress in text-to-video models, achieving precise control over text elements and animated graphics remains a significant challenge, especially in applications such as video advertisements. to address this limitation, we introduce animated layout generation, a novel approach to extend static graphic layouts with temporal dynamics. we propose a structured text representation for fine-grained video control through hierarchical visual elements. to demonstrate the effectiveness of our approach, we present vaker (video ad maker), a text-to-video advertisement generation pipeline that combines a three-stage generation process with unstructured text reasoning for seamless integration with llms. vaker fully automates video advertisement generation by incorporating dynamic layout trajectories for objects and graphics across specific video frames. through extensive evaluations, we demonstrate that vaker significantly outperforms existing methods in generating video advertisements. project page: https://yeonsangshin.github.io/projects/vaker",,2025-05-01,,"['yeonsang shin', 'jihwan kim', 'yumin song', 'kyungseung lee', 'hyunhee chung', 'taeyoung na']"
2505.0098,lmdepth: lightweight mamba-based monocular depth estimation for   real-world deployment,cs.cv,"monocular depth estimation provides an additional depth dimension to rgb images, making it widely applicable in various fields such as virtual reality, autonomous driving and robotic navigation. however, existing depth estimation algorithms often struggle to effectively balance performance and computational efficiency, which poses challenges for deployment on resource-constrained devices. to address this, we propose lmdepth, a lightweight mamba-based monocular depth estimation network, designed to reconstruct high-precision depth information while maintaining low computational overhead. specifically, we propose a modified pyramid spatial pooling module that serves as a multi-scale feature aggregator and context extractor, ensuring global spatial information for accurate depth estimation. moreover, we integrate multiple depth mamba blocks into the decoder. designed with linear computations, the mamba blocks enable lmdepth to efficiently decode depth information from global features, providing a lightweight alternative to transformer-based architectures that depend on complex attention mechanisms. extensive experiments on the nyudv2 and kitti datasets demonstrate the effectiveness of our proposed lmdepth. compared to previous lightweight depth estimation methods, lmdepth achieves higher performance with fewer parameters and lower computational complexity (measured by gflops). we further deploy lmdepth on an embedded platform with int8 quantization, validating its practicality for real-world edge applications.",,2025-05-02,,"['jiahuan long', 'xin zhou']"
2505.00986,on-demand test-time adaptation for edge devices,cs.lg cs.cv,"continual test-time adaptation (ctta) continuously adapts the deployed model on every incoming batch of data. while achieving optimal accuracy, existing ctta approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. in this work, we first introduce a novel paradigm -- on-demand tta -- which triggers adaptation only when a significant domain shift is detected. then, we present od-tta, an on-demand tta framework for accurate and efficient adaptation on edge devices. od-tta comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate tta only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled batch normalization (bn) update scheme to enable memory-efficient adaptation with small batch sizes. extensive experiments show that od-tta achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making tta a practical reality.",,2025-05-02,,"['xiao ma', 'young d. kwon', 'dong ma']"
2505.00995,optimizing indoor farm monitoring efficiency using uav: yield estimation   in a gnss-denied cherry tomato greenhouse,cs.ro cs.cv,"as the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. while unmanned ground vehicles (ugvs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. to address these issues, we develop a lightweight unmanned aerial vehicle (uav) equipped with an rgb-d camera, a 3d lidar, and an imu sensor. the uav employs a lidar-inertial odometry algorithm for precise navigation in gnss-denied environments and utilizes a 3d multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. we evaluate the system using two dataset: one from a harvesting row and another from a growing row. in the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. for the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. our findings demonstrate the potential of uavs for efficient robotic yield estimation in commercial greenhouses.",,2025-05-02,,"['taewook park', 'jinwoo lee', 'hyondong oh', 'won-jae yun', 'kyu-wha lee']"
2505.00998,deterministic-to-stochastic diverse latent feature mapping for human   motion synthesis,cs.cv,"human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. recent score-based generative models (sgms) have demonstrated impressive results on this task. however, their training process involves complex curvature trajectories, leading to unstable training process. in this paper, we propose a deterministic-to-stochastic diverse latent feature mapping (dsdfm) method for human motion synthesis. dsdfm consists of two stages. the first human motion reconstruction stage aims to learn the latent space distribution of human motions. the second diverse motion generation stage aims to build connections between the gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. this stage is achieved by the designed deterministic feature mapping procedure with derode and stochastic diverse output generation procedure with divsde.dsdfm is easy to train compared to previous sgms-based methods and can enhance diversity without introducing additional training parameters.through qualitative and quantitative experiments, dsdfm achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.",,2025-05-02,,"['yu hua', 'weiming liu', 'gui xu', 'yaqing hou', 'yew-soon ong', 'qiang zhang']"
2505.01003,3d human pose estimation via spatial graph order attention and temporal   body aware transformer,cs.cv,"nowadays, transformers and graph convolutional networks (gcns) are the prevailing techniques for 3d human pose estimation. however, transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while gcn-based methods often neglect the need for pose-specific representations. to address these problems, we propose a new method that exploits the graph modeling capability of gcn to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced graph order attention module that dynamically emphasizes the most representative orders for each joint. the resulting spatial features of the sequence are further processed using a proposed temporal body aware transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. given that our 3d pose output aligns with the central 2d pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. extensive experiments on human3.6m, mpiinf-3dhp, and humaneva-i datasets demonstrate the effectiveness of the proposed method. code and models are made available on github.",,2025-05-02,,"['kamel aouaidjia', 'aofan li', 'wenhao zhang', 'chongsheng zhang']"
2505.01007,towards the resistance of neural network watermarking to fine-tuning,cs.lg cs.ai cs.cl cs.cv,"this paper proves a new watermarking method to embed the ownership information into a deep neural network (dnn), which is robust to fine-tuning. specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised fourier transform to extract frequency components from the convolutional filter. additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. in this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. preliminary experiments demonstrate the effectiveness of our method.",,2025-05-02,,"['ling tang', 'yuefeng chen', 'hui xue', 'quanshi zhang']"
2505.01016,fine-tuning without forgetting: adaptation of yolov8 preserves coco   performance,cs.cv cs.ai,"the success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. while fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. the critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. we adapt a standard yolov8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original coco validation set. our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\% absolute map50) on the fine-grained fruit task compared to only training the head. strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\% absolute map difference) on the coco benchmark across all tested freeze levels. we conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.",,2025-05-02,,"['vishal gandhi', 'sagar gandhi']"
2505.01032,edge-preserving image denoising via multi-scale adaptive statistical   independence testing,cs.cv,"edge detection is crucial in image processing, but existing methods often produce overly detailed edge maps, affecting clarity. fixed-window statistical testing faces issues like scale mismatch and computational redundancy. to address these, we propose a novel multi-scale adaptive independence testing-based edge detection and denoising (edd-mait), a multi-scale adaptive statistical testing-based edge detection and denoising method that integrates a channel attention mechanism with independence testing. a gradient-driven adaptive window strategy adjusts window sizes dynamically, improving detail preservation and noise suppression. edd-mait achieves better robustness, accuracy, and efficiency, outperforming traditional and learning-based methods on bsds500 and biped datasets, with improvements in f-score, mse, psnr, and reduced runtime. it also shows robustness against gaussian noise, generating accurate and clean edge maps in noisy environments.",,2025-05-02,,"['ruyu yan', 'da-qing zhang']"
2505.0104,edge detection based on channel attention and inter-region independence   test,cs.cv,"existing edge detection methods often suffer from noise amplification and excessive retention of non-salient details, limiting their applicability in high-precision industrial scenarios. to address these challenges, we propose cam-edit, a novel framework that integrates channel attention mechanism (cam) and edge detection via independence testing (edit). the cam module adaptively enhances discriminative edge features through multi-channel fusion, while the edit module employs region-wise statistical independence analysis (using fisher's exact test and chi-square test) to suppress uncorrelated noise.extensive experiments on bsds500 and nyudv2 datasets demonstrate state-of-the-art performance. among the nine comparison algorithms, the f-measure scores of cam-edit are 0.635 and 0.460, representing improvements of 19.2\% to 26.5\% over traditional methods (canny, cannysr), and better than the latest learning based methods (tip2020, mscngp). noise robustness evaluations further reveal a 2.2\% psnr improvement under gaussian noise compared to baseline methods. qualitative results exhibit cleaner edge maps with reduced artifacts, demonstrating its potential for high-precision industrial applications.",,2025-05-02,,"['ru-yu yan', 'da-qing zhang']"
2505.0105,transferable adversarial attacks on black-box vision-language models,cs.cv cs.lg,"vision large language models (vllms) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. while prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for vllms. we present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary vllms such as gpt-4o, claude, and gemini. we show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary vllms. our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of vllms.",,2025-05-02,,"['kai hu', 'weichen yu', 'li zhang', 'alexander robey', 'andy zou', 'chengming xu', 'haoqi hu', 'matt fredrikson']"
2505.01057,gelovec: higher dimensional geometric smoothing for coherent visual   feature extraction in image segmentation,cs.cv,"this paper introduces gelovec, a new cnn-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. while existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. gelovec combines modified chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. the core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. the multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean intersection over union (miou) gains of 2.1%, 2.7%, and 2.4% on caltech birds-200, lsdsc, and fssd datasets respectively compared to state-of-the-art methods. gelovec's mathematical foundation in riemannian geometry provides theoretical guarantees on segmentation stability. importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.",,2025-05-02,,"['boris kriuk', 'matey yordanov']"
2505.01064,efficient vocabulary-free fine-grained visual recognition in the age of   multimodal llms,cs.cv cs.lg,"fine-grained visual recognition (fgvr) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. in domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. in such scenarios lacking labeled data, an fgvr model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. we refer to this task as vocabulary-free fgvr (vf-fgvr), where a model must predict labels from an unconstrained output space without prior label information. while recent multimodal large language models (mllms) show potential for vf-fgvr, querying these models for each test input is impractical because of high costs and prohibitive inference times. to address these limitations, we introduce \textbf{nea}rest-neighbor label \textbf{r}efinement (near), a novel approach that fine-tunes a downstream clip model using labels generated by an mllm. our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging mllms for label generation. near is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by mllms, and establishes a new benchmark for efficient vf-fgvr.",,2025-05-02,,"['hari chandana kuchibhotla', 'sai srinivas kancheti', 'abbavaram gowtham reddy', 'vineeth n balasubramanian']"
2505.01079,improving editability in image generation with layer-wise memory,cs.cv eess.iv,"most real-world image editing tasks require multiple sequential edits to achieve desired results. current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. these limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. we address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. we propose background consistency guidance that leverages memorized latents to maintain scene coherence and multi-query disentanglement in cross-attention that ensures natural adaptation to existing content. to evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.",,2025-05-02,,"['daneul kim', 'jaeah lee', 'jaesik park']"
2505.01091,any-to-any vision-language model for multimodal x-ray imaging and   radiological report generation,cs.cv cs.ai,"generative models have revolutionized artificial intelligence (ai), particularly in multimodal applications. however, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. in this work, we introduce a framework specifically designed for multimodal medical data generation. by enabling the generation of multi-view chest x-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. leveraging the mimic-cxr dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. our quantitative evaluation reveals significant results in terms of fid and bleu scores, showcasing the quality of the generated data. notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. this study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.",,2025-05-02,,"['daniele molino', 'francesco di feola', 'linlin shen', 'paolo soda', 'valerio guarrasi']"
2505.01096,evaluating vision language model adaptations for radiology report   generation in low-resource languages,cs.cv cs.cl,"the integration of artificial intelligence in healthcare has opened new horizons for improving medical diagnostics and patient care. however, challenges persist in developing systems capable of generating accurate and contextually relevant radiology reports, particularly in low-resource languages. in this study, we present a comprehensive benchmark to evaluate the performance of instruction-tuned vision-language models (vlms) in the specialized task of radiology report generation across three low-resource languages: italian, german, and spanish. employing the llava architectural framework, we conducted a systematic evaluation of pre-trained models utilizing general datasets, domain-specific datasets, and low-resource language-specific datasets. in light of the unavailability of models that possess prior knowledge of both the medical domain and low-resource languages, we analyzed various adaptations to determine the most effective approach for these contexts. the results revealed that language-specific models substantially outperformed both general and domain-specific models in generating radiology reports, emphasizing the critical role of linguistic adaptation. additionally, models fine-tuned with medical terminology exhibited enhanced performance across all languages compared to models with generic knowledge, highlighting the importance of domain-specific training. we also explored the influence of the temperature parameter on the coherence of report generation, providing insights for optimal model settings. our findings highlight the importance of tailored language and domain-specific training for improving the quality and accuracy of radiological reports in multilingual settings. this research not only advances our understanding of vlms adaptability in healthcare but also points to significant avenues for future investigations into model tuning and language-specific adaptations.",,2025-05-02,,"['marco salmè', 'rosa sicilia', 'paolo soda', 'valerio guarrasi']"
2505.01104,vsc: visual search compositional text-to-image diffusion model,cs.cv,"text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. this challenge primarily arises from the limitations of commonly used text encoders, such as clip, which can fail to encode complex linguistic relationships and modifiers effectively. existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. in this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. by applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. our approaches outperform existing compositional text-to-image diffusion models on the benchmark t2i compbench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.",,2025-05-02,,"['do huu dat', 'nam hyeonu', 'po-yuan mao', 'tae-hyun oh']"
2505.01109,self-supervision enhances instance-based multiple instance learning   methods in digital pathology: a benchmark study,cs.cv cs.ai,"multiple instance learning (mil) has emerged as the best solution for whole slide image (wsi) classification. it consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. mil includes two main approaches: instance-based and embedding-based. in the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. in the latter, bag classification is performed after aggregating patch embeddings. even if instance-based methods are naturally more interpretable, embedding-based mils have usually been preferred in the past due to their robustness to poor feature extractors. however, recently, the quality of feature embeddings has drastically increased using self-supervised learning (ssl). nevertheless, many authors continue to endorse the superiority of embedding-based mil. to investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 mil strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. furthermore, we introduce 4 instance-based mil methods never used before in the pathology domain. through these extensive experiments, we show that with a good ssl feature extractor, simple instance-based mils, with very few parameters, obtain similar or better performance than complex, state-of-the-art (sota) embedding-based mil methods, setting new sota results on the bracs and camelyon16 datasets. since simple instance-based mil methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted ssl methods for wsi rather than into complex embedding-based mil methods.",,2025-05-02,,"['ali mammadov', 'loic le folgoc', 'julien adam', 'anne buronfosse', 'gilles hayem', 'guillaume hocquet', 'pietro gori']"
2505.01113,neuroloc: encoding navigation cells for 6-dof camera localization,cs.ro cs.cv cs.ne,"recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. however, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. to address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method, namely neuroloc. firstly, we designed a hebbian learning module driven by place cells to save and replay historical information, aiming to restore the details of historical representations and solve the issue of scene fuzziness. secondly, we utilized the head direction cell-inspired internal direction learning as multi-head attention embedding to help restore the true orientation in similar scenes. finally, we added a 3d grid center prediction in the pose regression module to reduce the final wrong prediction. we evaluate the proposed neuroloc on commonly used benchmark indoor and outdoor datasets. the experimental results show that our neuroloc can enhance the robustness in complex environments and improve the performance of pose regression by using only a single image.",,2025-05-02,,"['xun li', 'jian yang', 'fenli jia', 'muyu wang', 'qi wu', 'jun wu', 'jinpeng mi', 'jilin hu', 'peidong liang', 'xuan tang', 'ke li', 'xiong you', 'xian wei']"
2505.01172,freepca: integrating consistency information across long-short frames in   training-free long video generation via principal component analysis,cs.cv,"long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. it necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. in this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying principal component analysis (pca), allowing for refined complementary integration of global consistency and local quality. with this insight, we propose freepca, a training-free long video generation paradigm based on pca that simultaneously achieves high consistency and quality. concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. experiments demonstrate that freepca can be applied to various video diffusion models without requiring training, leading to substantial improvements. code is available at https://github.com/josephtitan/freepca.",,2025-05-02,,"['jiangtong tan', 'hu yu', 'jie huang', 'jie xiao', 'feng zhao']"
2505.01182,tstmotion: training-free scene-aware text-to-motion generation,cs.cv cs.ai,"text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. however, human motions commonly occur within diverse 3d scenes, which has prompted exploration into scene-aware text-to-motion generation methods. yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3d scenes, which poses practical challenges due to the expensive cost. to mitigate this challenge, we are the first to propose a \textbf{t}raining-free \textbf{s}cene-aware \textbf{t}ext-to-\textbf{motion} framework, dubbed as \textbf{tstmotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. specifically, conditioned on the given 3d scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. extensive experiments demonstrate the efficacy and generalizability of our proposed framework. we release our code in \href{https://tstmotion.github.io/}{project page}.",,2025-05-02,2025-05-05,"['ziyan guo', 'haoxuan qu', 'hossein rahmani', 'dewen soh', 'ping hu', 'qiuhong ke', 'jun liu']"
2505.01203,efficient vision-based vehicle speed estimation,cs.cv,"this paper presents a computationally efficient method for vehicle speed estimation from traffic camera footage. building upon previous work that utilizes 3d bounding boxes derived from 2d detections and vanishing point geometry, we introduce several improvements to enhance real-time performance. we evaluate our method in several variants on the brnocompspeed dataset in terms of vehicle detection and speed estimation accuracy. our extensive evaluation across various hardware platforms, including edge devices, demonstrates significant gains in frames per second (fps) compared to the prior state-of-the-art, while maintaining comparable or improved speed estimation accuracy. we analyze the trade-off between accuracy and computational cost, showing that smaller models utilizing post-training quantization offer the best balance for real-world deployment. our best performing model beats previous state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs. 83.32%) while also being 5.5 times faster.",,2025-05-02,,"['andrej macko', 'lukáš gajdošech', 'viktor kocur']"
2505.01207,t-graph: enhancing sparse-view camera pose estimation by pairwise   translation graph,cs.cv,"sparse-view camera pose estimation, which aims to estimate the 6-degree-of-freedom (6-dof) poses from a limited number of images captured from different viewpoints, is a fundamental yet challenging problem in remote sensing applications. existing methods often overlook the translation information between each pair of viewpoints, leading to suboptimal performance in sparse-view scenarios. to address this limitation, we introduce t-graph, a lightweight, plug-and-play module to enhance camera pose estimation in sparse-view settings. t-graph takes paired image features as input and maps them through a multilayer perceptron (mlp). it then constructs a fully connected translation graph, where nodes represent cameras and edges encode their translation relationships. it can be seamlessly integrated into existing models as an additional branch in parallel with the original prediction, maintaining efficiency and ease of use. furthermore, we introduce two pairwise translation representations, relative-t and pair-t, formulated under different local coordinate systems. while relative-t captures intuitive spatial relationships, pair-t offers a rotation-disentangled alternative. the two representations contribute to enhanced adaptability across diverse application scenarios, further improving our module's robustness. extensive experiments on two state-of-the-art methods (relpose++ and forge) using public datasets (c03d and imc phototourism) validate both the effectiveness and generalizability of t-graph. the results demonstrate consistent improvements across various metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8 viewpoints.",,2025-05-02,,"['qingyu xian', 'weiqin jiao', 'hao cheng', 'berend jan van der zwaag', 'yanqiu huang']"
2505.01224,rd-uie: relation-driven state space modeling for underwater image   enhancement,cs.cv eess.iv,"underwater image enhancement (uie) is a critical preprocessing step for marine vision applications, where wavelength-dependent attenuation causes severe content degradation and color distortion. while recent state space models like mamba show potential for long-range dependency modeling, their unfolding operations and fixed scan paths on 1d sequences fail to adapt to local object semantics and global relation modeling, limiting their efficacy in complex underwater environments. to address this, we enhance conventional mamba with the sorting-based scanning mechanism that dynamically reorders scanning sequences based on statistical distribution of spatial correlation of all pixels. in this way, it encourages the network to prioritize the most informative components--structural and semantic features. upon building this mechanism, we devise a visually self-adaptive state block (vssb) that harmonizes dynamic sorting of mamba with input-dependent dynamic convolution, enabling coherent integration of global context and local relational cues. this exquisite design helps eliminate global focus bias, especially for widely distributed contents, which greatly weakens the statistical frequency. for robust feature extraction and refinement, we design a cross-feature bridge (cfb) to adaptively fuse multi-scale representations. these efforts compose the novel relation-driven mamba framework for effective uie (rd-uie). extensive experiments on underwater enhancement benchmarks demonstrate rd-uie outperforms the state-of-the-art approach wmamba in both quantitative metrics and visual fidelity, averagely achieving 0.55 db performance gain on the three benchmarks. our code is available at https://github.com/kkoucy/rd-uie/tree/main",,2025-05-02,,"['kui jiang', 'yan luo', 'junjun jiang', 'xin xu', 'fei ma', 'fei yu']"
2505.01225,core-set selection for data-efficient land cover segmentation,cs.cv,"the increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many earth observation tasks. traditionally, such models must be trained on large datasets. however, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. therefore, effective solutions should consider both the quantity and quality of data. in this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. we benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: dfc2022, vaihingen, and potsdam. in each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. this result shows the importance and potential of data-centric learning for the remote sensing domain. the code is available at https://github.com/keillernogueira/data-centric-rs-classification/.",,2025-05-02,,"['keiller nogueira', 'akram zaytar', 'wanli ma', 'ribana roscher', 'ronny hänsch', 'caleb robinson', 'anthony ortiz', 'simone nsutezo', 'rahul dodhia', 'juan m. lavista ferres', 'oktay karakuş', 'paul l. rosin']"
2505.01235,compensating spatiotemporally inconsistent observations for online   dynamic 3d gaussian splatting,cs.cv,"online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. however, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. this paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. we propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. we show that our method restores the ideal observation by subtracting the learned error. we demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. code, video results, and checkpoints are available at https://bbangsik13.github.io/or2.",10.1145/3721238.3730678,2025-05-02,,"['youngsik yun', 'jeongmin bae', 'hyunseung son', 'seoha kim', 'hahyun lee', 'gun bang', 'youngjung uh']"
2505.01239,can foundation models really segment tumors? a benchmarking odyssey in   lung ct imaging,eess.iv cs.cv,"accurate lung tumor segmentation is crucial for improving diagnosis, treatment planning, and patient outcomes in oncology. however, the complexity of tumor morphology, size, and location poses significant challenges for automated segmentation. this study presents a comprehensive benchmarking analysis of deep learning-based segmentation models, comparing traditional architectures such as u-net and deeplabv3, self-configuring models like nnunet, and foundation models like medsam, and medsam~2. evaluating performance across two lung tumor segmentation datasets, we assess segmentation accuracy and computational efficiency under various learning paradigms, including few-shot learning and fine-tuning. the results reveal that while traditional models struggle with tumor delineation, foundation models, particularly medsam~2, outperform them in both accuracy and computational efficiency. these findings underscore the potential of foundation models for lung tumor segmentation, highlighting their applicability in improving clinical workflows and patient outcomes.",,2025-05-02,,"['elena mulero ayllón', 'massimiliano mantegna', 'linlin shen', 'paolo soda', 'valerio guarrasi', 'matteo tortora']"
2505.01249,fusing foveal fixations using linear retinal transformations and   bayesian experimental design,cs.cv cs.lg,"humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. in this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. this linear transformation allows us to carry out exact inference for the latent variables in factor analysis (fa) and mixtures of fa models of the scene. further, this allows us to formulate and solve the choice of ""where to look next"" as a bayesian experimental design problem using the expected information gain criterion. experiments on the frey faces and mnist datasets demonstrate the effectiveness of our models.",,2025-05-02,,['christopher k. i. williams']
2505.01257,cameltrack: context-aware multi-cue exploitation for online multi-object   tracking,cs.cv cs.lg,"online multi-object tracking has been recently dominated by tracking-by-detection (tbd) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. the key strength of tbd lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. however, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. in this work, we introduce camel, a novel association module for context-aware multi-cue exploitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining tbd's valuable modularity. at its core, camel employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. our proposed online tracking pipeline, cameltrack, achieves state-of-the-art performance on multiple tracking benchmarks. our code is available at https://github.com/trackinglaboratory/cameltrack.",,2025-05-02,,"['vladimir somers', 'baptiste standaert', 'victor joos', 'alexandre alahi', 'christophe de vleeschouwer']"
2505.01263,flowdubber: movie dubbing with llm-based semantic-aware learning and   flow matching based voice enhancing,cs.mm cs.cv cs.sd eess.as,"movie dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. to address these issues, we propose a large language model (llm) based flow matching architecture for dubbing, named flowdubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. first, we introduce qwen2.5 as the backbone of llm to learn the in-context sequence from movie scripts and reference audio. then, the proposed semantic-aware learning focuses on capturing llm semantic knowledge at the phoneme level. next, dual contrastive aligning (dca) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. finally, the proposed flow-based voice enhancing (fve) improves acoustic quality in two aspects, which introduces an llm-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. the demos are available at {\href{https://galaxycong.github.io/llm-flow-dubber/}{\textcolor{red}{https://galaxycong.github.io/llm-flow-dubber/}}}.",,2025-05-02,,"['gaoxiang cong', 'liang li', 'jiadong pan', 'zhedong zhang', 'amin beheshti', 'anton van den hengel', 'yuankai qi', 'qingming huang']"
2505.01267,diffusion-based adversarial purification from the perspective of the   frequency domain,cs.cv,"the diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. we turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. we find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. this means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. for the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.",,2025-05-02,,"['gaozheng pei', 'ke ma', 'yingfei sun', 'qianqian xu', 'qingming huang']"
2505.01313,a neural architecture search method using auxiliary evaluation metric   based on resnet architecture,cs.ne cs.cv,"this paper proposes a neural architecture search space using resnet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. in addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. the experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the mnist, fashion-mnist and cifar100 datasets.",,2025-05-02,,"['shang wang', 'huanrong tang', 'jianquan ouyang']"
2505.01322,freeinsert: disentangled text-guided object insertion in 3d gaussian   scene without spatial priors,cs.cv,"text-driven object insertion in 3d scenes is an emerging task that enables intuitive scene editing through natural language. however, existing 2d editing-based methods often rely on spatial priors such as 2d masks or 3d bounding boxes, and they struggle to ensure consistency of the inserted object. these limitations hinder flexibility and scalability in real-world applications. in this paper, we propose freeinsert, a novel framework that leverages foundation models including mllms, lgms, and diffusion models to disentangle object generation from spatial placement. this enables unsupervised and flexible object insertion in 3d scenes without spatial priors. freeinsert starts with an mllm-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. these semantics guide both the reconstruction of the inserted object for 3d consistency and the learning of its degrees of freedom. we leverage the spatial reasoning capabilities of mllms to initialize object pose and scale. a hierarchical, spatially aware refinement stage further integrates spatial semantics and mllm-inferred priors to enhance placement. finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. experimental results demonstrate that freeinsert achieves semantically coherent, spatially precise, and visually realistic 3d insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.",,2025-05-02,,"['chenxi li', 'weijie wang', 'qiang li', 'bruno lepri', 'nicu sebe', 'weizhi nie']"
2505.01364,monitoring morphometric drift in lifelong learning segmentation of the   spinal cord,cs.cv,"morphometric measures derived from spinal cord segmentations can serve as diagnostic and prognostic biomarkers in neurological diseases and injuries affecting the spinal cord. while robust, automatic segmentation methods to a wide variety of contrasts and pathologies have been developed over the past few years, whether their predictions are stable as the model is updated using new datasets has not been assessed. this is particularly important for deriving normative values from healthy participants. in this study, we present a spinal cord segmentation model trained on a multisite $(n=75)$ dataset, including 9 different mri contrasts and several spinal cord pathologies. we also introduce a lifelong learning framework to automatically monitor the morphometric drift as the model is updated using additional datasets. the framework is triggered by an automatic github actions workflow every time a new model is created, recording the morphometric values derived from the model's predictions over time. as a real-world application of the proposed framework, we employed the spinal cord segmentation model to update a recently-introduced normative database of healthy participants containing commonly used measures of spinal cord morphometry. results showed that: (i) our model outperforms previous versions and pathology-specific models on challenging lumbar spinal cord cases, achieving an average dice score of $0.95 \pm 0.03$; (ii) the automatic workflow for monitoring morphometric drift provides a quick feedback loop for developing future segmentation models; and (iii) the scaling factor required to update the database of morphometric measures is nearly constant among slices across the given vertebral levels, showing minimum drift between the current and previous versions of the model monitored by the framework. the model is freely available in spinal cord toolbox v7.0.",,2025-05-02,,"['enamundram naga karthik', 'sandrine bédard', 'jan valošek', 'christoph s. aigner', 'elise bannier', 'josef bednařík', 'virginie callot', 'anna combes', 'armin curt', 'gergely david', 'falk eippert', 'lynn farner', 'michael g fehlings', 'patrick freund', 'tobias granberg', 'cristina granziera', 'rhscir network imaging group', 'ulrike horn', 'tomáš horák', 'suzanne humphreys', 'markus hupp', 'anne kerbrat', 'nawal kinany', 'shannon kolind', 'petr kudlička', 'anna lebret', 'lisa eunyoung lee', 'caterina mainero', 'allan r. martin', 'megan mcgrath', 'govind nair', ""kristin p. o'grady"", 'jiwon oh', 'russell ouellette', 'nikolai pfender', 'dario pfyffer', 'pierre-françois pradat', 'alexandre prat', 'emanuele pravatà', 'daniel s. reich', 'ilaria ricchi', 'naama rotem-kohavi', 'simon schading-sassenhausen', 'maryam seif', 'andrew smith', 'seth a smith', 'grace sweeney', 'roger tam', 'anthony traboulsee', 'constantina andrada treaba', 'charidimos tsagkas', 'zachary vavasour', 'dimitri van de ville', 'kenneth arnold weber', 'sarath chandar', 'julien cohen-adad']"
2505.01385,global collinearity-aware polygonizer for polygonal building mapping in   remote sensing,cs.cv cs.lg,"this paper addresses the challenge of mapping polygonal buildings from remote sensing images and introduces a novel algorithm, the global collinearity-aware polygonizer (gcp). gcp, built upon an instance segmentation framework, processes binary masks produced by any instance segmentation model. the algorithm begins by collecting polylines sampled along the contours of the binary masks. these polylines undergo a refinement process using a transformer-based regression module to ensure they accurately fit the contours of the targeted building instances. subsequently, a collinearity-aware polygon simplification module simplifies these refined polylines and generate the final polygon representation. this module employs dynamic programming technique to optimize an objective function that balances the simplicity and fidelity of the polygons, achieving globally optimal solutions. furthermore, the optimized collinearity-aware objective is seamlessly integrated into network training, enhancing the cohesiveness of the entire pipeline. the effectiveness of gcp has been validated on two public benchmarks for polygonal building mapping. further experiments reveal that applying the collinearity-aware polygon simplification module to arbitrary polylines, without prior knowledge, enhances accuracy over traditional methods such as the douglas-peucker algorithm. this finding underscores the broad applicability of gcp. the code for the proposed method will be made available at https://github.com/zhu-xlab.",,2025-05-02,,"['fahong zhang', 'yilei shi', 'xiao xiang zhu']"
2505.0139,multimodal doctor-in-the-loop: a clinically-guided explainable framework   for predicting pathological response in non-small cell lung cancer,cs.cv cs.ai cs.lg,"this study proposes a novel approach combining multimodal deep learning with intrinsic explainable artificial intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. the proposed multimodal doctor-in-the-loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.",,2025-05-02,,"['alice natalina caragliano', 'claudia tacconi', 'carlo greco', 'lorenzo nibid', 'edy ippolito', 'michele fiore', 'giuseppe perrone', 'sara ramella', 'paolo soda', 'valerio guarrasi']"
2505.01406,vidstamp: a temporally-aware watermark for ownership and integrity in   video diffusion models,cs.cv cs.cr cs.lg,"the rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. in this work, we introduce vidstamp, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. by fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, vidstamp learns to embed high-capacity, flexible watermarks with minimal perceptual impact. leveraging architectural components such as 3d convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. vidstamp embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log p-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. code: code: \url{https://github.com/spin-umass/vidstamp}",,2025-05-02,,"['mohammadreza teymoorianfard', 'shiqing ma', 'amir houmansadr']"
2505.01425,genmo: a generalist model for human motion,cs.gr cs.ai cs.cv cs.lg cs.ro,"human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. we present genmo, a unified generalist model for human motion that bridges motion estimation and generation in a single framework. our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. leveraging the synergy between regression and diffusion, genmo achieves accurate global motion estimation while enabling diverse motion generation. we also introduce an estimation-guided training objective that exploits in-the-wild videos with 2d annotations and text descriptions to enhance generative diversity. furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. this unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. extensive experiments demonstrate genmo's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.",,2025-05-02,,"['jiefeng li', 'jinkun cao', 'haotian zhang', 'davis rempe', 'jan kautz', 'umar iqbal', 'ye yuan']"
2505.0149,worldgenbench: a world-knowledge-integrated benchmark for   reasoning-driven text-to-image generation,cs.cv,"recent advances in text-to-image (t2i) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. to address this gap, we introduce \textbf{worldgenbench}, a benchmark designed to systematically evaluate t2i models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. we propose the \textbf{knowledge checklist score}, a structured metric that measures how well generated images satisfy key semantic expectations. experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like gpt-4o exhibit significantly stronger reasoning and knowledge integration. our findings highlight the need for deeper understanding and inference capabilities in next-generation t2i systems. project page: \href{https://dwanzhang-ai.github.io/worldgenbench/}{https://dwanzhang-ai.github.io/worldgenbench/}",,2025-05-02,,"['daoan zhang', 'che jiang', 'ruoshi xu', 'biaoxiang chen', 'zijian jin', 'yutian lu', 'jianguo zhang', 'liang yong', 'jiebo luo', 'shengda luo']"
2505.0153,automated parsing of engineering drawings for structured information   extraction using a fine-tuned document understanding transformer,cs.cv cs.ai,"accurate extraction of key information from 2d engineering drawings is crucial for high-precision manufacturing. manual extraction is time-consuming and error-prone, while traditional optical character recognition (ocr) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. to address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (obb) detection model with a transformer-based document parsing model (donut). an in-house annotated dataset is used to train yolov11 for detecting nine key categories: geometric dimensioning and tolerancing (gd&t), general tolerances, measures, materials, notes, radii, surface roughness, threads, and title blocks. detected obbs are cropped into images and labeled to fine-tune donut for structured json output. fine-tuning strategies include a single model trained across all categories and category-specific models. results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for gd&t), recall (100% for most), and f1 score (97.3%), while reducing hallucination (5.23%). the proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.",,2025-05-02,,"['muhammad tayyab khan', 'zane yong', 'lequn chen', 'jun ming tan', 'wenhe feng', 'seung ki moon']"
2505.01548,rethinking rgb-event semantic segmentation with a novel bidirectional   motion-enhanced event representation,cs.cv,"event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. however, rgb-event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of rgb modality. to tackle these challenges, we propose a novel event representation, motion-enhanced event tensor (met), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. in addition, we introduce a frequency-aware bidirectional flow aggregation module (bfam) and a temporal fusion module (tfm). bfam leverages the frequency domain and met to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art rgb-event semantic segmentation approaches. our code is available at: https://github.com/zyaocoder/brenet.",,2025-05-02,,"['zhen yao', 'xiaowen ying', 'mooi choo chuah']"
2505.01558,a sensor agnostic domain generalization framework for leveraging   geospatial foundation models: enhancing semantic segmentation viasynergistic   pseudo-labeling and generative learning,cs.cv,"remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. domain adaptation offers a promising solution to improve model generalization. this paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. we further provide new mathematical insights into mae-based generative learning for domain-invariant feature learning. experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.",,2025-05-02,,"['anan yaghmour', 'melba m. crawford', 'saurabh prasad']"
2505.01578,grounding task assistance with multimodal cues from a single   demonstration,cs.cv,"a person's demonstration often serves as a key reference for others learning the same task. however, rgb video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. this sensory gap fundamentally limits the ability of vision language models (vlms) to reason about why actions occur and how they should adapt to individual users. to address this, we introduce mica (multimodal interactive contextualized assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. mica segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. these results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world ai task assistance.",,2025-05-02,,"['gabriel sarch', 'balasaravanan thoravi kumaravel', 'sahithya ravi', 'vibhav vineet', 'andrew d. wilson']"
2505.01583,tempura: temporal event masked prediction and understanding for   reasoning in action,cs.cv cs.ai,"understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. we propose tempura (temporal event masked prediction and understanding for reasoning in action), a two-stage training framework that enhances video temporal understanding. tempura first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. tempura then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. we train tempura on ver, a large-scale dataset curated by us that comprises 1m training instances and 500k videos with temporally aligned event descriptions and structured reasoning steps. experiments on temporal grounding and highlight detection benchmarks demonstrate that tempura outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.",,2025-05-02,,"['jen-hao cheng', 'vivian wang', 'huayu wang', 'huapeng zhou', 'yi-hao peng', 'hou-i liu', 'hsiang-wei huang', 'kuang-ming chen', 'cheng-yen yang', 'wenhao chai', 'yi-ling chen', 'vibhav vineet', 'qin cai', 'jenq-neng hwang']"
2505.01615,multimodal and multiview deep fusion for autonomous marine navigation,cs.cv cs.ai,we propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. the model deeply fuses multiview rgb and long wave infrared images with sparse lidar point clouds. training also integrates x band radar and electronic chart data to inform predictions. the resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.,,2025-05-02,,"['dimitrios dagdilelis', 'panagiotis grigoriadis', 'roberto galeazzi']"
2505.01638,seeing heat with color -- rgb-only wildfire temperature inference from   sam-guided multimodal distillation using radiometric ground truth,eess.iv cs.ai cs.cv,"high-fidelity wildfire monitoring using unmanned aerial vehicles (uavs) typically requires multimodal sensing - especially rgb and thermal imagery - which increases hardware cost and power consumption. this paper introduces sam-tiff, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using rgb input only. a multimodal teacher network trained on paired rgb-thermal imagery and radiometric tiff ground truth distills knowledge to a unimodal rgb student network, enabling thermal-sensor-free inference. segmentation supervision is generated using a hybrid approach of segment anything (sam)-guided mask generation, and selection via topsis, along with canny edge detection and otsu's thresholding pipeline for automatic point prompt selection. our method is the first to perform per-pixel temperature regression from rgb uav data, demonstrating strong generalization on the recent flame 3 dataset. this work lays the foundation for lightweight, cost-effective uav-based wildfire monitoring systems without thermal sensors.",,2025-05-02,,"['michael marinaccio', 'fatemeh afghah']"
2505.01644,a dual-task synergy-driven generalization framework for pancreatic   cancer segmentation in ct scans,eess.iv cs.cv,"pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. the generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. this framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model's generalization ability from a dual-task perspective. besides, dual self-supervised learning in feature spaces and output spaces augments the model's representational capability and stability across different imaging views. experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (dice: 84.07%). more importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. the codes will be released at https://github.com/sjtubme-qianlab/dual-task-seg.",10.1109/tmi.2025.3566376,2025-05-02,,"['jun li', 'yijue zhang', 'haibo shi', 'minhong li', 'qiwei li', 'xiaohua qian']"
2505.0165,toward onboard ai-enabled solutions to space object detection for space   sustainability,cs.cv eess.iv,"the rapid expansion of advanced low-earth orbit (leo) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. a solution to the challenge is effective space object detection (sod) for collision assessment and avoidance. in sod, an leo satellite must detect other satellites and objects with high precision and minimal delay. this paper investigates the feasibility and effectiveness of employing vision sensors for sod tasks based on deep learning (dl) models. it introduces models based on the squeeze-and-excitation (se) layer, vision transformer (vit), and the generalized efficient layer aggregation network (gelan) and evaluates their performance under sod scenarios. experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (map50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (map50:95) scores of up to 0.280. compared to the baseline gelan-t model, the proposed gelan-vit-se model increases the average map50 from 0.721 to 0.751, improves the map50:95 from 0.266 to 0.274, reduces giga floating point operations (gflops) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mw to 2028.7 mw by 2.5\%.",,2025-05-02,,"['wenxuan zhang', 'peng hu']"
2505.01656,a novel waveinst-based network for tree trunk structure extraction and   pattern analysis in forest inventory,cs.cv,"the pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. the current trunk and branch extraction technologies are mainly lidar-based or uav-based. the former approaches obtain high-precision 3d data, but its equipment cost is high and the three-dimensional (3d) data processing is complex. the latter approaches efficiently capture canopy information, but they miss the 3-d structure of trees. in order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel waveinst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. experimental results of the proposed model show superior performance on synthtree43k, canetree100, urban street and our poplardataset. moreover, we present a new phenotypic dataset poplardataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. the proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2d images directly. this study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.",,2025-05-02,,"['chenyang fan', 'xujie zhu', 'taige luo', 'sheng xu', 'zhulin chen', 'hongxin yang']"
2505.01657,ragar: retrieval augment personalized image generation guided by   recommendation,cs.ir cs.cv,"personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. although effective, existing methods face two main issues. first, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. to address these issues, we propose retrieval augment personalized image generation guided by recommendation (ragar). our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. extensive experiments and human evaluations on three real-world datasets demonstrate that ragar achieves significant improvements in both personalization and semantic metrics compared to five baselines.",,2025-05-02,,"['run ling', 'wenji wang', 'yuting liu', 'guibing guo', 'linying jiang', 'xingwei wang']"
2505.01664,soft-masked semi-dual optimal transport for partial domain adaptation,cs.cv cs.ai,"visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. partial domain adaptation (pda) is a general and practical scenario in which the target label space is a subset of the source one. the challenges of pda exist due to not only domain shift but also the non-identical label spaces of domains. in this paper, a soft-masked semi-dual optimal transport (ssot) method is proposed to deal with the pda problem. specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. a soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. to deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized kantorovich problem is employed since it can be optimized by gradient-based algorithms. further, a neural network is exploited to approximate the kantorovich potential due to its strong fitting ability. this network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. the ssot model is built upon neural networks, which can be optimized alternately in an end-to-end manner. extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of ssot.",,2025-05-02,,"['yi-ming zhai', 'chuan-xian ren', 'hong yan']"
2505.0167,efficient multi subject visual reconstruction from fmri using aligned   representations,eess.iv cs.cv cs.lg,"this work introduces a novel approach to fmri-based visual image reconstruction using a subject-agnostic common representation space. we show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. this is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. our approach excels in low-data scenarios. we evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.",,2025-05-02,,"['christos zangos', 'danish ebadulla', 'thomas christopher sprague', 'ambuj singh']"
2505.0168,"automated arat scoring using multimodal video analysis, multi-view   fusion, and hierarchical bayesian models: a clinician study",cs.cv cs.ai cs.hc math.pr,"manual scoring of the action research arm test (arat) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. we propose an automated arat scoring system integrating multimodal video analysis with slowfast, i3d, and transformer-based models using openpose keypoints and object locations. our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. hierarchical bayesian models (hbms) infer movement quality components, enhancing interpretability. a clinician dashboard displays task scores, execution times, and quality assessments. we conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with hbms aligning closely with manual assessments. this work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.",,2025-05-03,,"['tamim ahmed', 'thanassis rikakis']"
2505.01694,topology-aware clip few-shot learning,cs.cv cs.ai,"efficiently adapting large vision-language models (vlms) like clip for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. existing methods often overlook valuable structural information within the vlm's latent space. we introduce a topology-aware tuning approach integrating representation topology divergence (rtd) into the task residual (tr) framework. by explicitly aligning the topological structures of visual and text representations using a combined rtd and cross-entropy loss, while freezing base vlm encoders, our method enhances few-shot performance. we optimize only lightweight task residual parameters, effectively leveraging topological information. across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\% over relevant baseline methods in few-shot settings. this work presents an effective strategy to boost vlm few-shot capabilities by incorporating topological alignment.",,2025-05-03,,['dazhi huang']
2505.01699,component-based fairness in face attribute classification with bayesian   network-informed meta learning,cs.cv cs.ai,"the widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. while previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. in this paper, we focus on face component fairness, a fairness notion defined by biological face features. to our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. in this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. to address these issues, we propose \textbf{b}ayesian \textbf{n}etwork-informed \textbf{m}eta \textbf{r}eweighting (bnmr), which incorporates a bayesian network calibrator to guide an adaptive meta-learning-based sample reweighting process. during the training process of our approach, the bayesian network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. to demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. our results show that bnmr is able to consistently outperform recent face bias mitigation baselines. moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \textit{gender}). our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. the code for our work is publicly available~\footnote{https://github.com/yliuaa/bnmr-faircompface.git}.",,2025-05-03,,"['yifan liu', 'ruichen yao', 'yaokun liu', 'ruohan zong', 'zelin li', 'yang zhang', 'dong wang']"
2505.01709,robridge: a hierarchical architecture bridging cognition and execution   for general robotic manipulation,cs.ro cs.ai cs.cv,"operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. while recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. existing methods often compromise cognitive and executive capabilities. to address these challenges, in this paper, we propose robridge, a hierarchical intelligent architecture for general robotic manipulation. it consists of a high-level cognitive planner (hcp) based on a large-scale pre-trained vision-language model (vlm), an invariant operable representation (ior) serving as a symbolic bridge, and a generalist embodied agent (gea). robridge maintains the declarative skill of vlm and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. robridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. this work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.",,2025-05-03,2025-05-07,"['kaidong zhang', 'rongtao xu', 'pengzhen ren', 'junfan lin', 'hefeng wu', 'liang lin', 'xiaodan liang']"
2505.01711,knowledge-augmented language models interpreting structured chest x-ray   findings,cs.cv,"automated interpretation of chest x-rays (cxr) is a critical task with the potential to significantly improve clinical workflow and patient care. while recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (llms) for this visual task remains an underexplored area. this paper introduces cxr-textinter, a novel framework that repurposes powerful text-centric llms for cxr interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. we augment this llm-centric approach with an integrated medical knowledge module to enhance clinical reasoning. to facilitate training and evaluation, we developed the mediinstruct-cxr dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the cxr-clineval benchmark for comprehensive assessment across various interpretation tasks. extensive experiments on cxr-clineval demonstrate that cxr-textinter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. ablation studies confirm the critical contribution of the knowledge integration module. furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by cxr-textinter. our work validates an alternative paradigm for medical image ai, showcasing the potential of harnessing advanced llm capabilities when visual information is effectively structured and domain knowledge is integrated.",,2025-05-03,,"['alexander davis', 'rafael souza', 'jia-hao lim']"
2505.01713,vision and intention boost large language model in long-term action   anticipation,cs.cv,"long-term action anticipation (lta) aims to predict future actions over an extended period. previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. recent researches leverage large language models (llms) by utilizing text-based inputs which suffer severe information loss. to tackle these limitations single-modality methods face, we propose a novel intention-conditioned vision-language (icvl) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of llms. considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (vlm) to infer behavioral intentions as comprehensive textual features directly from video inputs. the inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. these enhanced visual representations, along with textual prompts, are fed into llm for future action anticipation. furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. extensive experiments with state-of-the-art performance on ego4d, epic-kitchens-55, and egtea gaze+ datasets fully demonstrate the effectiveness and superiority of the proposed method.",,2025-05-03,,"['congqi cao', 'lanshu hu', 'yating yu', 'yanning zhang']"
2505.01729,posepilot: steering camera pose for generative world models with   self-supervised depth,cs.cv,"recent advancements in autonomous driving (ad) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. however, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. in this paper, we introduce posepilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. drawing inspiration from self-supervised depth estimation, posepilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. these outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. to further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. extensive experiments on autonomous driving and general-domain video datasets demonstrate that posepilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. by steering camera pose with self-supervised depth, posepilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.",,2025-05-03,,"['bu jin', 'weize li', 'baihan yang', 'zhenxin zhu', 'junpeng jiang', 'huan-ang gao', 'haiyang sun', 'kun zhan', 'hengtong hu', 'xueyang zhang', 'peng jia', 'hao zhao']"
2505.01737,learning multi-frame and monocular prior for estimating geometry in   dynamic scenes,cs.cv,"in monocular videos that capture dynamic scenes, estimating the 3d geometry of video contents has been a fundamental challenge in computer vision. specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. to address the challenge, we present a new model, coined mmp, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. specifically, based on the recent siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. in our experiments, we find mmp can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error.",,2025-05-03,,"['seong hyeon park', 'jinwoo shin']"
2505.01741,clog-cd: curriculum learning based on oscillating granularity of class   decomposed medical image classification,eess.iv cs.cv,"curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. it has the ability to improve the final model's performance and accelerate the training process. however, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. in this paper, we present a novel convolutional neural network (cnn) training method based on the curriculum learning strategy and the class decomposition approach, which we call clog-cd, to improve the performance of medical image classification. we evaluated our method on four different imbalanced medical image datasets, such as chest x-ray (cxr), brain tumour, digital knee x-ray, and histopathology colorectal cancer (crc). clog-cd utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e., anti-curriculum technique). we also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. we used two pre-trained networks, resnet-50 and densenet-121, as the backbone for clog-cd. the results with resnet-50 show that clog-cd has the ability to improve classification performance with an accuracy of 96.08% for the cxr dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee x-ray, and 99.17% for the crc dataset, compared to other training strategies. in addition, with densenet-121, clog-cd has achieved 94.86%, 94.63%, 76.19%, and 99.45% for cxr, brain tumour, digital knee x-ray, and crc datasets, respectively",10.1109/tetc.2025.3562620,2025-05-03,,"['asmaa abbas', 'mohamed gaber', 'mohammed m. abdelsamea']"
2505.01743,an llm-empowered low-resolution vision system for on-device human   behavior understanding,cs.cv cs.ai cs.lg,"the rapid advancements in large vision language models (lvlms) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (hbu) in low-resolution vision systems, such as depth, thermal, and infrared. however, existing large vision language model (lvlm) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as rgb images. a quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. in this paper, we propose a novel, labor-saving system, llambda, designed to support low-resolution hbu. the core idea is to leverage limited labeled data and a large amount of unlabeled data to guide llms in generating informative captions, which can be combined with raw data to effectively fine-tune lvlm models for understanding low-resolution videos in hbu. first, we propose a contrastive-oriented data labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. second, we propose a physical-knowledge guided captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. therefore, it can improve llms' understanding of sequential data and then generate high-quality video captions. finally, to ensure on-device deployability, we employ lora-based efficient fine-tuning to adapt lvlms for low-resolution data. we evaluate llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that llambda outperforms several state-of-the-art lvlm systems up to $40.03\%$ on average bert-score.",,2025-05-03,,"['siyang jiang', 'bufang yang', 'lilin xu', 'mu yuan', 'yeerzhati abudunuer', 'kaiwei liu', 'liekang zeng', 'hongkai chen', 'zhenyu yan', 'xiaofan jiang', 'guoliang xing']"
2505.01746,co$^{3}$gesture: towards coherent concurrent co-speech 3d gesture   generation with interactive diffusion,cs.cv,"generating gestures from human speech has gained tremendous progress in animating virtual avatars. while the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. to fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7m frames for diverse two-person interactive posture sequences, dubbed ges-inter. additionally, we propose co$^3$gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a temporal interaction module (tim). tim can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected ges-inter dataset. the dataset and source code are publicly available at \href{https://mattie-e.github.io/co3/}{\textit{https://mattie-e.github.io/co3/}}.",,2025-05-03,,"['xingqun qi', 'yatian wang', 'hengyuan zhang', 'jiahao pan', 'wei xue', 'shanghang zhang', 'wenhan luo', 'qifeng liu', 'yike guo']"
2505.01755,lensnet: an end-to-end learning framework for empirical point spread   function modeling and lensless imaging reconstruction,eess.iv cs.cv,"lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. however, such systems are fundamentally governed by the point spread function (psf), which dictates how a point source contributes to the final captured signal. traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate psf models. these rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. in this paper, we propose lensnet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. central to our approach is a learnable coded mask simulator (cms) that enables dynamic, data-driven estimation of the psf during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. by embedding a wiener filtering component, lensnet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. extensive experiments demonstrate lensnet's robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. the proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. the link of code is https://github.com/baijiesong/lensnet.",,2025-05-03,,"['jiesong bai', 'yuhao yin', 'yihang dong', 'xiaofeng zhang', 'chi-man pun', 'xuhang chen']"
2505.01766,multimodal graph representation learning for robust surgical workflow   recognition with adversarial feature disentanglement,cs.cv cs.ro,"surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. however, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. in this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. we propose a multimodal graph representation network with adversarial feature disentanglement (grad) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. specifically, we introduce a multimodal disentanglement graph network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. to align feature spaces across modalities, we propose a vision-kinematic adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. furthermore, we design a contextual calibrated decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.",,2025-05-03,,"['long bai', 'boyi ma', 'ruohan wang', 'guankun wang', 'beilei cui', 'zhongliang jiang', 'mobarakol islam', 'zhe min', 'jiewen lai', 'nassir navab', 'hongliang ren']"
2505.01768,continuous filtered backprojection by learnable interpolation network,eess.iv cs.cv,"accurate reconstruction of computed tomography (ct) images is crucial in medical imaging field. however, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. in this study, to address this issue, we propose a novel deep learning model, named leanable-interpolation-based fbp or linfbp shortly, to enhance the reconstructed ct image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (fbp) and alleviates the interpolation errors. specifically, in the proposed linfbp, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in fbp. extensive experiments, which encompass diverse ct scenarios, demonstrate the effectiveness of the proposed linfbp in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability.",,2025-05-03,,"['hui lin', 'dong zeng', 'qi xie', 'zerui mao', 'jianhua ma', 'deyu meng']"
2505.0179,enhancing the learning experience: using vision-language models to   generate questions for educational videos,cs.cv cs.cl cs.mm,"web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. however, improving user engagement and knowledge retention remains a challenge. automatically generated questions can activate learners and support their knowledge acquisition. further, they can help teachers and learners assess their understanding. while large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. in this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. we assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. we identify requirements for future multimodal datasets and outline promising research directions.",,2025-05-03,,"['markos stamatakis', 'joshua berger', 'christian wartena', 'ralph ewerth', 'anett hoppe']"
2505.01799,aquags: fast underwater scene reconstruction with sfm-free gaussian   splatting,cs.cv,"underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3d models from images captured by underwater platforms. however, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of structure-from-motion (sfm) pose estimation, leading to subsequent reconstruction failures. additionally, sfm methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. in this paper, we introduce aquags, an sfm-free underwater scene reconstruction model based on the seathru algorithm, which facilitates rapid and accurate separation of scene details and medium features. our approach initializes gaussians by integrating state-of-the-art multi-view stereo (mvs) technology, employs implicit neural radiance fields (nerf) for rendering translucent media and utilizes the latest explicit 3d gaussian splatting (3dgs) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.",,2025-05-03,,"['junhao shi', 'jisheng xu', 'jianping he', 'zhiliang lin']"
2505.01802,efficient 3d full-body motion generation from sparse tracking inputs   with temporal windows,cs.cv,"to have a seamless user experience on immersive ar/vr applications, the importance of efficient and effective neural network (nn) models is undeniable, since missing body parts that cannot be captured by limited sensors should be generated using these models for a complete 3d full-body reconstruction in virtual environment. however, the state-of-the-art nn-models are typically computational expensive and they leverage longer sequences of sparse tracking inputs to generate full-body movements by capturing temporal context. inevitably, longer sequences increase the computation overhead and introduce noise in longer temporal dependencies that adversely affect the generation performance. in this paper, we propose a novel multi-layer perceptron (mlp)-based method that enhances the overall performance while balancing the computational cost and memory overhead for efficient 3d full-body generation. precisely, we introduce a nn-mechanism that divides the longer sequence of inputs into smaller temporal windows. later, the current motion is merged with the information from these windows through latent representations to utilize the past context for the generation. our experiments demonstrate that generation accuracy of our method with this nn-mechanism is significantly improved compared to the state-of-the-art methods while greatly reducing computational costs and memory overhead, making our method suitable for resource-constrained devices.",,2025-05-03,,"['georgios fotios angelis', 'savas ozkan', 'sinan mutlu', 'paul wisbey', 'anastasios drosou', 'mete ozay']"
2505.01805,not every tree is a forest: benchmarking forest types from satellite   remote sensing,cs.cv,"developing accurate and reliable models for forest types mapping is critical to support efforts for halting deforestation and for biodiversity conservation (such as european union deforestation regulation (eudr)). this work introduces forty, a benchmark for global-scale forest types mapping using multi-temporal satellite data1. the benchmark comprises 200,000 time series of image patches, each consisting of sentinel-2, sentinel-1, climate, and elevation data. each time series captures variations at monthly or seasonal cadence. per-pixel annotations, including forest types and other land use classes, support image segmentation tasks. unlike most existing land use products that often categorize all forest areas into a single class, our benchmark differentiates between three forest types classes: natural forest, planted forest, and tree crops. by leveraging multiple public data sources, we achieve global coverage with this benchmark. we evaluate the forest types dataset using several baseline models, including convolution neural networks and transformer-based models. additionally, we propose a novel transformer-based model specifically designed to handle multi-modal, multi-temporal satellite data for forest types mapping. our experimental results demonstrate that the proposed model surpasses the baseline models in performance.",,2025-05-03,,"['yuchang jiang', 'maxim neumann']"
2505.01809,3dwg: 3d weakly supervised visual grounding via category and   instance-level alignment,cs.cv,"the 3d weakly-supervised visual grounding task aims to localize oriented 3d boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. this setting presents two primary challenges: category-level ambiguity and instance-level complexity. category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. to address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. in the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. in the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. these designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: nr3d, sr3d, and scanref.",,2025-05-03,,"['xiaoqi li', 'jiaming liu', 'nuowei han', 'liang heng', 'yandong guo', 'hao dong', 'yang liu']"
2505.01831,multi-scale target-aware representation learning for fundus image   enhancement,eess.iv cs.cv,"high-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. recent years have witnessed promising progress in fundus image enhancement. however, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. to address these challenges, we propose a multi-scale target-aware representation learning framework (mtrl-fie) for efficient fundus image enhancement. specifically, we propose a multi-scale feature encoder (mfe) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. next, we design a structure-preserving hierarchical decoder (shd) to fuse multi-scale feature embeddings for real fundus image restoration. shd integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. meanwhile, a target-aware feature aggregation (tfa) module is used to enhance pathological regions and reduce artifacts. experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of mtrl-fie for fundus image enhancement. compared to state-of-the-art methods, mtrl-fie achieves superior enhancement performance with a more lightweight architecture. furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications.",,2025-05-03,,"['haofan wu', 'yin huang', 'yuqing wu', 'qiuyu yang', 'bingfang wang', 'li zhang', 'muhammad fahadullah khan', 'ali zia', 'm. saleh memon', 'syed sohail bukhari', 'abdul fattah memon', 'daizong ji', 'ya zhang', 'ghulam mustafa', 'yin fang']"
2505.01837,cvvnet: a cross-vertical-view network for gait recognition,cs.cv,"gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. while existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. our experiments show up to 60\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. current cnn and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. to tackle this challenge, we propose cvvnet (cross-vertical-view network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. cvvnet employs a high-low frequency extraction module (hlfe) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. we also introduce the dynamic gated aggregation (dga) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. the integration of our core multi-scale attention gated aggregation (msaga) module, hlfe and dga enables cvvnet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. experimental results show that our cvvnet achieves state-of-the-art performance, with $8.6\%$ improvement on dronegait and $2\%$ on gait3d compared with the best existing methods.",,2025-05-03,,"['xiangru li', 'wei song', 'yingda huang', 'wei meng', 'le chang']"
2505.01838,mvhumannet++: a large-scale dataset of multi-view daily dressing human   captures with richer annotations for 3d human digitization,cs.cv,"in this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. however, in the realm of 3d vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like objaverse and mvimgnet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. to bridge this gap, we present mvhumannet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. the primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2d and 3d keypoints, smpl/smplx parameters, and corresponding textual descriptions. additionally, the proposed mvhumannet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. to explore the potential of our proposed mvhumannet++ dataset in various 2d and 3d visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by mvhumannet++. as the current largest-scale 3d human dataset, we hope that the release of mvhumannet++ dataset with annotations will foster further innovations in the domain of 3d human-centric tasks at scale. mvhumannet++ is publicly available at https://kevinlee09.github.io/research/mvhumannet++/.",,2025-05-03,,"['chenghong li', 'hongjie liao', 'yihao zhi', 'xihe yang', 'zhengwentai sun', 'jiahao chang', 'shuguang cui', 'xiaoguang han']"
2505.01851,mitigating group-level fairness disparities in federated visual language   models,cs.cv,"visual language models (vlms) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (fl) environments. this paper addresses the critical issue of group fairness in federated vlms by introducing fvl-fp, a novel framework that combines fl with fair prompt tuning techniques. we focus on mitigating demographic biases while preserving model performance through three innovative components: (1) cross-layer demographic fair prompting (cdfp), which adjusts potentially biased embeddings through counterfactual regularization; (2) demographic subspace orthogonal projection (dsop), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) fair-aware prompt fusion (fpf), which dynamically balances client contributions based on both performance and fairness metrics. extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\% compared to standard fl approaches, while maintaining task performance within 6\% of state-of-the-art results. fvl-fp effectively addresses the challenges of non-iid data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.",,2025-05-03,,"['chaomeng chen', 'zitong yu', 'junhao dong', 'sen su', 'linlin shen', 'shutao xia', 'xiaochun cao']"
2505.01854,accelerating volumetric medical image annotation via short-long memory   sam 2,eess.iv cs.ai cs.cv,"manual annotation of volumetric medical images, such as magnetic resonance imaging (mri) and computed tomography (ct), is a labor-intensive and time-consuming process. recent advancements in foundation models for video object segmentation, such as segment anything model 2 (sam 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. however, the performance of sam 2 in this context varies. our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. to address this problem, we propose short-long memory sam 2 (slm-sam 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. we evaluate slm-sam 2 on three public datasets covering organs, bones, and muscles across mri and ct modalities. we show that the proposed method markedly outperforms the default sam 2, achieving average dice similarity coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. slm-sam 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development.",,2025-05-03,,"['yuwen chen', 'zafer yildiz', 'qihang li', 'yaqian chen', 'haoyu dong', 'hanxue gu', 'nicholas konz', 'maciej a. mazurowski']"
2505.01857,dualdiff: dual-branch diffusion model for autonomous driving with   semantic fusion,cs.cv,"accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. however, existing approaches, which primarily use 3d bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. in this paper, we propose dualdiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. we introduce occupancy ray sampling (ors), a semantic-rich 3d representation, alongside numerical driving scene representation, for comprehensive foreground and background control. to improve cross-modal information integration, we propose a semantic fusion attention (sfa) mechanism that aligns and fuses features across modalities. furthermore, we design a foreground-aware masked (fgm) loss to enhance the generation of tiny objects. dualdiff achieves state-of-the-art performance in fid score, as well as consistently better results in downstream bev segmentation and 3d object detection tasks.",,2025-05-03,,"['haoteng li', 'zhao yang', 'zezhong qian', 'gongpeng zhao', 'yuqi huang', 'jun yu', 'huazheng zhou', 'longjun liu']"
2505.01869,visual enhancement and 3d representation for underwater scenes: a review,cs.cv,"underwater visual enhancement (uve) and underwater 3d reconstruction pose significant challenges in   computer vision and ai-based tasks due to complex imaging conditions in aquatic environments. despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   uve and underwater 3d reconstruction remains absent. to advance research in these areas, we present an   in-depth review from multiple perspectives. first, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. we survey advanced methods for visual enhancement and   3d reconstruction specifically designed for underwater scenarios. the paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including neural radiance fields and 3d gaussian   splatting, discussing their effectiveness in handling underwater distortions. finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art uve and underwater 3d reconstruction algorithms across multiple   benchmark datasets. finally, we highlight key research directions for future advancements in underwater vision.",,2025-05-03,,"['guoxi huang', 'haoran wang', 'brett seymour', 'evan kovacs', 'john ellerbrock', 'dave blackham', 'nantheera anantrasirichai']"
2505.0188,weakly-supervised audio temporal forgery localization via progressive   audio-language co-learning network,cs.sd cs.cv cs.mm eess.as,"audio temporal forgery localization (atfl) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. existing atfl methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. to meet this challenge, in this paper, we propose a progressive audio-language co-learning network (loco) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. in this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. in addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. extensive experiments show that the proposed loco achieves sota performance on three public benchmarks.",,2025-05-03,2025-05-07,"['junyan wu', 'wenbo xu', 'wei lu', 'xiangyang luo', 'rui yang', 'shize guo']"
2505.01881,physnav-dg: a novel adaptive framework for robust vlm-sensor fusion in   navigation applications,cs.cv cs.ai cs.lg cs.mm cs.ro,"robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. we present physnav-dg, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. a modified adaptive kalman filter dynamically adjusts its noise parameters based on environmental context. it leverages several streams of raw sensor data along with semantic insights from models such as llama 3.2 11b and blip-2. to evaluate our approach, we introduce the md-nex benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. extensive experiments and ablations show that physnav-dg improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. this work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,2025-05-03,,"['trisanth srinivasan', 'santosh patapati']"
2505.01882,cmawrnet: multiple adverse weather removal via a unified quaternion   neural architecture,cs.cv,"images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. when designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. recently, deep-learning methods offered a solution for a single type of degradation. current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. this work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called cmawrnet. it is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. we also introduce a quaternion similarity loss function to preserve color information better. the quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed cmawrnet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. extensive computer simulations validate that cmawrnet improves the performance of downstream applications such as object detection. this is the first time the decomposition approach has been applied to the universal weather removal task.",,2025-05-03,,"['vladimir frants', 'sos agaian', 'karen panetta', 'peter huang']"
2505.01884,adversarial robustness of deep learning models for inland water body   segmentation from sar images,eess.iv cs.ai cs.cv cs.lg,"inland water body segmentation from synthetic aperture radar (sar) images is an important task needed for several applications, such as flood mapping. while sar sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from sar images is not straightforward. inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. u-net is a widely used deep learning model for land-water segmentation of sar images. in practice, manual annotation is often used to generate the corresponding water masks as ground truth. manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. in this work, we simulate manual errors in the form of adversarial attacks on the u-net model and study the robustness of the model to human errors in annotation. our results indicate that u-net can tolerate a certain level of corruption before its performance drops significantly. this finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. the code and the new dataset, along with adversarial examples for robust training, are publicly available. (github link - https://github.com/gvcl/iwseg-sar-poison.git)",,2025-05-03,2025-05-06,"['siddharth kothari', 'srinivasan murali', 'sankalp kothari', 'ujjwal verma', 'jaya sreevalsan-nair']"
2505.01888,rethinking score distilling sampling for 3d editing and generation,cs.cv,"score distillation sampling (sds) has emerged as a prominent method for text-to-3d generation by leveraging the strengths of 2d diffusion models. however, sds is limited to generation tasks and lacks the capability to edit existing 3d assets. conversely, variants of sds that introduce editing capabilities often can not generate new 3d assets effectively. in this work, we observe that the processes of generation and editing within sds and its variants have unified underlying gradient terms. building on this insight, we propose unified distillation sampling (uds), a method that seamlessly integrates both the generation and editing of 3d assets. essentially, uds refines the gradient terms used in vanilla sds methods, unifying them to support both tasks. extensive experiments demonstrate that uds not only outperforms baseline methods in generating 3d assets with richer details but also excels in editing tasks, thereby bridging the gap between 3d generation and editing. the code is available on: https://github.com/xingy038/uds.",,2025-05-03,,"['xingyu miao', 'haoran duan', 'yang long', 'jungong han']"
2505.01928,gensync: a generalized talking head framework for audio-driven   multi-subject lip-sync using 3d gaussian splatting,cs.cv,"we introduce gensync, a novel framework for multi-identity lip-synced video synthesis using 3d gaussian splatting. unlike most existing 3d methods that require training a new model for each identity , gensync learns a unified network that synthesizes lip-synced videos for multiple speakers. by incorporating a disentanglement module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. this design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",,2025-05-03,,"['anushka agarwal', 'muhammad yusuf hassan', 'talha chafekar']"
2505.01934,gaus-slam: dense rgb-d slam with gaussian surfels,cs.cv,"we propose gaus-slam, a dense rgb-d slam system that leverages 2d gaussian surfels to achieve robust tracking and high-fidelity mapping. our investigations reveal that gaussian-based scene representations exhibit geometry distortion under novel viewpoints, which significantly degrades the accuracy of gaussian-based tracking methods. these geometry inconsistencies arise primarily from the depth modeling of gaussian primitives and the mutual interference between surfaces during the depth blending. to address these, we propose a 2d gaussian-based incremental reconstruction strategy coupled with a surface-aware depth rendering mechanism, which significantly enhances geometry accuracy and multi-view consistency. additionally, the proposed local map design dynamically isolates visible surfaces during tracking, mitigating misalignment caused by occluded regions in global maps while maintaining computational efficiency with increasing gaussian density. extensive experiments across multiple datasets demonstrate that gaus-slam outperforms comparable methods, delivering superior tracking precision and rendering fidelity. the project page will be made available at https://gaus-slam.github.io.",,2025-05-03,,"['yongxin su', 'lin chen', 'kaiting zhang', 'zhongliang zhao', 'chenfeng hou', 'ziping yu']"
2505.01938,hybridgs: high-efficiency gaussian splatting data compression using   dual-channel sparse representation and point cloud encoder,cs.cv eess.iv,"most existing 3d gaussian splatting (3dgs) compression schemes focus on producing compact 3dgs representation via implicit data embedding. they have long coding times and highly customized data format, making it difficult for widespread deployment. this paper presents a new 3dgs compression framework called hybridgs, which takes advantage of both compact generation and standardized point cloud data encoding. hybridgs first generates compact and explicit 3dgs data. a dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. it then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. a simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. at the current stage, hybridgs does not include any modules aimed at improving 3dgs quality during generation. but experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. the code is publicly available at https://github.com/qi-yangsjtu/hybridgs.",,2025-05-03,,"['qi yang', 'le yang', 'geert van der auwera', 'zhu li']"
2505.0195,segment any rgb-thermal model with language-aided distillation,cs.cv cs.ai,"the recent segment anything model (sam) demonstrates strong instance segmentation performance across various downstream tasks. however, sam is trained solely on rgb data, limiting its direct applicability to rgb-thermal (rgb-t) semantic segmentation. given that rgb-t provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, sartm, which customizes the powerful sam for rgb-t semantic segmentation. our key idea is to unleash the potential of sam while introduce semantic understanding modules for rgb-t data pairs. specifically, our framework first involves fine tuning the original sam by adding extra lora layers, aiming at preserving sam's strong generalization and segmentation capabilities for downstream tasks. secondly, we introduce language information as guidance for training our sartm. to address cross-modal inconsistencies, we introduce a cross-modal knowledge distillation(cmkd) module that effectively achieves modality adaptation while maintaining its generalization capabilities. this semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. furthermore, we enhance the segmentation performance by adjusting the segmentation head of sam and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. extensive experiments are conducted across three multi-modal rgbt semantic segmentation benchmarks: mfnet, pst900, and fmb. both quantitative and qualitative results consistently demonstrate that the proposed sartm significantly outperforms state-of-the-art approaches across a variety of conditions.",,2025-05-03,,"['dong xing', 'xianxun zhu', 'wei zhou', 'qika lin', 'hang yang', 'yuqing wang']"
2505.01958,a comprehensive analysis for visual object hallucination in large   vision-language models,cs.cv cs.cl,"large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.",,2025-05-03,,"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']"
2505.01969,mc3d-ad: a unified geometry-aware reconstruction model for   multi-category 3d anomaly detection,cs.cv,"3d anomaly detection (ad) is a promising means of controlling the quality of manufactured products. however, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. therefore, this paper presents a novel unified model for multi-category 3d anomaly detection (mc3d-ad) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. first, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. this leads to local and global geometry-aware reconstructed feature tokens for the ad task. mc3d-ad is evaluated on two publicly available real3d-ad and anomaly-shapenet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement in object-level auroc over real3d-ad and anomaly-shapenet, respectively. the source code will be released upon acceptance.",,2025-05-03,,"['jiayi cheng', 'can gao', 'jie zhou', 'jiajun wen', 'tao dai', 'jinbao wang']"
2505.01973,visual dominance and emerging multimodal approaches in distracted   driving detection: a review of machine learning techniques,cs.cv,"distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. recent developments in machine learning (ml) and deep learning (dl) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. this systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ml/dl techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. the review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (cnns) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (rf) methods, offer privacy-aware alternatives. multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. these findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (adas) and road safety interventions.",,2025-05-03,,"['anthony dontoh', 'stephanie ivey', 'logan sirbaugh', 'andrews danyo', 'armstrong aboah']"
2505.01984,lifelong whole slide image analysis: online vision-language adaptation   and past-to-present gradient distillation,cs.cv,"whole slide images (wsis) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. however, the rapid growth of computational tasks involving wsis poses significant challenges. given that wsis are gigapixels in size, they present difficulties in terms of storage, processing, and model training. therefore, it is essential to develop lifelong learning approaches for wsi analysis. in scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. in this study, we introduce adafgrad, a method designed to enhance lifelong learning for whole-slide image (wsi) analysis. first, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. we construct a sequence of six tcga datasets for training and evaluation. experimental results show that adafgrad outperforms both state-of-the-art wsi-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). moreover, adafgrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules.",,2025-05-04,,"['doanh c. bui', 'hoai luan pham', 'vu trung duong le', 'tuan hai vu', 'van duy tran', 'khang nguyen', 'yasuhiko nakashima']"
2505.01986,drug classification based on x-ray spectroscopy combined with machine   learning,cs.cv,"the proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. traditional detection methods have high requirements for instruments and environments, making the operation complex. x-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. in this study, we constructed a classification model using convolutional neural networks (cnn), support vector machines (svm), and particle swarm optimization (pso) to classify and identify drugs based on their x-ray spectral profiles. in the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. we utilized cnn to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an svm model. we also utilized pso to optimize two critical initial parameters of the svm. the experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of pso and svm. therefore, the combined approach of x-ray absorption spectroscopy with cnn, pso, and svm provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application.",,2025-05-04,,"['yongming li', 'peng wang', 'bangdong han']"
2505.01996,always skip attention,cs.lg cs.cv,"we highlight a curious empirical result within modern vision transformers (vits). specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. this is in contrast to other elements of a vit that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\eg, cnns) exhibiting good performance in their absence. in this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. additionally, we propose token graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. we validate our approach in both supervised and self-supervised training methods.",,2025-05-04,,"['yiping ji', 'hemanth saratchandran', 'peyman moghaddam', 'simon lucey']"
2505.02001,hybrid image resolution quality metric (hirqm):a comprehensive   perceptual image quality assessment framework,eess.iv cs.cv,"traditional image quality assessment metrics like mean squared error and structural similarity index often fail to reflect perceptual quality under complex distortions. we propose the hybrid image resolution quality metric (hirqm), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. hirqm combines three components: probability density function for local pixel distribution analysis, multi-scale feature similarity for structural integrity across resolutions, and hierarchical deep image features using a pre-trained vgg16 network for semantic alignment with human perception. a dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. our contributions include a unified metric and dynamic weighting for better perceptual alignment. evaluated on tid2013 and live datasets, hirqm achieves pearson and spearman correlations of 0.92 and 0.90, outperforming traditional metrics. it excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration.",,2025-05-04,,['vineesh kumar reddy mondem']
2505.02005,learning heterogeneous mixture of scene experts for large-scale neural   radiance fields,cs.cv,"recent nerf methods on large-scale scenes have underlined the importance of scene decomposition for scalable nerfs. although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. in this paper, we introduce switch-nerf++, a heterogeneous mixture of hash experts (hmohe) network that addresses these challenges within a unified framework. it is a highly scalable nerf that learns heterogeneous decomposition and heterogeneous nerfs efficiently for large-scale scenes in an end-to-end manner. in our framework, a gating network learns to decomposes scenes and allocates 3d points to specialized nerf experts. this gating network is co-optimized with the experts, by our proposed sparsely gated mixture of experts (moe) nerf framework. we incorporate a hash-based gating network and distinct heterogeneous hash experts. the hash-based gating efficiently learns the decomposition of the large-scale scene. the distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. these design choices make our framework an end-to-end and highly scalable nerf solution for real-world large-scale scene modeling to achieve both quality and efficiency. we evaluate our accuracy and scalability on existing large-scale nerf datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from urbanbis. extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to switch-nerf. codes will be released in https://github.com/mizhenxing/switch-nerf.",,2025-05-04,,"['zhenxing mi', 'ping yin', 'xue xiao', 'dan xu']"
2505.02007,efficient noise calculation in deep learning-based mri reconstructions,cs.cv,"accelerated mri reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. noise analyses are central to mri reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. however, deep learning (dl)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. this work proposes a theoretically grounded, memory-efficient technique to calculate voxel-wise variance for quantifying uncertainty due to acquisition noise in accelerated mri reconstructions. our approach approximates noise covariance using the dl network's jacobian, which is intractable to calculate. to circumvent this, we derive an unbiased estimator for the diagonal of this covariance matrix (voxel-wise variance) and introduce a jacobian sketching technique to efficiently implement it. we evaluate our method on knee and brain mri datasets for both data- and physics-driven networks trained in supervised and unsupervised manners. compared to empirical references obtained via monte carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. our work reintroduces accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy dl-based mri. our code will be made publicly available upon acceptance.",,2025-05-04,,"['onat dalmaz', 'arjun d. desai', 'reinhard heckel', 'tolga çukur', 'akshay s. chaudhari', 'brian a. hargreaves']"
2505.02013,mllm-enhanced face forgery detection: a vision-language fusion solution,cs.cv,"reliable face forgery detection algorithms are crucial for countering the growing threat of deepfake-driven disinformation. previous research has demonstrated the potential of multimodal large language models (mllms) in identifying manipulated faces. however, existing methods typically depend on either the large language model (llm) alone or an external detector to generate classification results, which often leads to sub-optimal integration of visual and textual modalities. in this paper, we propose vlf-ffd, a novel vision-language fusion solution for mllm-enhanced face forgery detection. our key contributions are twofold. first, we present eff++, a frame-level, explainability-driven extension of the widely used faceforensics++ (ff++) dataset. in eff++, each manipulated video frame is paired with a textual annotation that describes both the forgery artifacts and the specific manipulation technique applied, enabling more effective and informative mllm training. second, we design a vision-language fusion network (vlf-net) that promotes bidirectional interaction between visual and textual features, supported by a three-stage training pipeline to fully leverage its potential. vlf-ffd achieves state-of-the-art (sota) performance in both cross-dataset and intra-dataset evaluations, underscoring its exceptional effectiveness in face forgery detection.",,2025-05-04,,"['siran peng', 'zipei wang', 'li gao', 'xiangyu zhu', 'tianshuo zhang', 'ajian liu', 'haoyuan zhang', 'zhen lei']"
2505.02018,r-bench: graduate-level multi-disciplinary benchmarks for llm & mllm   complex reasoning evaluation,cs.cv,"reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. in this paper, we introduce a graduate-level, multi-disciplinary, englishchinese benchmark, dubbed as reasoning bench (r-bench), for assessing the reasoning capability of both language and multimodal models. rbench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both english and chinese. these questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an olympiad-level multi-disciplinary benchmark. we evaluate widely used models, including openai o1, gpt-4o, deepseek-r1, etc. experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. even the top-performing model openai o1 achieves only 53.2% accuracy on our multimodal evaluation. data and code are made publicly available at here.",,2025-05-04,,"['meng-hao guo', 'jiajun xu', 'yi zhang', 'jiaxi song', 'haoyang peng', 'yi-xuan deng', 'xinzhi dong', 'kiyohiro nakayama', 'zhengyang geng', 'chen wang', 'bolin ni', 'guo-wei yang', 'yongming rao', 'houwen peng', 'han hu', 'gordon wetzstein', 'shi-min hu']"
2505.02025,a birotation solution for relative pose problems,cs.cv,"relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. in this article, we break the mold by tackling this traditional problem with a novel birotation solution. we first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be estimated and its corresponding basis transformation. three energy functions, designed based on these metrics, are then minimized on the riemannian manifold $\mathrm{so(3)}$ by iteratively updating the two rotation matrices. the two rotation matrices and the basis transformation corresponding to the minimum energy are ultimately utilized to recover the relative pose. extensive quantitative and qualitative evaluations across diverse relative pose estimation tasks demonstrate the superior performance of our proposed birotation solution. source code, demo video, and datasets will be available at \href{https://mias.group/birotation-solution}{mias.group/birotation-solution} upon publication.",,2025-05-04,,"['hongbo zhao', 'ziwei long', 'mengtan zhang', 'hanli wang', 'qijun chen', 'rui fan']"
2505.02046,a unet model for accelerated preprocessing of crism hyperspectral data   for mineral identification on mars,cs.cv,"accurate mineral identification on the martian surface is critical for understanding the planet's geological history. this paper presents a unet-based autoencoder model for efficient spectral preprocessing of crism mtrdr hyperspectral data, addressing the limitations of traditional methods that are computationally intensive and time-consuming. the proposed model automates key preprocessing steps, such as smoothing and continuum removal, while preserving essential mineral absorption features. trained on augmented spectra from the mica spectral library, the model introduces realistic variability to simulate mtrdr data conditions. by integrating this framework, preprocessing time for an 800x800 mtrdr scene is reduced from 1.5 hours to just 5 minutes on an nvidia t1600 gpu. the preprocessed spectra are subsequently classified using micanet, a deep learning model for martian mineral identification. evaluation on labeled crism trdr data demonstrates that the proposed approach achieves competitive accuracy while significantly enhancing preprocessing efficiency. this work highlights the potential of the unet-based preprocessing framework to improve the speed and reliability of mineral mapping on mars.",,2025-05-04,,"['priyanka kumari', 'sampriti soor', 'amba shetty', 'archana m. nair']"
2505.02048,regression is all you need for medical image translation,eess.iv cs.ai cs.cv,"the acquisition of information-rich images within a limited time budget is crucial in medical imaging. medical image translation (mit) can help enhance and supplement existing datasets by generating synthetic images from acquired data. while generative adversarial nets (gans) and diffusion models (dms) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. in fact, the imitation of acquisition noise or content hallucination hinder clinical utility. here, we introduce yoda (you only denoise once - or average), a novel 2.5d diffusion-based framework for volumetric mit. yoda unites diffusion and regression paradigms to produce realistic or noise-free outputs. furthermore, we propose expectation-approximation (expa) dm sampling, which draws inspiration from mri signal averaging. expa-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain mri and pelvic mri-ct - we show that diffusion and regression sampling yield similar results in practice. as such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. building on these insights, we demonstrate that yoda outperforms several state-of-the-art gan and dm methods. notably, yoda-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. our findings challenge the presumed advantages of dms in mit and pave the way for the practical application of mit in medical imaging.",,2025-05-04,2025-05-06,"['sebastian rassmann', 'david kügler', 'christian ewert', 'martin reuter']"
2505.02052,txp: reciprocal generation of ground pressure dynamics and activity   descriptions for improving human activity recognition,cs.ai cs.cv,"sensor-based human activity recognition (har) has predominantly focused on inertial measurement units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the har domain due to limited datasets. to bridge this gap, we propose to exploit generative foundation models with pressure-specific har techniques. specifically, we present a bidirectional text$\times$pressure model that uses generative foundation models to interpret pressure data as natural language. txp accomplishes two tasks: (1) text2pressure, converting activity text descriptions into pressure sequences, and (2) pressure2text, generating activity descriptions and classifications from dynamic pressure maps. leveraging pre-trained models like clip and llama 2 13b chat, txp is trained on our synthetic presslang dataset, containing over 81,100 text-pressure pairs. validated on real-world data for activities such as yoga and daily tasks, txp provides novel approaches to data augmentation and classification grounded in atomic actions. this consequently improved har performance by up to 12.4\% in macro f1 score compared to the state-of-the-art, advancing pressure-based har with broader applications and deeper insights into human movement.",,2025-05-04,,"['lala shakti swarup ray', 'lars krupp', 'vitor fortes rey', 'bo zhou', 'sungho suh', 'paul lukowicz']"
2505.02056,handling imbalanced pseudolabels for vision-language models with concept   alignment and confusion-aware calibrated margin,cs.cv cs.lg,"adapting vision-language models (vlms) to downstream tasks with pseudolabels has gained increasing attention. a major obstacle is that the pseudolabels generated by vlms tend to be imbalanced, leading to inferior performance. while existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. to fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. to mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. the core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the sota method. our code is avaliable at https://anonymous.4open.science/r/cap-c642/",,2025-05-04,,"['yuchen wang', 'xuefeng bai', 'xiucheng li', 'weili guan', 'liqiang nie', 'xinyang chen']"
2505.0206,transforming faces into video stories -- videoface2.0,cs.cv,"face detection and face recognition have been in the focus of vision community since the very beginnings. inspired by the success of the original videoface digitizer, a pioneering device that allowed users to capture video signals from any source, we have designed an advanced video analytics tool to efficiently create structured video stories, i.e. identity-based information catalogs. videoface2.0 is the name of the developed system for spatial and temporal localization of each unique face in the input video, i.e. face re-identification (reid), which also allows their cataloging, characterization and creation of structured video outputs for later downstream tasks. developed near real-time solution is primarily designed to be utilized in application scenarios involving tv production, media analysis, and as an efficient tool for creating large video datasets necessary for training machine learning (ml) models in challenging vision tasks such as lip reading and multimodal speech recognition. conducted experiments confirm applicability of the proposed face reid algorithm that is combining the concepts of face detection, face recognition and passive tracking-by-detection in order to achieve robust and efficient face reid. the system is envisioned as a compact and modular extensions of the existing video production equipment. presented results are based on test implementation that achieves between 18-25 fps on consumer type notebook. ablation experiments also confirmed that the proposed algorithm brings relative gain in the reduction of number of false identities in the range of 73%-93%. we hope that the presented work and shared code implementation will stimulate further interest in development of similar, application specific video analysis tools, and lower the entry barrier for production of high-quality multi-modal datasets in the future.",,2025-05-04,2025-05-08,"['branko brkljač', 'vladimir kalušev', 'branislav popović', 'milan sečujski']"
2505.02064,"rtv-bench: benchmarking mllm continuous perception, understanding and   reasoning through real-time video",cs.cv,"multimodal large language models (mllms) increasingly excel at perception, understanding, and reasoning. however, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. to bridge this gap, we introduce rtv-bench, a fine-grained benchmark for mllm real-time video analysis. rtv-bench uses three key principles: (1) multi-timestamp question answering (mtqa), where answers evolve with scene changes; (2) hierarchical question structure, combining basic and advanced queries; and (3) multi-dimensional evaluation, assessing the ability of continuous perception, understanding, and reasoning. rtv-bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality qa pairs. we evaluated leading mllms, including proprietary (gpt-4o, gemini 2.0), open-source offline (qwen2.5-vl, videollama3), and open-source real-time (vita-1.5, internlm-xcomposer2.5-omnilive) models. experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost rtv-bench performance, sometimes causing slight decreases. this underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with mllms. our benchmark toolkit is available at: https://github.com/ljungang/rtv-bench.",,2025-05-04,2025-05-05,"['shuhang xun', 'sicheng tao', 'jungang li', 'yibo shi', 'zhixin lin', 'zhanhui zhu', 'yibo yan', 'hanqian li', 'linghao zhang', 'shikang wang', 'yixin liu', 'hanbo zhang', 'ying ma', 'xuming hu']"
2505.02071,hierarchical compact clustering attention (coca) for unsupervised   object-centric learning,cs.cv cs.lg,"we propose the compact clustering attention (coca) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. coca is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as coca-net. at its core, coca utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. thanks to this strategy, coca-net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. additionally, coca-net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. we demonstrate coca-net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics.",,2025-05-04,,"['can küçüksözen', 'yücel yemez']"
2505.02075,benchmarking feature upsampling methods for vision foundation models   using interactive segmentation,cs.cv cs.ai cs.lg,"vision foundation models (vfms) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. as vfms' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. however, vfms typically produce low-resolution features, limiting their direct applicability in this context. one way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines vfm features resolution. to assess the effectiveness of this approach, we investigate interactive segmentation (is) as a novel benchmark for evaluating feature upsampling methods on vfms. due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, is creates a challenging environment that demands comprehensive visual scene understanding. our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves vfm features quality. the code is released at https://github.com/havrylovv/isegprobe",,2025-05-04,,"['volodymyr havrylov', 'haiwen huang', 'dan zhang', 'andreas geiger']"
2505.02079,handocc: nerf-based hand rendering with occupancy networks,cs.cv,"we propose handocc, a novel framework for hand rendering based upon occupancy. popular rendering methods such as nerf are often combined with parametric meshes to provide deformable hand models. however, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. the simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. it also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. this paper presents a pipeline for meshless 3d rendering, which we apply to the hands. by providing only a 3d skeleton, the desired appearance is extracted via a convolutional model. we do this by exploiting a nerf renderer conditioned upon an occupancy-based representation. the approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. on the benchmark interhand2.6m dataset, we achieved state-of-the-art results.",,2025-05-04,,"['maksym ivashechkin', 'oscar mendez', 'richard bowden']"
2505.02094,skillmimic-v2: learning robust and generalizable interaction skills from   sparse and noisy demonstrations,cs.lg cs.cv,"we address a fundamental challenge in reinforcement learning from interaction demonstration (rlid): demonstration noise and coverage limitations. while existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. building upon this insight, we present two data augmentation techniques: a stitched trajectory graph (stg) that discovers potential transitions between demonstration skills, and a state transition field (stf) that establishes unique connections for arbitrary states within the demonstration neighborhood. to enable effective rlid with augmented data, we develop an adaptive trajectory sampling (ats) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.",,2025-05-04,,"['runyi yu', 'yinhuai wang', 'qihan zhao', 'hok wai tsui', 'jingbo wang', 'ping tan', 'qifeng chen']"
2505.02108,signsplat: rendering sign language via gaussian splatting,cs.cv,"state-of-the-art approaches for conditional human body rendering via gaussian splatting typically focus on simple body motions captured from many views. this is often in the context of dancing or walking. however, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. the problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. the solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. we focus on how to achieve this, constraining mesh parameters to build an accurate gaussian splatting framework from few views capable of modelling subtle human motion. we leverage regularization techniques on the gaussian parameters to mitigate overfitting and rendering artifacts. additionally, we propose a new adaptive control method to densify gaussians and prune splat points on the mesh surface. to demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. on benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",,2025-05-04,,"['maksym ivashechkin', 'oscar mendez', 'richard bowden']"
2505.02109,unaligned rgb guided hyperspectral image super-resolution with   spatial-spectral concordance,cs.cv,"hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. the recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution hsis, presenting it as a favorable method. however, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. in this paper, we introduce a spatial-spectral concordance hyperspectral super-resolution (ssc-hsr) framework for unaligned reference rgb guided hsi sr to address the issues of inaccurate alignment and poor interactivity of the previous approaches. specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a two-stage image alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. to enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a feature aggregation module and an attention fusion module. in the feature aggregation module, we introduce an iterative deformable feature aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations.",,2025-05-04,,"['yingkai zhang', 'zeqiang lai', 'tao zhang', 'ying fu', 'chenghu zhou']"
2505.02134,hillie: human-in-the-loop training for low-light image enhancement,cs.cv,"developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (llie). in this paper, we propose a human-in-the-loop llie training framework that improves the visual quality of unsupervised llie model outputs through iterative training stages, named hillie. at each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. subsequently, we employ a tailored image quality assessment (iqa) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. with only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the iqa model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing llie results. extensive experiments demonstrate that our approach significantly improves unsupervised llie model performance in terms of both quantitative and qualitative performance. the code and collected ranking dataset will be available at https://github.com/labshuhanggu/hillie.",,2025-05-04,,"['xiaorui zhao', 'xinyue zhou', 'peibei cao', 'junyu lou', 'shuhang gu']"
2505.02147,local herb identification using transfer learning: a cnn-powered mobile   application for nepalese flora,cs.lg cs.cv,"herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as nepal. this study introduces a novel deep learning approach for classifying 60 different herb species using convolutional neural networks (cnns) and transfer learning techniques. using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. our research employed multiple model architectures, including densenet121, 50-layer residual network (resnet50), 16-layer visual geometry group network (vgg16), inceptionv3, efficientnetv2, and vision transformer (vit), with densenet121 ultimately demonstrating superior performance. data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. this work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.",,2025-05-04,,"['prajwal thapa', 'mridul sharma', 'jinu nyachhyon', 'yagya raj pandeya']"
2505.02148,spotting the unexpected (stu): a 3d lidar dataset for anomaly   segmentation in autonomous driving,cs.cv,"to operate safely, autonomous vehicles (avs) need to detect and handle unexpected objects or anomalies on the road. while significant research exists for anomaly detection and segmentation in 2d, research progress in 3d is underexplored. existing datasets lack high-quality multimodal data that are typically found in avs. this paper presents a novel dataset for anomaly segmentation in driving scenarios. to the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3d semantic labeling, incorporating both lidar and camera data, as well as sequential information to enable anomaly detection across various ranges. this capability is critical for the safe navigation of autonomous vehicles. we adapted and evaluated several baseline models for 3d segmentation, highlighting the challenges of 3d anomaly detection in driving environments. our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches.",,2025-05-04,,"['alexey nekrasov', 'malcolm burdorf', 'stewart worrall', 'bastian leibe', 'julie stephany berrio perez']"
2505.02159,"small clips, big gains: learning long-range refocused temporal   information for video super-resolution",cs.cv,"video super-resolution (vsr) can achieve better performance compared to single image super-resolution by additionally leveraging temporal information. in particular, the recurrent-based vsr model exploits long-range temporal information during inference and achieves superior detail restoration. however, effectively learning these long-term dependencies within long videos remains a key challenge. to address this, we propose lrti-vsr, a novel training framework for recurrent vsr that efficiently leverages long-range refocused temporal information. our framework includes a generic training strategy that utilizes temporal propagation features from long video clips while training on shorter video clips. additionally, we introduce a refocused intra&inter-frame transformer block which allows the vsr model to selectively prioritize useful temporal information through its attention module while further improving inter-frame information utilization in the ffn module. we evaluate lrti-vsr on both cnn and transformer-based vsr architectures, conducting extensive ablation studies to validate the contribution of each component. experiments on long-video test sets demonstrate that lrti-vsr achieves state-of-the-art performance while maintaining training and computational efficiency.",,2025-05-04,,"['xingyu zhou', 'wei long', 'jingbo lu', 'shiyin jiang', 'weiyi you', 'haifeng wu', 'shuhang gu']"
2505.02161,focus what matters: matchability-based reweighting for local feature   matching,cs.cv,"since the rise of transformers, many semi-dense matching methods have adopted attention mechanisms to extract feature descriptors. however, the attention weights, which capture dependencies between pixels or keypoints, are often learned from scratch. this approach can introduce redundancy and noisy interactions from irrelevant regions, as it treats all pixels or keypoints equally. drawing inspiration from keypoint selection processes, we propose to first classify all pixels into two categories: matchable and non-matchable. matchable pixels are expected to receive higher attention weights, while non-matchable ones are down-weighted. in this work, we propose a novel attention reweighting mechanism that simultaneously incorporates a learnable bias term into the attention logits and applies a matchability-informed rescaling to the input value features. the bias term, injected prior to the softmax operation, selectively adjusts attention scores based on the confidence of query-key interactions. concurrently, the feature rescaling acts post-attention by modulating the influence of each value vector in the final output. this dual design allows the attention mechanism to dynamically adjust both its internal weighting scheme and the magnitude of its output representations. extensive experiments conducted on three benchmark datasets validate the effectiveness of our method, consistently outperforming existing state-of-the-art approaches.",,2025-05-04,,['dongyue li']
2505.02175,sparsplat: fast multi-view reconstruction with generalizable 2d gaussian   splatting,cs.cv,"recovering 3d information from scenes via multi-view stereo reconstruction (mvs) and novel view synthesis (nvs) is inherently challenging, particularly in scenarios involving sparse-view setups. the advent of 3d gaussian splatting (3dgs) enabled real-time, photorealistic nvs. following this, 2d gaussian splatting (2dgs) leveraged perspective accurate 2d gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3d scene reconstruction while maintaining real-time performance. recent approaches have tackled the problem of sparse real-time nvs using 3dgs within a generalizable, mvs-based learning framework to regress 3d gaussian parameters. our work extends this line of research by addressing the challenge of generalizable sparse 3d reconstruction and nvs jointly, and manages to perform successfully at both tasks. we propose an mvs-based learning pipeline that regresses 2dgs surface element parameters in a feed-forward fashion to perform 3d shape reconstruction and nvs from sparse-view images. we further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. the resulting model attains the state-of-the-art results on the dtu sparse 3d reconstruction benchmark in terms of chamfer distance to ground-truth, as-well as state-of-the-art nvs. it also demonstrates strong generalization on the blendedmvs and tanks and temples datasets. we note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",,2025-05-04,,"['shubhendu jena', 'shishir reddy vutukur', 'adnane boukhayma']"
2505.02176,saliency-guided training for fingerprint presentation attack detection,cs.cv,"saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (pad) tasks. this paper presents its first application to fingerprint pad. we conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated ""pseudosaliency,"" including minutiae-based, image quality-based, and autoencoder-based saliency maps. evaluating on the 2021 fingerprint liveness detection competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. our findings demonstrate the effectiveness of saliency-guided training for fingerprint pad in both limited and large data contexts, and we present a configuration capable of earning the first place on the livdet-2021 benchmark. our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint pad. all collected saliency data and trained models are released with the paper to support reproducible research.",,2025-05-04,,"['samuel webster', 'adam czajka']"
2505.02178,sparfels: fast reconstruction from sparse unposed imagery,cs.cv,"we present a method for sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade gpu. while few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. differently, we propose an efficient and simple pipeline harnessing a single recent 3d foundation model. we leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2d gaussian splatting (2dgs) model, and image correspondences to guide camera optimization midst 2dgs training. key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. reducing this moment in training leads to more accurate shape reconstructions. we demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",,2025-05-04,,"['shubhendu jena', 'amine ouasfi', 'mae younes', 'adnane boukhayma']"
2505.02179,prodisc-vad: an efficient system for weakly-supervised anomaly detection   in video surveillance applications,cs.cv,"weakly-supervised video anomaly detection (ws-vad) using multiple instance learning (mil) suffers from label ambiguity, hindering discriminative feature learning. we propose prodisc-vad, an efficient framework tackling this via two synergistic components. the prototype interaction layer (pil) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. the pseudo-instance discriminative enhancement (pide) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). prodisc-vad achieves strong aucs (97.98% shanghaitech, 87.12% ucf-crime) using only 0.4m parameters, over 800x fewer than recent vit-based methods like vadclip, demonstrating exceptional efficiency alongside state-of-the-art performance. code is available at https://github.com/modadundun/prodisc-vad.",,2025-05-04,,"['tao zhu', 'qi yu', 'xinru dong', 'shiyu li', 'yue liu', 'jinlong jiang', 'lei shu']"
2505.02182,robust ai-generated face detection with imbalanced data,cs.cv,"deepfakes, created using advanced ai techniques such as variational autoencoder and generative adversarial networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. current deepfake detection techniques have evolved from cnn-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like clip, which capture global anomalies and improve cross-domain generalization. despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. to address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. the code is available at https://github.com/purdue-m2/sp_cup.",,2025-05-04,,"['yamini sri krubha', 'aryana hou', 'braden vester', 'web walker', 'xin wang', 'li lin', 'shu hu']"
2505.02192,dualreal: adaptive joint training for lossless identity-motion fusion in   video customization,cs.cv cs.ai,"customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. however, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. to address this, we introduce dualreal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. specifically, dualreal is composed of two units: (1) dual-aware adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) stageblender controller leverages the denoising stages and diffusion transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. we constructed a more comprehensive benchmark than existing methods. the experimental results show that dualreal improves clip-i and dino-i metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.",,2025-05-04,,"['wenchuan wang', 'mengqi huang', 'yijing tu', 'zhendong mao']"
2505.02211,csasn: a multitask attention-based framework for heterogeneous thyroid   carcinoma classification in ultrasound images,eess.iv cs.cv,"heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. to address this issue, we propose a novel multitask learning framework, channel-spatial attention synergy network (csasn), which integrates a dual-branch feature extractor - combining efficientnet for local spatial encoding and vit for global semantic modeling, with a cascaded channel-spatial attention refinement module. a residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as ftc and mtc carcinomas. experimental results show that csasn outperforms existing single-stream cnn or transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. this framework provides a promising strategy for ai-assisted thyroid cancer diagnosis.",,2025-05-04,,"['peiqi li', 'yincheng gao', 'renxing li', 'haojie yang', 'yunyun liu', 'boji liu', 'jiahui ni', 'ying zhang', 'yulu wu', 'xiaowei fang', 'lehang guo', 'liping sun', 'jiangang chen']"
2505.02236,improving physical object state representation in text-to-image   generative systems,cs.cv cs.ai,"current text-to-image generative models struggle to accurately represent object states (e.g., ""a table without a bottle,"" ""an empty tumbler""). in this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. next, we fine-tune several open-source text-to-image models on this synthetic data. we evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using gpt4o-mini, and achieve an average absolute improvement of 8+% across four models on the public genai-bench dataset. we also curate a collection of 200 prompts with a specific focus on common objects in various physical states. we demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. we release all evaluation prompts and code.",,2025-05-04,,"['tianle chen', 'chaitanya chakka', 'deepti ghadiyaram']"
2505.02242,quantizing diffusion models from a sampling-aware perspective,cs.cv,"diffusion models have recently emerged as the dominant approach in visual generation tasks. however, the lengthy denoising chains and the computationally intensive noise estimation networks hinder their applicability in low-latency and resource-limited environments. previous research has endeavored to address these limitations in a decoupled manner, utilizing either advanced samplers or efficient model quantization techniques. in this study, we uncover that quantization-induced noise disrupts directional estimation at each sampling step, further distorting the precise directional estimations of higher-order samplers when solving the sampling equations through discretized numerical methods, thereby altering the optimal sampling trajectory. to attain dual acceleration with high fidelity, we propose a sampling-aware quantization strategy, wherein a mixed-order trajectory alignment technique is devised to impose a more stringent constraint on the error bounds at each sampling step, facilitating a more linear probability flow. extensive experiments on sparse-step fast sampling across multiple datasets demonstrate that our approach preserves the rapid convergence characteristics of high-speed samplers while maintaining superior generation quality. code will be made publicly available soon.",,2025-05-04,,"['qian zeng', 'jie song', 'yuanyu wan', 'huiqiong wang', 'mingli song']"
2505.02246,cricket: a self-powered chirping pixel,cs.cv,"we present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. our sensor, called cricket, harvests energy from incident light. it is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. the carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. we have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. we have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. we show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. we also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.",10.1145/3658196,2025-05-04,,"['shree k. nayar', 'jeremy klotz', 'nikhil nanda', 'mikhail fridberg']"
2505.02278,compositional image-text matching and retrieval by grounding entities,cs.cv,"vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current vision-language models. while with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. however, a notable weakness of pretrained models like clip, is their inability to perform entity grounding and compositional image and text matching~\cite{jiang2024comclip, yang2023amc, rajabi2023groundedvsr, learninglocalizecvpr24}. in this work we propose a novel learning-free zero-shot augmentation of clip embeddings that has favorable compositional properties. we compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % the final embedding is obtained by computing a weighted combination of the sub-image embeddings. the resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\% improvement in image-text matching accuracy on the visual genome and svo probes datasets~\cite{krishna2017visualgenome, svo}. notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the flickr30k and ms-coco retrieval benchmarks~\cite{flickr30ke, mscoco}, improving the state-of-the-art recall@1 by 12\% and 0.4\%, respectively. our code is available at https://github.com/madhukarreddyvongala/groundingclip.",,2025-05-04,,"['madhukar reddy vongala', 'saurabh srivastava', 'jana košecká']"
2505.02287,continuous normalizing flows for uncertainty-aware human pose estimation,cs.cv,"human pose estimation (hpe) is increasingly important for applications like virtual reality and motion analysis, yet current methods struggle with balancing accuracy, computational efficiency, and reliable uncertainty quantification (uq). traditional regression-based methods assume fixed distributions, which might lead to poor uq. heatmap-based methods effectively model the output distribution using likelihood heatmaps, however, they demand significant resources. to address this, we propose continuous flow residual estimation (cfre), an integration of continuous normalizing flows (cnfs) into regression-based models, which allows for dynamic distribution adaptation. through extensive experiments, we show that cfre leads to better accuracy and uncertainty quantification with retained computational efficiency on both 2d and 3d human pose estimation tasks.",,2025-05-04,,"['shipeng liu', 'ziliang xiong', 'bastian wandt', 'per-erik forssén']"
2505.02304,generative sign-description prompts with multi-positive contrastive   learning for sign language recognition,cs.cl cs.cv,"sign language recognition (slr) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. to the best of our knowledge, this is the first work to integrate generative large language models (llms) into slr tasks. we propose a novel generative sign-description prompts multi-positive contrastive learning (gsp-mc) method that leverages retrieval-augmented generation (rag) with domain-specific llms, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. the gsp-mc method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. our approach combines global and part-level losses, optimizing kl divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. experiments demonstrate state-of-the-art performance against existing methods on the chinese slr500 (reaching 97.1%) and turkish autsl datasets (97.07% accuracy). the method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.",,2025-05-04,,"['siyu liang', 'yunan li', 'wentian xin', 'huizhou chen', 'xujie liu', 'kang liu', 'qiguang miao']"
2505.02325,teda: boosting vision-lanuage models for zero-shot 3d object retrieval   via testing-time distribution alignment,cs.cv,"learning discriminative 3d representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3d applications. existing well-established methods often struggle to attain this goal due to insufficient 3d training data from broader concepts. meanwhile, pre-trained large vision-language models (e.g., clip) have shown remarkable zero-shot generalization capabilities. yet, they are limited in extracting suitable 3d representations due to substantial gaps between their 2d training and 3d testing distributions. to address these challenges, we propose testing-time distribution alignment (teda), a novel framework that adapts a pretrained 2d vision-language model clip for unknown 3d object retrieval at test time. to our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3d feature learning. teda projects 3d objects into multi-view images, extracts features using clip, and refines 3d query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. additionally, teda integrates textual descriptions generated by a multimodal language model (internvl) to enhance 3d object understanding, leveraging clip's aligned feature space to fuse visual and textual cues. extensive experiments on four open-set 3d object retrieval benchmarks demonstrate that teda greatly outperforms state-of-the-art methods, even those requiring extensive training. we also experimented with depth maps on objaverse-lvis, further validating its effectiveness. code is available at https://github.com/wangzhichuan123/teda.",,2025-05-04,,"['zhichuan wang', 'yang zhou', 'jinhai xiang', 'yulong wang', 'xinwei he']"
2505.02331,vaemo: efficient representation learning for visual-audio emotion with   knowledge injection,cs.cv cs.sd,"audiovisual emotion recognition (aver) aims to infer human emotions from nonverbal visual-audio (va) cues, offering modality-complementary and language-agnostic advantages. however, aver remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. recent self-supervised aver approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. to address these issues, we propose vaemo, an efficient two-stage framework for emotion-centric joint va representation learning with external knowledge injection. in stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric va corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. in stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of va samples; these rich textual semantics are then injected by aligning their corresponding embeddings with va representations through dual-path contrastive learning, further bridging the emotion gap. extensive experiments on multiple downstream aver benchmarks show that vaemo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable va emotion representations.",,2025-05-04,,"['hao cheng', 'zhiwei zhao', 'yichao he', 'zhenzhen hu', 'jia li', 'meng wang', 'richang hong']"
2505.02335,6d pose estimation on spoons and hands,cs.cv,"accurate dietary monitoring is essential for promoting healthier eating habits. a key area of research is how people interact and consume food using utensils and hands. by tracking their position and orientation, it is possible to estimate the volume of food being consumed, or monitor eating behaviours, highly useful insights into nutritional intake that can be more reliable than popular methods such as self-reporting. hence, this paper implements a system that analyzes stationary video feed of people eating, using 6d pose estimation to track hand and spoon movements to capture spatial position and orientation. in doing so, we examine the performance of two state-of-the-art (sota) video object segmentation (vos) models, both quantitatively and qualitatively, and identify main sources of error within the system.",,2025-05-04,,"['kevin tan', 'fan yang', 'yuhao chen']"
2505.02364,quaternion infrared visible image fusion,cs.cv,"visible images provide rich details and color information only under well-lighted conditions while infrared images effectively highlight thermal targets under challenging conditions such as low visibility and adverse weather. infrared-visible image fusion aims to integrate complementary information from infrared and visible images to generate a high-quality fused image. existing methods exhibit critical limitations such as neglecting color structure information in visible images and performance degradation when processing low-quality color-visible inputs. to address these issues, we propose a quaternion infrared-visible image fusion (qivif) framework to generate high-quality fused images completely in the quaternion domain. qivif proposes a quaternion low-visibility feature learning model to adaptively extract salient thermal targets and fine-grained texture details from input infrared and visible images respectively under diverse degraded conditions. qivif then develops a quaternion adaptive unsharp masking method to adaptively improve high-frequency feature enhancement with balanced illumination. qivif further proposes a quaternion hierarchical bayesian fusion model to integrate infrared saliency and enhanced visible details to obtain high-quality fused images. extensive experiments across diverse datasets demonstrate that our qivif surpasses state-of-the-art methods under challenging low-visibility conditions.",,2025-05-05,,"['weihua yang', 'yicong zhou']"
2505.02365,quaternion multi-focus color image fusion,cs.cv,"multi-focus color image fusion refers to integrating multiple partially focused color images to create a single all-in-focus color image. however, existing methods struggle with complex real-world scenarios due to limitations in handling color information and intricate textures. to address these challenges, this paper proposes a quaternion multi-focus color image fusion framework to perform high-quality color image fusion completely in the quaternion domain. this framework introduces 1) a quaternion sparse decomposition model to jointly learn fine-scale image details and structure information of color images in an iterative fashion for high-precision focus detection, 2) a quaternion base-detail fusion strategy to individually fuse base-scale and detail-scale results across multiple color images for preserving structure and detail information, and 3) a quaternion structural similarity refinement strategy to adaptively select optimal patches from initial fusion results and obtain the final fused result for preserving fine details and ensuring spatially consistent outputs. extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods.",,2025-05-05,,"['weihua yang', 'yicong zhou']"
2505.02369,sharpness-aware minimization with z-score gradient filtering for neural   networks,cs.lg cs.ai cs.cv cs.it cs.ne math.it,"sharpness-aware minimization (sam) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. we introduce zsharp, a refined sharpness-aware optimization method that incorporates layer-wise z-score normalization followed by percentile-based filtering. this process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. zsharp retains the standard two-phase sam structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. we evaluate zsharp on cifar-10, cifar-100, and tiny-imagenet using a range of models including resnet, vgg, and vision transformers. across all architectures and datasets, zsharp consistently achieves higher test accuracy compared to sam, asam, and friendly-sam. these results indicate that z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.",,2025-05-05,2025-05-07,['juyoung yun']
2505.0237,superedit: rectifying and facilitating supervision for instruction-based   image editing,cs.cv cs.ai cs.lg,"due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (vlms) but fail to resolve this fundamental issue. in this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. this includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. based on these prior attributes, we define a unified guide for vlms to rectify editing instructions. however, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. to this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. our method does not require the vlm modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. compared with previous sota smartedit, we achieve 9.19% improvements on the real-edit benchmark with 30x less training data and 13x smaller model size.",,2025-05-05,,"['ming li', 'xin gu', 'fan chen', 'xiaoying xing', 'longyin wen', 'chen chen', 'sijie zhu']"
2505.02385,an arbitrary-modal fusion network for volumetric cranial nerves tract   segmentation,eess.iv cs.cv,"the segmentation of cranial nerves (cns) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual cns. multimodal cns tract segmentation networks, e.g., cntseg, which combine structural magnetic resonance imaging (mri) and diffusion mri, have achieved promising segmentation performance. however, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. in this work, we propose a novel arbitrary-modal fusion network for volumetric cns tract segmentation, called cntseg-v2, which trains one model to handle different combinations of available modalities. instead of directly combining all the modalities, we select t1-weighted (t1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. our model encompasses an arbitrary-modal collaboration module (acm) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of t1w images. meanwhile, we construct a deep distance-guided multi-stage (ddm) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. we evaluate our cntseg-v2 on the human connectome project (hcp) dataset and the clinical multi-shell diffusion mri (mdm) dataset. extensive experimental results show that our cntseg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods.",,2025-05-05,,"['lei xie', 'huajun zhou', 'junxiong huang', 'jiahao huang', 'qingrun zeng', 'jianzhong he', 'jiawei zhang', 'baohua fan', 'mingchu li', 'guoqiang xie', 'hao chen', 'yuanjing feng']"
2505.02388,metascenes: towards automated replica creation for real-world 3d scans,cs.cv cs.ai cs.lg cs.ro,"embodied ai (eai) research requires high-quality, diverse 3d scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. achieving these quality standards, however, necessitates the precise replication of real-world object diversity. existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. to scalably produce realistic and interactive 3d scenes, we first present metascenes, a large-scale, simulatable 3d scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. then, we introduce scan2sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3d scenes. we further propose two benchmarks to evaluate metascenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (vln) to validate cross-domain transfer. results confirm metascene's potential to enhance eai by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for eai research. project website: https://meta-scenes.github.io/.",,2025-05-05,,"['huangyue yu', 'baoxiong jia', 'yixin chen', 'yandan yang', 'puhao li', 'rongpeng su', 'jiaxin li', 'qing li', 'wei liang', 'song-chun zhu', 'tengyu liu', 'siyuan huang']"
2505.02393,uncertainty-weighted image-event multimodal fusion for video anomaly   detection,cs.cv,"most existing video anomaly detectors rely solely on rgb frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. to address this limitation, we propose image-event fusion for video anomaly detection (ief-vad), a framework that synthesizes event representations directly from rgb videos and fuses them with image features through a principled, uncertainty-aware process. the system (i) models heavy-tailed sensor noise with a student`s-t likelihood, deriving value-level inverse-variance weights via a laplace approximation; (ii) applies kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. without any dedicated event sensor or frame-level labels, ief-vad sets a new state of the art across multiple real-world anomaly detection benchmarks. these findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in rgb frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. code and models are available at https://github.com/eavnjeong/ief-vad.",,2025-05-05,2025-05-08,"['sungheon jeong', 'jihong park', 'mohsen imani']"
2505.02396,diagnostic uncertainty in pneumonia detection using cnn mobilenetv2 and   cnn from scratch,eess.iv cs.ai cs.cv,"pneumonia diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. this uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest x-rays, and the presence of co-existing respiratory conditions. this research proposes one of the supervised learning methods, cnn. using mobilenetv2 as the pre-trained one with resnet101v2 architecture and using keras api as the built from scratch model, for identifying lung diseases especially pneumonia. the datasets used in this research were obtained from the website through kaggle. the result shows that by implementing cnn mobilenetv2 and cnn from scratch the result is promising. while validating data, mobilenetv2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. nonetheless, mobilenetv2 is more stable. although it takes more time to train each epoch. meanwhile, after the 10th epoch, the scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. with these results, resnet101v2 offers stability, and the scratch model offers high accuracy.",,2025-05-05,,"['kennard norbert sudiardjo', 'islam nur alam', 'wilson wijaya', 'lili ayu wulandhari']"
2505.02405,estimating commonsense scene composition on belief scene graphs,cs.ro cs.cv,"this work establishes the concept of commonsense scene composition, with a focus on extending belief scene graphs by estimating the spatial distribution of unseen objects. specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. the proposed framework includes two variants of a correlation information (ceci) model for learning probability distributions: (i) a baseline approach based on a graph convolutional network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on large language models (llms). furthermore, this article provides a detailed description of the dataset generation process for such tasks. finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.",,2025-05-05,,"['mario a. v. saucedo', 'vignesh kottayam viswanathan', 'christoforos kanellakis', 'george nikolakopoulos']"
2505.02406,token coordinated prompt attention is needed for visual prompting,cs.cv,"visual prompting techniques are widely used to efficiently fine-tune pretrained vision transformers (vit) by learning a small set of shared prompts for all tokens. however, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of vit. this often leads to indistinguishable and biased prompt-extracted features, hindering performance. to address this issue, we propose a plug-and-play token coordinated prompt attention (tcpa) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. firstly, recognizing the distinct functions of cls and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into cls prompts and image prompts, which interact exclusively with cls tokens and image tokens through attention mechanisms. this enhances their respective discriminative abilities. furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. this enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. extensive experiments across various benchmarks demonstrate that tcpa significantly enhances the diversity and discriminative power of the extracted features. the code is available at https://github.com/zhoujiahuan1991/icml2025-tcpa.",,2025-05-05,2025-05-06,"['zichen liu', 'xu zou', 'gang hua', 'jiahuan zhou']"
2505.02448,recent advances in out-of-distribution detection with clip-like models:   a survey,cs.cv,"out-of-distribution detection (ood) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (id) data during testing. recent advances in ai, particularly vision-language models (vlms) like clip, have revolutionized ood detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. this shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of id images, adhering to a unimodal paradigm. to better align with clip's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. specifically, we categorize existing methods based on how visual and textual information of ood data is utilized within image + text modalities, and further divide them into four groups: ood images (i.e., outliers) seen or unseen, and ood texts (i.e., learnable vectors or class names) known or unknown, across two training strategies (i.e., train-free or training-required). more importantly, we discuss open problems in clip-like ood detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.",,2025-05-05,,"['chaohua li', 'enhao zhang', 'chuanxing geng', 'songcan chen']"
2505.02467,timing is everything: finding the optimal fusion points in multimodal   medical imaging,cs.cv cs.ai,"multimodal deep learning harnesses diverse imaging modalities, such as mri sequences, to enhance diagnostic accuracy in medical imaging. a key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. we propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. at each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. this process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. the approach is validated on two multimodal mri datasets, each addressing different classification tasks. our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. these architectures demonstrated superior accuracy, f-score, and specificity while maintaining competitive or improved auc values. furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. by systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. it provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical ai applications.",,2025-05-05,,"['valerio guarrasi', 'klara mogensen', 'sara tassinari', 'sara qvarlander', 'paolo soda']"
2505.02471,ming-lite-uni: advancements in unified architecture for natural   multimodal interaction,cs.cv,"we introduce ming-lite-uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. specifically, this project provides an open-source implementation of the integrated metaqueries and m2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. by leveraging a fixed mllm and a learnable diffusion model, ming-lite-uni enables native multimodal ar models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. our experimental results demonstrate the strong performance of ming-lite-uni and illustrate the impressive fluid nature of its interactive process. all code and model weights are open-sourced to foster further exploration within the community. notably, this work aligns with concurrent multimodal ai milestones - such as chatgpt-4o with native image generation updated in march 25, 2025 - underscoring the broader significance of unified models like ming-lite-uni on the path toward agi. ming-lite-uni is in alpha stage and will soon be further refined.",,2025-05-05,2025-05-07,"['inclusion ai', 'biao gong', 'cheng zou', 'dandan zheng', 'hu yu', 'jingdong chen', 'jianxin sun', 'junbo zhao', 'jun zhou', 'kaixiang ji', 'lixiang ru', 'libin wang', 'qingpei guo', 'rui liu', 'weilong chai', 'xinyu xiao', 'ziyuan huang']"
2505.02476,point cloud recombination: systematic real data augmentation using   robotic targets for lidar perception validation,cs.ro cs.cv eess.iv,"the validation of lidar-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. in contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. however, these methods do not consider validation and remain limited in controllability because they rely on empirical data. we solve these limitations by proposing point cloud recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3d meshes. using the ouster os1-128 rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. we show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. by providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.",,2025-05-05,,"['hubert padusinski', 'christian steinhauser', 'christian scherl', 'julian gaal', 'jacob langner']"
2505.02481,finger pose estimation for under-screen fingerprint sensor,cs.cv,"two-dimensional pose estimation plays a crucial role in fingerprint recognition by facilitating global alignment and reduce pose-induced variations. however, existing methods are still unsatisfactory when handling with large angle or small area inputs. these limitations are particularly pronounced on fingerprints captured by under-screen fingerprint sensors in smartphones. in this paper, we present a novel dual-modal input based network for under-screen fingerprint pose estimation. our approach effectively integrates two distinct yet complementary modalities: texture details extracted from ridge patches through the under-screen fingerprint sensor, and rough contours derived from capacitive images obtained via the touch screen. this collaborative integration endows our network with more comprehensive and discriminative information, substantially improving the accuracy and stability of pose estimation. a decoupled probability distribution prediction task is designed, instead of the traditional supervised forms of numerical regression or heatmap voting, to facilitate the training process. additionally, we incorporate a mixture of experts (moe) based feature fusion mechanism and a relationship driven cross-domain knowledge transfer strategy to further strengthen feature extraction and fusion capabilities. extensive experiments are conducted on several public datasets and two private datasets. the results indicate that our method is significantly superior to previous state-of-the-art (sota) methods and remarkably boosts the recognition ability of fingerprint recognition algorithms. our code is available at https://github.com/xiongjunguan/draco.",,2025-05-05,,"['xiongjun guan', 'zhiyu pan', 'jianjiang feng', 'jie zhou']"
2505.02501,corr2distrib: making ambiguous correspondences an ally to predict   reliable 6d pose distributions,cs.cv cs.ai cs.lg cs.ro,"we introduce corr2distrib, the first correspondence-based method which estimates a 6d camera pose distribution from an rgb image, explaining the observations. indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. while a few recent methods tackle this problem, they do not rely on local correspondences which, according to the bop challenge, are currently the most effective way to estimate a single 6dof pose solution. using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of pnp. with corr2distrib, we turn these ambiguities into an advantage to recover all valid poses. corr2distrib first learns a symmetry-aware representation for each 3d point on the object's surface, characterized by a descriptor and a local frame. this representation enables the generation of 3dof rotation hypotheses from single 2d-3d correspondences. next, we refine these hypotheses into a 6dof pose distribution using pnp and pose scoring. our experimental evaluations on complex non-synthetic scenes show that corr2distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an rgb image, demonstrating the potential of correspondences-based approaches.",,2025-05-05,,"['asma brazi', 'boris meden', 'fabrice mayran de chamisso', 'steve bourgeois', 'vincent lepetit']"
2505.02527,text to image generation and editing: a survey,cs.cv,"text-to-image generation (t2i) refers to the text-guided generation of high-quality images. in the past few years, t2i has attracted widespread attention and numerous works have emerged. in this survey, we comprehensively review 141 works conducted from 2021 to 2024. first, we introduce four foundation model architectures of t2i (autoregression, non-autoregression, gan and diffusion) and the commonly used key technologies (autoencoder, attention and classifier-free guidance). secondly, we systematically compare the methods of these studies in two directions, t2i generation and t2i editing, including the encoders and the key technologies they use. in addition, we also compare the performance of these researches side by side in terms of datasets, evaluation metrics, training resources, and inference speed. in addition to the four foundation models, we survey other works on t2i, such as energy-based models and recent mamba and multimodality. we also investigate the potential social impact of t2i and provide some solutions. finally, we propose unique insights of improving the performance of t2i models and possible future development directions. in summary, this survey is the first systematic and comprehensive overview of t2i, aiming to provide a valuable guide for future researchers and stimulate continued progress in this field.",,2025-05-05,,"['pengfei yang', 'ngai-man cheung', 'xinda ma']"
2505.02529,robsurv: vector quantization-based multi-modal learning for robust   cancer survival prediction,eess.iv cs.cv,"cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. current approaches struggle to extract consistent features from heterogeneous ct and pet images, limiting their clinical applicability. we address these challenges by introducing robsurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. the key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. this dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via transformer-based processing. in extensive evaluations across three diverse datasets (hecktor, h\&n1, and nsclc radiogenomics), robsurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\% compared to 8-12\% in baseline methods. these results, combined with strong generalization across different cancer types and imaging protocols, establish robsurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.",,2025-05-05,,"['aiman farooq', 'azad singh', 'deepak mishra', 'santanu chaudhury']"
2505.02549,robust duality learning for unsupervised visible-infrared person   re-identification,cs.cv cs.mm,"unsupervised visible-infrared person re-identification (uvi-reid) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. in practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. to address this, we introduce a new learning paradigm that explicitly considers pseudo-label noise (pln), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. to this end, we propose a novel robust duality learning framework (rode) for uvi-reid to mitigate the effects of noisy pseudo-labels. first, to combat noise overfitting, a robust adaptive learning mechanism (ral) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. second, to alleviate error accumulation-where the model reinforces its own mistakes-rode employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. however, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. to resolve this, we introduce cluster consistency matching (ccm), which aligns clusters across models and modalities by measuring cross-cluster similarity. extensive experiments on three benchmarks demonstrate the effectiveness of rode.",10.1109/tifs.2025.3536613,2025-05-05,2025-05-06,"['yongxiang li', 'yuan sun', 'yang qin', 'dezhong peng', 'xi peng', 'peng hu']"
2505.02593,delta: dense depth from events and lidar using transformer's attention,cs.cv,"event cameras and lidars provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. to this day, few works have explored the combination of these two modalities. in this article, we propose a novel neural-network-based method for fusing event and lidar data in order to estimate dense depth maps. our architecture, delta, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and lidar data. following a thorough evaluation, we demonstrate that delta sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous sota.",,2025-05-05,,"['vincent brebion', 'julien moreau', 'franck davoine']"
2505.02626,"detect, classify, act: categorizing industrial anomalies with   multi-modal large language models",cs.cv,"recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. however, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. to address this gap, we propose velm, a novel llm-based pipeline for anomaly classification. given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. if an anomaly is detected, the llm then classifies its type. a key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. to address this limitation, we introduce mvtec-ac and visa-ac, refined versions of the widely used mvtec-ad and visa datasets, which include accurate anomaly class labels for rigorous evaluation. our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on mvtec-ad, exceeding the prior baselines by 5%, and 84% on mvtec-ac, demonstrating the effectiveness of velm in understanding and categorizing anomalies. we hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.",,2025-05-05,,"['sassan mokhtar', 'arian mousakhan', 'silvio galesso', 'jawad tayyub', 'thomas brox']"
2505.02628,deepsparse: a foundation model for sparse-view cbct reconstruction,eess.iv cs.cv,"cone-beam computed tomography (cbct) is a critical 3d imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. sparse-view reconstruction reduces radiation by using fewer x-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. to overcome these limitations, we propose deepsparse, the first foundation model for sparse-view cbct reconstruction, featuring dice (dual-dimensional cross-scale embedding), a novel network that integrates multi-view 2d features and multi-scale 3d features. additionally, we introduce the hyvip (hybrid view sampling pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. extensive experiments and ablation studies demonstrate that our proposed deepsparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient cbct imaging.",,2025-05-05,,"['yiqun lin', 'hualiang wang', 'jixiang chen', 'jiewen yang', 'jiarong guo', 'xiaomeng li']"
2505.02648,mccd: multi-agent collaboration-based compositional diffusion for   complex text-to-image generation,cs.cv,"diffusion models have shown excellent performance in text-to-image generation. nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. therefore, we propose a multi-agent collaboration-based compositional diffusion (mccd) for text-to-image generation for complex scenes. specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing mllms to extract various scene elements effectively. in addition, hierarchical compositional diffusion utilizes a gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. comprehensive experiments demonstrate that our mccd significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.",,2025-05-05,2025-05-06,"['mingcheng li', 'xiaolu hou', 'ziyang liu', 'dingkang yang', 'ziyun qian', 'jiawei chen', 'jinjie wei', 'yue jiang', 'qingyao xu', 'lihua zhang']"
2505.02654,sim2real in endoscopy segmentation with a novel structure aware image   translation,cs.cv,"automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. however, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. while ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. the main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. our approach produces realistic images in different endoscopy scenarios. we demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. in particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. we run experiments both on a novel simulated dataset for fold segmentation, and real data from the endomapper (em) dataset. all our new generated data and new em metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation.",10.1007/978-3-031-73281-2_9,2025-05-05,,"['clara tomasini', 'luis riazuelo', 'ana c. murillo']"
2505.02664,grasp the graph (gtg) 2.0: ensemble of gnns for high-precision grasp   pose detection in clutter,cs.ro cs.cv cs.lg,"grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. this paper introduces grasp the graph 2.0 (gtg 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of graph neural networks for efficient geometric reasoning from point cloud data. building on the success of gtg 1.0, which demonstrated the potential of graph neural networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-dof grasping, gtg 2.0 employs a conventional grasp pose generator to efficiently produce 7-dof grasp candidates. candidates are assessed with an ensemble graph neural network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). this improved representation boosts grasp detection performance over previous methods using the same generator. gtg 2.0 shows up to a 35% improvement in average precision on the graspnet-1billion benchmark compared to hypothesis-and-test and graph neural network-based methods, ranking it among the top three frameworks. experiments with a 3-dof delta parallel robot and kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.",,2025-05-05,,"['ali rashidi moghadam', 'sayedmohammadreza rastegari', 'mehdi tale masouleh', 'ahmad kalhor']"
2505.02677,multimodal deep learning for stroke prediction and detection using   retinal imaging and clinical data,eess.iv cs.cv,"stroke is a major public health problem, affecting millions worldwide. deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. however, existing methods rely on costly medical imaging modalities, such as computed tomography. recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. we propose a multimodal deep neural network that processes optical coherence tomography (oct) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. we pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. the experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\% auroc improvement as compared to the unimodal image-only baseline, and $8$\% improvement compared to an existing state-of-the-art foundation model. in conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.",,2025-05-05,,"['saeed shurrab', 'aadim nepal', 'terrence j. lee-st. john', 'nicola g. ghazi', 'bartlomiej piechowski-jozwiak', 'farah e. shamout']"
2505.0269,dance of fireworks: an interactive broadcast gymnastics training system   based on pose estimation,cs.cv,"this study introduces dance of fireworks, an interactive system designed to combat sedentary health risks by enhancing engagement in radio calisthenics. leveraging mobile device cameras and lightweight pose estimation (posenet/tensorflow lite), the system extracts body keypoints, computes joint angles, and compares them with standardized motions to deliver real-time corrective feedback. to incentivize participation, it dynamically maps users' movements (such as joint angles and velocity) to customizable fireworks animations, rewarding improved accuracy with richer visual effects. experiments involving 136 participants demonstrated a significant reduction in average joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four sessions, with 93.4 percent of users affirming its exercise-promoting efficacy and 85.4 percent praising its entertainment value. the system operates without predefined motion templates or specialised hardware, enabling seamless integration into office environments. future enhancements will focus on improving pose recognition accuracy, reducing latency, and adding features such as multiplayer interaction and music synchronisation. this work presents a cost-effective, engaging solution to promote physical activity in sedentary populations.",,2025-05-05,,"['haotian chen', 'ziyu liu', 'xi cheng', 'chuangqi li']"
2505.02703,structure causal models and llms integration in medical visual question   answering,cs.cv,"medical visual question answering (medvqa) aims to answer medical questions according to medical images. however, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. such cross-modal bias makes it challenging to infer medically meaningful answers. in this work, we propose a causal inference framework for the medvqa task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (qa) session. we are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. during optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. in addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. extensive experiments on three medvqa datasets demonstrate that 1) our method significantly improves the accuracy of medvqa, and 2) our method achieves true causal correlations in the face of complex medical data.",10.1109/tmi.2025.3564320,2025-05-05,,"['zibo xu', 'qiang li', 'weizhi nie', 'weijie wang', 'anan liu']"
2505.02704,vgld: visually-guided linguistic disambiguation for monocular depth   scale recovery,cs.cv,"we propose a robust method for monocular depth scale recovery. monocular depth estimation can be divided into two main directions: (1) relative depth estimation, which provides normalized or inverse depth without scale information, and (2) metric depth estimation, which involves recovering depth with absolute scale. to obtain absolute scale information for practical downstream tasks, utilizing textual information to recover the scale of a relative depth map is a highly promising approach. however, since a single image can have multiple descriptions from different perspectives or with varying styles, it has been shown that different textual descriptions can significantly affect the scale recovery process. to address this issue, our method, vgld, stabilizes the influence of textual information by incorporating high-level semantic information from the corresponding image alongside the textual description. this approach resolves textual ambiguities and robustly outputs a set of linear transformation parameters (scalars) that can be globally applied to the relative depth map, ultimately generating depth predictions with metric-scale accuracy. we validate our method across several popular relative depth models(midas, depthanything), using both indoor scenes (nyuv2) and outdoor scenes (kitti). our results demonstrate that vgld functions as a universal alignment module when trained on multiple datasets, achieving strong performance even in zero-shot scenarios. code is available at: https://github.com/pakinwu/vgld.",,2025-05-05,2025-05-05,"['bojin wu', 'jing chen']"
2505.02705,multi-view learning with context-guided receptance for image denoising,eess.iv cs.cv,"image denoising is essential in low-level vision applications such as photography and automated driving. existing methods struggle with distinguishing complex noise patterns in real-world scenes and consume significant computational resources due to reliance on transformer-based models. in this work, the context-guided receptance weighted key-value (\m) model is proposed, combining enhanced multi-view feature integration with efficient sequence modeling. our approach introduces the context-guided token shift (cts) paradigm, which effectively captures local spatial dependencies and enhance the model's ability to model real-world noise distributions. additionally, the frequency mix (fmix) module extracting frequency-domain features is designed to isolate noise in high-frequency spectra, and is integrated with spatial representations through a multi-view learning process. to improve computational efficiency, the bidirectional wkv (biwkv) mechanism is adopted, enabling full pixel-sequence interaction with linear complexity while overcoming the causal selection constraints. the model is validated on multiple real-world image denoising datasets, outperforming the existing state-of-the-art methods quantitatively and reducing inference time up to 40\%. qualitative results further demonstrate the ability of our model to restore fine details in various scenes.",,2025-05-05,,"['binghong chen', 'tingting chai', 'wei jiang', 'yuanrong xu', 'guanglu zhou', 'xiangqian wu']"
2505.0272,a rate-quality model for learned video coding,cs.cv,"learned video coding (lvc) has recently achieved superior coding performance. in this paper, we model the rate-quality (r-q) relationship for learned video coding by a parametric function. we learn a neural network, termed rqnet, to characterize the relationship between the bitrate and quality level according to video content and coding context. the predicted (r,q) results are further integrated with those from previously coded frames using the least-squares method to determine the parameters of our r-q model on-the-fly. compared to the conventional approaches, our method accurately estimates the r-q relationship, enabling the online adaptation of model parameters to enhance both flexibility and precision. experimental results show that our r-q model achieves significantly smaller bitrate deviations than the baseline method on commonly used datasets with minimal additional complexity.",,2025-05-05,,"['sang nguyenquang', 'cheng-wei chen', 'xiem hoangvan', 'wen-hsiao peng']"
2505.02746,using knowledge graphs to harvest datasets for efficient clip model   training,cs.cv cs.cl cs.ir cs.lg,"training high-quality clip models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest clip models do not cover well -- and drives up training costs. this poses challenges for scientific research that needs fine-grained control over the training procedure of clip models. in this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust clip model can be trained from scratch with considerably less data. specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10m images. moreover, we introduce entitynet, a dataset comprising 33m images paired with 46m text descriptions, which enables the training of a generic clip model in significantly reduced time.",,2025-05-05,,"['simon ging', 'sebastian walter', 'jelena bratulić', 'johannes dienert', 'hannah bast', 'thomas brox']"
2505.02751,platelet enumeration in dense aggregates,eess.iv cs.cv,"identifying and counting blood components such as red blood cells, various types of white blood cells, and platelets is a critical task for healthcare practitioners. deep learning approaches, particularly convolutional neural networks (cnns) using supervised learning strategies, have shown considerable success for such tasks. however, cnn based architectures such as u-net, often struggles to accurately identify platelets due to their sizes and high variability of features. to address these challenges, researchers have commonly employed strategies such as class weighted loss functions, which have demonstrated some success. however, this does not address the more significant challenge of platelet variability in size and tendency to form aggregates and associations with other blood components. in this study, we explored an alternative approach by investigating the role of convolutional kernels in mitigating these issues. we also assigned separate classes to singular platelets and platelet aggregates and performed semantic segmentation using various u-net architectures for identifying platelets. we then evaluated and compared two common methods (pixel area method and connected component analysis) for counting platelets and proposed an alternative approach specialized for single platelets and platelet aggregates. our experiments provided results that showed significant improvements in the identification of platelets, highlighting the importance of optimizing convolutional operations and class designations. we show that the common practice of pixel area-based counting often over estimate platelet counts, whereas the proposed method presented in this work offers significant improvements. we discuss in detail about these methods from segmentation masks.",,2025-05-05,,"['h. martin gillis', 'yogeshwar shendye', 'paul hollensen', 'alan fine', 'thomas trappenberg']"
2505.02753,advancing generalizable tumor segmentation with anomaly-aware   open-vocabulary attention maps and frozen foundation diffusion models,cs.cv,"we explore generalizable tumor segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. in this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named diffugts. diffugts creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. to further improve and refine anomaly segmentation masks, diffugts leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. codes are available at https://github.com/yankai96/diffugts.",,2025-05-05,,"['yankai jiang', 'peng zhang', 'donglin yang', 'yuan tian', 'hai lin', 'xiaosong wang']"
2505.02779,unsupervised deep learning-based keypoint localization estimating   descriptor matching performance,cs.cv,"retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.   in this work, we present a novel unsupervised registration pipeline that entirely eliminates the need for labeled data. our approach is based on the principle that locations with distinctive descriptors constitute reliable keypoints. this fully inverts the conventional state-of-the-art approach, conditioning the detector on the descriptor rather than the opposite.   first, we propose an innovative descriptor learning method that operates without keypoint detection or any labels, generating descriptors for arbitrary locations in retinal images. next, we introduce a novel, label-free keypoint detector network which works by estimating descriptor performance directly from the input image.   we validate our method through a comprehensive evaluation on four hold-out datasets, demonstrating that our unsupervised descriptor outperforms state-of-the-art supervised descriptors and that our unsupervised detector significantly outperforms existing unsupervised detection methods. finally, our full registration pipeline achieves performance comparable to the leading supervised methods, while not employing any labeled data. additionally, the label-free nature and design of our method enable direct adaptation to other domains and modalities.",,2025-05-05,,"['david rivas-villar', 'álvaro s. hervella', 'josé rouco', 'jorge novo']"
2505.02784,advances in automated fetal brain mri segmentation and biometry:   insights from the feta 2024 challenge,cs.cv,"accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. the feta challenge 2024 advanced automated fetal brain mri analysis by introducing biometry prediction as a new task alongside tissue segmentation. for the first time, our diverse multi-centric test set included data from a new low-field (0.55t) mri dataset. evaluation metrics were also expanded to include the topology-specific euler characteristic difference (ed). sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. however, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. the ed metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. overall, feta 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain mri, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable ai tools.",,2025-05-05,2025-05-08,"['vladyslav zalevskyi', 'thomas sanchez', 'misha kaandorp', 'margaux roulet', 'diego fajardo-rojas', 'liu li', 'jana hutter', 'hongwei bran li', 'matthew barkovich', 'hui ji', 'luca wilhelmi', 'aline dändliker', 'céline steger', 'mériam koob', 'yvan gomez', 'anton jakovčić', 'melita klaić', 'ana adžić', 'pavel marković', 'gracia grabarić', 'milan rados', 'jordina aviles verdera', 'gregor kasprian', 'gregor dovjak', 'raphael gaubert-rachmühl', 'maurice aschwanden', 'qi zeng', 'davood karimi', 'denis peruzzo', 'tommaso ciceri', 'giorgio longari', 'rachika e. hamadache', 'amina bouzid', 'xavier lladó', 'simone chiarella', 'gerard martí-juan', 'miguel ángel gonzález ballester', 'marco castellaro', 'marco pinamonti', 'valentina visani', 'robin cremese', 'keïn sam', 'fleur gaudfernau', 'param ahir', 'mehul parikh', 'maximilian zenk', 'michael baumgartner', 'klaus maier-hein', 'li tianhong', 'yang hong', 'zhao longfei', 'domen preloznik', 'žiga špiclin', 'jae won choi', 'muyang li', 'jia fu', 'guotai wang', 'jingwen jiang', 'lyuyang tong', 'bo du', 'andrea gondova', 'sungmin you', 'kiho im', 'abdul qayyum', 'moona mazher', 'steven a niederer', 'andras jakab', 'roxane licandro', 'kelly payette', 'meritxell bach cuadra']"
2505.02787,unsupervised training of keypoint-agnostic descriptors for flexible   retinal image registration,cs.cv,"current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. this enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference.   to validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. additionally, it demonstrates accurate performance regardless of the keypoint detector used. thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain.",,2025-05-05,,"['david rivas-villar', 'álvaro s. hervella', 'josé rouco', 'jorge novo']"
2505.02797,dpnet: dynamic pooling network for tiny object detection,cs.cv,"in unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. resizing images is a common strategy to improve detection accuracy, particularly for small objects. however, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. this paper proposes a dynamic pooling network (dpnet) for tiny object detection to mitigate these issues. dpnet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. thus, we achieve input-aware downsampling. we also design an adaptive normalization module (anm) to make a unified detector compatible with different dfs. a guidance loss supervises the predictor's training. dpnet dynamically allocates computing resources to trade off between detection accuracy and efficiency. experiments on the tinycoco and tinyperson datasets show that dpnet can save over 35% and 25% gflops, respectively, while maintaining comparable detection performance. the code will be made publicly available.",10.1109/jiot.2025.3559921,2025-05-05,,"['luqi gong', 'haotian chen', 'yikun chen', 'tianliang yao', 'chao li', 'shuai zhao', 'guangjie han']"
2505.02815,database-agnostic gait enrollment using settransformers,cs.cv,"gait recognition has emerged as a powerful tool for unobtrusive and long-range identity analysis, with growing relevance in surveillance and monitoring applications. although recent advances in deep learning and large-scale datasets have enabled highly accurate recognition under closed-set conditions, real-world deployment demands open-set gait enrollment, which means determining whether a new gait sample corresponds to a known identity or represents a previously unseen individual. in this work, we introduce a transformer-based framework for open-set gait enrollment that is both dataset-agnostic and recognition-architecture-agnostic. our method leverages a settransformer to make enrollment decisions based on the embedding of a probe sample and a context set drawn from the gallery, without requiring task-specific thresholds or retraining for new environments. by decoupling enrollment from the main recognition pipeline, our model is generalized across different datasets, gallery sizes, and identity distributions. we propose an evaluation protocol that uses existing datasets in different ratios of identities and walks per identity. we instantiate our method using skeleton-based gait representations and evaluate it on two benchmark datasets (casia-b and psymo), using embeddings from three state-of-the-art recognition models (gaitgraph, gaitformer, and gaitpt). we show that our method is flexible, is able to accurately perform enrollment in different scenarios, and scales better with data compared to traditional approaches. we will make the code and dataset scenarios publicly available.",,2025-05-05,,"['nicoleta basoc', 'adrian cosma', 'andy cǎtrunǎ', 'emilian rǎdoi']"
2505.02823,musar: exploring multi-subject customization from single-subject dataset   via attention routing,cs.cv,"current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. to bridge these gaps, we propose musar - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. firstly, to break the data limitation, we introduce debiased diptych learning. it constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch lora. secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. this design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. comprehensive experiments demonstrate that our musar outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.",,2025-05-05,,"['zinan guo', 'pengze zhang', 'yanze wu', 'chong mou', 'songtao zhao', 'qian he']"
2505.02824,towards dataset copyright evasion attack against personalized   text-to-image diffusion models,cs.cv cs.ai cs.cr,"text-to-image (t2i) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. however, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. to combat this, dataset ownership verification (dov) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. these watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. despite the promise of dov for t2i diffusion models, its robustness against copyright evasion attacks (cea) remains unexplored. in this paper, we explore how attackers can bypass these mechanisms through cea, allowing models to circumvent watermarks even when trained on watermarked datasets. we propose the first copyright evasion attack (i.e., ceat2i) specifically designed to undermine dov in t2i diffusion models. concretely, our ceat2i comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. a key insight driving our approach is that t2i models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. leveraging this, ceat2i can reliably detect the watermarked samples. then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. finally, we adopt a closed-form concept erasure method to remove the injected watermark. extensive experiments show that our ceat2i effectively evades dov mechanisms while preserving model performance.",,2025-05-05,,"['kuofeng gao', 'yufei zhu', 'yiming li', 'jiawang bai', 'yong yang', 'zhifeng li', 'shu-tao xia']"
2505.02825,towards application-specific evaluation of vision models: case studies   in ecology and biology,cs.cv,"computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. however, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. we argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. to support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3d posture estimator. we show that even models with strong machine learning performance (e.g., 87% map) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows.",,2025-05-05,2025-05-06,"['alex hoi hang chan', 'otto brookes', 'urs waldmann', 'hemal naik', 'iain d. couzin', 'majid mirmehdi', 'noël adiko houa', 'emmanuelle normand', 'christophe boesch', 'lukas boesch', 'mimi arandjelovic', 'hjalmar kühl', 'tilo burghardt', 'fumihiro kano']"
2505.0283,aor: anatomical ontology-guided reasoning for medical large multimodal   model in chest x-ray interpretation,cs.cv cs.cl,"chest x-rays (cxrs) are the most frequently performed imaging examinations in clinical settings. recent advancements in large multimodal models (lmms) have enabled automated cxr interpretation, enhancing diagnostic accuracy and efficiency. however, despite their strong visual understanding, current medical lmms (mlmms) still face two major challenges: (1) insufficient region-level understanding and interaction, and (2) limited accuracy and interpretability due to single-step reasoning. in this paper, we empower mlmms with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. specifically, we first propose an anatomical ontology-guided reasoning (aor) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. next, under the guidance of expert physicians, we develop aor-instruction, a large instruction dataset for mlmms training. our experiments demonstrate aor's superior performance in both vqa and report generation tasks.",,2025-05-05,,"['qingqiu li', 'zihang cui', 'seongsu bae', 'jilan xu', 'runtian yuan', 'yuejie zhang', 'rui feng', 'quanli shen', 'xiaobo zhang', 'junjun he', 'shujun wang']"
2505.02833,twist: teleoperated whole-body imitation system,cs.ro cs.cv cs.lg,"teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. we present the teleoperated whole-body imitation system (twist), a system for humanoid teleoperation through whole-body motion imitation. we first generate reference motion clips by retargeting human motion capture data to the humanoid robot. we then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (rl+bc). through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (mocap) data improves tracking accuracy. twist enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. our project website: https://humanoid-teleop.github.io",,2025-05-05,,"['yanjie ze', 'zixuan chen', 'joão pedro araújo', 'zi-ang cao', 'xue bin peng', 'jiajun wu', 'c. karen liu']"
2505.02836,scenethesis: a language and vision agentic framework for 3d scene   generation,cs.cv,"synthesizing interactive 3d scenes from text is essential for gaming, virtual reality, and embodied ai. however, existing methods face several challenges. learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. while large language models (llms) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that llms lack. to this end, we introduce scenethesis, a training-free agentic framework that integrates llm-based scene planning with vision-guided layout refinement. given a text prompt, scenethesis first employs an llm to draft a coarse layout. a vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. finally, a judge module verifies spatial coherence. comprehensive experiments show that scenethesis generates diverse, realistic, and physically plausible 3d interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied ai research.",,2025-05-05,,"['lu ling', 'chen-hsuan lin', 'tsung-yi lin', 'yifan ding', 'yu zeng', 'yichen sheng', 'yunhao ge', 'ming-yu liu', 'aniket bera', 'zhaoshuo li']"
2505.02867,resanything: attribute prompting for arbitrary referring segmentation,cs.cv,"we present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (res), targeting input expressions that are more general than what prior works were designed to handle. specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object/part function, design, style, material, etc. our model, coined resanything, leverages chain-of-thoughts (cot) reasoning, where the key idea is attribute prompting. we generate detailed descriptions of object/part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (llm), where the proposals are produced by a foundational image segmentation model. our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. as the first zero-shot and llm-based res method, resanything achieves clearly superior performance among zero-shot methods on traditional res benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. finally, we contribute a new benchmark dataset to offer ~3k carefully curated res instances to assess part-level, arbitrary res solutions.",,2025-05-03,,"['ruiqi wang', 'hao zhang']"
2505.02877,a wireless collaborated inference acceleration framework for plant   disease recognition,cs.lg cs.ai cs.cv,"plant disease is a critical factor affecting agricultural production. traditional manual recognition methods face significant drawbacks, including low accuracy, high costs, and inefficiency. deep learning techniques have demonstrated significant benefits in identifying plant diseases, but they still face challenges such as inference delays and high energy consumption. deep learning algorithms are difficult to run on resource-limited embedded devices. offloading these models to cloud servers is confronted with the restriction of communication bandwidth, and all of these factors will influence the inference's efficiency. we propose a collaborative inference framework for recognizing plant diseases between edge devices and cloud servers to enhance inference speed. the dnn model for plant disease recognition is pruned through deep reinforcement learning to improve the inference speed and reduce energy consumption. then the optimal split point is determined by a greedy strategy to achieve the best collaborated inference acceleration. finally, the system for collaborative inference acceleration in plant disease recognition has been implemented using gradio to facilitate friendly human-machine interaction. experiments indicate that the proposed collaborative inference framework significantly increases inference speed while maintaining acceptable recognition accuracy, offering a novel solution for rapidly diagnosing and preventing plant diseases.",,2025-05-04,,"['hele zhu', 'xinyi huang', 'haojia gao', 'mengfei jiang', 'haohua que', 'lei mu']"
2505.02949,gone with the bits: revealing racial bias in low-rate neural compression   for facial images,cs.cv,"neural compression methods are gaining popularity due to their superior rate-distortion performance over traditional methods, even at extremely low bitrates below 0.1 bpp. as deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. in this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. using this framework, we investigate racial bias in neural compression algorithms by analyzing nine popular models and their variants. through this investigation, we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. we then examine the relationship between bias and realism in the decoded images and demonstrate a trade-off across models. finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy. we additionally show the bias can be attributed to compression model bias and classification model bias. we believe that this work is a first step towards evaluating and eliminating bias in neural image compression models.",,2025-05-05,,"['tian qiu', 'arjun nichani', 'rasta tadayontahmasebi', 'haewon jeong']"
2505.02966,generating narrated lecture videos from slides with synchronized   highlights,cs.cv cs.ai,"turning static slides into engaging video lectures takes considerable time and effort, requiring presenters to record explanations and visually guide their audience through the material. we introduce an end-to-end system designed to automate this process entirely. given a slide deck, this system synthesizes a video lecture featuring ai-generated narration synchronized precisely with dynamic visual highlights. these highlights automatically draw attention to the specific concept being discussed, much like an effective presenter would. the core technical contribution is a novel highlight alignment module. this module accurately maps spoken phrases to locations on a given slide using diverse strategies (e.g., levenshtein distance, llm-based semantic analysis) at selectable granularities (line or word level) and utilizes timestamp-providing text-to-speech (tts) for timing synchronization. we demonstrate the system's effectiveness through a technical evaluation using a manually annotated slide dataset with 1000 samples, finding that llm-based alignment achieves high location accuracy (f1 > 92%), significantly outperforming simpler methods, especially on complex, math-heavy content. furthermore, the calculated generation cost averages under $1 per hour of video, offering potential savings of two orders of magnitude compared to conservative estimates of manual production costs. this combination of high accuracy and extremely low cost positions this approach as a practical and scalable tool for transforming static slides into effective, visually-guided video lectures.",,2025-05-05,,['alexander holmberg']
2505.02971,adversarial robustness analysis of vision-language models in medical   image segmentation,cs.cv,"adversarial attacks have been fairly explored for computer vision and vision-language models. however, the avenue of adversarial attack for the vision language segmentation models (vlsms) is still under-explored, especially for medical image analysis.   thus, we have investigated the robustness of vlsms against adversarial attacks for 2d medical images with different modalities with radiology, photography, and endoscopy. the main idea of this project was to assess the robustness of the fine-tuned vlsms specially in the medical domain setting to address the high risk scenario.   first, we have fine-tuned pre-trained vlsms for medical image segmentation with adapters.   then, we have employed adversarial attacks -- projected gradient descent (pgd) and fast gradient sign method (fgsm) -- on that fine-tuned model to determine its robustness against adversaries.   we have reported models' performance decline to analyze the adversaries' impact.   the results exhibit significant drops in the dsc and iou scores after the introduction of these adversaries. furthermore, we also explored universal perturbation but were not able to find for the medical images.   \footnote{https://github.com/anjilab/secure-private-ai}",,2025-05-05,,"['anjila budathoki', 'manish dhakal']"
2505.0298,completing spatial transcriptomics data for gene expression prediction   benchmarking,cs.cv,"spatial transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. among the various spatial transcriptomics techniques available, visium has emerged as the most widely adopted. however, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. to address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. to bridge this gap, we introduce spared, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. we further propose spackle, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. finally, we establish the spared benchmark, evaluating eight state-of-the-art prediction models on both raw and spackle-completed data, demonstrating spackle substantially improves the results across all the gene expression prediction models. altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on spatial transcriptomics.",,2025-05-05,,"['daniela ruiz', 'paula cardenas', 'leonardo manrique', 'daniela vega', 'gabriel mejia', 'pablo arbelaez']"
2505.03007,ntire 2025 challenge on ugc video enhancement: methods and results,cs.cv,"this paper presents an overview of the ntire 2025 challenge on ugc video enhancement. the challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. the goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. given the widespread use of ugc on short-form video platforms, this task holds substantial practical importance. the evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. the challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. the outcomes may provide insights into the state-of-the-art in ugc video enhancement and highlight emerging trends and effective strategies in this evolving research area. all data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/ntire25_ugc_video_enhancement.",,2025-05-05,,"['nikolay safonov', 'alexey bryncev', 'andrey moskalenko', 'dmitry kulikov', 'dmitry vatolin', 'radu timofte', 'haibo lei', 'qifan gao', 'qing luo', 'yaqing li', 'jie song', 'shaozhe hao', 'meisong zheng', 'jingyi xu', 'chengbin wu', 'jiahui liu', 'ying chen', 'xin deng', 'mai xu', 'peipei liang', 'jie ma', 'junjie jin', 'yingxue pang', 'fangzhou luo', 'kai chen', 'shijie zhao', 'mingyang wu', 'renjie li', 'yushen zuo', 'shengyun zhong', 'zhengzhong tu']"
2505.03012,gif: generative inspiration for face recognition at scale,cs.cv,"aiming to reduce the computational cost of softmax in massive label space of face recognition (fr) benchmarks, recent studies estimate the output using a subset of identities. although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. a shared characteristic among available fr methods is the employment of atomic scalar labels during training. consequently, the input to label matching is through a dot product between the feature vector of the input and the softmax centroids. inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. then, we train an fr backbone to predict the code for each input instead of its scalar label. as a result, the associated computational cost becomes logarithmic w.r.t. number of identities. we demonstrate the benefits of the proposed method by conducting experiments. in particular, our method outperforms its competitors by 1.52%, and 0.6% at tar@far$=1e-4$ on ijb-b and ijb-c, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. see code at https://github.com/msed-ebrahimi/gif",,2025-05-05,,"['saeed ebrahimi', 'sahar rahimi', 'ali dabouei', 'srinjoy das', 'jeremy m. dawson', 'nasser m. nasrabadi']"
2505.03018,lesion-aware generative artificial intelligence for virtual   contrast-enhanced mammography in breast cancer,cs.cv cs.ai,"contrast-enhanced spectral mammography (cesm) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. it acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. while cesm offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. to address these limitations, we propose seg-cyclegan, a generative deep learning framework for virtual contrast enhancement in cesm. the model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. building upon the standard cyclegan architecture, seg-cyclegan introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. experiments on the cesm@ucbm dataset demonstrate that seg-cyclegan outperforms the baseline in terms of psnr and ssim, while maintaining competitive mse and vif. qualitative evaluations further confirm improved lesion fidelity in the generated images. these results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free cesm alternatives.",,2025-05-05,,"['aurora rofena', 'arianna manchia', 'claudia lucia piccolo', 'bruno beomonte zobel', 'paolo soda', 'valerio guarrasi']"
2505.03037,dual prompting for diverse count-level pet denoising,eess.iv cs.cv physics.med-ph,"the to-be-denoised positron emission tomography (pet) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. in this work, we resort to the recently flourished prompt learning to achieve generalizable pet denoising with different count levels. specifically, we propose dual prompts to guide the pet denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential pet denoising knowledge. then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. the prompts are able to dynamically guide the noise-conditioned denoising process. therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. we evaluated on 1940 low-count pet 3d volumes with uniformly randomly selected 13-22\% fractions of events from 97 $^{18}$f-mk6240 tau pet studies. it shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.",,2025-05-05,,"['xiaofeng liu', 'yongsong huang', 'thibault marin', 'samira vafay eslahi', 'tiss amal', 'yanis chemli', 'keith johnson', 'georges el fakhri', 'jinsong ouyang']"
2505.03039,an explainable anomaly detection framework for monitoring depression and   anxiety using consumer wearable devices,cs.cv stat.ap,"continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. in this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. leveraging data from 2,023 participants with defined healthy baselines, our lstm autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). the model achieved an adjusted f1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (f1 = 0.84) and for more pronounced symptom changes (>=10-point increases, f1 = 0.85). model interpretability was supported by shap-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings.",,2025-05-05,,"['yuezhou zhang', 'amos a. folarin', 'callum stewart', 'heet sankesara', 'yatharth ranjan', 'pauline conde', 'akash roy choudhury', 'shaoxiong sun', 'zulqarnain rashid', 'richard j. b. dobson']"
2505.03046,sim2real transfer for vision-based grasp verification,cs.ro cs.cv,"the verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. in this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. our method employs a two-stage architecture; first yolo-based object detection model to detect and locate the robot's gripper and then a resnet-based classifier determines the presence of an object. to address the limitations of real-world data capture, we introduce hsr-graspsynth, a synthetic dataset designed to simulate diverse grasping scenarios. furthermore, we explore the use of visual question answering capabilities as a zero-shot baseline to which we compare our model. experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. code and datasets are publicly available at https://github.com/pauamargant/hsr-graspsynth .",,2025-05-05,,"['pau amargant', 'peter hönig', 'markus vincze']"
2505.03097,not all parameters matter: masking diffusion models for enhancing   generation ability,cs.cv,"the diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., resnet or gans) which captures or generates the image semantic information at different layers. this difference inspires us to explore the time-wise diffusion models. we initially investigate the key contributions of the u-net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. capitalizing on this discovery, we propose a simple yet effective method-termed ``maskunet''- that enhances generation quality with negligible parameter numbers. our method fully leverages timestep- and sample-dependent effective u-net parameters. to optimize maskunet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. in zero-shot inference on the coco dataset, maskunet achieves the best fid score and further demonstrates its effectiveness in downstream task evaluations. project page: https://gudaochangsheng.github.io/maskunet-page/",,2025-05-05,,"['lei wang', 'senmao li', 'fei yang', 'jianye wang', 'ziheng zhang', 'yuhan liu', 'yaxing wang', 'jian yang']"
2505.03114,path and bone-contour regularized unpaired mri-to-ct translation,cs.cv,"accurate mri-to-ct translation promises the integration of complementary imaging information without the need for additional imaging sessions. given the practical challenges associated with acquiring paired mri and ct scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the mri-to-ct translation. current unpaired mri-to-ct translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on ct but less distinguishable on mri, such as bone structures. this limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. to address this challenge, we propose a path- and bone-contour regularized approach for unpaired mri-to-ct translation. in our method, mri and ct images are projected to a shared latent space, where the mri-to-ct mapping is modeled as a continuous flow governed by neural ordinary differential equations. the optimal mapping is obtained by minimizing the transition path length of the flow. to enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from mri and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired mri-to-ct translation approaches, achieving lower overall error rates. moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. our code is available at: https://github.com/kennysyp/pabot.",,2025-05-05,,"['teng zhou', 'jax luo', 'yuping sun', 'yiheng tan', 'shun yao', 'nazim haouchine', 'scott raymond']"
2505.03116,timetracker: event-based continuous point tracking for video frame   interpolation with non-linear motion,cs.cv,"video frame interpolation (vfi) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. a hurdle for event-based vfi is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. unfortunately, motion errors often degrade the vfi quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. in this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. in light of this, we propose a novel continuous point tracking-based vfi framework, named timetracker. specifically, we first design a scene-aware region segmentation (sars) module to divide the scene into similar patches. then, a continuous trajectory guided motion estimation (ctme) module is proposed to track the continuous motion trajectory of each patch through events. finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. moreover, we collect a real-world dataset that features fast non-linear motion. extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.",,2025-05-05,,"['haoyue liu', 'jinghan xu', 'yi chang', 'hanyu zhou', 'haozhi zhao', 'lin wang', 'luxin yan']"
2505.03123,stg: spatiotemporal graph neural network with fusion and spatiotemporal   decoupling learning for prognostic prediction of colorectal cancer liver   metastasis,eess.iv cs.cv cs.mm,"we propose a multimodal spatiotemporal graph neural network (stg) framework to predict colorectal cancer liver metastasis (crlm) progression. current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. our stg framework combines preoperative ct imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. the framework uses graphsage to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. a lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. the model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. experimental results on the mskcc crlm dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. the innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.",,2025-05-05,,"['yiran zhu', 'wei yang', 'yan su', 'zesheng li', 'chengchang pan', 'honggang qi']"
2505.03132,vislix: an xai framework for validating vision models with slice   discovery and analysis,cs.cv cs.ai cs.hc,"real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. the evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. first, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. to overcome these limitations and better support the machine learning operations lifecycle, we introduce vislix, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. we evaluate vislix with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.",,2025-05-05,,"['xinyuan yan', 'xiwei xuan', 'jorge piazentin ono', 'jiajing guo', 'vikram mohanty', 'shekar arvind kumar', 'liang gou', 'bei wang', 'liu ren']"
2505.03134,enhancing glass defect detection with diffusion models: addressing   imbalanced datasets in manufacturing quality control,cs.cv cs.lg,"visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. this paper presents a novel approach using denoising diffusion probabilistic models (ddpms) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. the methodology significantly enhances image classification performance of standard cnn architectures (resnet50v2, efficientnetb0, and mobilenetv2) in detecting anomalies by increasing the minority class representation. experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. the most dramatic improvement was observed in resnet50v2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. this work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.",,2025-05-05,,"['sajjad rezvani boroujeni', 'hossein abedi', 'tom bush']"
2505.03149,motion-compensated cardiac mri using low-rank diffeomorphic flow (dmoco),cs.cv cs.ai,"we introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3d cardiac magnetic resonance imaging (mri). we express the image volume corresponding to each specific motion phase as the deformation of a single static image template. the main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. the diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. the velocity field at different phases is represented using a low-rank model. the static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. the more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3d cine mri.",,2025-05-05,2025-05-07,"['joseph kettelkamp', 'ludovica romanin', 'sarv priya', 'mathews jacob']"
2505.03153,robust fairness vision-language learning for medical image analysis,cs.cv,"the advent of vision-language models (vlms) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. however, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. in this paper, we introduce a framework for ensuring robustness and fairness of vlm models. this framework modifies the loss function at training by identifying and adjusting faulty image-text pairs through a dynamic bad pair mining algorithm and also utilizing sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled auc.",,2025-05-05,,"['sparsh bansal', 'mingyang wu', 'xin wang', 'shu hu']"
2505.03154,stablemotion: training motion cleanup models with unpaired corrupted   data,cs.cv cs.ai cs.gr,"motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. in this work, we present stablemotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. the core component of our method is the introduction of motion quality indicators, which can be easily annotated through manual labeling or heuristic algorithms and enable training of quality-aware motion generation models on raw motion data with mixed quality. at test time, the model can be prompted to generate high-quality motions using the quality indicators. our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. we demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying stablemotion to soccermocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. the trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. see https://youtu.be/3y7mmah02b4 for more results.",,2025-05-06,,"['yuxuan mu', 'hung yu ling', 'yi shi', 'ismael baira ojeda', 'pengcheng xi', 'chang shu', 'fabio zinno', 'xue bin peng']"
2505.03173,ravu: retrieval augmented video understanding with compositional   reasoning over graph,cs.cv cs.ai,"comprehending long videos remains a significant challenge for large multi-modal models (lmms). current lmms struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. to address this limitation, we propose ravu (retrieval augmented video understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. we construct a graph representation of the video, capturing both spatial and temporal relationships between entities. this graph serves as a long-term memory, allowing us to track objects and their actions across time. to answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other sota methods and baselines on two major video qa datasets, next-qa and egoschema.",,2025-05-06,,"['sameer malik', 'moyuru yamada', 'ayush singh', 'dishank aggarwal']"
2505.03174,automated data curation using gps & nlp to generate instruction-action   pairs for autonomous vehicle vision-language navigation datasets,cs.ro cs.cv cs.lg,"instruction-action (ia) data pairs are valuable for training robotic systems, especially autonomous vehicles (avs), but having humans manually annotate this data is costly and time-inefficient. this paper explores the potential of using mobile application global positioning system (gps) references and natural language processing (nlp) to automatically generate large volumes of ia commands and responses without having a human generate or retroactively tag the data. in our pilot data collection, by driving to various destinations and collecting voice instructions from gps applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. we provide details on our completely automated data collection prototype system, advlat-engine. we characterize collected gps voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. through research and exploration into the automation of ia data pairs using gps references, the potential to increase the speed and volume at which high-quality ia datasets are created, while minimizing cost, can pave the way for robust vision-language-action (vla) models to serve tasks in vision-language navigation (vln) and human-interactive autonomous systems.",,2025-05-06,,"['guillermo roque', 'erika maquiling', 'jose giovanni tapia lopez', 'ross greer']"
2505.03184,interactive instance annotation with siamese networks,cs.cv,"annotating instance masks is time-consuming and labor-intensive. a promising solution is to predict contours using a deep learning model and then allow users to refine them. however, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. in this paper, we propose siamanno, a framework inspired by the use of siamese networks in object tracking. siamanno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. trained on one dataset and tested on another without fine-tuning, siamanno achieves state-of-the-art (sota) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. we also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. to our knowledge, siamanno is the first model to explore siamese architecture for instance annotation.",,2025-05-06,,"['xiang xu', 'ruotong li', 'mengjun yi', 'baile xu', 'furao shen', 'jian zhao']"
2505.03203,pico: enhancing text-image alignment with improved noise selection and   precise mask control in diffusion models,cs.cv,"advanced diffusion models have made notable progress in text-to-image compositional generation. however, it is still a challenge for existing models to achieve text-image alignment when confronted with complex text prompts. in this work, we highlight two factors that affect this alignment: the quality of the randomly initialized noise and the reliability of the generated controlling mask. we then propose pico (pick-and-control), a novel training-free approach with two key components to tackle these two factors. first, we develop a noise selection module to assess the quality of the random noise and determine whether the noise is suitable for the target text. a fast sampling strategy is utilized to ensure efficiency in the noise selection stage. second, we introduce a referring mask module to generate pixel-level masks and to precisely modulate the cross-attention maps. the referring mask is applied to the standard diffusion process to guide the reasonable interaction between text and image features. extensive experiments have been conducted to verify the effectiveness of pico in liberating users from the tedious process of random generation and in enhancing the text-image alignment for diverse text descriptions.",,2025-05-06,,"['chang xie', 'chenyi zhuang', 'pan gao']"
2505.03204,dcs-st for classification of breast cancer histopathology images with   limited annotations,cs.cv cs.ai,"deep learning methods have shown promise in classifying breast cancer histopathology images, but their performance often declines with limited annotated data, a critical challenge in medical imaging due to the high cost and expertise required for annotations.",,2025-05-06,2025-05-07,"['liu suxing', 'byungwon min']"
2505.0322,dual-domain masked image modeling: a self-supervised pretraining   strategy using spatial and frequency domain masking for hyperspectral data,cs.cv,"hyperspectral images (hsis) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. however, the scarcity of labeled hsi data limits the full potential of deep learning, especially for transformer-based architectures that require large-scale training. to address this constraint, we propose spatial-frequency masked image modeling (sfmim), a self-supervised pretraining strategy for hyperspectral data that utilizes the large portion of unlabeled data. our method introduces a novel dual-domain masking mechanism that operates in both spatial and frequency domains. the input hsi cube is initially divided into non-overlapping patches along the spatial dimension, with each patch comprising the entire spectrum of its corresponding spatial location. in spatial masking, we randomly mask selected patches and train the model to reconstruct the masked inputs using the visible patches. concurrently, in frequency masking, we remove portions of the frequency components of the input spectra and predict the missing frequencies. by learning to reconstruct these masked components, the transformer-based encoder captures higher-order spectral-spatial correlations. we evaluate our approach on three publicly available hsi classification benchmarks and demonstrate that it achieves state-of-the-art performance. notably, our model shows rapid convergence during fine-tuning, highlighting the efficiency of our pretraining strategy.",,2025-05-06,,"['shaheer mohamed', 'tharindu fernando', 'sridha sridharan', 'peyman moghadam', 'clinton fookes']"
2505.03242,seeing the abstract: translating the abstract language for vision   language models,cs.cv cs.ai,"natural language goes beyond dryly describing visual content. it contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. yet, current research in vision language models (vlms) has not shed light on abstract-oriented language. our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. by analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. however, a critical challenge emerges: current general-purpose or fashion-specific vlms are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. we propose a training-free and model-agnostic method, abstract-to-concrete translator (act), to shift abstract representations towards well-represented concrete ones in the vlm latent space, using pre-trained models and existing multimodal databases. on the text-to-image retrieval task, despite being training-free, act outperforms the fine-tuned vlms in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. moreover, the improvement introduced by act is consistent with various vlms, making it a plug-and-play solution.",,2025-05-06,,"['davide talon', 'federico girella', 'ziyue liu', 'marco cristani', 'yiming wang']"
2505.03254,prom: prioritize reduction of multiplications over lower bit-widths for   efficient cnns,cs.cv cs.lg,"convolutional neural networks (cnns) are crucial for computer vision tasks on resource-constrained devices. quantization effectively compresses these models, reducing storage size and energy cost. however, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. by applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. to this end, we introduce prom, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. applying prom to mobilenetv2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on imagenet. our method advances the pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on imagenet. prom addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.",,2025-05-06,,"['lukas meiner', 'jens mehnert', 'alexandru paul condurache']"
2505.03261,diffvqa: video quality assessment using diffusion feature extractor,cs.cv eess.iv,"video quality assessment (vqa) aims to evaluate video quality based on perceptual distortions and human preferences. despite the promising performance of existing methods using convolutional neural networks (cnns) and vision transformers (vits), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. this challenge is exacerbated by the limited scale and diversity of available datasets. to address this limitation, we introduce a novel vqa framework, diffvqa, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. our framework adapts these models to reconstruct identical input frames through a control module. the adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. to enhance the model's ability to handle long-term temporal dynamics, a parallel mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. experiments across multiple datasets demonstrate diffvqa's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. these results confirm that leveraging a diffusion model as a feature extractor can offer enhanced vqa performance compared to cnn and vit backbones.",,2025-05-06,,"['wei-ting chen', 'yu-jiet vong', 'yi-tsung lee', 'sy-yen kuo', 'qiang gao', 'sizhuo ma', 'jian wang']"
2505.03284,occcylindrical: multi-modal fusion with cylindrical representation for   3d semantic occupancy prediction,cs.cv cs.ro,"the safe operation of autonomous vehicles (avs) is highly dependent on their understanding of the surroundings. for this, the task of 3d semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. recent perception models have used multisensor fusion to perform this task. however, existing multisensor fusion-based approaches focus mainly on using sensor information in the cartesian coordinate system. this ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. in this paper, we propose occcylindrical that merges and refines the different modality features under cylindrical coordinates. our method preserves more fine-grained geometry detail that leads to better performance. extensive experiments conducted on the nuscenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. the code will be available at: https://github.com/danielming123/occcylindrical",,2025-05-06,,"['zhenxing ming', 'julie stephany berrio', 'mao shan', 'yaoqi huang', 'hongyu lyu', 'nguyen hoang khoi tran', 'tzu-yun tseng', 'stewart worrall']"
2505.03286,base-detail feature learning framework for visible-infrared person   re-identification,cs.cv,"visible-infrared person re-identification (vireid) provides a solution for reid tasks in 24-hour scenarios; however, significant challenges persist in achieving satisfactory performance due to the substantial discrepancies between visible (vis) and infrared (ir) modalities. existing methods inadequately leverage information from different modalities, primarily focusing on digging distinguishing features from modality-shared information while neglecting modality-specific details. to fully utilize differentiated minutiae, we propose a base-detail feature learning framework (bdlf) that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information. specifically, the proposed bdlf mines detail and base features through a lossless detail feature extraction module and a complementary base embedding generation mechanism, respectively, supported by a novel correlation restriction method that ensures the features gained by bdlf enrich both detail and base knowledge across vis and ir features. comprehensive experiments conducted on the sysu-mm01, regdb, and llcm datasets validate the effectiveness of bdlf.",,2025-05-06,,"['zhihao gong', 'lian wu', 'yong xu']"
2505.03299,towards efficient benchmarking of foundation models in remote sensing: a   capabilities encoding approach,cs.cv cs.ai,"foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. in the field of earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. however, none has consistently outperformed the others across all available downstream tasks. to facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. this method is based on what we call ""capabilities encoding."" the utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. codes are available at https://github.com/pierreadorni/capabilities-encoding.",,2025-05-06,,"['pierre adorni', 'minh-tan pham', 'stéphane may', 'sébastien lefèvre']"
2505.033,3d can be explored in 2d: pseudo-label generation for lidar point clouds   using sensor-intensity-based 2d semantic segmentation,cs.cv,"semantic segmentation of 3d lidar point clouds, essential for autonomous driving and infrastructure management, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. we introduce a new 3d semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2d segmentation methods, avoiding the need for direct 3d annotation or reliance on additional modalities such as camera images at inference time. our approach generates 2d views from lidar scans colored by sensor intensity and applies 2d semantic segmentation to these views using a camera-domain pretrained model. the segmented 2d outputs are then back-projected onto the 3d points, with a simple voting-based estimator that merges the labels associated to each 3d point. our main contribution is a global pipeline for 3d semantic segmentation requiring no prior 3d annotation and not other modality for inference, which can be used for pseudo-label generation. we conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the unsupervised domain adaptation task.",10.1109/iv55156.2024.10588443,2025-05-06,,"['andrew caunes', 'thierry chateau', 'vincent frémont']"
2505.03303,comparative analysis of lightweight deep learning models for   memory-constrained devices,cs.cv cs.ai,"this paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. five state-of-the-art architectures - mobilenetv3 small, resnet18, squeezenet, efficientnetv2-s, and shufflenetv2 - are benchmarked across three diverse datasets: cifar-10, cifar-100, and tiny imagenet. the models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (flops), and model size. additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on mobilenetv3 small. our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like tiny imagenet. efficientnetv2 consistently achieves the highest accuracy, while mobilenetv3 offers the best balance between accuracy and efficiency, and squeezenet excels in inference speed and compactness. this study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. by addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.",,2025-05-06,,['tasnim shahriar']
2505.0331,3d gaussian splatting data compression with mixture of priors,cs.cv,"3d gaussian splatting (3dgs) data compression is crucial for enabling efficient storage and transmission in 3d scene modeling. however, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. in this work, we propose a novel mixture of priors (mop) strategy to simultaneously address these two challenges. specifically, inspired by the mixture-of-experts (moe) paradigm, our mop approach processes hyperprior information through multiple lightweight mlps to generate diverse prior features, which are subsequently integrated into the mop feature via a gating mechanism. to enhance lossless compression, the resulting mop feature is utilized as a hyperprior to improve conditional entropy modeling. meanwhile, for lossy compression, we employ the mop feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided coarse-to-fine quantization (c2fq) strategy with a predefined quantization step value. specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the mop feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. extensive experiments demonstrate that our proposed 3dgs data compression framework achieves state-of-the-art performance across multiple benchmarks, including mip-nerf360, bungeenerf, deepblending, and tank&temples.",,2025-05-06,,"['lei liu', 'zhenghao chen', 'dong xu']"
2505.03318,unified multimodal chain-of-thought reward model through reinforcement   fine-tuning,cs.cv,"recent advances in multimodal reward models (rms) have shown significant promise in delivering reward signals to align vision models with human preferences. however, current rms are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. we posit that incorporating explicit long chains of thought (cot) into the reward reasoning process can significantly strengthen their reliability and robustness. furthermore, we believe that once rms internalize cot reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. to this end, this paper proposes unifiedreward-think, the first unified multimodal cot-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) we first use a small amount of image generation preference data to distill the reasoning process of gpt-4o, which is then used for the model's cold start to learn the format and structure of cot reasoning. (2) subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. during this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for group relative policy optimization (grpo) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. extensive experiments across various vision reward tasks demonstrate the superiority of our model.",,2025-05-06,,"['yibin wang', 'zhimin li', 'yuhang zang', 'chunyu wang', 'qinglin lu', 'cheng jin', 'jiaqi wang']"
2505.03319,sd-vsum: a method and dataset for script-driven video summarization,cs.cv cs.ai cs.mm,"in this work, we introduce the task of script-driven video summarization, which aims to produce a summary of the full-length video by selecting the parts that are most relevant to a user-provided script outlining the visual content of the desired summary. following, we extend a recently-introduced large-scale dataset for generic video summarization (videoxum) by producing natural language descriptions of the different human-annotated summaries that are available per video. in this way we make it compatible with the introduced task, since the available triplets of ``video, summary and summary description'' can be used for training a method that is able to produce different summaries for a given video, driven by the provided script about the content of each summary. finally, we develop a new network architecture for script-driven video summarization (sd-vsum), that relies on the use of a cross-modal attention mechanism for aligning and fusing information from the visual and text modalities. our experimental evaluations demonstrate the advanced performance of sd-vsum against state-of-the-art approaches for query-driven and generic (unimodal and multimodal) summarization from the literature, and document its capacity to produce video summaries that are adapted to each user's needs about their content.",,2025-05-06,,"['manolis mylonas', 'evlampios apostolidis', 'vasileios mezaris']"
2505.03327,very high-resolution forest mapping with tandem-x insar data and   self-supervised learning,cs.cv cs.ai cs.lg eess.iv,"deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with tandem-x interferometric sar data. such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. in this work, our aim is to exploit the high-resolution capabilities of the tandem-x mission to map forests at 6 m. the goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. to cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. a 1 m resolution forest/non-forest reference map over pennsylvania, usa, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. we select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the amazon rainforest, where only very few labeled data at high resolution are available. in this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with tandem-x data.",,2025-05-06,,"['josé-luis bueso-bello', 'benjamin chauvel', 'daniel carcereri', 'philipp posovszky', 'pietro milillo', 'jennifer ruiz', 'juan-carlos fernández-diaz', 'carolina gonzález', 'michele martone', 'ronny hänsch', 'paola rizzoli']"
2505.03329,flux-text: a simple and advanced diffusion transformer baseline for   scene text editing,cs.cv,"the task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. recent works based on latent diffusion models (ldm) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-latin ones (\eg, chinese), which have complex glyph structures. to address these issues, we present flux-text, a simple and advanced multilingual scene text editing framework based on flux-fill. specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. to retain the original generative capabilities of flux-fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. owning to the lightweight design, flux-text is trained only with $100k$ training examples compared to current popular methods trained with 2.9m ones. with no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity.",,2025-05-06,,"['rui lan', 'yancheng bai', 'xu duan', 'mingxing li', 'lei sun', 'xiangxiang chu']"
2505.03334,from word to sentence: a large-scale multi-instance dataset for open-set   aerial detection,cs.cv cs.db,"in recent years, language-guided open-world aerial object detection has gained significant attention due to its better alignment with real-world application needs. however, due to limited datasets, most existing language-guided methods primarily focus on vocabulary, which fails to meet the demands of more fine-grained open-world detection. to address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. centered around an open-source large vision-language model and integrating image-operation-based preprocessing with bert-based postprocessing, we present the os-w2s label engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called multi-instance open-set aerial dataset (mi-oad), addressing the limitations of current remote sensing grounding data and enabling effective open-set aerial detection. specifically, mi-oad contains 163,023 images and 2 million image-caption pairs, approximately 40 times larger than comparable datasets. we also employ state-of-the-art open-set methods from the natural image domain, trained on our proposed dataset, to validate the model's open-set detection capabilities. for instance, when trained on our dataset, grounding dino achieves improvements of 29.5 ap_{50} and 33.7 recall@10 for sentence inputs under zero-shot transfer conditions. both the dataset and the label engine will be released publicly.",,2025-05-06,,"['guoting wei', 'yu liu', 'xia yuan', 'xizhe xue', 'linlin guo', 'yifan yang', 'chunxia zhao', 'zongwen bai', 'haokui zhang', 'rong xiao']"
2505.0335,a vision-language model for focal liver lesion classification,cs.cv,"accurate classification of focal liver lesions is crucial for diagnosis and treatment in hepatology. however, traditional supervised deep learning models depend on large-scale annotated datasets, which are often limited in medical imaging. recently, vision-language models (vlms) such as contrastive language-image pre-training model (clip) has been applied to image classifications. compared to the conventional convolutional neural network (cnn), which classifiers image based on visual information only, vlm leverages multimodal learning with text and images, allowing it to learn effectively even with a limited amount of labeled data. inspired by clip, we pro-pose a liver-vlm, a model specifically designed for focal liver lesions (flls) classification. first, liver-vlm incorporates class information into the text encoder without introducing additional inference overhead. second, by calculating the pairwise cosine similarities between image and text embeddings and optimizing the model with a cross-entropy loss, liver-vlm ef-fectively aligns image features with class-level text features. experimental results on mpct-flls dataset demonstrate that the liver-vlm model out-performs both the standard clip and medclip models in terms of accuracy and area under the curve (auc). further analysis shows that using a lightweight resnet18 backbone enhances classification performance, particularly under data-constrained conditions.",,2025-05-06,,"['song jian', 'hu yuchang', 'wang hui', 'chen yen-wei']"
2505.03351,guava: generalizable upper body 3d gaussian avatar,cs.cv,"reconstructing a high-quality, animatable 3d human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3d human avatar reconstruction typically requires multi-view or monocular videos and training on individual ids, which is both complex and time-consuming. furthermore, limited by smplx's expressiveness, these methods often focus on body motion but struggle with facial expressions. to address these challenges, we first introduce an expressive human model (ehm) to enhance facial expression capabilities and develop an accurate tracking method. based on this template model, we propose guava, the first framework for fast animatable upper-body 3d gaussian avatar reconstruction. we leverage inverse texture mapping and projection sampling techniques to infer ubody (upper-body) gaussians from a single image. the rendered images are refined through a neural refiner. experimental results demonstrate that guava significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.",,2025-05-06,,"['dongbin zhang', 'yunfei liu', 'lijian lin', 'ye zhu', 'yang li', 'minghan qin', 'yu li', 'haoqian wang']"
2505.03361,interpretable zero-shot learning with infinite class concepts,cs.cv,"zero-shot learning (zsl) aims to recognize unseen classes by aligning images with intermediate class semantics, like human-annotated concepts or class definitions. an emerging alternative leverages large-scale language models (llms) to automatically generate class documents. however, these methods often face challenges with transparency in the classification process and may suffer from the notorious hallucination problem in llms, resulting in non-visual class semantics. this paper redefines class semantics in zsl with a focus on transferability and discriminability, introducing a novel framework called zero-shot learning with infinite class concepts (infzsl). our approach leverages the powerful capabilities of llms to dynamically generate an unlimited array of phrase-level class concepts. to address the hallucination challenge, we introduce an entropy-based scoring process that incorporates a ``goodness"" concept selection mechanism, ensuring that only the most transferable and discriminative concepts are selected. our infzsl framework not only demonstrates significant improvements on three popular benchmark datasets but also generates highly interpretable, image-grounded concepts. code will be released upon acceptance.",,2025-05-06,,"['zihan ye', 'shreyank n gowda', 'shiming chen', 'yaochu jin', 'kaizhu huang', 'xiaobo jin']"
2505.03362,3d surface reconstruction with enhanced high-frequency details,cs.cv,"neural implicit 3d reconstruction can reproduce shapes without 3d supervision, and it learns the 3d scene through volume rendering methods and neural implicit representations. current neural surface reconstruction methods tend to randomly sample the entire image, making it difficult to learn high-frequency details on the surface, and thus the reconstruction results tend to be too smooth. we designed a method (freneus) based on high-frequency information to solve the problem of insufficient surface detail. specifically, freneus uses pixel gradient changes to easily acquire high-frequency regions in an image and uses the obtained high-frequency information to guide surface detail reconstruction. high-frequency information is first used to guide the dynamic sampling of rays, applying different sampling strategies according to variations in high-frequency regions. to further enhance the focus on surface details, we have designed a high-frequency weighting method that constrains the representation of high-frequency details during the reconstruction process. qualitative and quantitative results show that our method can reconstruct fine surface details and obtain better surface reconstruction quality compared to existing methods. in addition, our method is more applicable and can be generalized to any neus-based work.",,2025-05-06,,"['shikun zhang', 'yiqun wang', 'cunjian chen', 'yong li', 'qiuhong ke']"
2505.03374,reducing annotation burden in physical activity research using   vision-language models,cs.cv,"introduction: data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. one common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. methods: we compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in oxfordshire, united kingdom and sichuan, china, respectively, using the autographer (omg life, defunct) wearable camera. results: we found that the best open-source vision-language model (vlm) and fine-tuned discriminative model (dm) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the oxfordshire study; median f1-scores: vlm = 0.89 (0.84, 0.92), dm = 0.91 (0.86, 0.95). performance declined for light (vlm = 0.60 (0.56,0.67), dm = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (vlm = 0.66 (0.53, 0.85); dm = 0.72 (0.58, 0.84)). when applied to the external sichuan study, performance fell across all intensity categories, with median cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the vlm, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the dm. conclusion: freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden.",,2025-05-06,,"['abram schonfeldt', 'benjamin maylor', 'xiaofang chen', 'ronald clark', 'aiden doherty']"
2505.0338,reinforced correlation between vision and language for precise medical   ai assistant,cs.cv cs.ai eess.iv,"medical ai assistants support doctors in disease diagnosis, medical image analysis, and report generation. however, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. we propose rcmed, a full-stack ai assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. a self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. this correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. trained on 20 million image-mask-description triplets, rcmed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. it achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. rcmed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. this work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric ai healthcare.",,2025-05-06,,"['haonan wang', 'jiaji mao', 'lehan wang', 'qixiang zhang', 'marawan elbatel', 'yi qin', 'huijun hu', 'baoxun li', 'wenhui deng', 'weifeng qin', 'hongrui li', 'jialin liang', 'jun shen', 'xiaomeng li']"
2505.03383,attention-aggregated attack for boosting the transferability of facial   adversarial examples,cs.cv,"adversarial examples have revealed the vulnerability of deep learning models and raised serious concerns about information security. the transfer-based attack is a hot topic in black-box attacks that are practical to real-world scenarios where the training datasets, parameters, and structure of the target model are unknown to the attacker. however, few methods consider the particularity of class-specific deep models for fine-grained vision tasks, such as face recognition (fr), giving rise to unsatisfactory attacking performance. in this work, we first investigate what in a face exactly contributes to the embedding learning of fr models and find that both decisive and auxiliary facial features are specific to each fr model, which is quite different from the biological mechanism of human visual system. accordingly we then propose a novel attack method named attention-aggregated attack (aaa) to enhance the transferability of adversarial examples against fr, which is inspired by the attention divergence and aims to destroy the facial features that are critical for the decision-making of other fr models by imitating their attentions on the clean face images. extensive experiments conducted on various fr models validate the superiority and robust effectiveness of the proposed method over existing methods.",,2025-05-06,,"['jian-wei li', 'wen-ze shao']"
2505.03394,eopose : exemplar-based object reposing using generalized pose   correspondences,cs.cv,"reposing objects in images has a myriad of applications, especially for e-commerce where several variants of product images need to be produced quickly. in this work, we leverage the recent advances in unsupervised keypoint correspondence detection between different object images of the same class to propose an end-to-end framework for generic object reposing. our method, eopose, takes a target pose-guidance image as input and uses its keypoint correspondence with the source object image to warp and re-render the latter into the target pose using a novel three-step approach. unlike generative approaches, our method also preserves the fine-grained details of the object such as its exact colors, textures, and brand marks. we also prepare a new dataset of paired objects based on the objaverse dataset to train and test our network. eopose produces high-quality reposing output as evidenced by different image quality metrics (psnr, ssim and fid). besides a description of the method and the dataset, the paper also includes detailed ablation and user studies to indicate the efficacy of the proposed method",,2025-05-06,,"['sarthak mehrotra', 'rishabh jain', 'mayur hemani', 'balaji krishnamurthy', 'mausoom sarkar']"
2505.03401,ddatr: dynamic difference-aware temporal residual network for   longitudinal radiology report generation,cs.cv cs.ai,"radiology report generation (rrg) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. longitudinal radiology report generation (lrrg) extends rrg by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. existing lrrg approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. however, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in lrrg. to address this, we develop a novel dynamic difference-aware temporal residual network (ddatr). in ddatr, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. the dynamic feature alignment module (dfam) is designed to align prior features across modalities for the integrity of prior clinical information. prompted by the enriched prior features, the dynamic difference-aware module (ddam) captures favorable difference information by identifying relationships across exams. furthermore, our ddatr employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both rrg and lrrg tasks.",,2025-05-06,,"['shanshan song', 'hui tang', 'honglong yang', 'xiaomeng li']"
2505.03412,cxr-ad: component x-ray image dataset for industrial anomaly detection,cs.cv,"internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. however, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available x-ray datasets targeting internal defects in components. to address this gap, we construct the first publicly accessible component x-ray anomaly detection (cxr-ad) dataset, comprising real-world x-ray images. the dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. we systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in x-ray imaging, and (3) significant variations in defect scales and morphologies. to evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). experimental results demonstrate a 29.78% average performance degradation on cxr-ad compared to mvtec ad, highlighting the limitations of current algorithms in handling internal defect detection tasks. to the best of our knowledge, cxr-ad represents the first publicly available x-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.",,2025-05-06,,"['haoyu bai', 'jie wang', 'gaomin li', 'xuan li', 'xiaohu zhang', 'xia yang']"
2505.03422,liftfeat: 3d geometry-aware local feature matching,cs.cv cs.ro,"robust and efficient local feature matching plays a crucial role in applications such as slam and visual localization for robotics. despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. in this paper, we propose a new lightweight network called \textit{liftfeat}, which lifts the robustness of raw descriptor by aggregating 3d geometric feature. specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3d geometric feature in terms of predicted surface normal. we then design a 3d geometry-aware feature lifting module to fuse surface normal feature with raw 2d descriptor feature. integrating such 3d geometric feature enhances the discriminative ability of 2d feature description in extreme conditions. extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our liftfeat outperforms some lightweight state-of-the-art methods. code will be released at : https://github.com/lyp-deeplearning/liftfeat.",,2025-05-06,,"['yepeng liu', 'wenpeng lai', 'zhou zhao', 'yuxuan xiong', 'jinchi zhu', 'jun cheng', 'yongchao xu']"
2505.03426,phenotype-guided generative model for high-fidelity cardiac mri   synthesis: advancing pretraining and clinical applications,cs.cv cs.ai,"cardiac magnetic resonance (cmr) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. however, the limited availability of large-scale, high-quality cmr datasets poses a major challenge to the effective application of artificial intelligence (ai) in this domain. even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of ai models on downstream tasks. in this study, we present cardiac phenotype-guided cmr generation (cpgg), a novel approach for generating diverse cmr data that covers a wide spectrum of cardiac health status. the cpgg framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from cmr data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity cmr cine sequences that capture both structural and functional features of the heart in a fine-grained manner. we synthesized a massive amount of cmr to expand the pretraining data. experimental results show that cpgg generates high-quality synthetic cmr data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. these gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. code is availabel at https://anonymous.4open.science/r/cpgg.",,2025-05-06,,"['ziyu li', 'yujian hu', 'zhengyao ding', 'yiheng mao', 'haitao li', 'fan yi', 'hongkun zhang', 'zhengxing huang']"
2505.03431,a fusion-guided inception network for hyperspectral image   super-resolution,cs.cv,"the fusion of low-spatial-resolution hyperspectral images (hsis) with high-spatial-resolution conventional images (e.g., panchromatic or rgb) has played a significant role in recent advancements in hsi super-resolution. however, this fusion process relies on the availability of precise alignment between image pairs, which is often challenging in real-world scenarios. to mitigate this limitation, we propose a single-image super-resolution model called the fusion-guided inception network (fgin). specifically, we first employ a spectral-spatial fusion module to effectively integrate spectral and spatial information at an early stage. next, an inception-like hierarchical feature extraction strategy is used to capture multiscale spatial dependencies, followed by a dedicated multi-scale fusion block. to further enhance reconstruction quality, we incorporate an optimized upsampling module that combines bilinear interpolation with depthwise separable convolutions. experimental evaluations on two publicly available hyperspectral datasets demonstrate the competitive performance of our method.",,2025-05-06,,"['usman muhammad', 'jorma laaksonen']"
2505.03435,robustness in ai-generated detection: enhancing resistance to   adversarial attacks,cs.cv,"the rapid advancement of generative image technology has introduced significant security concerns, particularly in the domain of face generation detection. this paper investigates the vulnerabilities of current ai-generated face detection systems. our study reveals that while existing detection methods often achieve high accuracy under standard conditions, they exhibit limited robustness against adversarial attacks. to address these challenges, we propose an approach that integrates adversarial training to mitigate the impact of adversarial examples. furthermore, we utilize diffusion inversion and reconstruction to further enhance detection robustness. experimental results demonstrate that minor adversarial perturbations can easily bypass existing detection systems, but our method significantly improves the robustness of these systems. additionally, we provide an in-depth analysis of adversarial and benign examples, offering insights into the intrinsic characteristics of ai-generated content. all associated code will be made publicly available in a dedicated repository to facilitate further research and verification.",,2025-05-06,,"['sun haoxuan', 'hong yan', 'zhan jiahui', 'chen haoxing', 'lan jun', 'zhu huijia', 'wang weiqiang', 'zhang liqing', 'zhang jianfu']"
2505.03445,polar coordinate-based 2d pose prior with neural distance field,cs.cv,"human pose capture is essential for sports analysis, enabling precise evaluation of athletes' movements. while deep learning-based human pose estimation (hpe) models from rgb videos have achieved impressive performance on public datasets, their effectiveness in real-world sports scenarios is often hindered by motion blur, occlusions, and domain shifts across different pose representations. fine-tuning these models can partially alleviate such challenges but typically requires large-scale annotated data and still struggles to generalize across diverse sports environments. to address these limitations, we propose a 2d pose prior-guided refinement approach based on neural distance fields (ndf). unlike existing approaches that rely solely on angular representations of human poses, we introduce a polar coordinate-based representation that explicitly incorporates joint connection lengths, enabling a more accurate correction of erroneous pose estimations. additionally, we define a novel non-geodesic distance metric that separates angular and radial discrepancies, which we demonstrate is better suited for polar representations than traditional geodesic distances. to mitigate data scarcity, we develop a gradient-based batch-projection augmentation strategy, which synthesizes realistic pose samples through iterative refinement. our method is evaluated on a long jump dataset, demonstrating its ability to improve 2d pose estimation across multiple pose representations, making it robust across different domains. experimental results show that our approach enhances pose plausibility while requiring only limited training data. code is available at: https://github.com/qgan2019/polar-ndf.",,2025-05-06,,"['qi gan', 'sao mai nguyen', 'eric fenaux', 'stephan clémençon', 'mounîm el yacoubi']"
2505.03463,nonperiodic dynamic ct reconstruction using backward-warping inr with   regularization of diffeomorphism (bird),cs.cv physics.med-ph,"dynamic computed tomography (ct) reconstruction faces significant challenges in addressing motion artifacts, particularly for nonperiodic rapid movements such as cardiac imaging with fast heart rates. traditional methods struggle with the extreme limited-angle problems inherent in nonperiodic cases. deep learning methods have improved performance but face generalization challenges. recent implicit neural representation (inr) techniques show promise through self-supervised deep learning, but have critical limitations: computational inefficiency due to forward-warping modeling, difficulty balancing dvf complexity with anatomical plausibility, and challenges in preserving fine details without additional patient-specific pre-scans. this paper presents a novel inr-based framework, bird, for nonperiodic dynamic ct reconstruction. it addresses these challenges through four key contributions: (1) backward-warping deformation that enables direct computation of each dynamic voxel with significantly reduced computational cost, (2) diffeomorphism-based dvf regularization that ensures anatomically plausible deformations while maintaining representational capacity, (3) motion-compensated analytical reconstruction that enhances fine details without requiring additional pre-scans, and (4) dimensional-reduction design for efficient 4d coordinate encoding. through various simulations and practical studies, including digital and physical phantoms and retrospective patient data, we demonstrate the effectiveness of our approach for nonperiodic dynamic ct reconstruction with enhanced details and reduced motion artifacts. the proposed framework enables more accurate dynamic ct reconstruction with potential clinical applications, such as one-beat cardiac reconstruction, cinematic image sequences for functional imaging, and motion artifact reduction in conventional ct scans.",,2025-05-06,,"['muge du', 'zhuozhao zheng', 'wenying wang', 'guotao quan', 'wuliang shi', 'le shen', 'li zhang', 'liang li', 'yinong liu', 'yuxiang xing']"
2505.0347,blending 3d geometry and machine learning for multi-view stereopsis,cs.cv cs.ai cs.cg cs.lg,"traditional multi-view stereo (mvs) methods primarily depend on photometric and geometric consistency constraints. in contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3d geometry, applying explicit geometric consistency (gc) checks only as a post-processing step, with no impact on the learning process itself. in this work, we introduce gc mvsnet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see fig. 1). this integrated gc check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other mvs methods. furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. extensive experiments demonstrate that our approach achieves a new state of the art on the dtu and blendedmvs datasets and secures second place on the tanks and temples benchmark. to our knowledge, gc mvsnet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. our code is available.",,2025-05-06,,"['vibhas vats', 'md. alimoor reza', 'david crandall', 'soon-heung jung']"
2505.03494,upmad-net: a brain tumor segmentation network with uncertainty guidance   and adaptive multimodal feature fusion,cs.cv,"background: brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. objective: we propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. methods: the proposed method utilizes a multi-scale feature fusion (msff) module and adaptive attention mechanisms (aam) to extract multi-scale features and capture global contextual information. to enhance the model's robustness in low-confidence regions, the monte carlo dropout (mc dropout) strategy is employed for uncertainty estimation. results: extensive experiments demonstrate that the proposed method achieves superior performance on brain tumor segmentation (brats) datasets, significantly outperforming various state-of-the-art methods. on the brats2021 dataset, the test dice scores are 89.18% for enhancing tumor (et) segmentation, 93.67% for whole tumor (wt) segmentation, and 91.23% for tumor core (tc) segmentation. on the brats2019 validation set, the validation dice scores are 87.43%, 90.92%, and 90.40% for et, wt, and tc segmentation, respectively. ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. conclusion: this study proposed a novel 3d brain tumor segmentation network based on the u-net architecture. by incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. the code for the proposed method is available at https://github.com/chenzhao2023/upmad_net_brainseg.",,2025-05-06,,"['zhanyuan jia', 'ni yao', 'danyang sun', 'chuang han', 'yanting li', 'jiaofen nan', 'fubao zhu', 'chen zhao', 'weihua zhou']"
2505.03498,mri motion correction via efficient residual-guided denoising diffusion   probabilistic models,cs.cv physics.med-ph,"purpose: motion artifacts in magnetic resonance imaging (mri) significantly degrade image quality and impair quantitative analysis. conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. this study introduces res-mocodiff, an efficient denoising diffusion probabilistic model tailored for mri motion artifact correction. methods: res-mocodiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. a u-net backbone enhanced with swin-transformer blocks conventional attention layers, improving adaptability across resolutions. training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. res-mocodiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. comparative analyses were conducted against established methods, including cyclegan, pix2pix, and mt-ddpm using quantitative metrics such as peak signal-to-noise ratio (psnr), structural similarity index measure (ssim), and normalized mean squared error (nmse). results: the proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. res-mocodiff consistently achieved the highest ssim and the lowest nmse values, with a psnr of up to 41.91+-2.94 db for minor distortions. notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.",,2025-05-06,,"['mojtaba safari', 'shansong wang', 'qiang li', 'zach eidex', 'richard l. j. qiu', 'chih-wei chang', 'hui mao', 'xiaofeng yang']"
2505.03507,modality-guided dynamic graph fusion and temporal diffusion for   self-supervised rgb-t tracking,cs.cv,"to reduce the reliance on large-scale annotations, self-supervised rgb-t tracking approaches have garnered significant attention. however, the omission of the object region by erroneous pseudo-label or the introduction of background noise affects the efficiency of modality fusion, while pseudo-label noise triggered by similar object noise can further affect the tracking performance. in this paper, we propose gdstrack, a novel approach that introduces dynamic graph fusion and temporal diffusion to address the above challenges in self-supervised rgb-t tracking. gdstrack dynamically fuses the modalities of neighboring frames, treats them as distractor noise, and leverages the denoising capability of a generative model. specifically, by constructing an adjacency matrix via an adjacency matrix generator (amg), the proposed modality-guided dynamic graph fusion (mdgf) module uses a dynamic adjacency matrix to guide graph attention, focusing on and fusing the object's coherent regions. temporal graph-informed diffusion (tgid) models mdgf features from neighboring frames as interference, and thus improving robustness against similar-object noise. extensive experiments conducted on four public rgb-t tracking datasets demonstrate that gdstrack outperforms the existing state-of-the-art methods. the source code is available at https://github.com/lishenglana/gdstrack.",,2025-05-06,,"['shenglan li', 'rui yao', 'yong zhou', 'hancheng zhu', 'kunyang sun', 'bing liu', 'zhiwen shao', 'jiaqi zhao']"
2505.0351,from neurons to computation: biological reservoir computing for pattern   recognition,cs.ne cs.ai cs.cv,"in this paper, we introduce a novel paradigm for reservoir computing (rc) that leverages a pool of cultured biological neurons as the reservoir substrate, creating a biological reservoir computing (brc). this system operates similarly to an echo state network (esn), with the key distinction that the neural activity is generated by a network of cultured neurons, rather than being modeled by traditional artificial computational units. the neuronal activity is recorded using a multi-electrode array (mea), which enables high-throughput recording of neural signals. in our approach, inputs are introduced into the network through a subset of the mea electrodes, while the remaining electrodes capture the resulting neural activity. this generates a nonlinear mapping of the input data to a high-dimensional biological feature space, where distinguishing between data becomes more efficient and straightforward, allowing a simple linear classifier to perform pattern recognition tasks effectively. to evaluate the performance of our proposed system, we present an experimental study that includes various input patterns, such as positional codes, bars with different orientations, and a digit recognition task. the results demonstrate the feasibility of using biological neural networks to perform tasks traditionally handled by artificial neural networks, paving the way for further exploration of biologically-inspired computing systems, with potential applications in neuromorphic engineering and bio-hybrid computing.",,2025-05-06,,"['ludovico iannello', 'luca ciampi', 'gabriele lagani', 'fabrizio tonelli', 'eleonora crocco', 'lucio maria calcagnile', 'angelo di garbo', 'federico cremisi', 'giuseppe amato']"
2505.03522,optimization of module transferability in single image super-resolution:   universality assessment and cycle residual blocks,cs.cv cs.ai,"deep learning has substantially advanced the single image super-resolution (sisr). however, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. in this paper, we introduce the concept of ""universality"" and its associated definitions which extend the traditional notion of ""generalization"" to encompass the modules' ease of transferability, thus revealing the relationships between module universality and model generalizability. then we propose the universality assessment equation (uae), a metric for quantifying how readily a given module could be transplanted across models. guided by the uae results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, cycle residual block (crb) and depth-wise cycle residual block (dcrb). through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, extreme-industrial imagery and on-device deployments, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a psnr enhancement of up to 0.83db or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity.",,2025-05-06,,"['haotong cheng', 'zhiqi zhang', 'hao li', 'xinshang zhang']"
2505.03528,coop-wd: cooperative perception with weighting and denoising for robust   v2v communication,cs.cv,"cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (v2v) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. existing works have explored the effects of v2v communication impairments on perception precision, but they lack generalization to different levels of impairments. in this work, we propose a joint weighting and denoising framework, coop-wd, to enhance cooperative perception subject to v2v channel impairments. in this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. an efficient variant model, coop-wd-eco, is proposed to selectively deactivate denoising to reduce processing overhead. rician fading, non-stationarity, and time-varying distortion are considered. simulation results demonstrate that the proposed coop-wd outperforms conventional benchmarks in all types of channels. qualitative analysis with visual examples further proves the superiority of our proposed method. the proposed coop-wd-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.",,2025-05-06,,"['chenguang liu', 'jianjun chen', 'yunfei chen', 'yubei he', 'zhuangkun wei', 'hongjian sun', 'haiyan lu', 'qi hao']"
2505.03538,rail: region-aware instructive learning for semi-supervised tooth   segmentation in cbct,cs.cv,"semi-supervised learning has become a compelling approach for 3d tooth segmentation from cbct scans, where labeled data is minimal. however, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. to address these problems, we propose region-aware instructive learning (rail), a dual-group dual-student, semi-supervised framework. each group contains two student models guided by a shared teacher network. by alternating training between the two groups, rail promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. specifically, rail introduces two instructive mechanisms. disagreement-focused supervision (dfs) controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. in the unsupervised phase, confidence-aware learning (cal) modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. this helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. extensive experiments on four cbct tooth segmentation datasets show that rail surpasses state-of-the-art methods under limited annotation. our code will be available at https://github.com/tournesol-saturday/rail.",,2025-05-06,,"['chuyu zhao', 'hao huang', 'jiashuo guo', 'ziyu shen', 'zhongwei zhou', 'jie liu', 'zekuan yu']"
2505.03539,panoramic out-of-distribution segmentation,cs.cv cs.ro eess.iv,"panoramic imaging enables capturing 360{\deg} images with an ultra-wide field-of-view (fov) for dense omnidirectional perception. however, current panoramic semantic segmentation methods fail to identify outliers, and pinhole out-of-distribution segmentation (oos) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. to address these issues, we introduce a new task, panoramic out-of-distribution segmentation (panoos), achieving oos for panoramas. furthermore, we propose the first solution, pos, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. specifically, pos integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of clip. the proposed prompt-based restoration attention (pra) optimizes semantic decoding by prompt guidance and self-adaptive correction, while bilevel prompt distribution learning (bpdl) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. besides, to compensate for the scarcity of panoos datasets, we establish two benchmarks: denseoos, which features diverse outliers in complex environments, and quadoos, captured by a quadruped robot with a panoramic annular lens system. extensive experiments demonstrate superior performance of pos, with auprc improving by 34.25% and fpr95 decreasing by 21.42% on denseoos, outperforming state-of-the-art pinhole-oos methods. moreover, pos achieves leading closed-set segmentation capabilities. code and datasets will be available at https://github.com/mengfeid/panoos.",,2025-05-06,,"['mengfei duan', 'kailun yang', 'yuheng zhang', 'yihong cao', 'fei teng', 'kai luo', 'jiaming zhang', 'zhiyong li', 'shutao li']"
2505.03554,read my ears! horse ear movement detection for equine affective state   assessment,cs.cv,"the equine facial action coding system (equifacs) enables the systematic annotation of facial movements through distinct action units (aus). it serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. however, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial aus is both time-consuming and costly. to address this challenge, automated annotation systems are essential for leveraging existing datasets and improving affective states detection tools. in this work, we study different methods for specific ear au detection and localization from horse videos. we leverage past works on deep learning-based video feature extraction combined with recurrent neural networks for the video classification task, as well as a classic optical flow based approach. we achieve 87.5% classification accuracy of ear movement presence on a public horse video dataset, demonstrating the potential of our approach. we discuss future directions to develop these systems, with the aim of bridging the gap between automated au detection and practical applications in equine welfare and veterinary diagnostics. our code will be made publicly available at https://github.com/jmalves5/read-my-ears.",,2025-05-06,,"['joão alves', 'pia haubro andersen', 'rikke gade']"
2505.03557,generating synthetic data via augmentations for improved facial   resemblance in dreambooth and instantid,cs.cv cs.ai,"the personalization of stable diffusion for generating professional portraits from amateur photographs is a burgeoning area, with applications in various downstream contexts. this paper investigates the impact of augmentations on improving facial resemblance when using two prominent personalization techniques: dreambooth and instantid. through a series of experiments with diverse subject datasets, we assessed the effectiveness of various augmentation strategies on the generated headshots' fidelity to the original subject. we introduce facedistance, a wrapper around facenet, to rank the generations based on facial similarity, which aided in our assessment. ultimately, this research provides insights into the role of augmentations in enhancing facial resemblance in sdxl-generated portraits, informing strategies for their effective deployment in downstream applications.",,2025-05-06,,"['koray ulusan', 'benjamin kiefer']"
2505.03562,real-time person image synthesis using a flow matching model,cs.cv cs.ai,"pose-guided person image synthesis (pgpis) generates realistic person images conditioned on a target pose and a source image. this task plays a key role in various real-world applications, such as sign language video generation, ar/vr, gaming, and live streaming. in these scenarios, real-time pgpis is critical for providing immediate visual feedback and maintaining user immersion.however, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-fidelity images from diverse and dynamic human poses. recent diffusion-based methods have shown impressive image quality in pgpis, but their slow sampling speeds hinder deployment in time-sensitive applications. this latency is particularly problematic in tasks like generating sign language videos during live broadcasts, where rapid image updates are required. therefore, developing a fast and reliable pgpis model is a crucial step toward enabling real-time interactive systems. to address this challenge, we propose a generative model based on flow matching (fm). our approach enables faster, more stable, and more efficient training and sampling. furthermore, the proposed model supports conditional generation and can operate in latent space, making it especially suitable for real-time pgpis applications where both speed and quality are critical. we evaluate our proposed method, real-time person image synthesis using a flow matching model (rpfm), on the widely used deepfashion dataset for pgpis tasks. our results show that rpfm achieves near-real-time sampling speeds while maintaining performance comparable to the state-of-the-art models. our methodology trades off a slight, acceptable decrease in generated-image accuracy for over a twofold increase in generation speed, thereby ensuring real-time performance.",,2025-05-06,,"['jiwoo jeong', 'kirok kim', 'wooju kim', 'nam-joon kim']"
2505.03567,uncertainty-aware prototype semantic decoupling for text-based person   search in full images,cs.cv,"text-based pedestrian search (tbps) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. however, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. to address this, we propose upd-tbps, a novel framework comprising three modules: multi-granularity uncertainty estimation (mue), prototype-based uncertainty decoupling (pud), and cross-modal re-identification (reid). mue conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. pud leverages visual context decoupling and prototype mining to extract features of the target pedestrian described in the query. it separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. reid evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. experiments on cuhk-sysu-tbps and prw-tbps datasets validate the effectiveness of our framework.",,2025-05-06,2025-05-06,"['zengli luo', 'canlong zhang', 'xiaochun lu', 'zhixin li', 'zhiwen wang']"
2505.03569,corner cases: how size and position of objects challenge   imagenet-trained models,cs.cv,"backgrounds in images play a major role in contributing to spurious correlations among different data points. owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. in this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. to better illustrate our findings, we propose a synthetic dataset derived from imagenet1k, hard-spurious-imagenet, which contains images with various backgrounds, object positions, and object sizes. by evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (roi) to image ratio is small and the object is far from the center of the image. moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change.",,2025-05-06,,"['mishal fatima', 'steffen jung', 'margret keuper']"
2505.03575,supervised and unsupervised textile classification via near-infrared   hyperspectral imaging and deep learning,cs.cv physics.app-ph,"recycling textile fibers is critical to reducing the environmental impact of the textile industry. hyperspectral near-infrared (nir) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. in this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. we show that optimized convolutional neural networks (cnns) and autoencoder networks achieve robust generalization under varying conditions. these results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.",10.5445/ksp/1000178356,2025-05-06,,"['maria kainz', 'johannes k. krondorfer', 'malte jaschik', 'maria jernej', 'harald ganster']"
2505.03581,dygenc: encoding a sequence of textual scene graphs to reason and answer   questions in dynamic scenes,cs.cv,"the analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. current approaches predominantly utilize visual models. however, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. to address this issue we introduce dygenc - a novel method for encoding a dynamic graph. this method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. the purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. extended evaluations on the star and agqa datasets indicate that dygenc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. we hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. code is available at github.com/linukc/dygenc.",,2025-05-06,,"['sergey linok', 'vadim semenov', 'anastasia trunova', 'oleg bulichev', 'dmitry yudin']"
2505.03597,fixed-length dense fingerprint representation,cs.cv,"fixed-length fingerprint representations, which map each fingerprint to a compact and fixed-size feature vector, are computationally efficient and well-suited for large-scale matching. however, designing a robust representation that effectively handles diverse fingerprint modalities, pose variations, and noise interference remains a significant challenge. in this work, we propose a fixed-length dense descriptor of fingerprints, and introduce flare-a fingerprint matching framework that integrates the fixed-length dense descriptor with pose-based alignment and robust enhancement. this fixed-length representation employs a three-dimensional dense descriptor to effectively capture spatial relationships among fingerprint ridge structures, enabling robust and locally discriminative representations. to ensure consistency within this dense feature space, flare incorporates pose-based alignment using complementary estimation methods, along with dual enhancement strategies that refine ridge clarity while preserving the original fingerprint modality. the proposed dense descriptor supports fixed-length representation while maintaining spatial correspondence, enabling fast and accurate similarity computation. extensive experiments demonstrate that flare achieves superior performance across rolled, plain, latent, and contactless fingerprints, significantly outperforming existing methods in cross-modality and low-quality scenarios. further analysis validates the effectiveness of the dense descriptor design, as well as the impact of alignment and enhancement modules on the accuracy of dense descriptor matching. experimental results highlight the effectiveness and generalizability of flare as a unified and scalable solution for robust fingerprint representation and matching. the implementation and code will be publicly available at https://github.com/yu-yy/flare.",,2025-05-06,,"['zhiyu pan', 'xiongjun guan', 'yongjie duan', 'jianjiang feng', 'jie zhou']"
2505.03599,from pixels to polygons: a survey of deep learning approaches for   medical image-to-mesh reconstruction,cs.cv,"deep learning-based medical image-to-mesh reconstruction has rapidly evolved, enabling the transformation of medical imaging data into three-dimensional mesh models that are critical in computational medicine and in silico trials for advancing our understanding of disease mechanisms, and diagnostic and therapeutic techniques in modern medicine. this survey systematically categorizes existing approaches into four main categories: template models, statistical models, generative models, and implicit models. each category is analysed in detail, examining their methodological foundations, strengths, limitations, and applicability to different anatomical structures and imaging modalities. we provide an extensive evaluation of these methods across various anatomical applications, from cardiac imaging to neurological studies, supported by quantitative comparisons using standard metrics. additionally, we compile and analyze major public datasets available for medical mesh reconstruction tasks and discuss commonly used evaluation metrics and loss functions. the survey identifies current challenges in the field, including requirements for topological correctness, geometric accuracy, and multi-modality integration. finally, we present promising future research directions in this domain. this systematic review aims to serve as a comprehensive reference for researchers and practitioners in medical image analysis and computational medicine.",,2025-05-06,,"['fengming lin', 'arezoo zakeri', 'yidan xue', 'michael macraild', 'haoran dou', 'zherui zhou', 'ziwei zou', 'ali sarrami-foroushani', 'jinming duan', 'alejandro f. frangi']"
2505.0361,learning knowledge-based prompts for robust 3d mask presentation attack   detection,cs.cv,"3d mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3d mask attacks. while most existing methods utilize multimodal features or remote photoplethysmography (rppg) signals to distinguish between real faces and 3d masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. detection-related text descriptions offer concise, universal information and are cost-effective to obtain. however, the potential of vision-language multimodal features for 3d mask presentation attack detection remains unexplored. in this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3d mask presentation attack detection. specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. during training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.",,2025-05-06,,"['fangling jiang', 'qi li', 'bing liu', 'weining wang', 'caifeng shan', 'zhenan sun', 'ming-hsuan yang']"
2505.03611,learning unknown spoof prompts for generalized face anti-spoofing using   only real face images,cs.cv,"face anti-spoofing is a critical technology for ensuring the security of face recognition systems. however, its ability to generalize across diverse scenarios remains a significant challenge. in this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. to address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model's ability to generalize to unseen target domains. specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. this framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images.",,2025-05-06,,"['fangling jiang', 'qi li', 'weining wang', 'wei shen', 'bing liu', 'zhenan sun']"
2505.03621,physllm: harnessing large language models for cross-modal remote   physiological sensing,cs.cv,"remote photoplethysmography (rppg) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. large language models (llms) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rppg signals due to their text-centric design. to bridge this gap, we introduce physllm, a collaborative optimization framework that synergizes llms with domain-specific rppg components. specifically, the text prototype guidance (tpg) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into llm-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. besides, a novel dual-domain stationary (dds) algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. finally, rppg task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. evaluation on four benchmark datasets, physllm achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.",,2025-05-06,,"['yiping xie', 'bo zhao', 'mingtong dai', 'jian-ping zhou', 'yue sun', 'tao tan', 'weicheng xie', 'linlin shen', 'zitong yu']"
2505.03623,bounding box-guided diffusion for synthesizing industrial images and   segmentation map,cs.cv,"synthetic dataset generation in computer vision, particularly for industrial applications, is still underexplored. industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. to address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. we introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. the code is publicly available at https://github.com/covisionlab/diffusion_labeling.",,2025-05-06,,"['alessandro simoni', 'francesco pelosin']"
2505.03631,breaking annotation barriers: generalized video quality assessment via   ranking-based self-supervision,cs.cv,"video quality assessment (vqa) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. while recent supervised vqa models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. to bridge this gap, we introduce a self-supervised learning framework for vqa to learn quality assessment capabilities from large-scale, unlabeled web videos. our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (lmm) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing vqa models and relative quality ranking based on synthetic distortion simulations. furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. by training on a dataset $10\times$ larger than the existing vqa benchmarks, our model: (1) achieves zero-shot performance on in-domain vqa benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (ood) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. extensive experimental results validate the effectiveness of our self-supervised approach in training generalized vqa models. the datasets and code will be publicly released to facilitate future research.",,2025-05-06,2025-05-07,"['linhan cao', 'wei sun', 'kaiwei zhang', 'yicong peng', 'guangtao zhai', 'xiongkuo min']"
2505.03638,towards smart point-and-shoot photography,cs.cv,"hundreds of millions of people routinely take photos using their smartphones as point and shoot (pas) cameras, yet very few would have the photography skills to compose a good shot of a scene. while traditional pas cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. in this paper, we present a first of its kind smart point and shoot (spas) system to help users to take good photos. our spas proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. we first constructed a large dataset containing 320k images with camera pose information from 4000 scenes. we then developed an innovative clip-based composition quality assessment (ccqa) model to assign pseudo labels to these images. the ccqa introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. and finally we have developed a camera pose adjustment model (cpam) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. the two tasks of cpam make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the cpam in an end-to-end manner. we will present extensive results to demonstrate the performances of our spas system using publicly available image composition datasets.",,2025-05-06,,"['jiawan li', 'fei zhou', 'zhipeng zhong', 'jiongzhi lin', 'guoping qiu']"
2505.03646,alma: aggregated lipschitz maximization attack on auto-encoders,cs.lg cs.ai cs.cv,"despite the extensive use of deep autoencoders (aes) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. ae robustness is characterized by the lipschitz bounds of its components. existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in aes. in the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. to address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local lipschitz bounds by enhancing loss gradient information propagation during attack optimization. we demonstrate through extensive experiments on state-of-the-art aes that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. as a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.",,2025-05-06,,"['chethan krishnamurthy ramanaik', 'arjun roy', 'eirini ntoutsi']"
2505.03662,revolutionizing brain tumor imaging: generating synthetic 3d fa maps   from t1-weighted mri using cyclegan models,cs.cv cs.ai,"fractional anisotropy (fa) and directionally encoded colour (dec) maps are essential for evaluating white matter integrity and structural connectivity in neuroimaging. however, the spatial misalignment between fa maps and tractography atlases hinders their effective integration into predictive models. to address this issue, we propose a cyclegan based approach for generating fa maps directly from t1-weighted mri scans, representing the first application of this technique to both healthy and tumour-affected tissues. our model, trained on unpaired data, produces high fidelity maps, which have been rigorously evaluated using structural similarity index (ssim) and peak signal-to-noise ratio (psnr), demonstrating particularly robust performance in tumour regions. radiological assessments further underscore the model's potential to enhance clinical workflows by providing an ai-driven alternative that reduces the necessity for additional scans.",,2025-05-06,,"['xin du', 'francesca m. cozzi', 'rajesh jena']"
2505.03667,distribution-conditional generation: from class distribution to creative   generation,cs.cv,"text-to-image (t2i) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose distribution-conditional generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. building on this, we propose distok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. distok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. to enforce distributional consistency, latent vectors sampled from a gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. the resulting tokens are added to the concept pool for subsequent composition. extensive experiments demonstrate that distok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores.",,2025-05-06,,"['fu feng', 'yucheng xie', 'xu yang', 'jing wang', 'xin geng']"
2505.03679,caraffusion: improving 2d semantic segmentation with camera-radar point   cloud fusion and zero-shot image inpainting,cs.cv,"segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. in contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. therefore, a promising approach is to fuse information from both sensors. in this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. we leverage radar point features to create pseudo-masks using the segment-anything model, treating the projected radar points as point prompts. additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. our method improves the camera-only segmentation baseline by 2.63% in miou and enhances our camera-radar fusion architecture by 1.48% in miou on the waterscenes dataset. this demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.",,2025-05-06,,"['huawei sun', 'bora kunter sahin', 'georg stettinger', 'maximilian bernhard', 'matthias schubert', 'robert wille']"
2505.03692,matching distance and geometric distribution aided learning multiview   point cloud registration,cs.cv cs.ro,"multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. this paper concentrates on pose graph construction and motion synchronization within multiview registration. previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. to identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. for motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. the source code is available at https://github.com/shi-qi-li/mdgd.",10.1109/lra.2024.3455783,2025-05-06,,"['shiqi li', 'jihua zhu', 'yifan xie', 'naiwen hu', 'di wang']"
2505.03703,fill the gap: quantifying and reducing the modality gap in image-text   representation learning,cs.cv cs.lg,"vision-language models (vlms) allow to embed texts and images in a shared representation space. however, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. while this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. we therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. our code is available at the url provided in the paper's abstract.",,2025-05-06,,"['françois role', 'sébastien meyer', 'victor amblard']"
2505.03715,disarm++: beyond scanner-free harmonization,cs.cv,"harmonization of t1-weighted mr images across different scanners is crucial for ensuring consistency in neuroimaging studies. this study introduces a novel approach to direct image harmonization, moving beyond feature standardization to ensure that extracted features remain inherently reliable for downstream analysis. our method enables image transfer in two ways: (1) mapping images to a scanner-free space for uniform appearance across all scanners, and (2) transforming images into the domain of a specific scanner used in model training, embedding its unique characteristics. our approach presents strong generalization capability, even for unseen scanners not included in the training phase. we validated our method using mr images from diverse cohorts, including healthy controls, traveling subjects, and individuals with alzheimer's disease (ad). the model's effectiveness is tested in multiple applications, such as brain age prediction (r2 = 0.60 \pm 0.05), biomarker extraction, ad classification (test accuracy = 0.86 \pm 0.03), and diagnosis prediction (auc = 0.95). in all cases, our harmonization technique outperforms state-of-the-art methods, showing improvements in both reliability and predictive accuracy. moreover, our approach eliminates the need for extensive preprocessing steps, such as skull-stripping, which can introduce errors by misclassifying brain and non-brain structures. this makes our method particularly suitable for applications that require full-head analysis, including research on head trauma and cranial deformities. additionally, our harmonization model does not require retraining for new datasets, allowing smooth integration into various neuroimaging workflows. by ensuring scanner-invariant image quality, our approach provides a robust and efficient solution for improving neuroimaging studies across diverse settings. the code is available at this link.",,2025-05-06,,"['luca caldera', 'lara cavinato', 'alessio cirone', 'isabella cama', 'sara garbarino', 'raffaele lodi', 'fabrizio tagliavini', 'anna nigri', 'silvia de francesco', 'andrea cappozzo', 'michele piana', 'francesca ieva']"
2505.0373,flexiact: towards flexible action control in heterogeneous scenarios,cs.cv cs.ai cs.mm,"action customization involves generating videos where the subject performs actions dictated by input control signals. current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. to overcome these limitations, we propose flexiact, which transfers actions from a reference video to an arbitrary target image. unlike existing methods, flexiact allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. achieving this requires precise action control, spatial structure adaptation, and consistency preservation. to this end, we introduce refadapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. so we propose fae (frequency-aware action extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. we release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/flexiact/",,2025-05-06,,"['shiyi zhang', 'junhao zhuang', 'zhaoyang zhang', 'ying shan', 'yansong tang']"
2505.03735,multi-agent system for comprehensive soccer understanding,cs.cv,"recent advancements in ai-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. to bridge this gap, we propose a comprehensive framework for holistic soccer understanding. specifically, we make the following contributions in this paper: (i) we construct soccerwiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present soccerbench, the largest and most comprehensive soccer-specific benchmark, featuring around 10k standardized multimodal (text, image, video) multi-choice qa pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce socceragent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from soccerwiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art mllms on soccerbench, highlighting the superiority of our proposed agentic system. all data and code are publicly available at: https://jyrao.github.io/socceragent/.",,2025-05-06,,"['jiayuan rao', 'zifeng li', 'haoning wu', 'ya zhang', 'yanfeng wang', 'weidi xie']"
2505.038,design description of wisdom computing persperctive,cs.ai cs.cv cs.hc,"this course design aims to develop and research a handwriting matrix recognition and step-by-step visual calculation process display system, addressing the issue of abstract formulas and complex calculation steps that students find difficult to understand when learning mathematics. by integrating artificial intelligence with visualization animation technology, the system enhances precise recognition of handwritten matrix content through the introduction of mamba backbone networks, completes digital extraction and matrix reconstruction using the yolo model, and simultaneously combines coordattention coordinate attention mechanisms to improve the accurate grasp of character spatial positions. the calculation process is demonstrated frame by frame through the manim animation engine, vividly showcasing each mathematical calculation step, helping students intuitively understand the intrinsic logic of mathematical operations. through dynamically generating animation processes for different computational tasks, the system exhibits high modularity and flexibility, capable of generating various mathematical operation examples in real-time according to student needs. by innovating human-computer interaction methods, it brings mathematical calculation processes to life, helping students bridge the gap between knowledge and understanding on a deeper level, ultimately achieving a learning experience where ""every step is understood."" the system's scalability and interactivity make it an intuitive, user-friendly, and efficient auxiliary tool in education.",,2025-05-02,,['tianyi yu']
2505.03807,facilitating video story interaction with multi-agent collaborative   system,cs.hc cs.ai cs.cv cs.ma,"video story interaction enables viewers to engage with and explore narrative content for personalized experiences. however, existing methods are limited to user selection, specially designed narratives, and lack customization. to address this, we propose an interactive system based on user intent. our system uses a vision language model (vlm) to enable machines to understand video stories, combining retrieval-augmented generation (rag) and a multi-agent system (mas) to create evolving characters and scene experiences. it includes three stages: 1) video story processing, utilizing vlm and prior knowledge to simulate human understanding of stories across three modalities. 2) multi-space chat, creating growth-oriented characters through mas interactions based on user queries and story stages. 3) scene customization, expanding and visualizing various story scenes mentioned in dialogue. applied to the harry potter series, our study shows the system effectively portrays emergent character social behavior and growth, enhancing the interactive experience in the video story world.",,2025-05-02,,"['yiwen zhang', 'jianing hao', 'zhan wang', 'hongling sheng', 'wei zeng']"
2505.03808,"ai-driven multi-source data fusion for algal bloom severity   classification in small inland water bodies: leveraging sentinel-2, dem, and   noaa climate data",cs.lg cs.cv physics.ao-ph,"harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. this research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. key data sources include copernicus sentinel-2 optical imagery, the copernicus digital elevation model (dem), and noaa's high-resolution rapid refresh (hrrr) climate data, all efficiently retrieved using platforms like google earth engine (gee) and microsoft planetary computer (mpc). the nir and two swir bands from sentinel-2, the altitude from the elevation model, the temperature and wind from noaa as well as the longitude and latitude were the most important features. the approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. while the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. the method leverages high-resolution satellite imagery and ai-driven analysis to monitor algal blooms dynamically, and although initially developed for a nasa competition in the u.s., it shows potential for global application. the complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and ai to address critical environmental challenges (https://github.com/ioannisnasios/harmfulalgalbloomdetection).",,2025-05-02,,['ioannis nasios']
2505.03821,beyond recognition: evaluating visual perspective taking in vision   language models,cs.cv cs.ai,"we investigate the ability of vision language models (vlms) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. by systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. our evaluation of several state-of-the-art models, including gpt-4-turbo, gpt-4o, llama-3.2-11b-vision-instruct, and variants of claude sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future vlm development.",,2025-05-02,,"['gracjan góral', 'alicja ziarko', 'piotr miłoś', 'michał nauman', 'maciej wołczyk', 'michał kosiński']"
2505.03826,in-situ and non-contact etch depth prediction in plasma etching via   machine learning (ann & bnn) and digital image colorimetry,cs.cv cs.ai,"precise monitoring of etch depth and the thickness of insulating materials, such as silicon dioxide and silicon nitride, is critical to ensuring device performance and yield in semiconductor manufacturing. while conventional ex-situ analysis methods are accurate, they are constrained by time delays and contamination risks. to address these limitations, this study proposes a non-contact, in-situ etch depth prediction framework based on machine learning (ml) techniques. two scenarios are explored. in the first scenario, an artificial neural network (ann) is trained to predict average etch depth from process parameters, achieving a significantly lower mean squared error (mse) compared to a linear baseline model. the approach is then extended to incorporate variability from repeated measurements using a bayesian neural network (bnn) to capture both aleatoric and epistemic uncertainty. coverage analysis confirms the bnn's capability to provide reliable uncertainty estimates. in the second scenario, we demonstrate the feasibility of using rgb data from digital image colorimetry (dic) as input for etch depth prediction, achieving strong performance even in the absence of explicit process parameters. these results suggest that the integration of dic and ml offers a viable, cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to enhanced process stability, and manufacturing efficiency.",,2025-05-03,,"['minji kang', 'seongho kim', 'eunseo go', 'donghyeon paek', 'geon lim', 'muyoung kim', 'soyeun kim', 'sung kyu jang', 'min sup choi', 'woo seok kang', 'jaehyun kim', 'jaekwang kim', 'hyeong-u kim']"
2505.03829,videollm benchmarks and evaluation: a survey,cs.cv cs.ai,"the rapid development of large language models (llms) has catalyzed significant advancements in video understanding technologies. this survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for video large language models (videollms). we examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. the paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. we highlight the performance trends of state-of-the-art videollms across these benchmarks and identify key challenges in current evaluation frameworks. additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. this survey aims to equip researchers with a structured understanding of how to effectively evaluate videollms and identify promising avenues for advancing the field of video understanding with large language models.",,2025-05-03,,['yogesh kumar']
2505.03832,video forgery detection for surveillance cameras: a review,cs.cv cs.ai,"the widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. surveillance footage plays a crucial role in security, law enforcement, and judicial processes. however, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. this paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. the findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.",,2025-05-04,,"['noor b. tayfor', 'tarik a. rashid', 'shko m. qader', 'bryar a. hassan', 'mohammed h. abdalla', 'jafar majidpour', 'aram m. ahmed', 'hussein m. ali', 'aso m. aladdin', 'abdulhady a. abdullah', 'ahmed s. shamsaldin', 'haval m. sidqi', 'abdulrahman salih', 'zaher m. yaseen', 'azad a. ameen', 'janmenjoy nayak', 'mahmood yashar hamza']"
2505.03833,pointexplainer: towards transparent parkinson's disease diagnosis,cs.cv cs.ai cs.lg,"deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of parkinson's disease. however, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. in this paper, we propose pointexplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. specifically, pointexplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3d point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. we also introduce consistency measures to further address the issue of faithfulness in explanations. extensive experiments on two benchmark datasets and a newly constructed dataset show that pointexplainer can provide intuitive explanations with no diagnostic performance degradation. the source code is available at https://github.com/chaoxuewang/pointexplainer.",,2025-05-04,,"['xuechao wang', 'sven nomm', 'junqing huang', 'kadri medijainen', 'aaro toomela', 'michael ruzhansky']"
2505.03836,obd-finder: explainable coarse-to-fine text-centric oracle bone   duplicates discovery,cs.ir cs.ai cs.cv,"oracle bone inscription (obi) is the earliest systematic writing system in china, while the identification of oracle bone (ob) duplicates is a fundamental issue in obi research. in this work, we design a progressive ob duplicate discovery framework that combines unsupervised low-level keypoints matching with high-level text-centric content-based matching to refine and rank the candidate ob duplicates with semantic awareness and interpretability. we compare our approach with state-of-the-art content-based image retrieval and image matching methods, showing that our approach yields comparable recall performance and the highest simplified mean reciprocal rank scores for both top-5 and top-15 retrieval results, and with significantly accelerated computation efficiency. we have discovered over 60 pairs of new ob duplicates in real-world deployment, which were missed by obi researchers for decades. the models, video illustration and demonstration of this work are available at: https://github.com/cszhanglmu/obd-finder/.",,2025-05-04,,"['chongsheng zhang', 'shuwen wu', 'yingqi chen', 'matthias aßenmacher', 'christian heumann', 'yi men', 'gaojuan fan', 'joão gama']"
2505.03837,explainable face recognition via improved localization,cs.cv cs.ai,"biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. face is the most common form of biometric modality that has proven effective. deep learning-based face recognition systems are now commonly used across different domains. however, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. this is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. this paper addresses this problem by applying an efficient method for explainable face recognition systems. we use a class activation mapping (cam)-based discriminative localization (very narrow/specific localization) technique called scaled directed divergence (sdd) to visually explain the results of deep learning-based face recognition systems. we perform fine localization of the face features relevant to the deep learning model for its prediction/decision. our experiments show that the sdd class activation map (cam) highlights the relevant face features very specifically compared to the traditional cam and very accurately. the provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.",,2025-05-04,,"['rashik shadman', 'daqing hou', 'faraz hussain', 'm g sarwar murshed']"
2505.03838,intellicardiac: an intelligent platform for cardiac image segmentation   and classification,eess.iv cs.ai cs.cv,"precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. we introduce intellicardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4d cardiac images and disease classification, utilizing an ai model trained on the publicly accessible acdc dataset. the system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. the system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patient's cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. intellicardiac combines a deep learning-based segmentation model with a two-step classification pipeline. the segmentation module gains an overall accuracy of 92.6%. the classification module, trained on characteristics taken from segmented heart structures, achieves 98% accuracy in five categories. these results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. intellicardiac, which supports real-time visualization, workflow integration, and ai-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis.",,2025-05-05,2025-05-07,"['ting yu tsai', 'an yu', 'meghana spurthi maadugundu', 'ishrat jahan mohima', 'umme habiba barsha', 'mei-hwa f. chen', 'balakrishnan prabhakaran', 'ming-ching chang']"
2505.03845,a deep learning approach for depressive symptoms assessment in   parkinson's disease patients using facial videos,eess.iv cs.ai cs.cv cs.lg,"parkinson's disease (pd) is a neurodegenerative disorder, manifesting with motor and non-motor symptoms. depressive symptoms are prevalent in pd, affecting up to 45% of patients. they are often underdiagnosed due to overlapping motor features, such as hypomimia. this study explores deep learning (dl) models-vivit, video swin tiny, and 3d cnn-lstm with attention layers-to assess the presence and severity of depressive symptoms, as detected by the geriatric depression scale (gds), in pd patients through facial video analysis. the same parameters were assessed in a secondary analysis taking into account whether patients were one hour after (on-medication state) or 12 hours without (off-medication state) dopaminergic medication. using a dataset of 1,875 videos from 178 patients, the video swin tiny model achieved the highest performance, with up to 94% accuracy and 93.7% f1-score in binary classification (presence of absence of depressive symptoms), and 87.1% accuracy with an 85.4% f1-score in multiclass tasks (absence or mild or severe depressive symptoms).",,2025-05-05,,"['ioannis kyprakis', 'vasileios skaramagkas', 'iro boura', 'georgios karamanis', 'dimitrios i. fotiadis', 'zinovia kefalopoulou', 'cleanthe spanaki', 'manolis tsiknakis']"
2505.03846,game: learning multimodal interactions via graph structures for   personality trait estimation,cs.cv cs.ai,"apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. in this paper, we propose game, a graph-augmented multimodal encoder designed to robustly model and fuse multi-source features for automatic personality prediction. for the visual stream, we construct a facial graph and introduce a dual-branch geo two-stream network, which combines graph convolutional networks (gcns) and convolutional neural net-works (cnns) with attention mechanisms to capture both structural and appearance-based facial cues. complementing this, global context and iden-tity features are extracted using pretrained resnet18 and vggface back-bones. to capture temporal dynamics, frame-level features are processed by a bigru enhanced with temporal attention modules. meanwhile, audio representations are derived from the vggish network, and linguistic se-mantics are captured via the xlm-roberta transformer. to achieve effective multimodal integration, we propose a channel attention-based fusion module, followed by a multi-layer perceptron (mlp) regression head for predicting personality traits. extensive experiments show that game con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.",,2025-05-05,,"['kangsheng wang', 'yuhang li', 'chengwei ye', 'yufei lin', 'huanzhen zhang', 'bohan hu', 'linuo xu', 'shuyan liu']"
2505.03848,advanced clustering framework for semiconductor image analytics   integrating deep tda with self-supervised and transfer learning techniques,cs.cv cs.ai cs.et cs.lg,"semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. this paper introduces an advanced clustering framework that integrates deep topological data analysis (tda) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. tda captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. this study highlights the transformative potential of combining tda, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.",,2025-05-05,,"['janhavi giri', 'attila lengyel', 'don kent', 'edward kibardin']"
2505.03856,an active inference model of covert and overt visual attention,cs.cv cs.ai q-bio.nc,"the ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. this paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. the model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. to test the effectiveness of the model, we analyze its behavior in the posner cueing task and a simple target focus task using two-dimensional(2d) visual data. reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. the results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.",,2025-05-06,,"['tin mišić', 'karlo koledić', 'fabio bonsignorio', 'ivan petrović', 'ivan marković']"
2505.03859,deepfakes on demand: the rise of accessible non-consensual deepfake   image generators,cs.cy cs.ai cs.cv,"advances in multimodal machine learning have made text-to-image (t2i) models increasingly accessible and popular. however, t2i models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. this paper presents an empirical study exploring the accessibility of deepfake model variants online. through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, hugging face and civitai, we demonstrate a huge rise in easily accessible deepfake models. almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on civitai. these deepfake models have been downloaded almost 15 million times since november 2022, with the models targeting a range of individuals from global celebrities to instagram users with under 10,000 followers. both stable diffusion and flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (ncii). deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (lora), requiring as few as 20 images, 24gb vram, and 15 minutes of time, making this process widely accessible via consumer-grade computers. despite these models violating the terms of service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and ncii.",,2025-05-06,,"['will hawkins', 'chris russell', 'brent mittelstadt']"
2505.03896,novel extraction of discriminative fine-grained feature to improve   retinal vessel segmentation,cs.cv cs.ai,"retinal vessel segmentation is a vital early detection method for several severe ocular diseases. despite significant progress in retinal vessel segmentation with the advancement of neural networks, there are still challenges to overcome. specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. to address these issues, we propose a novel attention u-shaped kolmogorov-arnold network named attukan along with a novel label-guided pixel-wise contrastive loss for retinal vessel segmentation. specifically, we implement attention gates into kolmogorov-arnold networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of kan blocks. additionally, we also design a novel label-guided pixel-wise contrastive loss to supervise our proposed attukan to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. experiments are conducted across four public datasets including drive, stare, chase_db1, hrf and our private dataset. attukan achieves f1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with miou scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. quantitative and qualitative results show that our attukan achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. our code will be available at https://github.com/stevezs315/attukan.",,2025-05-06,,"['shuang zeng', 'chee hong lee', 'micky c nnamdi', 'wenqi shi', 'j ben tamo', 'lei zhu', 'hangzhou he', 'xinliang zhang', 'qian chen', 'may d. wang', 'yanye lu', 'qiushi ren']"
2505.03912,"openhelix: a short survey, empirical analysis, and open-source   dual-system vla model for robotic manipulation",cs.ro cs.cv,"dual-system vla (vision-language-action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. to address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. ultimately, it will provide a low-cost open-source model for further exploration. of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. project page: https://openhelix-robot.github.io/.",,2025-05-06,,"['can cui', 'pengxiang ding', 'wenxuan song', 'shuanghao bai', 'xinyang tong', 'zirui ge', 'runze suo', 'wanqi zhou', 'yang liu', 'bofang jia', 'han zhao', 'siteng huang', 'donglin wang']"
2505.03974,deep learning framework for infrastructure maintenance: crack detection   and high-resolution imaging of infrastructure surfaces,cs.cv cs.ai eess.iv,"recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. however, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. a few studies used super-resolution techniques to address the problem of low-resolution images. nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. in order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (cnn) and efficient sub-pixel convolutional neural network (espcnn). cnn accurately classified both the classes. espcnn, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from cnn. the espcnn outperformed bicubic interpolation in all the evaluation metrics for super-resolution. based on the performance metrics, the combination of cnn and espcnn was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. the visual inspection showed that epscnn is able to capture crack propagation, complex geometry of even minor cracks. the proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.",,2025-05-06,,"['nikhil m. pawar', 'jorge a. prozzi', 'feng hong', 'surya sarat chandra congress']"
2505.03991,"action spotting and precise event detection in sports: datasets,   methods, and challenges",cs.cv,"video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. recent advancements in deep learning, particularly convolutional neural networks (cnns) and transformers, have significantly improved accuracy and efficiency in temporal action localization (tal), action spotting (as), and precise event spotting (pes). this survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. we thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. this survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.",,2025-05-06,,"['hao xu', 'arbind agrahari baniya', 'sam well', 'mohamed reda bouadjenek', 'richard dazeley', 'sunil aryal']"
2505.04003,prototype-based information compensation network for multi-source remote   sensing data classification,eess.iv cs.cv,"multi-source remote sensing data joint classification aims to provide accuracy and reliability of land cover classification by leveraging the complementary information from multiple data sources. existing methods confront two challenges: inter-frequency multi-source feature coupling and inconsistency of complementary information exploration. to solve these issues, we present a prototype-based information compensation network (picnet) for land cover classification based on hsi and sar/lidar data. specifically, we first design a frequency interaction module to enhance the inter-frequency coupling in multi-source feature extraction. the multi-source features are first decoupled into high- and low-frequency components. then, these features are recoupled to achieve efficient inter-frequency communication. afterward, we design a prototype-based information compensation module to model the global multi-source complementary information. two sets of learnable modality prototypes are introduced to represent the global modality information of multi-source data. subsequently, cross-modal feature integration and alignment are achieved through cross-attention computation between the modality-specific prototype vectors and the raw feature representations. extensive experiments on three public datasets demonstrate the significant superiority of our picnet over state-of-the-art methods. the codes are available at https://github.com/oucailab/picnet.",,2025-05-06,,"['feng gao', 'sheng liu', 'chuanzheng gong', 'xiaowei zhou', 'jiayi wang', 'junyu dong', 'qian du']"
2505.04006,the eye as a window to systemic health: a survey of retinal imaging from   classical techniques to oculomics,eess.iv cs.cv,"the unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. the retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. the advancement in imaging technology leveraging artificial intelligence has seized this opportunity to bridge the gap between the eye and human health. this track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. the new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. in this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of ai-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. we also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.",,2025-05-06,,"['n/a inamullah', 'imran razzak', 'shoaib jameel']"
2505.0405,terrafusion: joint generation of terrain geometry and texture using   latent diffusion models,cs.gr cs.cv,"3d terrain models are essential in fields such as video game development and film production. since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. however, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. in this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. first, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.",,2025-05-06,,"['kazuki higo', 'toshiki kanai', 'yuki endo', 'yoshihiro kanamori']"
2505.04052,person-in-situ: scene-consistent human image insertion with   occlusion-aware pose control,cs.gr cs.cv,"compositing human figures into scene images has broad applications in areas such as entertainment and advertising. however, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer. moreover, they offer limited control over the inserted person's pose. to address these challenges, we propose two methods. both allow explicit pose control via a 3d body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks. the first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly. the second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision. quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses.",,2025-05-06,,"['shun masuda', 'yuki endo', 'yoshihiro kanamori']"
2505.04055,foodtrack: estimating handheld food portions with egocentric video,cs.cv,"accurately tracking food consumption is crucial for nutrition and health monitoring. traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. we propose the foodtrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. foodtrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. we achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.",,2025-05-06,,"['ervin wang', 'yuhao chen']"
2505.04087,seva: leveraging single-step ensemble of vicinal augmentations for   test-time adaptation,cs.cv,"test-time adaptation (tta) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. while existing tta methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. in this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. to address this limitation, we propose a novel tta approach named single-step ensemble of vicinal augmentations (seva), which can take advantage of data augmentations without increasing the computational burden. specifically, instead of explicitly utilizing the augmentation strategy to generate new data, seva develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of tta. the comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of seva. the code will be publicly available.",,2025-05-06,,"['zixuan hu', 'yichun hu', 'ling-yu duan']"
2505.04095,scalable aerial gnss localization for marine robots,cs.ro cs.cv,"accurate localization is crucial for water robotics, yet traditional onboard global navigation satellite system (gnss) approaches are difficult or ineffective due to signal reflection on the water's surface and its high cost of aquatic gnss receivers. existing approaches, such as inertial navigation, doppler velocity loggers (dvl), slam, and acoustic-based methods, face challenges like error accumulation and high computational complexity. therefore, a more efficient and scalable solution remains necessary. this paper proposes an alternative approach that leverages an aerial drone equipped with gnss localization to track and localize a marine robot once it is near the surface of the water. our results show that this novel adaptation enables accurate single and multi-robot marine robot localization.",,2025-05-06,,"['shuo wen', 'edwin meriaux', 'mariana sosa guzmán', 'charlotte morissette', 'chloe si', 'bobak baghi', 'gregory dudek']"
2505.04097,3d brain mri classification for alzheimer diagnosis using cnn with data   augmentation,eess.iv cs.cv cs.lg,"a three-dimensional convolutional neural network was developed to classify t1-weighted brain mri scans as healthy or alzheimer. the network comprises 3d convolution, pooling, batch normalization, dense relu layers, and a sigmoid output. using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the roc curve of 0.961, an improvement of approximately 0.027 over resizing alone. sensitivity and specificity both exceeded 0.90. these results align with prior work reporting up to 0.10 gain via synthetic augmentation. the findings demonstrate the effectiveness of simple augmentation for 3d mri classification and motivate future exploration of advanced augmentation methods and architectures such as 3d u-net and vision transformers.",,2025-05-06,,"['thien nhan vo', 'bac nam ho', 'thanh xuan truong']"
2505.04109,one2any: one-reference 6d pose estimation for any object,cs.cv,"6d object pose estimation remains challenging for many applications due to dependencies on complete 3d models, multi-view images, or training limited to specific object categories. these requirements make generalization to novel objects difficult for which neither 3d models nor multi-view images may be available. to address this, we propose a novel method one2any that estimates the relative 6-degrees of freedom (dof) object pose using only a single reference-single query rgb-d image, without prior knowledge of its 3d model, multi-view data, or category constraints. we treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive reference object pose embedding (rope) that encodes an object shape, orientation, and texture from a single reference view. using this embedding, a u-net-based pose decoding module produces reference object coordinate (roc) for new views, enabling fast and accurate pose estimation. this simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or cad inputs, at a fraction of compute.",,2025-05-06,,"['mengya liu', 'siyuan li', 'ajad chhatkuli', 'prune truong', 'luc van gool', 'federico tombari']"
2505.04147,"r^3-vqa: ""read the room"" by video social reasoning",cs.cv cs.ai,"""read the room"" is a significant social reasoning capability in human daily life. humans can infer others' mental states from subtle social cues. previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. in this paper, we contribute a valuable, high-quality, and comprehensive video dataset named r^3-vqa with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. moreover, we include human-annotated and model-generated qas. our task r^3-vqa includes three aspects: social event understanding, mental state estimation, and social causal reasoning. as a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (lvlms). comprehensive experiments show that (i) lvlms are still far from human-level consistent social reasoning in complex social scenarios; (ii) theory of mind (tom) prompting can help lvlms perform better on social reasoning tasks. we provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.",,2025-05-07,,"['lixing niu', 'jiapeng li', 'xingping yu', 'shu wang', 'ruining feng', 'bo wu', 'ping wei', 'yisen wang', 'lifeng fan']"
2505.0415,learning from similarity proportion loss for classifying skeletal muscle   recovery stages,cs.cv cs.lg,"evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. the conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. there is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. given the limited availability of fully labeled data, a possible approach is learning from label proportions (llp), a weakly supervised learning method using class label proportions. however, current llp methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. to address these issues, we propose ordinal scale learning from similarity proportion (oslsp), which uses a similarity proportion loss derived from two bag combinations. oslsp can update the feature extractor by using class proportion attention to the ordinal scale of the class. our model with oslsp outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.",,2025-05-07,2025-05-08,"['yu yamaoka', 'weng ian chan', 'shigeto seno', 'soichiro fukada', 'hideo matsuda']"
2505.04173,diffpattern-flex: efficient layout pattern generation via discrete   diffusion,cs.lg cs.cv,"recent advancements in layout pattern generation have been dominated by deep generative models. however, relying solely on neural networks for legality guarantees raises concerns in many practical applications. in this paper, we present \tool{diffpattern}-flex, a novel approach designed to generate reliable layout patterns efficiently. \tool{diffpattern}-flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. to ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. experimental results across various benchmarks demonstrate that \tool{diffpattern}-flex significantly outperforms existing methods and excels at producing reliable layout patterns.",,2025-05-07,,"['zixiao wang', 'wenqian zhao', 'yunheng shen', 'yang bai', 'guojin chen', 'farzan farnia', 'bei yu']"
2505.04175,dota: deformable optimized transformer architecture for end-to-end text   recognition with retrieval-augmented generation,cs.cv cs.ai,"text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. this paper introduces a novel end-to-end framework that combines resnet and vision transformer backbones with advanced methodologies, including deformable convolutions, retrieval-augmented generation, and conditional random fields (crf). these innovations collectively enhance feature representation and improve optical character recognition (ocr) performance. specifically, the framework substitutes standard convolution layers in the third and fourth blocks with deformable convolutions, leverages adaptive dropout for regularization, and incorporates crf for more refined sequence modeling. extensive experiments conducted on six benchmark datasets ic13, ic15, svt, iiit5k, svtp, and cute80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on ic13, 58.26% on ic15, 88.10% on svt, 74.13% on iiit5k, 82.17% on svtp, and 66.67% on cute80, resulting in an average accuracy of 77.77%. these results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.",,2025-05-07,,"['naphat nithisopa', 'teerapong panboonyuen']"
2505.04185,s3d: sketch-driven 3d model generation,cs.cv cs.ai,"generating high-quality 3d models from 2d sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. in this paper, we present s3d, a novel framework that converts simple hand-drawn sketches into detailed 3d models. our method utilizes a u-net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3d representation that can be rendered from novel views. to ensure robust consistency between the sketch domain and the 3d output, we introduce a novel style-alignment loss that aligns the u-net bottleneck features with the initial encoder outputs of the 3d generation module, significantly enhancing reconstruction fidelity. to further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. this streamlined framework demonstrates the effectiveness of s3d in generating high-quality 3d models from sketch inputs. the source code for this project is publicly available at https://github.com/hailsong/s3d.",,2025-05-07,,"['hail song', 'wonsik shin', 'naeun lee', 'soomin chung', 'nojun kwak', 'woontack woo']"
2505.04192,videopath-llava: pathology diagnostic reasoning through video   instruction tuning,cs.cv cs.ai cs.cl,"we present videopath-llava, the first large multimodal model (lmm) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. by generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, videopath-llava bridges visual narratives with diagnostic reasoning.   central to our approach is the videopath-instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on youtube. although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. to overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. videopath-llava establishes a new benchmark in pathology video analysis and offers a promising foundation for future ai systems that support clinical decision-making through integrated visual and diagnostic reasoning. our code, data, and model are publicly available at https://github.com/trinhvg/videopath-llava.",,2025-05-07,,"['trinh t. l. vuong', 'jin tae kwak']"
2505.04201,stola: self-adaptive touch-language framework with tactile commonsense   reasoning in open-ended scenarios,cs.cv,"this paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. we identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. to overcome these challenges, we introduce stola, a self-adaptive touch-language framework. stola utilizes mixture of experts (moe) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. experiments show stola exhibits competitive performance compared to existing models on the physiclear benchmark and self-constructed datasets, proving the effectiveness of the mixture of experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.",,2025-05-07,,"['ning cheng', 'jinan xu', 'jialing chen', 'wenjuan han']"
2505.04214,cm1 -- a dataset for evaluating few-shot information extraction with   large vision language models,cs.cv,"the automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. a reliable extraction is a prerequisite for the mass digitization efforts of many archives. large vision language models (lvlm) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. in this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of lvlms. the cm1 documents are a historic collection of forms with handwritten entries created in europe to administer the care and maintenance program after world war two. the dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. we provide baseline results for two different lvlms and compare performances to an established full-page extraction model. while the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered lvlms benefit from their size and heavy pretraining and outperform the classical approach.",,2025-05-07,,"['fabian wolf', 'oliver tüselmann', 'arthur matei', 'lukas hennies', 'christoph rass', 'gernot a. fink']"
2505.04228,low resolution next best view for robot packing,cs.ro cs.cv,"automating the packing of objects with robots is a key challenge in industrial automation, where efficient object perception plays a fundamental role. this paper focuses on scenarios where precise 3d reconstruction is not required, prioritizing cost-effective and scalable solutions. the proposed low-resolution next best view (lr-nbv) algorithm leverages a utility function that balances pose redundancy and acquisition density, ensuring efficient object reconstruction. experimental validation demonstrates that lr-nbv consistently outperforms standard nbv approaches, achieving comparable accuracy with significantly fewer poses. this method proves highly suitable for applications requiring efficiency, scalability, and adaptability without relying on high-precision sensing.",,2025-05-07,,"['giuseppe fabio preziosa', 'chiara castellano', 'andrea maria zanchettin', 'marco faroni', 'paolo rocco']"
2505.04262,bridging geometry-coherent text-to-3d generation with multi-view   diffusion priors and gaussian splatting,cs.cv,"score distillation sampling (sds) leverages pretrained 2d diffusion models to advance text-to-3d generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3d content. in this work, we propose coupled score distillation (csd), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3d generation while enabling the stable and direct optimization of 3d gaussian splatting. specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3d assets. additionally, we propose a framework that directly optimizes 3d gaussian splatting (3d-gs) with random initialization to generate geometrically consistent 3d content. we further employ a deformable tetrahedral grid, initialized from 3d-gs and refined through csd, to produce high-quality, refined meshes. quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",,2025-05-07,,"['feng yang', 'wenliang qian', 'wangmeng zuo', 'hui li']"
2505.0427,object-shot enhanced grounding network for egocentric video,cs.cv cs.ai,"egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. to address these limitations, we propose osgnet, an object-shot enhanced grounding network for egocentric video. specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. experiments conducted on three datasets demonstrate that osgnet achieves state-of-the-art performance, validating the effectiveness of our approach. our code can be found at https://github.com/yisen-feng/osgnet.",,2025-05-07,,"['yisen feng', 'haoyu zhang', 'meng liu', 'weili guan', 'liqiang nie']"
2505.04276,hdifftg: a lightweight hybrid diffusion-transformer-gcn architecture for   3d human pose estimation,cs.cv cs.mm,"we propose hdifftg, a novel 3d human pose estimation (3dhpe) method that integrates transformer, graph convolutional network (gcn), and diffusion model into a unified framework. hdifftg leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. the transformer captures global spatiotemporal dependencies, the gcn models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. this integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. evaluation results on the human3.6m and mpi-inf-3dhp datasets demonstrate that hdifftg achieves state-of-the-art (sota) performance on the mpi-inf-3dhp dataset while excelling in both accuracy and computational efficiency. additionally, the model exhibits exceptional robustness in noisy and occluded environments. source codes and models are available at https://github.com/circejie/hdifftg",,2025-05-07,,"['yajie fu', 'chaorui huang', 'junwei li', 'hui kong', 'yibin tian', 'huakang li', 'zhiyuan zhang']"
2505.04281,ts-diff: two-stage diffusion model for low-light raw image enhancement,cs.cv eess.iv,"this paper presents a novel two-stage diffusion model (ts-diff) for enhancing extremely low-light raw images. in the pre-training stage, ts-diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. camera feature integration (cfi) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. during the aligning stage, cfis are averaged to create a target-specific cfi$^t$, which is fine-tuned using a small amount of real raw data to adapt to the noise characteristics of specific cameras. a structural reparameterization technique further simplifies cfi$^t$ for efficient deployment. to address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. additionally, a novel dataset, qid, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. experimental results demonstrate that ts-diff achieves state-of-the-art performance on multiple datasets, including qid, sid, and eld, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. these findings highlight the robustness and versatility of ts-diff, making it a practical solution for low-light imaging applications. source codes and models are available at https://github.com/circcclek/ts-diff",,2025-05-07,,"['yi li', 'zhiyuan zhang', 'jiangnan xia', 'jianghan cheng', 'qilong wu', 'junwei li', 'yibin tian', 'hui kong']"
2505.04306,mode: mixture of diffusion experts for any occluded face recognition,cs.cv,"with the continuous impact of epidemics, people have become accustomed to wearing masks. however, most current occluded face recognition (ofr) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. in this paper, we propose an identity-gated mixture of diffusion experts (mode) for ofr. each diffusion-based generative expert estimates one possible complete image for occluded faces. considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. to ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. moreover, our mode is a plug-and-play module for most existing face recognition models. extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.",,2025-05-07,,"['qiannan fan', 'zhuoyang li', 'jitong li', 'chenyang cao']"
2505.0432,multi-turn consistent image editing,cs.cv,"many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. however, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. as a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. to address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. our approach leverages flow matching for accurate image inversion and a dual-objective linear quadratic regulators (lqr) for stable sampling, effectively mitigating error accumulation. additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.",,2025-05-07,,"['zijun zhou', 'yingying deng', 'xiangyu he', 'weiming dong', 'fan tang']"
2505.04347,countdiffusion: text-to-image synthesis with training-free   counting-guidance diffusion,cs.cv,"stable diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. in this paper, we propose countdiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. countdiffusion consists of two stages. in the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. in the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. the proposed countdiffusion can be plugged into any diffusion-based text-to-image (t2i) generation models without further training. experiment results demonstrate the superiority of our proposed countdiffusion, which improves the accurate object quantity generation ability of t2i models by a large margin.",,2025-05-07,,"['yanyu li', 'pencheng wan', 'liang han', 'yaowei wang', 'liqiang nie', 'min zhang']"
2505.04369,wdmamba: when wavelet degradation prior meets vision mamba for image   dehazing,cs.cv,"in this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. exploiting this insight, we propose a novel dehazing framework, wdmamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. this coarse-to-fine strategy enables wdmamba to effectively capture features specific to each stage of the dehazing process, resulting in high-quality restored images. specifically, in the low-frequency restoration stage, we integrate mamba blocks to reconstruct global structures with linear complexity, efficiently removing overall haze and producing a coarse restored image. thereafter, the detail enhancement stage reinstates fine-grained information that may have been overlooked during the previous phase, culminating in the final dehazed output. furthermore, to enhance detail retention and achieve more natural dehazing, we introduce a self-guided contrastive regularization during network training. by utilizing the coarse restored output as a hard negative example, our model learns more discriminative representations, substantially boosting the overall dehazing performance. extensive evaluations on public dehazing benchmarks demonstrate that our method surpasses state-of-the-art approaches both qualitatively and quantitatively. code is available at https://github.com/sunj000/wdmamba.",,2025-05-07,,"['jie sun', 'heng liu', 'yongzhen wang', 'xiao-ping zhang', 'mingqiang wei']"
2505.04375,"balancing accuracy, calibration, and efficiency in active learning with   vision transformers under label noise",cs.cv cs.ai cs.lg,"fine-tuning pre-trained convolutional neural networks on imagenet for downstream tasks is well-established. still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. we explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (base and large with 16x16 and 32x32 patch sizes) and three swin transformer configurations (tiny, small, and base) on cifar10 and cifar100 datasets, under varying label noise rates. our findings show that larger vit models (vitl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while swin transformers exhibit weaker robustness across all noise levels. we find that smaller patch sizes do not always lead to better performance, as vitl16 performs consistently worse than vitl32 while incurring a higher computational cost. we also find that information-based active learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. we hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.",,2025-05-07,,"[""moseli mots'oehli"", 'hope mogale', 'kyungim baek']"
2505.04376,label-efficient single photon images classification via active learning,eess.iv cs.cv,"single-photon lidar achieves high-precision 3d imaging in extreme environments through quantum-level photon detection technology. current research primarily focuses on reconstructing 3d scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. this paper presents the first active learning framework for single-photon image classification. the core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. by identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. on real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. this illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.",,2025-05-07,,"['zili zhang', 'ziting wen', 'yiheng qiang', 'hongzhou dong', 'wenle dong', 'xinyang li', 'xiaofan wang', 'xiaoqiang ren']"
2505.0438,tetrahedron-net for medical image registration,eess.iv cs.cv cs.ir,"medical image registration plays a vital role in medical image processing. extracting expressive representations for medical images is crucial for improving the registration quality. one common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. the de facto structure, u-net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. in this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. the new decoder is designed to interact with both the original encoder and decoder. in this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. the new architecture is concise yet generalized, with only one encoder and two decoders forming a ``tetrahedron'' structure, thereby dubbed tetrahedron-net. three instantiations of tetrahedron-net are further constructed regarding the different structures of the appended decoder. our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. finally, such a ``tetrahedron'' design can also be easily integrated into popular u-net-like architectures including voxelmorph, vit-v-net, and transmorph, leading to consistent performance gains.",,2025-05-07,,"['jinhai xiang', 'shuai guo', 'qianru han', 'dantong shi', 'xinwei he', 'xiang bai']"
2505.04384,data: multi-disentanglement based contrastive learning for open-world   semi-supervised deepfake attribution,cs.cv,"deepfake attribution (dfa) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. however, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. to address these issues, in this paper we propose an innovative multi-disentanglement based contrastive learning framework, data, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (oss-dfa) task. specifically, since all generation techniques can be abstracted into a similar architecture, data defines the concept of 'orthonormal deepfake basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. additionally, to enhance the standardization and discrimination of features, data uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. extensive experimental evaluations show that data achieves state-of-the-art performance on the oss-dfa benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.",,2025-05-07,,"['ming-hui liu', 'xiao-qian liu', 'xin luo', 'xin-shun xu']"
2505.04387,geometry-aware texture generation for 3d head modeling with   artist-driven control,cs.gr cs.cv,"creating realistic 3d head assets for virtual characters that match a precise artistic vision remains labor-intensive. we present a novel framework that streamlines this process by providing artists with intuitive control over generated 3d heads. our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. the framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. experiments demonstrate that our method produces diverse results with clean geometries. we showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. this integrated approach aims to streamline the artistic workflow in virtual character creation.",,2025-05-07,,"['amin fadaeinejad', 'abdallah dib', 'luiz gustavo hafemann', 'emeline got', 'trevor anderson', 'amaury depierre', 'nikolaus f. troje', 'marcus a. brubaker', 'marc-andré carbonneau']"
2505.04392,predicting road surface anomalies by visual tracking of a preceding   vehicle,cs.cv,"a novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. the method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. the method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. a challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. the method is effective and performs in real time on standard consumer hardware.",,2025-05-07,,"['petr jahoda', 'jan cech']"
2505.04394,swinlip: an efficient visual speech encoder for lip reading using swin   transformer,cs.cv cs.sd eess.as,"this paper presents an efficient visual speech encoder for lip reading. while most recent lip reading studies have been based on the resnet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). to overcome the limitations of convolutional neural network (cnn)-based models, we apply the hierarchical structure and window self-attention of the swin transformer to lip reading. we configure a new lightweight scale of the swin transformer suitable for processing lip reading data and present the swinlip visual speech encoder, which efficiently reduces computational load by integrating modified convolution-augmented transformer (conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. through extensive experiments, we have validated that our swinlip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. in particular, our swinlip demonstrated robust performance in both english lrw and mandarin lrw-1000 datasets and achieved state-of-the-art performance on the mandarin lrw-1000 dataset with less computation compared to the existing state-of-the-art model.",10.1016/j.neucom.2025.130289,2025-05-07,,"['young-hu park', 'rae-hong park', 'hyung-min park']"
2505.04397,deep residual learning with product units,cs.cv cs.ai cs.lg eess.iv,"we propose a deep product-unit residual neural network (pure) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. pure replaces conventional convolutional layers with 2d product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. we validate pure on three benchmark datasets. on galaxy10 decals, pure34 achieves the highest test accuracy of 84.89%, surpassing the much deeper resnet152, while converging nearly five times faster and demonstrating strong robustness to poisson noise. on imagenet, pure architectures outperform standard resnet models at similar depths, with pure34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper resnet variants (resnet50, resnet101) while utilizing significantly fewer parameters and computational resources. on cifar-10, pure consistently outperforms resnet variants across varying depths, with pure272 reaching 95.01% test accuracy, comparable to resnet1001 but at less than half the model size. these results demonstrate that pure achieves a favorable balance between accuracy, efficiency, and robustness. compared to traditional residual networks, pure not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.",,2025-05-07,,"['ziyuan li', 'uwe jaekel', 'babette dellen']"
2505.04408,mfseg: efficient multi-frame 3d semantic segmentation,cs.cv,"we propose mfseg, an efficient multi-frame 3d semantic segmentation framework. by aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, mfseg reduces computational overhead while maintaining high accuracy. moreover, by employing a lightweight mlp-based point decoder, our method eliminates the need to upsample redundant points from past frames. experiments on the nuscenes and waymo datasets show that mfseg outperforms existing methods, demonstrating its effectiveness and efficiency.",,2025-05-07,,"['chengjie huang', 'krzysztof czarnecki']"
2505.0441,declip: decoupled learning for open-vocabulary dense perception,cs.cv,"dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. while vision-language models (vlms) like clip have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. in this work, we present our observation that clip's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. to address this issue, we propose declip, a novel framework that enhances clip by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. the ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as dino. extensive experiments demonstrate that declip significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/declip}.",,2025-05-07,,"['junjie wang', 'bin chen', 'yulin li', 'bin kang', 'yichi chen', 'zhuotao tian']"
2505.04424,rlministyler: light-weight rl style agent for arbitrary sequential   neural style generation,cs.cv,"arbitrary style transfer aims to apply the style of any given artistic image to another content image. still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer rlministyler. this framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. through a series of experiments across image various resolutions, we have validated the advantages of rlministyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. codes are available at https://github.com/fengxiaoming520/rlministyler.",,2025-05-07,,"['jing hu', 'chengming feng', 'shu hu', 'ming-ching chang', 'xin li', 'xi wu', 'xin wang']"
2505.0446,learning real facial concepts for independent deepfake detection,cs.cv,"deepfake detection models often struggle with generalization to unseen datasets, manifesting as misclassifying real instances as fake in target domains. this is primarily due to an overreliance on forgery artifacts and a limited understanding of real faces. to address this challenge, we propose a novel approach realid to enhance generalization by learning a comprehensive concept of real faces while assessing the probabilities of belonging to the real and fake classes independently. realid comprises two key modules: the real concept capture module (realc2) and the independent dual-decision classifier (idc). with the assistance of a multireal memory, realc2 maintains various prototypes for real faces, allowing the model to capture a comprehensive concept of real class. meanwhile, idc redefines the classification strategy by making independent decisions based on the concept of the real class and the presence of forgery artifacts. through the combined effect of the above modules, the influence of forgery-irrelevant patterns is alleviated, and extensive experiments on five widely used datasets demonstrate that realid significantly outperforms existing state-of-the-art methods, achieving a 1.74% improvement in average accuracy.",,2025-05-07,,"['ming-hui liu', 'harry cheng', 'tianyi wang', 'xin luo', 'xin-shun xu']"
2505.04481,cad-llama: leveraging large language models for computer-aided design   parametric 3d model generation,cs.cv,"recently, large language models (llms) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. this study investigates the generation of parametric sequences for computer-aided design (cad) models using llms. this endeavor represents an initial step towards creating parametric 3d shapes with llms, as cad model parameters directly correlate with shapes in three-dimensional space. despite the formidable generative capacities of llms, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3d structures. to address this, we present cad-llama, a framework designed to enhance pretrained llms for generating parametric 3d cad models. specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3d cad command sequences into structured parametric cad code (spcc), incorporating hierarchical semantic descriptions. furthermore, we propose an adaptive pretraining approach utilizing spcc, followed by an instruction tuning process aligned with cad-specific guidelines. this methodology aims to equip llms with the spatial knowledge inherent in parametric sequences. experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing llm baselines.",,2025-05-07,,"['jiahao li', 'weijian ma', 'xueyang li', 'yunzhong lou', 'guichun zhou', 'xiangdong zhou']"
2505.04485,fa-kpconv: introducing euclidean symmetries to kpconv via frame   averaging,cs.cv,"we present frame-averaging kernel-point convolution (fa-kpconv), a neural network architecture built on top of the well-known kpconv, a widely adopted backbone for 3d point cloud analysis. even though invariance and/or equivariance to euclidean transformations are required for many common tasks, kpconv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. using frame averaging, we allow to flexibly customize point cloud neural networks built with kpconv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. by simply wrapping around an existing kpconv-based network, fa-kpconv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. we showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.",,2025-05-07,2025-05-08,"['ali alawieh', 'alexandru p. condurache']"
2505.04488,"""i can see forever!"": evaluating real-time videollms for assisting   individuals with visual impairments",cs.cv cs.ai cs.hc cs.mm,"the visually impaired population, especially the severely visually impaired, is currently large in scale, and daily activities pose significant challenges for them. although many studies use large language and vision-language models to assist the blind, most focus on static content and fail to meet real-time perception needs in dynamic and complex environments, such as daily activities. to provide them with more effective intelligent assistance, it is imperative to incorporate advanced visual understanding technologies. although real-time vision and speech interaction videollms demonstrate strong real-time visual understanding, no prior work has systematically evaluated their effectiveness in assisting visually impaired individuals. in this work, we conduct the first such evaluation. first, we construct a benchmark dataset (visassistdaily), covering three categories of assistive tasks for visually impaired individuals: basic skills, home life tasks, and social life tasks. the results show that gpt-4o achieves the highest task success rate. next, we conduct a user study to evaluate the models in both closed-world and open-world scenarios, further exploring the practical challenges of applying videollms in assistive contexts. one key issue we identify is the difficulty current models face in perceiving potential hazards in dynamic environments. to address this, we build an environment-awareness dataset named safevid and introduce a polling mechanism that enables the model to proactively detect environmental risks. we hope this work provides valuable insights and inspiration for future research in this field.",,2025-05-07,,"['ziyi zhang', 'zhen sun', 'zongmin zhang', 'zifan peng', 'yuemeng zhao', 'zichun wang', 'zeren luo', 'ruiting zuo', 'xinlei he']"
2505.04497,defining and quantifying creative behavior in popular image generators,cs.cv cs.ai,"creativity of generative ai models has been a subject of scientific debate in the last years, without a conclusive answer. in this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable ai model for a given task. we evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.",,2025-05-07,2025-05-08,"['aditi ramaswamy', 'hana chockler', 'melane navaratnarajah']"
2505.04502,leveraging simultaneous usage of edge gpu hardware engines for video   face detection and recognition,cs.cv cs.ar eess.iv,"video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. this paper aims to maximize the simultaneous usage of hardware engines available in edge gpus nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. this also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via gbps ethernet network. this constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. in addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. the results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent nvidia edge orin gpu, higher throughput, and a slight saving of power consumption of around 300 mw, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. the performance gets even higher by considering several video streams simultaneously. further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor rt framework for the face recognition task was lower. thus, the paper suggests some hardware improvements to the existing edge gpu processors to enhance their performance even higher.",,2025-05-07,,"['asma baobaid', 'mahmoud meribout']"
2505.04512,hunyuancustom: a multimodal-driven architecture for customized video   generation,cs.cv,"customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. in this paper, we propose hunyuancustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. built upon hunyuanvideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on llava for enhanced multi-modal understanding, along with an image id enhancement module that leverages temporal concatenation to reinforce identity features across frames. to enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an audionet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. extensive experiments on single- and multi-subject scenarios demonstrate that hunyuancustom significantly outperforms state-of-the-art open- and closed-source methods in terms of id consistency, realism, and text-video alignment. moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. all the code and models are available at https://hunyuancustom.github.io.",,2025-05-07,2025-05-08,"['teng hu', 'zhentao yu', 'zhengguang zhou', 'sen liang', 'yuan zhou', 'qin lin', 'qinglin lu']"
2505.04522,text2ct: towards 3d ct volume generation from free-text descriptions   using diffusion model,eess.iv cs.cv,"generating 3d ct volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. in this paper, we introduce text2ct, a novel approach for synthesizing 3d ct volumes from textual descriptions using the diffusion model. unlike previous methods that rely on fixed-format text input, text2ct employs a novel prompt formulation that enables generation from diverse, free-text descriptions. the proposed framework encodes medical text into latent representations and decodes them into high-resolution 3d ct scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3d framework. our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.",,2025-05-07,,"['pengfei guo', 'can zhao', 'dong yang', 'yufan he', 'vishwesh nath', 'ziyue xu', 'pedro r. a. s. bassi', 'zongwei zhou', 'benjamin d. simon', 'stephanie anne harmon', 'baris turkbey', 'daguang xu']"
2505.04524,edge-gpu based face tracking for face detection and recognition   acceleration,cs.cv cs.ar cs.lg eess.iv,"cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. however, despite their high performance, which could be reached using specialized edge or cloud ai hardware accelerators, there is still room for improvement in throughput and power consumption. this paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge gpus, namely nvidia jetson agx orin. first, it leverages the simultaneous usage of all its hardware engines to improve processing time. this offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the cpu or, to a higher extent, to the gpu core. additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. the results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the orin gpu and tracker integration into the pipeline yield an impressive throughput of 290 fps (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. additionally, a substantial saving of power consumption of around 800 mw was achieved when compared to running the task on the cpu/gpu engines only and without integrating a tracker into the orin gpu\'92s pipeline. this hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.",,2025-05-07,,"['asma baobaid', 'mahmoud meribout']"
2505.04526,dfvo: learning darkness-free visible and infrared image disentanglement   and fusion all at once,cs.cv cs.ai,"visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. however, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. to this end, a darkness-free network is proposed to handle visible and infrared image disentanglement and fusion all at once (dfvo), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. specifically, we construct a latent-common feature extractor (lcfe) to obtain latent features for the cascaded tasks strategy. firstly, a details-extraction module (dem) is devised to acquire high-frequency semantic information. secondly, we design a hyper cross-attention module (hcam) to extract low-frequency information and preserve texture features from source images. finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. particularly, dfvo can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the llvip dataset with 63.258 db psnr and 0.724 cc, providing more effective information for high-level vision tasks. our code is publicly accessible at https://github.com/davin-qi530/dfvo.",,2025-05-07,,"['qi zhou', 'yukai shi', 'xiaojun yang', 'xiaoyu xian', 'lunjia liao', 'ruimao zhang', 'liang lin']"
2505.04529,raft: robust augmentation of features for image segmentation,cs.cv,"image segmentation is a powerful computer vision technique for scene understanding. however, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. however, deep neural networks trained on synthetic data often face the syn2real problem, leading to poor performance in real-world deployments.   to mitigate the aforementioned gap in image segmentation, we propose raft, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. to validate raft, we perform experiments on the synthetic-to-real ""synthia->cityscapes"" and ""gtav->cityscapes"" benchmarks. we managed to surpass the previous state of the art, halo. synthia->cityscapes experiences an improvement in miou* upon domain adaptation of 2.1%/79.9%, and gtav->cityscapes experiences a 0.4%/78.2% improvement in miou. furthermore, we test our approach on the real-to-real benchmark of ""cityscapes->acdc"", and again surpass halo, with a gain in miou upon adaptation of 1.3%/73.2%. finally, we examine the effect of the allocated annotation budget and various components of raft upon the final transfer miou.",,2025-05-07,,"['edward humes', 'xiaomin lin', 'uttej kallakuri', 'tinoosh mohsenin']"
2505.0454,registration of 3d point sets using exponential-based similarity matrix,cs.cv,"point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3d point sets captured from varying viewpoints using depth sensors such as lidar or structured light. in modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. however, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. these challenges can lead to misalignments and, consequently, to inaccurate or distorted 3d reconstructions. in this work, we address both these limitations by proposing a robust modification to the classic iterative closest point (icp) algorithm. our method, termed exponential similarity matrix icp (esm-icp), integrates a gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. this matrix facilitates improved estimation of both rotational and translational components during alignment. we demonstrate the robustness of esm-icp in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-gaussian noise. our results show that esm-icp outperforms traditional geometric registration techniques as well as several recent learning-based methods. to encourage reproducibility and community engagement, our full implementation is made publicly available on github. https://github.com/aralab-unr/esm_icp",,2025-05-07,,"['ashutosh singandhupe', 'sanket lokhande', 'hung manh la']"
2505.04575,componential prompt-knowledge alignment for domain incremental learning,cs.cv cs.lg,"domain incremental learning (dil) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. this arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.to address this, we propose componential prompt-knowledge alignment (ka-prompt), a novel prompt-based dil method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. ka-prompt operates in two phases: (1) initial componential structure configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) online alignment preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. extensive experiments on dil benchmarks demonstrate the effectiveness of our ka-prompt. our source code is available at https://github.com/zhoujiahuan1991/icml2025-ka-prompt",,2025-05-07,,"['kunlun xu', 'xu zou', 'gang hua', 'jiahuan zhou']"
2505.04586,active sampling for mri-based sequential decision making,eess.iv cs.cv cs.lg,"despite the superior diagnostic capability of magnetic resonance imaging (mri), its use as a point-of-care (poc) device remains limited by high cost and complexity. to enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. such work shows that single diagnostic decisions can be made, but if we aspire to see mri as a true poc, multiple and sequential decisions are necessary while minimizing the number of samples acquired. we present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. our approach during inference actively adapts to sequential decisions to optimally sample. to achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. we evaluate our approach in two sequential knee pathology assessment tasks: acl sprain detection and cartilage thickness loss assessment. our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. our approach paves the way for the future of mri as a comprehensive and affordable poc device. our code is publicly available at https://github.com/vios-s/mri_sequential_active_sampling",,2025-05-07,,"['yuning du', 'jingshuai liu', 'rohan dharmakumar', 'sotirios a. tsaftaris']"
2505.0459,tetweave: isosurface extraction using on-the-fly delaunay tetrahedral   grids for gradient-based mesh optimization,cs.gr cs.cv,"we introduce tetweave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for marching tetrahedra and a novel directional signed distance at each point. tetweave constructs tetrahedral grids on-the-fly via delaunay triangulation, enabling increased flexibility compared to predefined grids. the extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. the flexibility of tetweave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. this leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. consequently, tetweave exhibits near-linear memory scaling relative to the vertex count of the output mesh - a substantial improvement over predefined grids. we demonstrate the applicability of tetweave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3d reconstruction, mesh compression and geometric texture generation.",10.1145/3730851,2025-05-07,2025-05-08,"['alexandre binninger', 'ruben wiersma', 'philipp herholz', 'olga sorkine-hornung']"
2505.04596,dynamic network flow optimization for task scheduling in ptz camera   surveillance systems,math.oc cs.cv cs.sy eess.sy,"this paper presents a novel approach for optimizing the scheduling and control of pan-tilt-zoom (ptz) cameras in dynamic surveillance environments. the proposed method integrates kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. by assigning kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. this prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. to further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. in addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. by adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.",,2025-05-07,,"['mohammad merati', 'david castañón']"
2505.04601,"openvision: a fully-open, cost-effective family of advanced vision   encoders for multimodal learning",cs.cv,"openai's clip, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. although recent alternatives such as siglip have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. this paper fills this gap with openvision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of openai's clip when integrated into multimodal frameworks like llava. openvision builds on existing works -- e.g., clips for training framework and recap-datacomp-1b for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. by releasing vision encoders spanning from 5.9m to 632.1m parameters, openvision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.",,2025-05-07,,"['xianhang li', 'yanqing liu', 'haoqin tu', 'hongru zhu', 'cihang xie']"
2505.04616,"person recognition at altitude and range: fusion of face, body shape and   gait",cs.cv,"we address the problem of whole-body person recognition in unconstrained environments. this problem arises in surveillance scenarios such as those in the iarpa biometric recognition and identification at altitude and range (briar) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). to this end, we propose farsight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. farsight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. these components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. extensive experiments on the briar dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of farsight. compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (tar@0.1% far), a 17.8% increase in closed-set identification (rank-20), and a 34.3% reduction in open-set identification errors (fnir@1% fpir). furthermore, farsight was evaluated in the 2025 nist rte face in video evaluation (five), which conducts standardized face recognition testing on the briar dataset. these results establish farsight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.",,2025-05-07,,"['feng liu', 'nicholas chimitt', 'lanqing guo', 'jitesh jain', 'aditya kane', 'minchul kim', 'wes robbins', 'yiyang su', 'dingqiang ye', 'xingguang zhang', 'jie zhu', 'siddharth satyakam', 'christopher perry', 'stanley h. chan', 'arun ross', 'humphrey shi', 'zhangyang wang', 'anil jain', 'xiaoming liu']"
2505.04619,merging and disentangling views in visual reinforcement learning for   robotic manipulation,cs.lg cs.cv cs.ro,"vision is well-known for its use in manipulation, especially using visual servoing. to make it robust, multiple cameras are needed to expand the field of view. that is computationally challenging. merging multiple views and using q-learning allows the design of more effective representations and optimization of sample efficiency. such a solution might be expensive to deploy. to mitigate this, we introduce a merge and disentanglement (mad) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. we demonstrate the efficiency and robustness of our approach using meta-world and maniskill3. for project website and code, see https://aalmuzairee.github.io/mad",,2025-05-07,,"['abdulaziz almuzairee', 'rohan patil', 'dwait bhatt', 'henrik i. christensen']"
2505.0462,on path to multimodal generalist: general-level and general-bench,cs.cv,"the multimodal large language model (mllm) is currently experiencing rapid growth, driven by the advanced capabilities of llms. unlike earlier specialists, existing mllms are evolving towards a multimodal generalist paradigm. initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. while many benchmarks exist to assess mllms, a critical question arises: can we simply assume that higher performance across tasks indicates a stronger mllm capability, bringing us closer to human-level ai? we argue that the answer is not as straightforward as it seems. this project introduces general-level, an evaluation framework that defines 5-scale levels of mllm performance and generality, offering a methodology to compare mllms and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards agi. at the core of the framework is the concept of synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. to support this evaluation, we present general-bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. the evaluation results that involve over 100 existing state-of-the-art mllms uncover the capability rankings of generalists, highlighting the challenges in reaching genuine ai. we expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of agi. project page: https://generalist.top/",,2025-05-07,,"['hao fei', 'yuan zhou', 'juncheng li', 'xiangtai li', 'qingshan xu', 'bobo li', 'shengqiong wu', 'yaoting wang', 'junbao zhou', 'jiahao meng', 'qingyu shi', 'zhiyuan zhou', 'liangtao shi', 'minghe gao', 'daoan zhang', 'zhiqi ge', 'weiming wu', 'siliang tang', 'kaihang pan', 'yaobo ye', 'haobo yuan', 'tao zhang', 'tianjie ju', 'zixiang meng', 'shilin xu', 'liyu jia', 'wentao hu', 'meng luo', 'jiebo luo', 'tat-seng chua', 'shuicheng yan', 'hanwang zhang']"
2505.04622,primitiveanything: human-crafted 3d primitive assembly generation with   auto-regressive transformer,cs.gr cs.cv,"shape primitive abstraction, which decomposes complex 3d shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. while recent advances in 3d content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. we present primitiveanything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. primitiveanything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. the proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. through extensive experiments, we demonstrate that primitiveanything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. it benefits various 3d applications and shows potential for enabling primitive-based user-generated content (ugc) in games. project page: https://primitiveanything.github.io",,2025-05-07,,"['jingwen ye', 'yuze he', 'yanning zhou', 'yiqin zhu', 'kaiwen xiao', 'yong-jin liu', 'wei yang', 'xiao han']"
2505.04623,echoink-r1: exploring audio-visual reasoning in multimodal llms via   reinforcement learning,eess.as cs.ai cs.cv cs.mm cs.sd,"multimodal large language models (mllms) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. we introduce echoink-r1, a reinforcement learning framework that enhances such reasoning in mllms. built upon the qwen2.5-omni-7b foundation and optimized with group relative policy optimization (grpo), echoink-r1 tackles multiple-choice question answering over synchronized audio-image pairs. to enable this, we curate avqa-r1-6k, a dataset pairing such audio-image inputs with multiple-choice questions derived from omniinstruct-v1. echoink-r1-7b achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. beyond accuracy, echoink-r1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. these results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in mllms. echoink-r1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. code and data are publicly released to facilitate further research.",,2025-05-07,,"['zhenghao xing', 'xiaowei hu', 'chi-wing fu', 'wenhai wang', 'jifeng dai', 'pheng-ann heng']"
2505.04647,channelexplorer: exploring class separability through activation channel   visualization,cs.gr cs.cv cs.lg,"deep neural networks (dnns) achieve state-of-the-art performance in many vision tasks, yet understanding their internal behavior remains challenging, particularly how different layers and activation channels contribute to class separability. we introduce channelexplorer, an interactive visual analytics tool for analyzing image-based outputs across model layers, emphasizing data-driven insights over architecture analysis for exploring class separability. channelexplorer summarizes activations across layers and visualizes them using three primary coordinated views: a scatterplot view to reveal inter- and intra-class confusion, a jaccard similarity view to quantify activation overlap, and a heatmap view to inspect activation channel patterns. our technique supports diverse model architectures, including cnns, gans, resnet and stable diffusion models. we demonstrate the capabilities of channelexplorer through four use-case scenarios: (1) generating class hierarchy in imagenet, (2) finding mislabeled images, (3) identifying activation channel contributions, and(4) locating latent states' position in stable diffusion model. finally, we evaluate the tool with expert users.",,2025-05-06,,"['md rahat-uz- zaman', 'bei wang', 'paul rosen']"
2505.04652,rethinking boundary detection in deep learning-based medical image   segmentation,eess.iv cs.cv,"medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. while current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. in this study, we propose a novel network architecture named cto, which combines convolutional neural networks (cnns), vision transformer (vit) models, and explicit edge detection operators to tackle this challenge. cto surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. specifically, cto adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream cnn stream for capturing local features and an auxiliary stitchvit stream for integrating long-range dependencies. furthermore, to enhance the model's ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. we validate the performance of cto through extensive experiments conducted on seven challenging medical image segmentation datasets, namely isic 2016, ph2, isic 2018, conic, lits17, and btcv. our experimental results unequivocally demonstrate that cto achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. the codes have been released at: https://github.com/xiaofang007/cto.",,2025-05-06,,"['yi lin', 'dong zhang', 'xiao fang', 'yufan chen', 'kwang-ting cheng', 'hao chen']"
2505.04653,advancing conversational diagnostic ai with multimodal reasoning,cs.cl cs.ai cs.cv cs.lg,"large language models (llms) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of llms to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. here we advance the conversational diagnosis and management performance of the articulate medical intelligence explorer (amie) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. leveraging gemini 2.0 flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. we compared amie to primary care physicians (pcps) in a randomized, blinded, osce-style study of chat-based consultations with patient actors. we constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ecgs, and pdfs of clinical documents across diverse conditions and demographics. our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. specialist evaluation showed amie to be superior to pcps on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). the results show clear progress in multimodal conversational diagnostic ai, but real-world translation needs further research.",,2025-05-06,,"['khaled saab', 'jan freyberg', 'chunjong park', 'tim strother', 'yong cheng', 'wei-hung weng', 'david g. t. barrett', 'david stutz', 'nenad tomasev', 'anil palepu', 'valentin liévin', 'yash sharma', 'roma ruparel', 'abdullah ahmed', 'elahe vedadi', 'kimberly kanada', 'cian hughes', 'yun liu', 'geoff brown', 'yang gao', 'sean li', 's. sara mahdavi', 'james manyika', 'katherine chou', 'yossi matias', 'avinatan hassidim', 'dale r. webster', 'pushmeet kohli', 's. m. ali eslami', 'joëlle barral', 'adam rodman', 'vivek natarajan', 'mike schaekermann', 'tao tu', 'alan karthikesalingam', 'ryutaro tanno']"
2505.0466,ai-generated fall data: assessing llms and diffusion model for wearable   fall detection,cs.cl cs.cv,"training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. to address this, we explore the potential of large language models (llms) for generating synthetic fall data. this study evaluates text-to-motion (t2m, sato, parco) and text-to-text models (gpt4o, gpt4, gemini) in simulating realistic fall scenarios. we generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a long short-term memory (lstm) model. additionally, we compare llm-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with llm-generated data performing best in low-frequency settings (e.g., 20hz) while showing instability in high-frequency datasets (e.g., 200hz). while text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. an ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. these findings provide insights into optimizing synthetic data generation for fall detection models.",,2025-05-06,,"['sana alamgeer', 'yasine souissi', 'anne h. h. ngu']"
2505.04664,advancing 3d medical image segmentation: unleashing the potential of   planarian neural networks in artificial intelligence,eess.iv cs.ai cs.cv,"our study presents pnn-unet as a method for constructing deep neural networks that replicate the planarian neural network (pnn) structure in the context of 3d medical image data. planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organism's neurological system. accordingly, pnn-unet comprises a deep-unet and a wide-unet as the nerve cords, with a densely connected autoencoder performing the role of the brain. this distinct architecture offers advantages over both monolithic (unet) and modular networks (ensemble-unet). our outcomes on a 3d mri hippocampus dataset, with and without data augmentation, demonstrate that pnn-unet outperforms the baseline unet and several other unet variants in image segmentation.",,2025-05-06,,"['ziyuan huang', 'kevin huggins', 'srikar bellur']"
2505.04672,histo-miner: deep learning based tissue features extraction pipeline   from h&e whole slide images of cutaneous squamous cell carcinoma,cs.cv q-bio.qm,"recent advancements in digital pathology have enabled comprehensive analysis of whole-slide images (wsi) from tissue samples, leveraging high-resolution microscopy and computational capabilities. despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. here we propose histo-miner, a deep learning-based pipeline for analysis of skin wsis and generate two datasets with labeled nuclei and tumor regions. we develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cscc), a frequent non-melanoma skin cancer. utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented wsis respectively, both from cscc patients, histo-miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. performance of trained models positively compares to state of the art with multi-class panoptic quality (mpq) of 0.569 for nucleus segmentation, macro-averaged f1 of 0.832 for nucleus classification and mean intersection over union (miou) of 0.884 for tumor region segmentation. from these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. here, we use histo-miner to predict cscc patient response to immunotherapy based on pre-treatment wsis from 45 patients. histo-miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. this highlights the applicability of histo-miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.",,2025-05-07,,"['lucas sancéré', 'carina lorenz', 'doris helbig', 'oana-diana persa', 'sonja dengler', 'alexander kreuter', 'martim laimer', 'anne fröhlich', 'jennifer landsberg', 'johannes brägelmann', 'katarzyna bozek']"
2505.04713,comparison of visual trackers for biomechanical analysis of running,cs.cv cs.lg,"human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. these developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.   this work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. the proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. the experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. we propose a post-processing module for outlier detection and fusion prediction in the joint angles.   the experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. when integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. the experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. however, there is still room for improvement in applications where high accuracy is required.",,2025-05-07,,"['luis f. gomez', 'gonzalo garrido-lopez', 'julian fierrez', 'aythami morales', 'ruben tolosana', 'javier rueda', 'enrique navarro']"
2505.04718,lay-your-scene: natural scene layout generation with diffusion   transformers,cs.cv cs.lg,"we present lay-your-scene (shorthand layousyn), a novel text-to-layout generation pipeline for natural scenes. prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. in this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion transformer architecture trained in an open-vocabulary manner for conditional layout generation. extensive experiments demonstrate that layousyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. additionally, we present two applications of layousyn. first, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. second, we present a pipeline for adding objects to images, demonstrating the potential of layousyn in image editing applications.",,2025-05-07,,"['divyansh srivastava', 'xiang zhang', 'he wen', 'chenru wen', 'zhuowen tu']"
2505.0472,false promises in medical imaging ai? assessing validity of   outperformance claims,cs.cv,"performance comparisons are fundamental in medical imaging artificial intelligence (ai) research, often driving claims of superiority based on relative improvements in common performance metrics. however, such claims frequently rely solely on empirical mean performance. in this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. we quantify the probability of false claims based on a bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. according to our results, the majority (>80%) of papers claims outperformance when introducing a new method. our analysis further revealed a high probability (>5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. these findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging ai are frequently unsubstantiated, posing a risk of misdirecting future research efforts.",,2025-05-07,,"['evangelia christodoulou', 'annika reinke', 'pascaline andrè', 'patrick godau', 'piotr kalinowski', 'rola houhou', 'selen erkan', 'carole h. sudre', 'ninon burgos', 'sofiène boutaj', 'sophie loizillon', 'maëlys solal', 'veronika cheplygina', 'charles heitz', 'michal kozubek', 'michela antonelli', 'nicola rieke', 'antoine gilson', 'leon d. mayer', 'minu d. tizabi', 'm. jorge cardoso', 'amber simpson', 'annette kopp-schneider', 'gaël varoquaux', 'olivier colliot', 'lena maier-hein']"
2505.0474,hyb-kan vit: hybrid kolmogorov-arnold networks augmented vision   transformer,cs.cv,"this study addresses the inherent limitations of multi-layer perceptrons (mlps) in vision transformers (vits) by introducing hybrid kolmogorov-arnold network (kan)-vit (hyb-kan vit), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the vit architecture and integration of edge detection capabilities of wavelet functions. we propose two key modules: efficient-kan (eff-kan), which replaces mlp layers with spline functions and wavelet-kan (wav-kan), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. these modules are systematically integrated in vit encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. experiments on imagenet-1k (image recognition), coco (object detection and instance segmentation), and ade20k (semantic segmentation) demonstrate state-of-the-art performance with hyb-kan vit. ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. the framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures.",,2025-05-07,,"['sainath dey', 'mitul goswami', 'jashika sethi', 'prasant kumar pattnaik']"
2505.04758,lightweight rgb-d salient object detection from a speed-accuracy   tradeoff perspective,cs.cv,"current rgb-d methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. to balance the efficiency and performance, we propose a speed-accuracy tradeoff network (satnet) for lightweight rgb-d sod from three fundamental perspectives: depth quality, modality fusion, and feature representation. concerning depth quality, we introduce the depth anything model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. for modality fusion, we propose a decoupled attention module (dam) to explore the consistency within and between modalities. here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. for feature representation, we develop a dual information representation module (dirm) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. dirm models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. finally, we design a dual feature aggregation module (dfam) in the decoder to aggregate texture and saliency features. extensive experiments on five public rgb-d sod datasets indicate that the proposed satnet excels state-of-the-art (sota) cnn-based heavyweight models and achieves a lightweight framework with 5.2 m parameters and 415 fps.",,2025-05-07,,"['songsong duan', 'xi yang', 'nannan wang', 'xinbo gao']"
2505.04769,"vision-language-action models: concepts, progress, applications and   challenges",cs.cv,"vision-language-action (vla) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. this foundational review presents a comprehensive synthesis of recent advancements in vision-language-action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. we begin by establishing the conceptual foundations of vla systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (vlms), action planners, and hierarchical controllers. our methodology adopts a rigorous literature review framework, covering over 80 vla models published in the past three years. key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. we explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. the review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. drawing from the state-of-the-art, we propose targeted solutions including agentic ai adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. in our forward-looking discussion, we outline a future roadmap where vla models, vlms, and agentic ai converge to power socially aligned, adaptive, and general-purpose embodied agents. this work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >vision-language-action, agentic ai, ai agents, vision-language models",,2025-05-07,,"['ranjan sapkota', 'yang cao', 'konstantinos i. roumeliotis', 'manoj karkee']"
2505.04793,detreidx: a stress-test dataset for real-world uav-based person   recognition,cs.cv,"person reidentification (reid) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. this paper introduces detreidx, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to reid under real-world conditions. detreidx is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. more important, as a key novelty, detreidx subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person reid. plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, reid, and action recognition. in order to provide empirical evidence of detreidx usefulness, we considered the specific tasks of human detection and reid, where sota methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in rank-1 reid) when exposed to detreidxs conditions. the dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/detreidx/",,2025-05-07,,"['kailash a. hambarde', 'nzakiese mbongo', 'pavan kumar mp', 'satish mekewad', 'carolina fernandes', 'gökhan silahtaroğlu', 'alice nithya', 'pawan wasnik', 'md. rashidunnabi', 'pranita samale', 'hugo proença']"
2505.04813,wir3d: visually-informed and geometry-aware 3d shape abstraction,cs.gr cs.cv,"we present wir3d, a technique for abstracting 3d shapes through a sparse set of visually meaningful curves in 3d. we optimize the parameters of bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. we leverage the intermediate activations of a pre-trained foundation model (clip) to guide our optimization process. we divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. our second phase supervision is spatially guided by a novel localized keypoint loss. this spatial guidance enables user control over abstracted features. we ensure fidelity to the original surface through a neural sdf loss, which allows the curves to be used as intuitive deformation handles. we successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.",,2025-05-07,,"['richard liu', 'daniel fu', 'noah tan', 'itai lang', 'rana hanocka']"
2505.04835,are synthetic corruptions a reliable proxy for real-world corruptions?,cs.cv,"deep learning (dl) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. collecting diverse real-world data for testing the robustness of dl models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. however, are synthetic corruptions a reliable proxy for real-world corruptions? to answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. we further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. open-source code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation",,2025-05-07,,"['shashank agnihotri', 'david schader', 'nico sharei', 'mehmet ege kaçar', 'margret keuper']"
2505.04836,integrated image reconstruction and target recognition based on deep   learning technique,eess.sp cs.cv,"computational microwave imaging (cmi) has gained attention as an alternative technique for conventional microwave imaging techniques, addressing their limitations such as hardware-intensive physical layer and slow data collection acquisition speed to name a few. despite these advantages, cmi still encounters notable computational bottlenecks, especially during the image reconstruction stage. in this setting, both image recovery and object classification present significant processing demands. to address these challenges, our previous work introduced classigan, which is a generative deep learning model designed to simultaneously reconstruct images and classify targets using only back-scattered signals. in this study, we build upon that framework by incorporating attention gate modules into classigan. these modules are intended to refine feature extraction and improve the identification of relevant information. by dynamically focusing on important features and suppressing irrelevant ones, the attention mechanism enhances the overall model performance. the proposed architecture, named att-classigan, significantly reduces the reconstruction time compared to traditional cmi approaches. furthermore, it outperforms current advanced methods, delivering improved normalized mean squared error (nmse), higher structural similarity index (ssim), and better classification outcomes for the reconstructed targets.",,2025-05-07,,"['cien zhang', 'jiaming zhang', 'jiajun he', 'okan yurduseven']"
2505.04838,seeing cells clearly: evaluating machine vision strategies for microglia   centroid detection in 3d images,cs.cv,"microglia are important cells in the brain, and their shape can tell us a lot about brain health. in this project, i test three different tools for finding the center points of microglia in 3d microscope images. the tools include ilastik, 3d morph, and omnipose. i look at how well each one finds the cells and how their results compare. my findings show that each tool sees the cells in its own way, and this can affect the kind of information we get from the images.",,2025-05-07,,['youjia zhang']
2505.0485,orxe: orchestrating experts for dynamically configurable efficiency,cs.cv,"this paper presents orxe, a modular and adaptable framework for achieving real-time configurable efficiency in ai models. by leveraging a collection of pre-trained experts with diverse computational costs and performance levels, orxe dynamically adjusts inference pathways based on the complexity of input samples. unlike conventional approaches that require complex metamodel training, orxe achieves high efficiency and flexibility without complicating the development process. the proposed system utilizes a confidence-based gating mechanism to allocate appropriate computational resources for each input. orxe also supports adjustments to the preference between inference cost and prediction performance across a wide range during runtime. we implemented a training-free orxe system for image classification tasks, evaluating its efficiency and accuracy across various devices. the results demonstrate that orxe achieves superior performance compared to individual experts and other dynamic models in most cases. this approach can be extended to other applications, providing a scalable solution for diverse real-world deployment scenarios.",,2025-05-07,,"['qingyuan wang', 'guoxin wang', 'barry cardiff', 'deepu john']"
2505.04851,craft: cultural russian-oriented dataset adaptation for focused   text-to-image generation,cs.ai cs.cl cs.cv cs.cy cs.lg,"despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. this is due to the content of existing large training datasets collected on the internet, which are predominantly based on western european or american popular culture. meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. in an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. we propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the russian one. we explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the kandinsky 3.1 text-to-image model. human evaluation results demonstrate an increase in the level of awareness of russian culture in the model.",10.1134/s1064562424602324,2025-05-07,,"['viacheslav vasilev', 'vladimir arkhipkin', 'julia agafonova', 'tatiana nikulina', 'evelina mironova', 'alisa shichanina', 'nikolai gerasimenko', 'mikhail shoytov', 'denis dimitrov']"
2505.0486,d-coda: diffusion for coordinated dual-arm data augmentation,cs.ro cs.ai cs.cv cs.lg,"learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. however, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. while prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. in this work, we propose diffusion for coordinated dual-arm data augmentation (d-coda), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. it employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. we evaluate d-coda on 5 simulated and 3 real-world tasks. our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. our project website is at: https://dcodaaug.github.io/d-coda/.",,2025-05-07,,"['i-chun arthur liu', 'jason chen', 'gaurav sukhatme', 'daniel seita']"
2505.04861,mix-qsam: mixed-precision quantization of the segment anything model,cs.cv,"the segment anything model (sam) is a popular vision foundation model; however, its high computational and memory demands make deployment on resource-constrained devices challenging. while post-training quantization (ptq) is a practical approach for reducing computational overhead, existing ptq methods rely on fixed bit-width quantization, leading to suboptimal accuracy and efficiency. to address this limitation, we propose mix-qsam, a mixed-precision ptq framework for sam. first, we introduce a layer-wise importance score, derived using kullback-leibler (kl) divergence, to quantify each layer's contribution to the model's output. second, we introduce cross-layer synergy, a novel metric based on causal mutual information, to capture dependencies between adjacent layers. this ensures that highly interdependent layers maintain similar bit-widths, preventing abrupt precision mismatches that degrade feature propagation and numerical stability. using these metrics, we formulate an integer quadratic programming (iqp) problem to determine optimal bit-width allocation under model size and bit-operation constraints, assigning higher precision to critical layers while minimizing bit-width in less influential layers. experimental results demonstrate that mix-qsam consistently outperforms existing ptq methods on instance segmentation and object detection tasks, achieving up to 20% higher average precision under 6-bit and 4-bit mixed-precision settings, while maintaining computational efficiency.",,2025-05-07,,"['navin ranjan', 'andreas savakis']"
2505.04864,auto-regressive transformation for image alignment,cs.cv cs.ai,"existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. we thus propose auto-regressive transformation (art), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. by incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. extensive experiments across diverse datasets demonstrate that art significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.",,2025-05-07,,"['kanggeon lee', 'soochahn lee', 'kyoung mu lee']"
2505.04888,cross-branch orthogonality for improved generalization in face deepfake   detection,cs.cv cs.ai,"remarkable advancements in generative ai technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. in particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. this is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. to combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. a novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. comprehensive experiments on three public benchmarks: faceforensics++, celeb-df, and the deepfake detection challenge (dfdc) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the celeb-df dataset and 7% on the dfdc dataset in a cross-dataset evaluation setting.",,2025-05-07,,"['tharindu fernando', 'clinton fookes', 'sridha sridharan', 'simon denman']"
2505.04899,owt: a foundational organ-wise tokenization framework for medical   imaging,cs.cv,"recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. these issues are especially critical in medical imaging. to address these limitations, we propose an organ-wise tokenization (owt) framework with a token group-based reconstruction (tgr) training paradigm. unlike conventional approaches that produce holistic features, owt explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. experiments on ct and mri datasets demonstrate the effectiveness of owt in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. these findings underscore the potential of owt as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.",,2025-05-07,,"['sifan song', 'siyeop yoon', 'pengfei jin', 'sekeun kim', 'matthew tivnan', 'yujin oh', 'runqi meng', 'ling chen', 'zhiliang lyu', 'dufan wu', 'ning guo', 'xiang li', 'quanzheng li']"
2505.04905,pro2sam: mask prompt to sam with grid points for weakly supervised   object localization,cs.cv,"weakly supervised object localization (wsol), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. current studies focus on the class activation map (cam) of cnn and the self-attention map of transformer to identify the region of objects. however, both cam and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of wsol. to address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in segment anything model (sam) to boost the activation of integral object regions. further, to alleviate the semantic ambiguity issue accrued in single point prompt-based sam, we propose an innovative mask prompt to sam (pro2sam) network with grid points for wsol task. first, we devise a global token transformer (gtformer) to generate a coarse-grained foreground map as a flexible mask prompt, where the gtformer jointly embeds patch tokens and novel global tokens to learn foreground semantics. secondly, we deliver grid points as dense prompts into sam to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to sam, where the mask with the highest score is viewed as the final localization map. experiments show that the proposed pro2sam achieves state-of-the-art performance on both cub-200-2011 and ilsvrc, with 84.03\% and 66.85\% top-1 loc, respectively.",,2025-05-07,,"['xi yang', 'songsong duan', 'nannan wang', 'xinbo gao']"
2505.04911,spatialprompting: keyframe-driven zero-shot spatial reasoning with   off-the-shelf multimodal large language models,cs.cv cs.ai cs.cl,"this study introduces spatialprompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3d) environments. unlike existing methods that rely on expensive 3d-specific fine-tuning with specialized 3d inputs such as point clouds or voxel-based features, spatialprompting employs a keyframe-driven prompt generation strategy. this framework uses metrics such as vision-language similarity, mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3d structures. the proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as scanqa and sqa3d, across several metrics. the proposed method effectively eliminates the need for specialized 3d inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.",,2025-05-07,,"['shun taguchi', 'hideki deguchi', 'takumi hamazaki', 'hiroyuki sakai']"
2505.04913,advanced 3d imaging approach to tsv/tgv metrology and inspection using   only optical microscopy,eess.iv cs.cv physics.optics,"this paper introduces an innovative approach to silicon and glass via inspection, which combines hybrid field microscopy with photometric stereo. conventional optical microscopy techniques are generally limited to superficial inspections and struggle to effectively visualize the internal structures of silicon and glass vias. by utilizing various lighting conditions for 3d reconstruction, the proposed method surpasses these limitations. by integrating photometric stereo to the traditional optical microscopy, the proposed method not only enhances the capability to detect micro-scale defects but also provides a detailed visualization of depth and edge abnormality, which are typically not visible with conventional optical microscopy inspection. the experimental results demonstrated that the proposed method effectively captures intricate surface details and internal structures. quantitative comparisons between the reconstructed models and actual measurements present the capability of the proposed method to significantly improve silicon and glass via inspection process. as a result, the proposed method achieves enhanced cost-effectiveness while maintaining high accuracy and repeatability, suggesting substantial advancements in silicon and glass via inspection techniques",,2025-05-07,,['gugeong sung']
2505.04915,glyphmastero: a glyph encoder for high-fidelity scene text editing,cs.cv,"scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. while diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. these methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like chinese. in such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. we present glyphmastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. our key insight is that existing methods, despite using pretrained ocr models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. to address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. meanwhile, our model implements a feature pyramid network to fuse the multi-scale ocr backbone features at the global-level. through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region fr\'echet inception distance by 53.28\%.",,2025-05-07,,"['tong wang', 'ting liu', 'xiaochao qu', 'chengjing wu', 'luoqi liu', 'xiaolin hu']"
2505.04917,a simple detector with frame dynamics is a strong tracker,cs.cv,"infrared object tracking plays a crucial role in anti-unmanned aerial vehicle (anti-uav) applications. existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. to address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. our method is based on object detection and achieves significant improvements through two key innovations. first, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared uav tracking scenarios. notably, we achieve state-of-the-art performance in the 4th anti-uav challenge, securing 1st place in track 1 and 2nd place in track 2.",,2025-05-07,,"['chenxu peng', 'chenxu wang', 'minrui zou', 'danyang li', 'zhengpeng yang', 'yimian dai', 'ming-ming cheng', 'xiang li']"
2505.04921,"perception, reason, think, and plan: a survey on large multimodal   reasoning models",cs.cv cs.cl,"reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. in artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. large multimodal reasoning models (lmrms) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. as research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. while instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. to address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. first, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. next, we examine recent approaches that unify reasoning into multimodal llms, with advances such as multimodal chain-of-thought (mcot) and multimodal reinforcement learning enabling richer and more structured reasoning chains. finally, drawing on empirical insights from challenging benchmarks and experimental cases of openai o3 and o4-mini, we discuss the conceptual direction of native large multimodal reasoning models (n-lmrms), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.",,2025-05-07,,"['yunxin li', 'zhenyu liu', 'zitao li', 'xuanyu zhang', 'zhenran xu', 'xinyu chen', 'haoyuan shi', 'shenyuan jiang', 'xintong wang', 'jifang wang', 'shouzheng huang', 'xinping zhao', 'borui jiang', 'lanqing hong', 'longyue wang', 'zhuotao tian', 'baoxing huai', 'wenhan luo', 'weihua luo', 'zheng zhang', 'baotian hu', 'min zhang']"
2505.04922,canny2palm: realistic and controllable palmprint generation for   large-scale pre-training,cs.cv,"palmprint recognition is a secure and privacy-friendly method of biometric identification. one of the major challenges to improve palmprint recognition accuracy is the scarcity of palmprint data. recently, a popular line of research revolves around the synthesis of virtual palmprints for large-scale pre-training purposes. in this paper, we propose a novel synthesis method named canny2palm that extracts palm textures with canny edge detector and uses them to condition a pix2pix network for realistic palmprint generation. by re-assembling palmprint textures from different identities, we are able to create new identities by seeding the generator with new assemblies. canny2palm not only synthesizes realistic data following the distribution of real palmprints but also enables controllable diversity to generate large-scale new identities. on open-set palmprint recognition benchmarks, models pre-trained with canny2palm synthetic data outperform the state-of-the-art with up to 7.2% higher identification accuracy. moreover, the performance of models pre-trained with canny2palm continues to improve given 10,000 synthetic ids while those with existing methods already saturate, demonstrating the potential of our method for large-scale pre-training.",,2025-05-07,,"['xingzeng lan', 'xing duan', 'chen chen', 'weiyu lin', 'bo wang']"
2505.04941,building-guided pseudo-label learning for cross-modal building damage   mapping,cs.cv,"accurate building damage assessment using bi-temporal multi-modal remote sensing images is essential for effective disaster response and recovery planning. this study proposes a novel building-guided pseudo-label learning framework to address the challenges of mapping building damage from pre-disaster optical and post-disaster sar images. first, we train a series of building extraction models using pre-disaster optical images and building labels. to enhance building segmentation, we employ multi-model fusion and test-time augmentation strategies to generate pseudo-probabilities, followed by a low-uncertainty pseudo-label training method for further refinement. next, a change detection model is trained on bi-temporal cross-modal images and damaged building labels. to improve damage classification accuracy, we introduce a building-guided low-uncertainty pseudo-label refinement strategy, which leverages building priors from the previous step to guide pseudo-label generation for damaged buildings, reducing uncertainty and enhancing reliability. experimental results on the 2025 ieee grss data fusion contest dataset demonstrate the effectiveness of our approach, which achieved the highest miou score (54.28%) and secured first place in the competition.",,2025-05-08,,"['jiepan li', 'he huang', 'yu sheng', 'yujun guo', 'wei he']"
2505.04946,t2vtextbench: a human evaluation benchmark for textual control in video   generation models,cs.cv cs.ai cs.cl cs.lg,"thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. however, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. in this work, we introduce t2vtextbench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. we evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. these results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.",,2025-05-08,,"['xuyang guo', 'jiayan huo', 'zhenmei shi', 'zhao song', 'jiahao zhang', 'jiale zhao']"
2505.04959,more-3dgsmr: motion-resolved reconstruction framework for free-breathing   pulmonary mri based on 3d gaussian representation,eess.iv cs.cv,"this study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (mri), utilizing a three-dimensional gaussian representation (3dgs). the proposed method leverages 3dgs to address the challenges of motion-resolved 3d isotropic pulmonary mri reconstruction by enabling data smoothing between voxels for continuous spatial representation. pulmonary mri data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. a 3dgs framework is then applied to reconstruct a reference image volume from the first motion state. subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (dvfs), which are used to generate the remaining motion states through spatial transformation of the reference volume. the proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. the experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary mr images. compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. the proposed unsupervised 3dgs-based reconstruction method enables accurate motion-resolved pulmonary mri with isotropic spatial resolution. its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary mr imaging.",,2025-05-08,,"['tengya peng', 'ruyi zha', 'qing zou']"
2505.04961,add: physics-based motion imitation with adversarial differential   discriminators,cs.gr cs.ai cs.cv cs.ro,"multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. the performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. these limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. to bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. the proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. we demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. results are best visualized through https://youtu.be/rz8byce9e2w.",,2025-05-08,,"['ziyu zhang', 'sergey bashkirov', 'dun yang', 'michael taylor', 'xue bin peng']"
2505.04962,an efficient method for accurate pose estimation and error correction of   cuboidal objects,cs.cv cs.ro,"the proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. this paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. however, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. this paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.",,2025-05-08,,"['utsav rai', 'hardik mehta', 'vismay vakharia', 'aditya choudhary', 'amit parmar', 'rolif lima', 'kaushik das']"
2505.04964,cag-vlm: fine-tuning of a large-scale model to recognize angiographic   images for next-generation diagnostic systems,cs.cv,"coronary angiography (cag) is the gold-standard imaging modality for evaluating coronary artery disease, but its interpretation and subsequent treatment planning rely heavily on expert cardiologists. to enable ai-based decision support, we introduce a two-stage, physician-curated pipeline and a bilingual (japanese/english) cag image-report dataset. first, we sample 14,686 frames from 539 exams and annotate them for key-frame detection and left/right laterality; a convnext-base cnn trained on this data achieves 0.96 f1 on laterality classification, even on low-contrast frames. second, we apply the cnn to 243 independent exams, extract 1,114 key frames, and pair each with its pre-procedure report and expert-validated diagnostic and treatment summary, yielding a parallel corpus. we then fine-tune three open-source vlms (paligemma2, gemma3, and conceptclip-enhanced gemma3) via lora and evaluate them using vlscore and cardiologist review. although paligemma2 w/lora attains the highest vlscore, gemma3 w/lora achieves the top clinician rating (mean 7.20/10); we designate this best-performing model as cag-vlm. these results demonstrate that specialized, fine-tuned vlms can effectively assist cardiologists in generating clinical reports and treatment recommendations from cag images.",,2025-05-08,,"['yuto nakamura', 'satoshi kodera', 'haruki settai', 'hiroki shinohara', 'masatsugu tamura', 'tomohiro noguchi', 'tatsuki furusawa', 'ryo takizawa', 'tempei kabayama', 'norihiko takeda']"
2505.04965,densegrounding: improving dense language-vision semantics for   ego-centric 3d visual grounding,cs.cv,"enabling intelligent agents to comprehend and interact with 3d environments through natural language is crucial for advancing robotics and human-computer interaction. a fundamental task in this field is ego-centric 3d visual grounding, where agents locate target objects in real-world 3d spaces based on verbal descriptions. however, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. we propose densegrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. for visual features, we introduce the hierarchical scene semantic enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. for text descriptions, we propose a language semantic enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. extensive experiments show that densegrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the sota in egocentric 3d visual grounding. our method also achieves 1st place and receives the innovation award in the cvpr 2024 autonomous grand challenge multi-view 3d visual grounding track, validating its effectiveness and robustness.",,2025-05-08,,"['henry zheng', 'hao shi', 'qihang peng', 'yong xien chng', 'rui huang', 'yepeng weng', 'zhongchao shi', 'gao huang']"
2505.04969,general transform: a unified framework for adaptive transform to enhance   representations,cs.lg cs.cl cs.cv,"discrete transforms, such as the discrete fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. however, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. in this work, we propose general transform (gt), an adaptive transform-based representation designed for machine learning applications. unlike conventional transforms, gt learns data-driven mapping tailored to the dataset and task of interest. here, we demonstrate that models incorporating gt outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.",,2025-05-08,,"['gekko budiutama', 'shunsuke daimon', 'hirofumi nishi', 'yu-ichiro matsushita']"
2505.04972,ai and vision based autonomous navigation of nano-drones in   partially-known environments,cs.ro cs.ai cs.cv cs.lg cs.ni,"the miniaturisation of sensors and processors, the advancements in connected edge intelligence, and the exponential interest in artificial intelligence are boosting the affirmation of autonomous nano-size drones in the internet of robotic things ecosystem. however, achieving safe autonomous navigation and high-level tasks such as exploration and surveillance with these tiny platforms is extremely challenging due to their limited resources. this work focuses on enabling the safe and autonomous flight of a pocket-size, 30-gram platform called crazyflie 2.1 in a partially known environment. we propose a novel ai-aided, vision-based reactive planning method for obstacle avoidance under the ambit of integrated sensing, computing and communication paradigm. we deal with the constraints of the nano-drone by splitting the navigation task into two parts: a deep learning-based object detector runs on the edge (external hardware) while the planning algorithm is executed onboard. the results show the ability to command the drone at $\sim8$ frames-per-second and a model performance reaching a coco mean-average-precision of $60.8$. field experiments demonstrate the feasibility of the solution with the drone flying at a top speed of $1$ m/s while steering away from an obstacle placed in an unknown position and reaching the target destination. the outcome highlights the compatibility of the communication delay and the model performance with the requirements of the real-time navigation task. we provide a feasible alternative to a fully onboard implementation that can be extended to autonomous exploration with nano-drones.",,2025-05-08,,"['mattia sartori', 'chetna singhal', 'neelabhro roy', 'davide brunelli', 'james gross']"
2505.04974,realign: bilingual text-to-motion generation via step-aware   reward-guided alignment,cs.cv,"bilingual text-to-motion generation, which synthesizes 3d human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. however, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. to address these challenges, we propose bihumanml3d, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. furthermore, we propose a bilingual motion diffusion model (bimd), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. building upon this, we propose reward-guided sampling alignment (realign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. this reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. project page: https://wengwanjiang.github.io/realign-page/.",,2025-05-08,,"['wanjiang weng', 'xiaofeng tan', 'hongsong wang', 'pan zhou']"
2505.04996,inter-diffusion generation model of speakers and listeners for effective   communication,cs.gr cs.cv cs.sd eess.as,"full-body gestures play a pivotal role in natural interactions and are crucial for achieving effective communication. nevertheless, most existing studies primarily focus on the gesture generation of speakers, overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them. this paper innovatively proposes an inter-diffusion generation model of speakers and listeners for effective communication. for the first time, we integrate the full-body gestures of listeners into the generation framework. by devising a novel inter-diffusion mechanism, this model can accurately capture the complex interaction patterns between speakers and listeners during communication. in the model construction process, based on the advanced diffusion model architecture, we innovatively introduce interaction conditions and the gan model to increase the denoising step size. as a result, when generating gesture sequences, the model can not only dynamically generate based on the speaker's speech information but also respond in realtime to the listener's feedback, enabling synergistic interaction between the two. abundant experimental results demonstrate that compared with the current state-of-the-art gesture generation methods, the model we proposed has achieved remarkable improvements in the naturalness, coherence, and speech-gesture synchronization of the generated gestures. in the subjective evaluation experiments, users highly praised the generated interaction scenarios, believing that they are closer to real life human communication situations. objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators, providing more powerful support for effective communication.",,2025-05-08,,"['jinhe huang', 'yongkang cheng', 'yuming hang', 'gaoge han', 'jinewei li', 'jing zhang', 'xingjian gu']"
2505.05001,stabstitch++: unsupervised online video stitching with spatiotemporal   bidirectional warps,cs.cv cs.ai,"we retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. to address this issue, we propose stabstitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. first, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. compared with stabstitch that sacrifices alignment for stabilization, stabstitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. to establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. experiments exhibit that stabstitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.",,2025-05-08,,"['lang nie', 'chunyu lin', 'kang liao', 'yun zhang', 'shuaicheng liu', 'yao zhao']"
2505.05004,automated thoracolumbar stump rib detection and analysis in a large ct   cohort,cs.cv cs.lg,"thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. while some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. to this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (dice score 0.997 vs. 0.779, p-value < 0.01). in addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. when analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. we show that with partially visible ribs, these features can achieve an f1-score of 0.84 in differentiating stump ribs from regular ones. we publish the model weights and masks for public use.",,2025-05-08,,"['hendrik möller', 'hanna schön', 'alina dima', 'benjamin keinert-weth', 'robert graf', 'matan atad', 'johannes paetzold', 'friederike jungmann', 'rickmer braren', 'florian kofler', 'bjoern menze', 'daniel rueckert', 'jan s. kirschke']"
2505.05008,adaptive contextual embedding for robust far-view borehole detection,cs.cv,"in controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. however, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. to address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., yolo) by explicitly leveraging consistent embedding representations derived through exponential moving average (ema)-based statistical updates.   our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. the pervasive use of ema in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline yolo-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios.",,2025-05-08,,"['xuesong liu', 'tianyu hao', 'emmett j. ientilucci']"
2505.05023,split matching for inductive zero-shot semantic segmentation,cs.cv,"zero-shot semantic segmentation (zss) aims to segment categories that are not annotated during training. while fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. as an alternative to fully supervised approaches, query-based segmentation has shown great latent in zss, as it enables object localization without relying on explicit labels. however, conventional hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of zss. to address this issue, we propose split matching (sm), a novel assignment strategy that decouples hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. to discover unseen candidates, we cluster clip dense features to generate pseudo masks and extract region-level embeddings using cls tokens. matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. additionally, we introduce a multi-scale feature enhancement (mfe) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. sm is the first to introduce decoupled hungarian matching under the inductive zss setting, and achieves state-of-the-art performance on two standard benchmarks.",,2025-05-08,,"['jialei chen', 'xu zheng', 'dongyue li', 'chong yi', 'seigo ito', 'danda pani paudel', 'luc van gool', 'hiroshi murase', 'daisuke deguchi']"
2505.0504,image-text relation prediction for multilingual tweets,cs.cl cs.ai cs.cv,"various social networks have been allowing media uploads for over a decade now. still, it has not always been clear what is their relation with the posted text or even if there is any at all. in this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from twitter posts in latvian along with their manual translations into english. we compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.",,2025-05-08,,"['matīss rikters', 'edison marrese-taylor']"
2505.05041,adnp-15: an open-source histopathological dataset for neuritic plaque   segmentation in human brain whole slide images with frequency domain image   enhancement for stain normalization,eess.iv cs.cv,"alzheimer's disease (ad) is a neurodegenerative disorder characterized by amyloid-beta plaques and tau neurofibrillary tangles, which serve as key histopathological features. the identification and segmentation of these lesions are crucial for understanding ad progression but remain challenging due to the lack of large-scale annotated datasets and the impact of staining variations on automated image analysis. deep learning has emerged as a powerful tool for pathology image segmentation; however, model performance is significantly influenced by variations in staining characteristics, necessitating effective stain normalization and enhancement techniques. in this study, we address these challenges by introducing an open-source dataset (adnp-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of dystrophic tau-positive neurites) in human brain whole slide images. we establish a comprehensive benchmark by evaluating five widely adopted deep learning models across four stain normalization techniques, providing deeper insights into their influence on neuritic plaque segmentation. additionally, we propose a novel image enhancement method that improves segmentation accuracy, particularly in complex tissue structures, by enhancing structural details and mitigating staining inconsistencies. our experimental results demonstrate that this enhancement strategy significantly boosts model generalization and segmentation accuracy. all datasets and code are open-source, ensuring transparency and reproducibility while enabling further advancements in the field.",,2025-05-08,,"['chenxi zhao', 'jianqiang li', 'qing zhao', 'jing bai', 'susana boluda', 'benoit delatour', 'lev stimmer', 'daniel racoceanu', 'gabriel jimenez', 'guanghui fu']"
2505.05043,xtrace: a facial expressive behaviour analysis tool for continuous   affect recognition,cs.cv,"recognising expressive behaviours in face videos is a long-standing challenge in affective computing. despite significant advancements in recent years, it still remains a challenge to build a robust and reliable system for naturalistic and in-the-wild facial expressive behaviour analysis in real time. this paper addresses two key challenges in building such a system: (1). the paucity of large-scale labelled facial affect video datasets with extensive coverage of the 2d emotion space, and (2). the difficulty of extracting facial video features that are discriminative, interpretable, robust, and computationally efficient. toward addressing these challenges, we introduce xtrace, a robust tool for facial expressive behaviour analysis and predicting continuous values of dimensional emotions, namely valence and arousal, from in-the-wild face videos.   to address challenge (1), our affect recognition model is trained on the largest facial affect video data set, containing ~450k videos that cover most emotion zones in the dimensional emotion space, making xtrace highly versatile in analysing a wide spectrum of naturalistic expressive behaviours. to address challenge (2), xtrace uses facial affect descriptors that are not only explainable, but can also achieve a high degree of accuracy and robustness with low computational complexity. the key components of xtrace are benchmarked against three existing tools: mediapipe, openface, and augsburg affect toolbox. on an in-the-wild validation set composed of 50k videos, xtrace achieves 0.86 mean ccc and 0.13 mean absolute error values. we present a detailed error analysis of affect predictions from xtrace, illustrating (a). its ability to recognise emotions with high accuracy across most bins in the 2d emotion space, (b). its robustness to non-frontal head pose angles, and (c). a strong correlation between its uncertainty estimates and its accuracy.",,2025-05-08,,"['mani kumar tellamekala', 'shashank jaiswal', 'thomas smith', 'timur alamev', 'gary mckeown', 'anthony brown', 'michel valstar']"
2505.05054,direct image classification from fourier ptychographic microscopy   measurements without reconstruction,eess.iv cs.ai cs.cv,"the computational imaging technique of fourier ptychographic microscopy (fpm) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. however, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. therefore, in this paper, we investigate the idea of classifying the image content in the fpm measurements directly without performing a reconstruction step first. we show that convolutional neural networks (cnn) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.",,2025-05-08,,"['navya sonal agarwal', 'jan philipp schneider', 'kanchana vaishnavi gandikota', 'syed muhammad kazim', 'john meshreki', 'ivo ihrke', 'michael moeller']"
2505.05062,ulfine: unbiased lightweight fine-tuning for foundation-model-assisted   long-tailed semi-supervised learning,cs.cv,"based on the success of large-scale visual foundation models like clip in various downstream tasks, this paper initially attempts to explore their impact on long-tailed semi-supervised learning (ltssl) by employing the foundation model with three strategies: linear probing (lp), lightweight fine-tuning (lft), and full fine-tuning (fft). our analysis presents the following insights: i) compared to ltssl algorithms trained from scratch, fft results in a decline in model performance, whereas lp and lft, although boosting overall model performance, exhibit negligible benefits to tail classes. ii) lp produces numerous false pseudo-labels due to \textit{underlearned} training data, while lft can reduce the number of these false labels but becomes overconfident about them owing to \textit{biased fitting} training data. this exacerbates the pseudo-labeled and classifier biases inherent in ltssl, limiting performance improvement in the tail classes. with these insights, we propose a unbiased lightweight fine-tuning strategy, \textbf{ulfine}, which mitigates the overconfidence via confidence-aware adaptive fitting of textual prototypes and counteracts the pseudo-labeled and classifier biases via complementary fusion of dual logits. extensive experiments demonstrate that ulfine markedly decreases training costs by over ten times and substantially increases prediction accuracies compared to state-of-the-art methods.",,2025-05-08,,"['enhao zhang', 'chaohua li', 'chuanxing geng', 'songcan chen']"
2505.05073,repsnet: a nucleus instance segmentation model based on boundary   regression and structural re-parameterization,eess.iv cs.cv,"pathological diagnosis is the gold standard for tumor diagnosis, and nucleus instance segmentation is a key step in digital pathology analysis and pathological diagnosis. however, the computational efficiency of the model and the treatment of overlapping targets are the major challenges in the studies of this problem. to this end, a neural network model repsnet was designed based on a nucleus boundary regression and a structural re-parameterization scheme for segmenting and classifying the nuclei in h\&e-stained histopathological images. first, repsnet estimates the boundary position information (bpi) of the parent nucleus for each pixel. the bpi estimation incorporates the local information of the pixel and the contextual information of the parent nucleus. then, the nucleus boundary is estimated by aggregating the bpis from a series of pixels using a proposed boundary voting mechanism (bvm), and the instance segmentation results are computed from the estimated nucleus boundary using a connected component analysis procedure. the bvm intrinsically achieves a kind of synergistic belief enhancement among the bpis from various pixels. therefore, different from the methods available in literature that obtain nucleus boundaries based on a direct pixel recognition scheme, repsnet computes its boundary decisions based on some guidances from macroscopic information using an integration mechanism. in addition, repsnet employs a re-parametrizable encoder-decoder structure. this model can not only aggregate features from some receptive fields with various scales which helps segmentation accuracy improvement, but also reduce the parameter amount and computational burdens in the model inference phase through the structural re-parameterization technique. extensive experiments demonstrated the superiorities of repsnet compared to several typical benchmark models.",10.1007/s11263-024-02332-z,2025-05-08,,"['shengchun xiong', 'xiangru li', 'yunpeng zhong', 'wanfen peng']"
2505.05074,visual affordances: enabling robots to understand object functionality,cs.cv cs.ro,"human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. in this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. to address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. to favour transparency, we introduce the affordance sheet, a document to detail the proposed solution, the datasets, and the validation. as the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.",,2025-05-08,,"['tommaso apicella', 'alessio xompero', 'andrea cavallaro']"
2505.05076,the city that never settles: simulation-based lidar dataset for   long-term place recognition under extreme structural changes,cs.ro cs.cv,"large-scale construction and demolition significantly challenge long-term place recognition (pr) by drastically reshaping urban and suburban environments. existing datasets predominantly reflect limited or indoor-focused changes, failing to adequately represent extensive outdoor transformations. to bridge this gap, we introduce the city that never settles (cns) dataset, a simulation-based dataset created using the carla simulator, capturing major structural changes-such as building construction and demolition-across diverse maps and sequences. additionally, we propose tcr_sym, a symmetric version of the original tcr metric, enabling consistent measurement of structural changes irrespective of source-target ordering. quantitative comparisons demonstrate that cns encompasses more extensive transformations than current real-world benchmarks. evaluations of state-of-the-art lidar-based pr methods on cns reveal substantial performance degradation, underscoring the need for robust algorithms capable of handling significant environmental changes. our dataset is available at https://github.com/hyunho111/cns_dataset.",,2025-05-08,,"['hyunho song', 'dongjae lee', 'seunghun oh', 'minwoo jung', 'ayoung kim']"
2505.05088,ssh-net: a self-supervised and hybrid network for noisy image watermark   removal,cs.mm cs.cv eess.iv,"visible watermark removal is challenging due to its inherent complexities and the noise carried within images. existing methods primarily rely on supervised learning approaches that require paired datasets of watermarked and watermark-free images, which are often impractical to obtain in real-world scenarios. to address this challenge, we propose ssh-net, a self-supervised and hybrid network specifically designed for noisy image watermark removal. ssh-net synthesizes reference watermark-free images using the watermark distribution in a self-supervised manner and adopts a dual-network design to address the task. the upper network, focused on the simpler task of noise removal, employs a lightweight cnn-based architecture, while the lower network, designed to handle the more complex task of simultaneously removing watermarks and noise, incorporates transformer blocks to model long-range dependencies and capture intricate image features. to enhance the model's effectiveness, a shared cnn-based feature encoder is introduced before dual networks to extract common features that both networks can leverage. our code will be available at https://github.com/wenyang001/ssh-net.",,2025-05-08,,"['wenyang liu', 'jianjun gao', 'kim-hui yap']"
2505.05089,nonlinear motion-guided and spatio-temporal aware network for   unsupervised event-based optical flow,cs.cv,"event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. however, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. in this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. therefore, we propose e-nmstflow, a novel unsupervised event-based optical flow network focusing on long-time sequences. we propose a spatio-temporal motion feature aware (stmfa) module and an adaptive motion feature enhancement (amfe) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. extensive experiments demonstrate the effectiveness and superiority of our method. remarkably, our method ranks first among unsupervised learning methods on the mvsec and dsec-flow datasets. our project page is available at https://wynelio.github.io/e-nmstflow.",,2025-05-08,,"['zuntao liu', 'hao zhuang', 'junjie jiang', 'yuhang song', 'zheng fang']"
2505.05091,dispbench: benchmarking disparity estimation to synthetic corruptions,cs.cv cs.lg,"deep learning (dl) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. one such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. however, dl-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.   to address this gap, we introduce dispbench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. dispbench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2d common corruptions across multiple datasets and diverse corruption scenarios. we conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. open-source code for dispbench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation",,2025-05-08,,"['shashank agnihotri', 'amaan ansari', 'annika dackermann', 'fabian rösch', 'margret keuper']"
2505.05098,x-driver: explainable autonomous driving with vision-language models,cs.ro cs.cl cs.cv cs.et,"end-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. however, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. in this paper, we introduce x-driver, a unified multi-modal large language models(mllms) framework designed for closed-loop autonomous driving, leveraging chain-of-thought(cot) and autoregressive modeling to enhance perception and decision-making. we validate x-driver across multiple autonomous driving tasks using public benchmarks in carla simulation environment, including bench2drive[6]. our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(sota) while improving the interpretability of driving decisions. these findings underscore the importance of structured reasoning in end-to-end driving and establish x-driver as a strong baseline for future research in closed-loop autonomous driving.",,2025-05-08,,"['wei liu', 'jiyuan zhang', 'binxiong zheng', 'yufeng hu', 'yingzhan lin', 'zengfeng zeng']"
2505.05112,mdaa-diff: ct-guided multi-dose adaptive attention diffusion model for   pet denoising,eess.iv cs.cv,"acquiring high-quality positron emission tomography (pet) images requires administering high-dose radiotracers, which increases radiation exposure risks. generating standard-dose pet (spet) from low-dose pet (lpet) has become a potential solution. however, previous studies have primarily focused on single low-dose pet denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from ct images. in this work, we propose a novel ct-guided multi-dose adaptive attention denoising diffusion model (mdaa-diff) for multi-dose pet denoising. our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. specifically, this approach incorporates a ct-guided high-frequency wavelet attention (hwa) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from ct images. these extracted features are then incorporated into pet imaging through an adaptive weighted fusion mechanism to enhance edge details. additionally, we propose the dose-adaptive attention (daa) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. extensive experiments on 18f-fdg and 68ga-fapi datasets demonstrate that mdaa-diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. our code is publicly available.",,2025-05-08,,"['xiaolong niu', 'zanting ye', 'xu han', 'yanchao huang', 'hao sun', 'hubing wu', 'lijun lu']"
2505.05136,automated vision-based assistance tools in bronchoscopy: stenosis   severity estimation,cs.cv,"purpose: subglottic stenosis refers to the narrowing of the subglottis, the airway between the vocal cords and the trachea. its severity is typically evaluated by estimating the percentage of obstructed airway. this estimation can be obtained from ct data or through visual inspection by experts exploring the region. however, visual inspections are inherently subjective, leading to less consistent and robust diagnoses. no public methods or datasets are currently available for automated evaluation of this condition from bronchoscopy video.   methods: we propose a pipeline for automated subglottic stenosis severity estimation during the bronchoscopy exploration, without requiring the physician to traverse the stenosed region. our approach exploits the physical effect of illumination decline in endoscopy to segment and track the lumen and obtain a 3d model of the airway. this 3d model is obtained from a single frame and is used to measure the airway narrowing.   results: our pipeline is the first to enable automated and robust subglottic stenosis severity measurement using bronchoscopy images. the results show consistency with ground-truth estimations from ct scans and expert estimations, and reliable repeatability across multiple estimations on the same patient. our evaluation is performed on our new subglottic stenosis dataset of real bronchoscopy procedures data.   conclusion: we demonstrate how to automate evaluation of subglottic stenosis severity using only bronchoscopy. our approach can assist with and shorten diagnosis and monitoring procedures, with automated and repeatable estimations and less exploration time, and save radiation exposure to patients as no ct is required. additionally, we release the first public benchmark for subglottic stenosis severity assessment.",,2025-05-08,,"['clara tomasini', 'javier rodriguez-puigvert', 'dinora polanco', 'manuel viñuales', 'luis riazuelo', 'ana cristina murillo']"
2505.05137,research on anomaly detection methods based on diffusion models,cs.lg cs.cv,"anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. in this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (dpms) to effectively identify anomalies in both image and audio data. the proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. to enhance the framework's performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. extensive experiments on benchmark datasets, including mvtec ad and urbansound8k, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. this research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications.",,2025-05-08,,['yi chen']
2505.05163,probabilistic embeddings for frozen vision-language models: uncertainty   quantification with gaussian process latent variable models,cs.cv cs.lg,"vision-language models (vlms) learn joint representations by mapping images and text into a shared latent space. however, recent research highlights that deterministic embeddings from standard vlms often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. existing approaches tackle this by learning probabilistic embeddings during vlm training, which demands large datasets and does not leverage the powerful representations already learned by large-scale vlms like clip. in this paper, we propose grove, a post-hoc approach to obtaining probabilistic embeddings from frozen vlms. grove builds on gaussian process latent variable model (gplvm) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. once trained, the gaussian process model generates uncertainty-aware probabilistic embeddings. evaluation shows that grove achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.",,2025-05-08,,"['aishwarya venkataramanan', 'paul bodesheim', 'joachim denzler']"
2505.05183,panicar: securing the perception of advanced driving assistance systems   against emergency vehicle lighting,cs.cv cs.lg,"the safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). while previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. in this research, we unveil panicar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. this vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. in addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (adass). we assess seven commercial adass (tesla model 3, ""manufacturer c"", hp, pelsee, azdome, imagebon, rexing), four object detectors (yolo, ssd, retinanet, faster r-cnn), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. we also evaluate four sota flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. to mitigate this risk, we propose caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. our evaluation shows that on yolov3 and faster rcnn, caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. in addition, caracetamol is capable of processing frames at a rate of between 30-50 fps, enabling real-time adas car detection.",,2025-05-08,,"['elad feldman', 'jacob shams', 'dudi biton', 'alfred chen', 'shaoyuan xie', 'satoru koda', 'yisroel mirsky', 'asaf shabtai', 'yuval elovici', 'ben nassi']"
2505.05189,biomed-dpt: dual modality prompt tuning for biomedical vision-language   models,cs.cv cs.ai,"prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (vlms) to the biomedical image classification tasks in few shot scenarios. however, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. in this work, we propose biomed-dpt, a knowledge-enhanced dual modality prompt tuning technique. in designing the text prompt, biomed-dpt constructs a dual prompt including the template-driven clinical prompts and the large language model (llm)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. in designing the vision prompt, biomed-dpt introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. biomed-dpt achieves an average classification accuracy of 66.14\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\% in base classes and 75.97\% in novel classes, surpassing the context optimization (coop) method by 6.20\%, 3.78\%, and 8.04\%, respectively. our code are available at \underline{https://github.com/kanyooo/biomed-dpt}.",,2025-05-08,,"['wei peng', 'kang liu', 'jianchen hu', 'meng zhang']"
2505.05195,concept-based unsupervised domain adaptation,cs.lg cs.ai cs.cv,"concept bottleneck models (cbms) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. this assumption often fails under domain shifts, leading to degraded performance and poor generalization. to address these limitations and improve the robustness of cbms, we propose the concept-based unsupervised domain adaptation (cuda) framework. cuda is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling cbms to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (da) with theoretical guarantees, improving interpretability and establishing new benchmarks for da. experiments demonstrate that our approach significantly outperforms the state-of-the-art cbm and da methods on real-world datasets.",,2025-05-08,,"['xinyue xu', 'yueying hu', 'hui tang', 'yi qin', 'lu mi', 'hao wang', 'xiaomeng li']"
2505.05208,improved brain tumor detection in mri: fuzzy sigmoid convolution in deep   learning,eess.iv cs.cv,"early detection and accurate diagnosis are essential to improving patient outcomes. the use of convolutional neural networks (cnns) for tumor detection has shown promise, but existing models often suffer from overparameterization, which limits their performance gains. in this study, fuzzy sigmoid convolution (fsc) is introduced along with two additional modules: top-of-the-funnel and middle-of-the-funnel. the proposed methodology significantly reduces the number of trainable parameters without compromising classification accuracy. a novel convolutional operator is central to this approach, effectively dilating the receptive field while preserving input data integrity. this enables efficient feature map reduction and enhances the model's tumor detection capability. in the fsc-based model, fuzzy sigmoid activation functions are incorporated within convolutional layers to improve feature extraction and classification. the inclusion of fuzzy logic into the architecture improves its adaptability and robustness. extensive experiments on three benchmark datasets demonstrate the superior performance and efficiency of the proposed model. the fsc-based architecture achieved classification accuracies of 99.17%, 99.75%, and 99.89% on three different datasets. the model employs 100 times fewer parameters than large-scale transfer learning architectures, highlighting its computational efficiency and suitability for detecting brain tumors early. this research offers lightweight, high-performance deep-learning models for medical imaging applications.",,2025-05-08,,"['muhammad irfan', 'anum nawaz', 'riku klen', 'abdulhamit subasi', 'tomi westerlund', 'wei chen']"
2505.05209,eam: enhancing anything with diffusion transformers for blind   super-resolution,cs.cv,"utilizing pre-trained text-to-image (t2i) diffusion models to guide blind super-resolution (bsr) has become a predominant approach in the field. while t2i models have traditionally relied on u-net architectures, recent advancements have demonstrated that diffusion transformers (dit) achieve significantly higher performance in this domain. in this work, we introduce enhancing anything model (eam), a novel bsr method that leverages dit and outperforms previous u-net-based approaches. we introduce a novel block, $\psi$-dit, which effectively guides the dit to enhance image restoration. this block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained dit. to fully exploit the prior guidance capabilities of t2i models and enhance their generalization in bsr, we introduce a progressive masked image modeling strategy, which also reduces training costs. additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. this strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of t2i diffusion priors. our experiments demonstrate that eam achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.",,2025-05-08,,"['haizhen xie', 'kunpeng du', 'qiangyu yan', 'sen lu', 'jianhong han', 'hanting chen', 'hailin hu', 'jie hu']"
2505.05212,hqc-nbv: a hybrid quantum-classical view planning approach,cs.cv,"efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. while classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. this study introduces hqc-nbv, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. we propose a specific hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. this work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.",,2025-05-08,,"['xiaotong yu', 'chang wen chen']"
2505.05215,diffusion model quantization: a review,cs.cv,"recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. to facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. this survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. first, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on u-net architectures and diffusion transformers (dit). we then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. from a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. from a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. in conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. the list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage https://github.com/taylorjocelyn/diffusion-model-quantization.",,2025-05-08,,"['qian zeng', 'chenggong hu', 'mingli song', 'jie song']"
2505.05223,multi-objective reinforcement learning for adaptive personalized   autonomous driving,cs.ro cs.cv cs.lg,"human drivers exhibit individual preferences regarding driving style. adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. however, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. we propose a novel approach using multi-objective reinforcement learning (morl) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\unicode{x2013}$without requiring policy retraining. our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the carla simulator. experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.",,2025-05-08,,"['hendrik surmann', 'jorge de heuvel', 'maren bennewitz']"
2505.05229,does clip perceive art the same way we do?,cs.cv cs.mm,"clip has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it ""see"" the same way humans do - especially when interpreting artworks? in this paper, we investigate clip's ability to extract high-level semantic and stylistic information from paintings, including both human-created and ai-generated imagery. we evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. by designing targeted probing tasks and comparing clip's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. our findings reveal both strengths and limitations in clip's visual representations, particularly in relation to aesthetic cues and artistic intent. we further discuss the implications of these insights for using clip as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.",,2025-05-08,,"['andrea asperti', 'leonardo dessì', 'maria chiara tonetti', 'nico wu']"
2505.0524,padriver: towards personalized autonomous driving,cs.cv,"in this paper, we propose padriver, a novel closed-loop framework for personalized autonomous driving (pad). built upon multi-modal large language model (mllm), padriver takes streaming frames and personalized textual prompts as inputs. it autoaggressively performs scene understanding, danger level estimation and action decision. the predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. moreover, we construct a closed-loop benchmark named pad-highway based on highway-env simulator to comprehensively evaluate the decision performance under traffic rules. the dataset contains 250 hours videos with high-quality annotation to facilitate the development of pad behavior analysis. experimental results on the constructed benchmark show that padriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.",,2025-05-08,,"['genghua kou', 'fan jia', 'weixin mao', 'yingfei liu', 'yucheng zhao', 'ziheng zhang', 'osamu yoshie', 'tiancai wang', 'ying li', 'xiangyu zhang']"
2505.05248,white light specular reflection data augmentation for deep learning   polyp detection,eess.iv cs.cv,"colorectal cancer is one of the deadliest cancers today, but it can be prevented through early detection of malignant polyps in the colon, primarily via colonoscopies. while this method has saved many lives, human error remains a significant challenge, as missing a polyp could have fatal consequences for the patient. deep learning (dl) polyp detectors offer a promising solution. however, existing dl polyp detectors often mistake white light reflections from the endoscope for polyps, which can lead to false positives.to address this challenge, in this paper, we propose a novel data augmentation approach that artificially adds more white light reflections to create harder training scenarios. specifically, we first generate a bank of artificial lights using the training dataset. then we find the regions of the training images that we should not add these artificial lights on. finally, we propose a sliding window method to add the artificial light to the areas that fit of the training images, resulting in augmented images. by providing the model with more opportunities to make mistakes, we hypothesize that it will also have more chances to learn from those mistakes, ultimately improving its performance in polyp detection. experimental results demonstrate the effectiveness of our new data augmentation method.",,2025-05-08,,"['jose angel nuñez', 'fabian vazquez', 'diego adame', 'xiaoyan fu', 'pengfei gu', 'bin fu']"
2505.05279,mtl-ue: learning to learn nothing for multi-task learning,cs.lg cs.cr cs.cv,"most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (stl) models with personal data. nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (mtl), targeting generalist and foundation models that can handle multiple tasks simultaneously. despite their growing importance, mtl data and models have been largely neglected while pursuing unlearnable strategies. this paper presents mtl-ue, the first unified framework for generating unlearnable examples for multi-task data and mtl models. instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. in addition, mtl-ue incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. furthermore, mtl-ue is versatile with good supports for dense prediction tasks in mtl. it is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. extensive experiments show that mtl-ue achieves superior attacking performance consistently across 4 mtl datasets, 3 base ue methods, 5 model backbones, and 5 mtl task-weighting strategies.",,2025-05-08,,"['yi yu', 'song xia', 'siyuan yang', 'chenqi kong', 'wenhan yang', 'shijian lu', 'yap-peng tan', 'alex c. kot']"
2505.05288,placeit3d: language-guided object placement in real 3d scenes,cs.cv cs.ai cs.ro,"we introduce the novel task of language-guided object placement in real 3d scenes. our model is given a 3d scene's point cloud, a 3d asset, and a textual prompt broadly describing where the 3d asset should be placed. the task here is to find a valid placement for the 3d asset that respects the prompt. compared with other language-guided localization tasks in 3d scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3d geometric relationships and free space. we inaugurate this task by proposing a new benchmark and evaluation protocol. we also introduce a new dataset for training 3d llms on this task, as well as the first method to serve as a non-trivial baseline. we believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3d llm models.",,2025-05-08,,"['ahmed abdelreheem', 'filippo aleotti', 'jamie watson', 'zawar qureshi', 'abdelrahman eldesokey', 'peter wonka', 'gabriel brostow', 'sara vicente', 'guillermo garcia-hernando']"
2505.05307,pre-mamba: a 4d state space model for ultra-high-frequent event camera   deraining,cs.cv,"event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. in this paper, we propose pre-mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. our framework introduces a 4d event cloud representation that integrates dual temporal scales to preserve high temporal precision, a spatio-temporal decoupling and fusion module (stdf) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a multi-scale state space model (ms3m) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. enhanced by frequency-domain regularization, pre-mamba achieves superior performance (0.95 sr, 0.91 nr, and 0.4s/m events) with only 0.26m parameters on eventrain-27k, a comprehensive dataset with labeled synthetic and real-world sequences. moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.",,2025-05-08,,"['ciyu ruan', 'ruishan guo', 'zihang gong', 'jingao xu', 'wenhan yang', 'xinlei chen']"
2505.05309,augmented deep contexts for spatially embedded video coding,eess.iv cs.cv,"most neural video codecs (nvcs) only employ temporal references to generate temporal-only contexts and latent prior. these temporal-only nvcs fail to handle large motions or emerging objects due to limited contexts and misaligned latent prior. to relieve the limitations, we propose a spatially embedded video codec (sevc), in which the low-resolution video is compressed for spatial references. firstly, our sevc leverages both spatial and temporal references to generate augmented motion vectors and hybrid spatial-temporal contexts. secondly, to address the misalignment issue in latent prior and enrich the prior information, we introduce a spatial-guided latent prior augmented by multiple temporal latent representations. at last, we design a joint spatial-temporal optimization to learn quality-adaptive bit allocation for spatial references, further boosting rate-distortion performance. experimental results show that our sevc effectively alleviates the limitations in handling large motions or emerging objects, and also reduces 11.9% more bitrate than the previous state-of-the-art nvc while providing an additional low-resolution bitstream. our code and model are available at https://github.com/esakak/sevc.",,2025-05-08,,"['yifan bian', 'chuanbo tang', 'li li', 'dong liu']"
2505.05318,"mapping user trust in vision language models: research landscape,   challenges, and prospects",cs.cv cs.ai cs.cy cs.hc cs.ro,"the rapid adoption of vision language models (vlms), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. this survey reviews studies on trust dynamics in user-vlm interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. literature insights and findings from a workshop with prospective vlm users inform preliminary requirements for future vlm trust studies.",,2025-05-08,,"['agnese chiatti', 'sara bernardini', 'lara shibelski godoy piccolo', 'viola schiaffonati', 'matteo matteucci']"
2505.05321,feature-augmented deep networks for multiscale building segmentation in   high-resolution uav and satellite imagery,cs.cv cs.ai,"accurate building segmentation from high-resolution rgb imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. in this study, we present a comprehensive deep learning framework for multiscale building segmentation using rgb aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. we curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including principal component analysis (pca), visible difference vegetation index (vdvi), morphological building index (mbi), and sobel edge filters from rgb channels. these features guide a res-u-net architecture in learning complex spatial patterns more effectively. we also propose training policies incorporating layer freezing, cyclical learning rates, and superconvergence to reduce training time and resource usage. evaluated on a held-out worldview-3 image, our model achieves an overall accuracy of 96.5%, an f1-score of 0.86, and an intersection over union (iou) of 0.80, outperforming existing rgb-based benchmarks. this study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.",,2025-05-08,,"['chintan b. maniyar', 'minakshi kumar', 'gengchen mai']"
2505.05331,aesthetics without semantics,cs.cv q-bio.nc stat.co,"while it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. we address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. the resulting minimum semantic content (msc) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. we next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.",,2025-05-08,,"['c. alejandro parraga', 'olivier penacchio', 'marcos muňoz gonzalez', 'bogdan raducanu', 'xavier otazu']"
2505.05336,progressive inertial poser: progressive real-time kinematic chain   estimation for 3d full-body pose from three imu sensors,cs.cv,"the motion capture system that supports full-body virtual representation is of key significance for virtual reality. compared to vision-based systems, full-body pose estimation from sparse tracking signals is not limited by environmental conditions or recording range. however, previous works either face the challenge of wearing additional sensors on the pelvis and lower-body or rely on external visual sensors to obtain global positions of key joints. to improve the practicality of the technology for virtual reality applications, we estimate full-body poses using only inertial data obtained from three inertial measurement unit (imu) sensors worn on the head and wrists, thereby reducing the complexity of the hardware system. in this work, we propose a method called progressive inertial poser (progip) for human pose estimation, which combines neural network estimation with a human dynamics model, considers the hierarchical structure of the kinematic chain, and employs a multi-stage progressive network estimation with increased depth to reconstruct full-body motion in real time. the encoder combines transformer encoder and bidirectional lstm (te-bilstm) to flexibly capture the temporal dependencies of the inertial sequence, while the decoder based on multi-layer perceptrons (mlps) transforms high-dimensional features and accurately projects them onto skinned multi-person linear (smpl) model parameters. quantitative and qualitative experimental results on multiple public datasets show that our method outperforms state-of-the-art methods with the same inputs, and is comparable to recent works using six imu sensors.",,2025-05-08,,"['zunjie zhu', 'yan zhao', 'yihan hu', 'guoxiang wang', 'hai qiu', 'bolun zheng', 'chenggang yan', 'feng xu']"
2505.05343,hearing and seeing through clip: a framework for self-supervised sound   source localization,cs.cv cs.sd eess.as,"large-scale vision-language models demonstrate strong multimodal alignment and generalization across diverse tasks. among them, clip stands out as one of the most successful approaches. in this work, we extend the application of clip to sound source localization, proposing a self-supervised method operates without explicit text input. we introduce a framework that maps audios into tokens compatible with clip's text encoder, producing audio-driven embeddings. these embeddings are used to generate sounding region masks, from which visual features are extracted and aligned with the audio embeddings through a contrastive audio-visual correspondence objective. our findings show that alignment knowledge of pre-trained multimodal foundation model enables our method to generate more complete and compact localization for sounding objects. we further propose an llm-guided extension that distills object-aware audio-visual scene understanding into the model during training to enhance alignment. extensive experiments across five diverse tasks demonstrate that our method, in all variants, outperforms state-of-the-art approaches and achieves strong generalization in zero-shot settings.",,2025-05-08,,"['sooyoung park', 'arda senocak', 'joon son chung']"
2505.05356,time of the flight of the gaussians: optimizing depth indirectly in   dynamic radiance fields,cs.gr cs.ai cs.cv,"we present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (c-tof) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. quickly achieving high-fidelity dynamic 3d reconstruction from a single viewpoint is a significant challenge in computer vision. in c-tof radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. this problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3d gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. we incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by gaussians. experimental results show that our approach produces accurate reconstructions under constrained c-tof sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf",,2025-05-08,,"['runfeng li', 'mikhail okunev', 'zixuan guo', 'anh ha duong', 'christian richardt', ""matthew o'toole"", 'james tompkin']"
2505.05367,joint super-resolution and segmentation for 1-m impervious surface area   mapping in china's yangtze river economic belt,cs.cv eess.iv,"we propose a novel joint framework by integrating super-resolution and segmentation, called jointseg, which enables the generation of 1-meter isa maps directly from freely available sentinel-2 imagery. jointseg was trained on multimodal cross-resolution inputs, offering a scalable and affordable alternative to traditional approaches. this synergistic design enables gradual resolution enhancement from 10m to 1m while preserving fine-grained spatial textures, and ensures high classification fidelity through effective cross-scale feature fusion. this method has been successfully applied to the yangtze river economic belt (yreb), a region characterized by complex urban-rural patterns and diverse topography. as a result, a comprehensive isa mapping product for 2021, referred to as isa-1, was generated, covering an area of over 2.2 million square kilometers. quantitative comparisons against the 10m esa worldcover and other benchmark products reveal that isa-1 achieves an f1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by 9.5%, and surpassing other isa datasets by 21.43%-61.07%. in densely urbanized areas (e.g., suzhou, nanjing), isa-1 reduces isa overestimation through improved discrimination of green spaces and water bodies. conversely, in mountainous regions (e.g., ganzi, zhaotong), it identifies significantly more isa due to its enhanced ability to detect fragmented anthropogenic features such as rural roads and sparse settlements, demonstrating its robustness across diverse landscapes. moreover, we present biennial isa maps from 2017 to 2023, capturing spatiotemporal urbanization dynamics across representative cities. the results highlight distinct regional growth patterns: rapid expansion in upstream cities, moderate growth in midstream regions, and saturation in downstream metropolitan areas.",,2025-05-08,,"['jie deng', 'danfeng hong', 'chenyu li', 'naoto yokoya']"
2505.05374,ocularage: a comparative study of iris and periocular images for   pediatric age estimation,eess.iv cs.cv,"estimating a child's age from ocular biometric images is challenging due to subtle physiological changes and the limited availability of longitudinal datasets. although most biometric age estimation studies have focused on facial features and adult subjects, pediatric-specific analysis, particularly of the iris and periocular regions, remains relatively unexplored. this study presents a comparative evaluation of iris and periocular images for estimating the ages of children aged between 4 and 16 years. we utilized a longitudinal dataset comprising more than 21,000 near-infrared (nir) images, collected from 288 pediatric subjects over eight years using two different imaging sensors. a multi-task deep learning framework was employed to jointly perform age prediction and age-group classification, enabling a systematic exploration of how different convolutional neural network (cnn) architectures, particularly those adapted for non-square ocular inputs, capture the complex variability inherent in pediatric eye images. the results show that periocular models consistently outperform iris-based models, achieving a mean absolute error (mae) of 1.33 years and an age-group classification accuracy of 83.82%. these results mark the first demonstration that reliable age estimation is feasible from children's ocular images, enabling privacy-preserving age checks in child-centric applications. this work establishes the first longitudinal benchmark for pediatric ocular age estimation, providing a foundation for designing robust, child-focused biometric systems. the developed models proved resilient across different imaging sensors, confirming their potential for real-world deployment. they also achieved inference speeds of less than 10 milliseconds per image on resource-constrained vr headsets, demonstrating their suitability for real-time applications.",,2025-05-08,,"['naveenkumar g venkataswamy', 'poorna ravi', 'stephanie schuckers', 'masudul h. imtiaz']"
2505.05397,pillarmamba: learning local-global context for roadside point cloud via   hybrid state space model,cs.cv,"serving the intelligent transport system (its) and vehicle-to-everything (v2x) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. however, roadside point cloud oriented 3d object detection has not been effectively explored. to some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. the recent emergence of mamba, based on state space model (ssm), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. in this work, we introduce mamba to pillar-based roadside point cloud perception and propose a framework based on cross-stage state-space group (csg), called pillarmamba. it enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. however, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. to address this, we propose the hybrid state-space block (hsb) to obtain the local-global context of roadside point cloud. specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. the proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: dair-v2x-i. the code will be released soon.",,2025-05-08,,"['zhang zhang', 'chao sun', 'chao yue', 'da wen', 'tianze wang', 'jianghao leng']"
2505.05422,toklip: marry visual tokens to clip for multimodal comprehension and   generation,cs.cv cs.ai cs.cl,"pioneering token-based works such as chameleon and emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. in this paper, we introduce toklip, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (vq) tokens and incorporating clip-level semantics while enabling end-to-end multimodal autoregressive training with standard vq tokens. toklip integrates a low-level discrete vq tokenizer with a vit-based token encoder to capture high-level continuous semantics. unlike previous approaches (e.g., vila-u) that discretize high-level features, toklip disentangles training objectives for comprehension and generation, allowing the direct application of advanced vq tokenizers without the need for tailored quantization operations. our empirical results demonstrate that toklip achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive transformers in both comprehension and generation tasks. the code and models are available at https://github.com/tencentarc/toklip.",,2025-05-08,,"['haokun lin', 'teng wang', 'yixiao ge', 'yuying ge', 'zhichao lu', 'ying wei', 'qingfu zhang', 'zhenan sun', 'ying shan']"
2505.05446,adaptive markup language generation for contextually-grounded visual   document understanding,cs.cv cs.cl,"visual document understanding has become essential with the increase of text-rich visual content. this field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. to address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as markdown, json, html, and tikz, to build highly structured document representations and deliver contextually-grounded responses. we introduce two fine-grained structured datasets: docmark-pile, comprising approximately 3.8m pretraining data pairs for document parsing, and docmark-instruct, featuring 624k fine-tuning data annotations for grounded instruction following. extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart mllms across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. our code and models are released at https://github. com/euphoria16/docmark.",,2025-05-08,,"['han xiao', 'yina xie', 'guanxin tan', 'yinghao chen', 'rui hu', 'ke wang', 'aojun zhou', 'hao li', 'hao shao', 'xudong lu', 'peng gao', 'yafei wen', 'xiaoxin chen', 'shuai ren', 'hongsheng li']"
2505.05456,site: towards spatial intelligence thorough evaluation,cs.cv,"spatial intelligence (si) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. we introduce site, a benchmark dataset towards si thorough evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and si factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental si factor. moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied ai task.",,2025-05-08,,"['wenqi wang', 'reuben tan', 'pengyue zhu', 'jianwei yang', 'zhengyuan yang', 'lijuan wang', 'andrey kolobov', 'jianfeng gao', 'boqing gong']"
2505.05467,streambridge: turning your offline video large language model into a   proactive streaming assistant,cs.cv cs.ai cs.cl,"we present streambridge, a simple yet effective framework that seamlessly transforms offline video-llms into streaming-capable models. it addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. specifically, streambridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing video-llms, enabling continuous proactive responses. to further support streambridge, we construct stream-it, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. extensive experiments show that streambridge significantly improves the streaming understanding capabilities of offline video-llms across various tasks, outperforming even proprietary models such as gpt-4o and gemini 1.5 pro. simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.",,2025-05-08,,"['haibo wang', 'bo feng', 'zhengfeng lai', 'mingze xu', 'shiyu li', 'weifeng ge', 'afshin dehghan', 'meng cao', 'ping huang']"
2505.05469,generating physically stable and buildable lego designs from text,cs.cv,"we introduce legogpt, the first approach for generating physically stable lego brick models from text prompts. to achieve this, we construct a large-scale, physically stable dataset of lego designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. to improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. our experiments show that legogpt produces stable, diverse, and aesthetically pleasing lego designs that align closely with the input text prompts. we also develop a text-based lego texturing method to generate colored and textured designs. we show that our designs can be assembled manually by humans and automatically by robotic arms. we also release our new dataset, stabletext2lego, containing over 47,000 lego structures of over 28,000 unique 3d objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/legogpt/.",,2025-05-08,,"['ava pun', 'kangle deng', 'ruixuan liu', 'deva ramanan', 'changliu liu', 'jun-yan zhu']"
2505.05473,diffusionsfm: predicting structure and motion via ray origin and   endpoint diffusion,cs.cv,"current structure-from-motion (sfm) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. in contrast, we propose a data-driven multi-view reasoning approach that directly infers 3d scene geometry and camera poses from multi-view images. our framework, diffusionsfm, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. to address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. we empirically validate diffusionsfm on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.",,2025-05-08,,"['qitao zhao', 'amy lin', 'jeff tan', 'jason y. zhang', 'deva ramanan', 'shubham tulsiani']"
2505.05474,3d scene generation: a survey,cs.cv,"3d scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied ai. early methods based on procedural rules offered scalability but limited diversity. recent advances in deep generative models (e.g., gans, diffusion models) and 3d representations (e.g., nerf, 3d gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. recent advances like diffusion models bridge 3d scene synthesis and photorealism by reframing generation as image or video synthesis problems. this survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3d-based generation, image-based generation, and video-based generation. we analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. we conclude by discussing key challenges in generation capacity, 3d representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. this review organizes recent advances in 3d scene generation and highlights promising directions at the intersection of generative ai, 3d vision, and embodied intelligence. to track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/awesome-3d-scene-generation.",,2025-05-08,,"['beichen wen', 'haozhe xie', 'zhaoxi chen', 'fangzhou hong', 'ziwei liu']"
2505.05475,svad: from single image to 3d avatar via synthetic data generation with   video diffusion and data augmentation,cs.cv,"creating high-quality animatable 3d human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3d information from a single viewpoint. current approaches face a clear limitation: 3d gaussian splatting (3dgs) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. we present svad, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3dgs avatars. comprehensive evaluations demonstrate that svad outperforms state-of-the-art (sota) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3dgs approaches. extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. by effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3dgs, our work establishes a new approach for high-fidelity avatar generation from a single image input.",,2025-05-08,,['yonwoo choi']

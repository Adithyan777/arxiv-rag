{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2a7bb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM config\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# IO-NET\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"Qwen/QwQ-32B\",\n",
    "#     base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "#     api_key=getenv(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "# smol = ChatOpenAI(\n",
    "#     model = \"Qwen/QwQ-32B\",\n",
    "#     base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "#     api_key=getenv(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "# LMSTUDIO\n",
    "llm = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-4b\",\n",
    "    base_url = \"http://127.0.0.1:1234/v1\"\n",
    ")\n",
    "\n",
    "smol = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-4b\",\n",
    "    base_url = \"http://127.0.0.1:1234/v1\"\n",
    ")\n",
    "\n",
    "# # OPENROUTER\n",
    "# smol = ChatOpenAI(\n",
    "#     model = \"qwen/qwen3-8b:free\",\n",
    "#     base_url = \"https://openrouter.ai/api/v1\",\n",
    "#     api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    "# )\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"qwen/qwen3-8b:free\",\n",
    "#     base_url = \"https://openrouter.ai/api/v1\",\n",
    "#     api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a98755b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "collection_name = \"arxiv-cvpr-main\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "   model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a89a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"How is visual hallucination still an issue in LVLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "28a54401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def get_rewritten_queries(question: str, llm) -> List[str]:\n",
    "    \"\"\"Generate multiple versions of the input question using an LLM.\"\"\"\n",
    "    multi_query_template = PromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database of research papers in the field Computer Vision and Pattern Recognition. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide ONLY the alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "    \n",
    "    multi_query_chain = multi_query_template | llm\n",
    "    queries = multi_query_chain.invoke({\"question\": question}).content.split('\\n')\n",
    "    \n",
    "    # Clean up queries and add original question\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "    queries.append(question)\n",
    "    return queries\n",
    "\n",
    "def get_top_paper_id(queries: List[str], vector_store) -> str:\n",
    "    \"\"\"Get the most frequent paper ID from multiple queries.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        results = vector_store.similarity_search(\n",
    "            query,\n",
    "            k=1,\n",
    "            score_threshold=0.4\n",
    "        )\n",
    "        if results:\n",
    "            all_results.append(results[0].metadata['id'])\n",
    "\n",
    "    print(f\"All results: {all_results}\")\n",
    "    # Return most common paper ID if we have results, else None\n",
    "    if all_results:\n",
    "        return Counter(all_results).most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def get_paper_id_from_search_query(search_query: str, abstracts_vector_store_collection_name) -> str:\n",
    "    \"\"\"Get the paper ID from a search query using query reconstruction and similarity search.\"\"\"\n",
    "\n",
    "    abstract_vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=abstracts_vector_store_collection_name,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    \n",
    "    # print(f\"\\nOriginal Question: {search_query}\")\n",
    "    queries = get_rewritten_queries(search_query, smol)\n",
    "    # print(\"Rewritten queries:\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "    \n",
    "    # Get most frequent paper ID from all queries\n",
    "    predicted_paper_id = get_top_paper_id(queries, abstract_vector_store)\n",
    "    # print(f\"Predicted Paper ID for re-written queries: {predicted_paper_id}\")\n",
    "\n",
    "    return predicted_paper_id, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d2ad31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <think>\n",
      "2. Okay, the user is asking about why visual hallucinations are still a problem in LVLMs (Large Vision Language Models). I need to come up with three different versions of this question to help retrieve relevant documents from a vector database. The goal is to generate alternative questions that might cover different angles or aspects of the original query, so that the search can find more diverse and relevant papers.\n",
      "3. First, let me break down the original question. The key points are \"visual hallucination\" and \"LVLMs\". The user is interested in understanding why this issue persists. To generate alternative questions, I should consider different perspectives or angles that might be covered in the literature.\n",
      "4. One angle could be the technical aspects. For example, how do LVLMs generate hallucinations? Maybe looking at factors like training data or model architecture. Another angle could be the impact on performance, like how hallucinations affect tasks like image captioning or object detection. A third angle might be comparing LVLMs to other models, such as traditional vision models or LLMs.\n",
      "5. I need to make sure each alternative question is distinct but related. Let me brainstorm a few options:\n",
      "6. 1. What factors contribute to the persistence of visual hallucinations in large vision language models?\n",
      "7. 2. How do current LVLMs handle or fail to handle visual hallucinations in tasks like image captioning?\n",
      "8. 3. What are the underlying causes of visual hallucination issues in large vision language models compared to traditional vision models?\n",
      "9. Wait, the third one compares to traditional models. That's a good angle. Also, maybe looking into the training process or data sources. Another option could be about the evaluation methods for hallucinations in LVLMs.\n",
      "10. Alternatively, maybe focusing on specific tasks where hallucinations are more prevalent. Or the role of language in causing hallucinations, since these models are multimodal.\n",
      "11. Another angle could be the balance between image and language understanding in LVLMs, leading to hallucinations. Or the limitations of current evaluation metrics for visual hallucination in these models.\n",
      "12. I need to make sure each alternative question is phrased differently but still captures the essence of the original query. Let me check if these are distinct enough and cover different aspects.\n",
      "13. So, the three versions could be:\n",
      "14. 1. What factors contribute to the persistence of visual hallucinations in large vision language models?\n",
      "15. 2. How do current LVLMs handle or fail to handle visual hallucinations in tasks like image captioning?\n",
      "16. 3. What are the underlying causes of visual hallucination issues in large vision language models compared to traditional vision models?\n",
      "17. Alternatively, maybe something about the training data or techniques used. Or the role of pre-training vs fine-tuning.\n",
      "18. Another option: What are the challenges in mitigating visual hallucinations in large vision language models, and how do they compare to other multimodal models?\n",
      "19. Hmm. Let me see if these are all distinct and cover different aspects. The first is about contributing factors, the second is about handling in specific tasks, and the third compares to traditional models. That should cover different angles. I need to make sure they are phrased as questions, not statements.\n",
      "20. </think>\n",
      "21. What factors contribute to the persistence of visual hallucinations in large vision language models?\n",
      "22. How do current LVLMs handle or fail to handle visual hallucinations in tasks like image captioning?\n",
      "23. What are the underlying causes of visual hallucination issues in large vision language models compared to traditional vision models?\n",
      "24. How is visual hallucination still an issue in LVLMs?\n",
      "All results: [2505.00703, 2505.01958, 2505.01958, 2505.01958, 2505.02018, 2505.01958, 2505.01958, 2505.01958, 2505.01958, 2505.01958, 2505.01958, 2505.02018, 2505.00681, 2505.01958, 2505.01958, 2505.01958, 2505.01016, 2505.01958, 2505.02018, 2505.00703, 2505.01958, 2505.01958, 2505.01958, 2505.01958]\n",
      "\n",
      "Predicted Paper ID: 2505.01958\n"
     ]
    }
   ],
   "source": [
    "paper_id, rewritten_queries = get_paper_id_from_search_query(\n",
    "    search_query,\n",
    "    \"arxiv-abstracts\"\n",
    ")\n",
    "\n",
    "print(f\"\\nPredicted Paper ID: {paper_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d2ac4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "def get_context_for_qa(paper_id: str, rewritten_queries: List[str], vector_store, k : int = 3) -> List[models.Record]:\n",
    "    \"\"\"Get context for QA from the vector store based on paper ID and search query.\"\"\"\n",
    "    results = []\n",
    "    for query in rewritten_queries:\n",
    "        # Perform similarity search with filter for the specific paper ID\n",
    "        individual_results = vector_store.similarity_search_with_score(\n",
    "            search_query, \n",
    "            k=k, \n",
    "            score_threshold=0.4,\n",
    "            filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"metadata.id\",\n",
    "                        match=models.MatchValue(value=str(paper_id))\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        if individual_results:\n",
    "            for doc in individual_results:\n",
    "                if doc not in results:\n",
    "                    results.append(doc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_context_for_qa_without_id(rewritten_queries: List[str], vector_store, k : int = 3) -> List[models.Record]:\n",
    "    \"\"\"Get context for QA from the vector store based on paper ID and search query.\"\"\"\n",
    "    results = []\n",
    "    for query in rewritten_queries:\n",
    "        # Perform similarity search with filter for the specific paper ID\n",
    "        individual_results = vector_store.similarity_search_with_score(\n",
    "            search_query, \n",
    "            k=k, \n",
    "            score_threshold=0.4,\n",
    "        )\n",
    "        if individual_results:\n",
    "            for doc in individual_results:\n",
    "                if doc not in results:\n",
    "                    results.append(doc)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b14a17c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '2505.01958', 'title': 'a comprehensive analysis for visual object hallucination in large   vision-language models', 'categories': 'cs.cv cs.cl', 'abstract': 'large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.', 'created': '2025-05-03', 'authors': \"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']\", 'heading': '6. Conclusion', '_id': 'e0d00299-475a-44a9-bd46-9a81d2937cc4', '_collection_name': 'arxiv-cvpr-main'}, page_content=\"6. Conclusion\\nIn this paper, our study delves into the visual hallucination problem in LVLMs, identifying its sources within the model's components. By independently analyzing the LLM, vision backbone, and projector, we propose targeted mitigation strategies. We introduce fine-grained hallucination benchmarks, QA-VisualGenome and QA-FB15k, to comprehensively evaluate hallucinations. Our methods demonstrate effectiveness in reducing hallucinations, contributing to the reliability and accuracy of LVLMs.\\nLimitations:  Our work primarily focuses on analyzing and improving hallucinations of general objects, such as tables and people, while neglecting the research topic of how to mitigate cognition-level hallucinations, such as the names of individuals and famous buildings.\"),\n",
       " Document(metadata={'id': '2505.01958', 'title': 'a comprehensive analysis for visual object hallucination in large   vision-language models', 'categories': 'cs.cv cs.cl', 'abstract': 'large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.', 'created': '2025-05-03', 'authors': \"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']\", 'heading': '2. Mitigating Object Hallucination Caused by Different Modules', 'chunk_index': 0, '_id': '3e227220-2f46-4d83-a587-77e721e8e053', '_collection_name': 'arxiv-cvpr-main'}, page_content=\"2. Mitigating Object Hallucination Caused by Different Modules\\nBased on the analysis, we further devised different methods to mitigate the object hallucination in different components in LVLMs.\\n2.1 How to alleviate the hallucination caused by CLIP?\\nThe vision backbone within LVLMs also contributes to hallucinations. The CLIP model, as the vision encoder of LLaVA, is trained on massive image-caption pairs from the internet with a contrastive loss objective. However, these captions are typically brief and noisy, and negative pairs often differ substantially from positive ones. Therefore, it is likely that the model can distinguish them without needing to capture the finer details in the images.\\nTo address this issue, we propose two methods to reduce hallucination caused by the vision backbone:\\n[<RawText children='Tuning CLIP with fine-grained data:'>]\\n[<RawText children='Fine-grained perception-based visual instruction tuning:'>]\\nFigure 2:  Tuning CLIP with fine-grained data (left) and fine-grained perception-based instruction tuning (right).\\n2.2 How to reduce hallucination caused by the projector?\\nHallucination introduced by the projector may be due to the inability of aligning visual and textual spaces, manifested by the low cosine similarity of caption embeddings and projected image features. Therefore, a straightforward remedy would be to explicitly bridge the image and caption representation during LLaVA's alignment stage.\\n2.2 How to reduce hallucination caused by the projector?\\n2.2.1 Loss Objectives\"),\n",
       " Document(metadata={'id': '2505.01958', 'title': 'a comprehensive analysis for visual object hallucination in large   vision-language models', 'categories': 'cs.cv cs.cl', 'abstract': 'large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.', 'created': '2025-05-03', 'authors': \"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']\", 'heading': 'Introduction', 'chunk_index': 1, '_id': '8b63f1e9-ee5f-48b9-90f3-3451d854e1e5', '_collection_name': 'arxiv-cvpr-main'}, page_content='In this work, we focus on visual object-related hallucination and LLaVA-like LVLMs, which typically consist of three modules: the large language model (LLM), the vision backbone, and the projector. Errors in any of these modules can lead to issues in the overall performance or functionality of the model. Therefore, we conduct an independent analysis of each component to identify potential sources of error and their impact.\\nOur main findings are:\\nThe LLM in LVLM is able to generate faithful content when captions of images are provided as input.\\nHallucinations exist in the perception process of the vision backbone.\\nThe projector is able to preserve visual features but has trouble aligning between visual and textual spaces.\\nBased on our observations, we propose methods for the two problematic components to mitigate their hallucination issue. To improve the vision backbone, we propose to finetune CLIP with fine-grained data and fine-grained perception-based visual instruction tuning, and find that both of them can reduce hallucination caused by the vision backbone. For the projector, we propose a contrastive alignment objective with three variations, which can all be integrated into the original training pipeline with minimal additional costs.')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = get_context_for_qa_without_id(rewritten_queries, vector_store)\n",
    "final_context = [res for res, score in results if res.page_content and len(res.page_content) > 0]\n",
    "\n",
    "final_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ccc2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with paper_id:  Visual hallucination remains a significant issue in Large Vision Language Models (LVLMs) due to the inherent challenges in aligning visual and textual information across different model components. The LLM, while capable of generating coherent responses based on image inputs, can still produce inaccurate or fabricated information when the visual and textual components do not align properly. The vision backbone, such as CLIP, is trained on image-caption pairs that are often brief and noisy, leading to a lack of fine-grained details in the learned visual representations. This results in the model being unable to capture subtle differences between images and captions, contributing to hallucinations. Additionally, the projector module struggles to align visual features with textual embeddings, as evidenced by low cosine similarity between caption embeddings and projected image features. These misalignments can cause the model to generate responses that are factually incorrect or inconsistent with the input images, even when the LLM itself is capable of producing faithful content given accurate visual inputs. The paper highlights that these issues arise from the design and training of individual components, necessitating targeted mitigation strategies such as fine-grained data tuning for CLIP and contrastive alignment objectives for the projector to reduce hallucinations.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# TODO: improve prompt to include more persona\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are an expert in CVPR topics and help students to learn by answering questions solely based on the provided context which are taken from research papers in arxiv.\n",
    "\n",
    "Focus on explaining concepts in detail and substantiate answers with relevant context from the given information.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Identify Key Concepts**: Upon receiving a question, pinpoint the core topics within CVPR relevant to the inquiry.\n",
    "2. **Contextual Analysis**: Thoroughly review the provided context to gather accurate and pertinent information specific to the question.\n",
    "3. **Detailed Explanation**: Craft a comprehensive explanation, incorporating key details and any relevant examples that illuminate the concept.\n",
    "4. **Clarification and Depth**: Ensure the response is clear, well-substantiated, and sufficiently detailed to aid student understanding.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Provide a paragraph elaborating the concept or answering the inquiry.\n",
    "- Ensure clarity and depth, utilizing examples if applicable.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always derive the response solely from the given context.\n",
    "- Ensure terminologies and technical details are accurately explained within the framework of the provided context.\n",
    "                                      \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer the question based on the context provided above. If the context is not sufficient, say \"I don't know\" or \"I don't have enough information to answer this question.\" Do not make up answers or provide information not present in the context.                                      \n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    prompt | llm \n",
    ")\n",
    "\n",
    "response1 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(final_context)})\n",
    "\n",
    "\n",
    "\n",
    "if(\"I don't\" in response1.content):\n",
    "    print(\"Retrying without paper_id...\")\n",
    "    results_without_id = get_context_for_qa_without_id(rewritten_queries, vector_store)\n",
    "    final_context_without_id = [res for res, score in results if res.page_content and len(res.page_content) > 0]\n",
    "    response2 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(final_context_without_id)})\n",
    "    print(\"Response without paper_id: \", response2.content)\n",
    "    print(\"\\n\\n\")\n",
    "else:\n",
    "    print(\"Response with paper_id: \", response1.content)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65da408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

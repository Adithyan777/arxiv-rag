{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7bb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM config\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# IO-NET\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"Qwen/QwQ-32B\",\n",
    "#     base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "#     api_key=getenv(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "# smol = ChatOpenAI(\n",
    "#     model = \"Qwen/QwQ-32B\",\n",
    "#     base_url= \"https://api.intelligence.io.solutions/api/v1\",\n",
    "#     api_key=getenv(\"OPENAI_API_KEY\")\n",
    "# )\n",
    "\n",
    "# # LMSTUDIO\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"qwen3-14b-cvpr-chat-full\",\n",
    "#     base_url = \"http://127.0.0.1:1234/v1\"\n",
    "# )\n",
    "\n",
    "# smol = ChatOpenAI(\n",
    "#     model = \"qqwen3-14b-cvpr-chat-full\",\n",
    "#     base_url = \"http://127.0.0.1:1234/v1\"\n",
    "# )\n",
    "\n",
    "# OPENROUTER\n",
    "smol = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-8b:free\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"qwen/qwen3-8b:free\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = getenv(\"OPENROUTER_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98755b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "collection_name = \"arxiv-cvpr-main\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "   model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"How is visual hallucination still an issue in LVLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a54401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def get_rewritten_queries(question: str, llm) -> List[str]:\n",
    "    \"\"\"Generate multiple versions of the input question using an LLM.\"\"\"\n",
    "    multi_query_template = PromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database of research papers in the field Computer Vision and Pattern Recognition. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide ONLY the alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "    \n",
    "    multi_query_chain = multi_query_template | llm\n",
    "    # TODO: make a custom parser to handle <think> tags in the response\n",
    "    queries = multi_query_chain.invoke({\"question\": question}).content.split('\\n')\n",
    "    \n",
    "    # Clean up queries and add original question\n",
    "    queries = [q.strip() for q in queries if q.strip()]\n",
    "    queries.append(question)\n",
    "    return queries\n",
    "\n",
    "def get_top_paper_id(queries: List[str], vector_store) -> str:\n",
    "    \"\"\"Get the most frequent paper ID from multiple queries.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        results = vector_store.similarity_search(\n",
    "            query,\n",
    "            k=1,\n",
    "            score_threshold=0.4\n",
    "        )\n",
    "        if results:\n",
    "            all_results.append(results[0].metadata['id'])\n",
    "\n",
    "    print(f\"All results: {all_results}\")\n",
    "    # Return most common paper ID if we have results, else None\n",
    "    if all_results:\n",
    "        return Counter(all_results).most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def get_paper_id_from_search_query(search_query: str, abstracts_vector_store_collection_name) -> str:\n",
    "    \"\"\"Get the paper ID from a search query using query reconstruction and similarity search.\"\"\"\n",
    "\n",
    "    abstract_vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=abstracts_vector_store_collection_name,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    \n",
    "    # print(f\"\\nOriginal Question: {search_query}\")\n",
    "    queries = get_rewritten_queries(search_query, smol)\n",
    "    # print(\"Rewritten queries:\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "    \n",
    "    # Get most frequent paper ID from all queries\n",
    "    predicted_paper_id = get_top_paper_id(queries, abstract_vector_store)\n",
    "    # print(f\"Predicted Paper ID for re-written queries: {predicted_paper_id}\")\n",
    "\n",
    "    return predicted_paper_id, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ad31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What are the underlying causes of visual hallucination in large vision-language models (LVLMs) that persist despite recent advancements?\n",
      "2. How do limitations in training data or model architecture specifically contribute to visual hallucination in LVLMs, and what are the consequences for their performance?\n",
      "3. What strategies are currently being explored to address visual hallucination in LVLMs, and why do these approaches still fall short in practical applications?\n",
      "4. How is visual hallucination still an issue in LVLMs?\n",
      "All results: [2505.01958, 2505.01958, 2505.01958, 2505.01958]\n",
      "\n",
      "Predicted Paper ID: 2505.01958\n"
     ]
    }
   ],
   "source": [
    "paper_id, rewritten_queries = get_paper_id_from_search_query(\n",
    "    search_query,\n",
    "    \"arxiv-abstracts\"\n",
    ")\n",
    "\n",
    "print(f\"\\nPredicted Paper ID: {paper_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2ac4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_context_for_qa(paper_id: str, rewritten_queries: List[str], vector_store, k : int = 3) -> List[models.Record]:\n",
    "    \"\"\"Get context for QA from the vector store based on paper ID and search query.\"\"\"\n",
    "    results = []\n",
    "    for query in rewritten_queries:\n",
    "        # Perform similarity search with filter for the specific paper ID\n",
    "        individual_results = vector_store.similarity_search_with_score(\n",
    "            query, \n",
    "            k=k, \n",
    "            score_threshold=0.4,\n",
    "            filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"metadata.id\",\n",
    "                        match=models.MatchValue(value=str(paper_id))\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        if individual_results:\n",
    "            if results:\n",
    "                for doc, score in individual_results:\n",
    "                    if not any(doc.page_content == existing_doc.page_content for existing_doc, score in results):\n",
    "                        results.append((doc, score))\n",
    "            else:\n",
    "                results = individual_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_context_for_qa_without_id(rewritten_queries: List[str], vector_store, k : int = 3) -> List[models.Record]:\n",
    "    \"\"\"Get context for QA from the vector store based on paper ID and search query.\"\"\"\n",
    "    results = []\n",
    "    for query in rewritten_queries:\n",
    "        # Perform similarity search with filter for the specific paper ID\n",
    "        individual_results = vector_store.similarity_search_with_score(\n",
    "            query, \n",
    "            k=k, \n",
    "            score_threshold=0.4,\n",
    "        )\n",
    "        if individual_results:\n",
    "            if results:\n",
    "                for doc, score in individual_results:\n",
    "                    if not any(doc.page_content == existing_doc.page_content for existing_doc, score in results):\n",
    "                        results.append((doc, score))\n",
    "            else:\n",
    "                results = individual_results\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b14a17c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = get_context_for_qa(paper_id, rewritten_queries, vector_store)\n",
    "final_context = [res for res, score in results if res.page_content and len(res.page_content) > 0]\n",
    "\n",
    "len(final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2ccc2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with paper_id:   class\n",
      "</think>\n",
      "\n",
      "Visual hallucination remains an issue in LVLMs because errors can occur in any of the three key components: the large language model (LLM), the vision backbone, and the projector. The LLM may produce faithful outputs when captions are provided but has limited capacity for perception-based hallucinations without adequate visual input. Errors in the vision backbone—such as misclassifying or missing objects due to reliance on noisy internet captions—can result in incorrect features being extracted from images. Lastly, the projector, responsible for aligning visual and textual spaces, may struggle with low cosine similarity between caption embeddings and projected image features, leading to hallucinations when mapping visual information into language space. These challenges highlight the need for targeted strategies like fine-grained perception-based instruction tuning or contrastive alignment objectives to reduce hallucination in LVLMs.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# TODO: improve prompt to include more persona\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are an expert in CVPR topics and help students to learn by answering questions solely based on the provided context which are taken from research papers in arxiv.\n",
    "\n",
    "Focus on explaining concepts in detail and substantiate answers with relevant context from the given information.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Identify Key Concepts**: Upon receiving a question, pinpoint the core topics within CVPR relevant to the inquiry.\n",
    "2. **Contextual Analysis**: Thoroughly review the provided context to gather accurate and pertinent information specific to the question.\n",
    "3. **Detailed Explanation**: Craft a comprehensive explanation, incorporating key details and any relevant examples that illuminate the concept.\n",
    "4. **Clarification and Depth**: Ensure the response is clear, well-substantiated, and sufficiently detailed to aid student understanding.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Provide a paragraph elaborating the concept or answering the inquiry.\n",
    "- Ensure clarity and depth, utilizing examples if applicable.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always derive the response solely from the given context.\n",
    "- Ensure terminologies and technical details are accurately explained within the framework of the provided context.\n",
    "                                      \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer the question based on the context provided above. If the context is not sufficient, say \"I don't know\" or \"I don't have enough information to answer this question.\" Do not make up answers or provide information not present in the context.                                      \n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    prompt | llm \n",
    ")\n",
    "\n",
    "response1 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(final_context)})\n",
    "\n",
    "\n",
    "\n",
    "if(\"I don't\" in response1.content):\n",
    "    print(\"Retrying without paper_id...\")\n",
    "    results_without_id = get_context_for_qa_without_id(rewritten_queries, vector_store)\n",
    "    final_context_without_id = [res for res, score in results if res.page_content and len(res.page_content) > 0]\n",
    "    response2 = rag_chain.invoke({\"question\": search_query, \"context\": format_docs(final_context_without_id)})\n",
    "    print(\"Response without paper_id: \", response2.content)\n",
    "    print(\"\\n\\n\")\n",
    "else:\n",
    "    print(\"Response with paper_id: \", response1.content)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65da408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

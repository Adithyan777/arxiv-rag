id,title,categories,abstract,doi,created,updated,authors
2505.00312,aware-net: adaptive weighted averaging for robust ensemble network in   deepfake detection,cs.cv,"deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. while multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. in response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: xception, res2net101, and efficientnet-b7. our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. our experiments achieved state-of-the-art intra-dataset performance with auc scores of 99.22% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.06% (ff++) and 99.94% (celebdf-v2) without augmentation. with augmentation, we achieve auc scores of 99.47% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.43% (ff++) and 99.95% (celebdf-v2). the framework demonstrates robust cross-dataset generalization, achieving auc scores of 88.20% and 72.52%, and f1 scores of 93.16% and 80.62% in cross-dataset evaluations.",10.1049/icp.2025.1162,2025-05-01,,"['muhammad salman', 'iqra tariq', 'mishal zulfiqar', 'muqadas jalal', 'sami aujla', 'sumbal fatima']"
2505.00337,t2vphysbench: a first-principles benchmark for physical consistency in   text-to-video generation,cs.lg cs.ai cs.cl cs.cv,"text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. to fill this gap, we introduce \textbf{t2vphysbench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including newtonian mechanics, conservation principles, and phenomenological effects. our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. the results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.",,2025-05-01,,"['xuyang guo', 'jiayan huo', 'zhenmei shi', 'zhao song', 'jiahao zhang', 'jiale zhao']"
2505.00426,leveraging pretrained diffusion models for zero-shot part assembly,cs.cv,"3d part assembly aims to understand part relationships and predict their 6-dof poses to construct realistic 3d shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. however, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. in this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an iterative closest point (icp) process. then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. to verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. the code has been released on https://github.com/ruiyuan-zhang/zero-shot-assembly.",,2025-05-01,,"['ruiyuan zhang', 'qi wang', 'jiaxiang liu', 'yu zhang', 'yuchi huo', 'chao wu']"
2505.00452,clearlines - camera calibration from straight lines,cs.cv,"the problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. however, its practical applicability remains limited, particularly in real-world outdoor scenarios. these environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3d lines, and varying lighting conditions, making the task notoriously difficult. furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. in this study, we present a small dataset named ""clearlines"", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3d line detection algorithms.",,2025-05-01,,"['gregory schroeder', 'mohamed sabry', 'cristina olaverri-monreal']"
2505.00462,"corstitch - a free, open source software for stitching and   georeferencing underwater coral reef videos",eess.iv cs.cv,"corstitch is an open-source software developed to automate the creation of accurate georeferenced reef mosaics from video transects obtained through automated rapid reef assessment system surveys. we utilized a fourier-based image correlation algorithm to stitch sequential video frames, aligning them with synchronized gnss timestamps. the resulting compressed keyhole markup language files, compatible with geographic information systems such as google earth, enable detailed spatial analysis. validation through comparative analysis of mosaics from two temporally distinct surveys of the same reef demonstrated the software's consistent and reliable performance.",,2025-05-01,,"['julian christopher l. maypa', 'johnenn r. manalang', 'maricor n. soriano']"
2505.00482,jointdit: enhancing rgb-depth joint modeling with diffusion transformers,cs.cv cs.ai,"we present jointdit, a diffusion transformer that models the joint distribution of rgb and depth. by leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, jointdit not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. this solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. with these techniques, we train our model across all noise levels for each modality, enabling jointdit to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. jointdit demonstrates outstanding joint generation performance. furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. the project page is available at https://byungki-k.github.io/jointdit/.",,2025-05-01,,"['kwon byung-ki', 'qi dai', 'lee hyoseok', 'chong luo', 'tae-hyun oh']"
2505.00497,keysync: a robust approach for leakage-free lip synchronization in high   resolution,cs.cv,"lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. however, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. to address these shortcomings, we present keysync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. we show that keysync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to lipleak, our novel leakage metric. furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. code and model weights can be found at https://antonibigata.github.io/keysync.",,2025-05-01,,"['antoni bigata', 'rodrigo mira', 'stella bounareli', 'michał stypułkowski', 'konstantinos vougioukas', 'stavros petridis', 'maja pantic']"
2505.00502,towards scalable human-aligned benchmark for text-guided image editing,cs.cv,"a variety of text-guided image editing models have been proposed recently. however, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. to address this, we introduce a novel human-aligned benchmark for text-guided image editing (hatie). providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. also, hatie provides a fully-automated and omnidirectional evaluation pipeline. particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. we empirically verify that the evaluation of hatie is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.",,2025-05-01,,"['suho ryu', 'kihyun kim', 'eugene baek', 'dongsoo shin', 'joonseok lee']"
2505.00584,synthesizing and identifying noise levels in autonomous vehicle camera   radar datasets,cs.cv cs.ai eess.iv eess.sp,"detecting and tracking objects is a crucial component of any autonomous navigation method. for the past decades, object detection has yielded promising results using neural networks on various datasets. while many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. in this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar autonomous vehicle (av) datasets. our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. we also present our results of a baseline lightweight noise recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.",,2025-05-01,,"['mathis morales', 'golnaz habibi']"
2505.00681,minerva: evaluating complex video reasoning,cs.lg cs.cv,"multimodal llms are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. this makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. to remedy this, we provide a new video reasoning dataset called minerva for modern multimodal models. each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. we perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. we use this to explore both human and llm-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. the dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.",,2025-05-01,,"['arsha nagrani', 'sachit menon', 'ahmet iscen', 'shyamal buch', 'ramin mehran', 'nilpa jha', 'anja hauth', 'yukun zhu', 'carl vondrick', 'mikhail sirotenko', 'cordelia schmid', 'tobias weyand']"
2505.00684,visual test-time scaling for gui agent grounding,cs.cv cs.ai cs.lg,"we introduce regionfocus, a visual test-time scaling approach for vision language model agents. understanding webpages is challenging due to the visual complexity of gui images and the large number of interface elements, making accurate action selection difficult. our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. to support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. even with a simple region selection strategy, we observe significant performance gains of 28+\% on screenspot-pro and 24+\% on webvoyager benchmarks on top of two state-of-the-art open vision language model agents, ui-tars and qwen2.5-vl, highlighting the effectiveness of visual test-time scaling in interactive settings. we achieve a new state-of-the-art grounding performance of 61.6\% on the screenspot-pro benchmark by applying regionfocus to a qwen2.5-vl-72b model. our code will be released publicly at https://github.com/tiangeluo/regionfocus.",,2025-05-01,,"['tiange luo', 'lajanugen logeswaran', 'justin johnson', 'honglak lee']"
2505.00703,t2i-r1: reinforcing image generation with collaborative semantic-level   and token-level cot,cs.cv cs.ai cs.cl cs.lg,"recent advancements in large language models have demonstrated how chain-of-thought (cot) and reinforcement learning (rl) can improve performance. however, applying such reasoning strategies to the visual generation domain remains largely unexplored. in this paper, we present t2i-r1, a novel reasoning-enhanced text-to-image generation model, powered by rl with a bi-level cot reasoning process. specifically, we identify two levels of cot that can be utilized to enhance different stages of generation: (1) the semantic-level cot for high-level planning of the prompt and (2) the token-level cot for low-level pixel processing during patch-by-patch generation. to better coordinate these two levels of cot, we introduce bicot-grpo with an ensemble of generation rewards, which seamlessly optimizes both generation cots within the same training step. by applying our reasoning strategies to the baseline model, janus-pro, we achieve superior performance with 13% improvement on t2i-compbench and 19% improvement on the wise benchmark, even surpassing the state-of-the-art model flux.1. code is available at: https://github.com/caraj7/t2i-r1",,2025-05-01,,"['dongzhi jiang', 'ziyu guo', 'renrui zhang', 'zhuofan zong', 'hao li', 'le zhuo', 'shilin yan', 'pheng-ann heng', 'hongsheng li']"
2505.00704,controllable weather synthesis and removal with video diffusion models,cs.gr cs.cv,"generating realistic and controllable weather effects in videos is valuable for many applications. physics-based weather simulation requires precise reconstructions that are hard to scale to in-the-wild videos, while current video editing often lacks realism and control. in this work, we introduce weatherweaver, a video diffusion model that synthesizes diverse weather effects -- including rain, snow, fog, and clouds -- directly into any input video without the need for 3d modeling. our model provides precise control over weather effect intensity and supports blending various weather types, ensuring both realism and adaptability. to overcome the scarcity of paired training data, we propose a novel data strategy combining synthetic videos, generative image editing, and auto-labeled real-world videos. extensive evaluations show that our method outperforms state-of-the-art methods in weather simulation and removal, providing high-quality, physically plausible, and scene-identity-preserving results over various real-world videos.",,2025-05-01,,"['chih-hao lin', 'zian wang', 'ruofan liang', 'yuxuan zhang', 'sanja fidler', 'shenlong wang', 'zan gojcic']"
2505.00935,autonomous embodied agents: when robotics meets deep learning reasoning,cs.ro cs.ai cs.cv,"the increase in available computing power and the deep learning revolution have allowed the exploration of new topics and frontiers in artificial intelligence research. a new field called embodied artificial intelligence, which places at the intersection of computer vision, robotics, and decision making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. the recent availability of large collections of 3d models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. these intelligent agents are intended to perform a certain task in a possibly unknown environment. to this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. this dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. we aim to contribute to research in embodied ai and autonomous agents, in order to foster future work in this field. we present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.",,2025-05-01,,['roberto bigazzi']
2505.00938,cdformer: cross-domain few-shot object detection transformer against   feature confusion,cs.cv cs.ai,"cross-domain few-shot object detection (cd-fsod) aims to detect novel objects across different domains with limited class instances. feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. in this work, we introduce cdformer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. the method specifically tackles feature confusion through two key modules: object-background distinguishing (obd) and object-object distinguishing (ood). the obd module leverages a learnable background token to differentiate between objects and background, while the ood module enhances the distinction between objects of different classes. experimental results demonstrate that cdformer outperforms previous state-of-the-art approaches, achieving 12.9% map, 11.0% map, and 10.4% map improvements under the 1/5/10 shot settings, respectively, when fine-tuned.",,2025-05-01,,"['boyuan meng', 'xiaohan zhang', 'peilin li', 'zhe wu', 'yiming li', 'wenkai zhao', 'beinan yu', 'hui-liang shen']"
2505.00986,on-demand test-time adaptation for edge devices,cs.lg cs.cv,"continual test-time adaptation (ctta) continuously adapts the deployed model on every incoming batch of data. while achieving optimal accuracy, existing ctta approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. in this work, we first introduce a novel paradigm -- on-demand tta -- which triggers adaptation only when a significant domain shift is detected. then, we present od-tta, an on-demand tta framework for accurate and efficient adaptation on edge devices. od-tta comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate tta only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled batch normalization (bn) update scheme to enable memory-efficient adaptation with small batch sizes. extensive experiments show that od-tta achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making tta a practical reality.",,2025-05-02,,"['xiao ma', 'young d. kwon', 'dong ma']"
2505.00998,deterministic-to-stochastic diverse latent feature mapping for human   motion synthesis,cs.cv,"human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. recent score-based generative models (sgms) have demonstrated impressive results on this task. however, their training process involves complex curvature trajectories, leading to unstable training process. in this paper, we propose a deterministic-to-stochastic diverse latent feature mapping (dsdfm) method for human motion synthesis. dsdfm consists of two stages. the first human motion reconstruction stage aims to learn the latent space distribution of human motions. the second diverse motion generation stage aims to build connections between the gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. this stage is achieved by the designed deterministic feature mapping procedure with derode and stochastic diverse output generation procedure with divsde.dsdfm is easy to train compared to previous sgms-based methods and can enhance diversity without introducing additional training parameters.through qualitative and quantitative experiments, dsdfm achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.",,2025-05-02,,"['yu hua', 'weiming liu', 'gui xu', 'yaqing hou', 'yew-soon ong', 'qiang zhang']"
2505.01007,towards the resistance of neural network watermarking to fine-tuning,cs.lg cs.ai cs.cl cs.cv,"this paper proves a new watermarking method to embed the ownership information into a deep neural network (dnn), which is robust to fine-tuning. specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised fourier transform to extract frequency components from the convolutional filter. additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. in this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. preliminary experiments demonstrate the effectiveness of our method.",,2025-05-02,,"['ling tang', 'yuefeng chen', 'hui xue', 'quanshi zhang']"
2505.01016,fine-tuning without forgetting: adaptation of yolov8 preserves coco   performance,cs.cv cs.ai,"the success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. while fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. the critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. we adapt a standard yolov8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original coco validation set. our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\% absolute map50) on the fine-grained fruit task compared to only training the head. strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\% absolute map difference) on the coco benchmark across all tested freeze levels. we conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.",,2025-05-02,,"['vishal gandhi', 'sagar gandhi']"
2505.01263,flowdubber: movie dubbing with llm-based semantic-aware learning and   flow matching based voice enhancing,cs.mm cs.cv cs.sd eess.as,"movie dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. to address these issues, we propose a large language model (llm) based flow matching architecture for dubbing, named flowdubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. first, we introduce qwen2.5 as the backbone of llm to learn the in-context sequence from movie scripts and reference audio. then, the proposed semantic-aware learning focuses on capturing llm semantic knowledge at the phoneme level. next, dual contrastive aligning (dca) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. finally, the proposed flow-based voice enhancing (fve) improves acoustic quality in two aspects, which introduces an llm-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. the demos are available at {\href{https://galaxycong.github.io/llm-flow-dubber/}{\textcolor{red}{https://galaxycong.github.io/llm-flow-dubber/}}}.",,2025-05-02,,"['gaoxiang cong', 'liang li', 'jiadong pan', 'zhedong zhang', 'amin beheshti', 'anton van den hengel', 'yuankai qi', 'qingming huang']"
2505.01313,a neural architecture search method using auxiliary evaluation metric   based on resnet architecture,cs.ne cs.cv,"this paper proposes a neural architecture search space using resnet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. in addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. the experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the mnist, fashion-mnist and cifar100 datasets.",,2025-05-02,,"['shang wang', 'huanrong tang', 'jianquan ouyang']"
2505.01406,vidstamp: a temporally-aware watermark for ownership and integrity in   video diffusion models,cs.cv cs.cr cs.lg,"the rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. in this work, we introduce vidstamp, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. by fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, vidstamp learns to embed high-capacity, flexible watermarks with minimal perceptual impact. leveraging architectural components such as 3d convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. vidstamp embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log p-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. code: code: \url{https://github.com/spin-umass/vidstamp}",,2025-05-02,,"['mohammadreza teymoorianfard', 'shiqing ma', 'amir houmansadr']"
2505.01583,tempura: temporal event masked prediction and understanding for   reasoning in action,cs.cv cs.ai,"understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. we propose tempura (temporal event masked prediction and understanding for reasoning in action), a two-stage training framework that enhances video temporal understanding. tempura first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. tempura then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. we train tempura on ver, a large-scale dataset curated by us that comprises 1m training instances and 500k videos with temporally aligned event descriptions and structured reasoning steps. experiments on temporal grounding and highlight detection benchmarks demonstrate that tempura outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.",,2025-05-02,,"['jen-hao cheng', 'vivian wang', 'huayu wang', 'huapeng zhou', 'yi-hao peng', 'hou-i liu', 'hsiang-wei huang', 'kuang-ming chen', 'cheng-yen yang', 'wenhao chai', 'yi-ling chen', 'vibhav vineet', 'qin cai', 'jenq-neng hwang']"
2505.01657,ragar: retrieval augment personalized image generation guided by   recommendation,cs.ir cs.cv,"personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. although effective, existing methods face two main issues. first, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. to address these issues, we propose retrieval augment personalized image generation guided by recommendation (ragar). our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. extensive experiments and human evaluations on three real-world datasets demonstrate that ragar achieves significant improvements in both personalization and semantic metrics compared to five baselines.",,2025-05-02,,"['run ling', 'wenji wang', 'yuting liu', 'guibing guo', 'linying jiang', 'xingwei wang']"
2505.01713,vision and intention boost large language model in long-term action   anticipation,cs.cv,"long-term action anticipation (lta) aims to predict future actions over an extended period. previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. recent researches leverage large language models (llms) by utilizing text-based inputs which suffer severe information loss. to tackle these limitations single-modality methods face, we propose a novel intention-conditioned vision-language (icvl) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of llms. considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (vlm) to infer behavioral intentions as comprehensive textual features directly from video inputs. the inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. these enhanced visual representations, along with textual prompts, are fed into llm for future action anticipation. furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. extensive experiments with state-of-the-art performance on ego4d, epic-kitchens-55, and egtea gaze+ datasets fully demonstrate the effectiveness and superiority of the proposed method.",,2025-05-03,,"['congqi cao', 'lanshu hu', 'yating yu', 'yanning zhang']"
2505.01743,an llm-empowered low-resolution vision system for on-device human   behavior understanding,cs.cv cs.ai cs.lg,"the rapid advancements in large vision language models (lvlms) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (hbu) in low-resolution vision systems, such as depth, thermal, and infrared. however, existing large vision language model (lvlm) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as rgb images. a quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. in this paper, we propose a novel, labor-saving system, llambda, designed to support low-resolution hbu. the core idea is to leverage limited labeled data and a large amount of unlabeled data to guide llms in generating informative captions, which can be combined with raw data to effectively fine-tune lvlm models for understanding low-resolution videos in hbu. first, we propose a contrastive-oriented data labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. second, we propose a physical-knowledge guided captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. therefore, it can improve llms' understanding of sequential data and then generate high-quality video captions. finally, to ensure on-device deployability, we employ lora-based efficient fine-tuning to adapt lvlms for low-resolution data. we evaluate llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that llambda outperforms several state-of-the-art lvlm systems up to $40.03\%$ on average bert-score.",,2025-05-03,,"['siyang jiang', 'bufang yang', 'lilin xu', 'mu yuan', 'yeerzhati abudunuer', 'kaiwei liu', 'liekang zeng', 'hongkai chen', 'zhenyu yan', 'xiaofan jiang', 'guoliang xing']"
2505.01958,a comprehensive analysis for visual object hallucination in large   vision-language models,cs.cv cs.cl,"large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.",,2025-05-03,,"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']"
2505.01996,always skip attention,cs.lg cs.cv,"we highlight a curious empirical result within modern vision transformers (vits). specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. this is in contrast to other elements of a vit that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\eg, cnns) exhibiting good performance in their absence. in this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. additionally, we propose token graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. we validate our approach in both supervised and self-supervised training methods.",,2025-05-04,,"['yiping ji', 'hemanth saratchandran', 'peyman moghaddam', 'simon lucey']"
2505.02005,learning heterogeneous mixture of scene experts for large-scale neural   radiance fields,cs.cv,"recent nerf methods on large-scale scenes have underlined the importance of scene decomposition for scalable nerfs. although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. in this paper, we introduce switch-nerf++, a heterogeneous mixture of hash experts (hmohe) network that addresses these challenges within a unified framework. it is a highly scalable nerf that learns heterogeneous decomposition and heterogeneous nerfs efficiently for large-scale scenes in an end-to-end manner. in our framework, a gating network learns to decomposes scenes and allocates 3d points to specialized nerf experts. this gating network is co-optimized with the experts, by our proposed sparsely gated mixture of experts (moe) nerf framework. we incorporate a hash-based gating network and distinct heterogeneous hash experts. the hash-based gating efficiently learns the decomposition of the large-scale scene. the distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. these design choices make our framework an end-to-end and highly scalable nerf solution for real-world large-scale scene modeling to achieve both quality and efficiency. we evaluate our accuracy and scalability on existing large-scale nerf datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from urbanbis. extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to switch-nerf. codes will be released in https://github.com/mizhenxing/switch-nerf.",,2025-05-04,,"['zhenxing mi', 'ping yin', 'xue xiao', 'dan xu']"
2505.02018,r-bench: graduate-level multi-disciplinary benchmarks for llm & mllm   complex reasoning evaluation,cs.cv,"reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. in this paper, we introduce a graduate-level, multi-disciplinary, englishchinese benchmark, dubbed as reasoning bench (r-bench), for assessing the reasoning capability of both language and multimodal models. rbench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both english and chinese. these questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an olympiad-level multi-disciplinary benchmark. we evaluate widely used models, including openai o1, gpt-4o, deepseek-r1, etc. experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. even the top-performing model openai o1 achieves only 53.2% accuracy on our multimodal evaluation. data and code are made publicly available at here.",,2025-05-04,,"['meng-hao guo', 'jiajun xu', 'yi zhang', 'jiaxi song', 'haoyang peng', 'yi-xuan deng', 'xinzhi dong', 'kiyohiro nakayama', 'zhengyang geng', 'chen wang', 'bolin ni', 'guo-wei yang', 'yongming rao', 'houwen peng', 'han hu', 'gordon wetzstein', 'shi-min hu']"
2505.02048,regression is all you need for medical image translation,eess.iv cs.ai cs.cv,"the acquisition of information-rich images within a limited time budget is crucial in medical imaging. medical image translation (mit) can help enhance and supplement existing datasets by generating synthetic images from acquired data. while generative adversarial nets (gans) and diffusion models (dms) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. in fact, the imitation of acquisition noise or content hallucination hinder clinical utility. here, we introduce yoda (you only denoise once - or average), a novel 2.5d diffusion-based framework for volumetric mit. yoda unites diffusion and regression paradigms to produce realistic or noise-free outputs. furthermore, we propose expectation-approximation (expa) dm sampling, which draws inspiration from mri signal averaging. expa-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain mri and pelvic mri-ct - we show that diffusion and regression sampling yield similar results in practice. as such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. building on these insights, we demonstrate that yoda outperforms several state-of-the-art gan and dm methods. notably, yoda-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. our findings challenge the presumed advantages of dms in mit and pave the way for the practical application of mit in medical imaging.",,2025-05-04,2025-05-06,"['sebastian rassmann', 'david kügler', 'christian ewert', 'martin reuter']"
2505.02134,hillie: human-in-the-loop training for low-light image enhancement,cs.cv,"developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (llie). in this paper, we propose a human-in-the-loop llie training framework that improves the visual quality of unsupervised llie model outputs through iterative training stages, named hillie. at each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. subsequently, we employ a tailored image quality assessment (iqa) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. with only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the iqa model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing llie results. extensive experiments demonstrate that our approach significantly improves unsupervised llie model performance in terms of both quantitative and qualitative performance. the code and collected ranking dataset will be available at https://github.com/labshuhanggu/hillie.",,2025-05-04,,"['xiaorui zhao', 'xinyue zhou', 'peibei cao', 'junyu lou', 'shuhang gu']"

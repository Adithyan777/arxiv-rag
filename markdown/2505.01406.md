# Abstract

The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches—whether passive, post-hoc, or adapted from image-based techniques—often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline—first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency—VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs.

# Introduction

The rapid advancement of AI-generated content—particularly video—has introduced unprecedented challenges to digital media integrity, security, and trust. Recent generative models are capable of synthesizing highly realistic video content from static images or natural language prompts, raising concerns about their potential misuse in misinformation, impersonation, and tampering. The growing accessibility of such tools poses a significant threat to content authenticity and highlights the urgent need for robust mechanisms that can ensure generative model accountability, enable provenance tracking, and support tamper detection. In this context, watermarking of AI-generated media has emerged as a promising strategy for content authentication and tamper evidence.

Generative diffusion models, especially latent diffusion models (LDMs), have revolutionized high-fidelity content synthesis by learning to reverse a noise process in a compressed latent space. While diffusion-based text-to-image models like Stable Diffusion have already demonstrated remarkable realism and scalability, recent efforts have extended this framework to the video domain by incorporating temporally-aware modules such as 3D convolutions and temporal attention into the decoder. These advancements enable the generation of temporally coherent videos that are nearly indistinguishable from real-world footage. However, the growing power and accessibility of such models also call for integrated watermarking solutions that can operate within the generation pipeline itself—ensuring authenticity without introducing post-processing steps or perceptual degradation.

Existing approaches to watermarking generative content either operate in a passive forensic manner or add external watermarks post-generation. Passive detectors—which attempt to identify artifacts or inconsistencies in generated content—are increasingly ineffective against modern diffusion models as they produce highly realistic outputs with minimal statistical traces. Meanwhile, post-hoc watermarking, where a signal is embedded into media after generation, is often brittle and vulnerable to simple removal techniques, particularly when the watermarking algorithm is publicly known. Recent work in the image domain demonstrates that training the generative model itself to embed an imperceptible signature during content synthesis offers a promising direction for watermarking. These methods embed the watermark directly into the latent decoder, enabling resilient and cost-free watermarking during generation. However, extending such techniques to the video domain introduces new challenges—naively applying image-based watermarking frame-by-frame fails to capture temporal dependencies and cannot protect against video-specific attacks like frame dropping, swapping, or insertion.

In this paper, we introduce VIDSTAMP, a novel framework for robust and efficient watermarking in latent video diffusion models. An overview of our proposed watermarking framework, VIDSTAMP, is shown in Figure 1.

**Figure 1:** VIDSTAMP framework overview. Our method embeds watermark messages directly into the latent space of a video diffusion model during generation. Each frame or segment is assigned a message that is recoverable via a pretrained extractor. This enables forensic verification, tamper detection, and source attribution without any post-processing.

Our method leverages the temporally-aware architecture of modern video diffusion decoders to embed a sequence of hidden messages—either per-frame or per-segment—directly into the generated video. We adopt a two-stage fine-tuning strategy: first, the decoder is fine-tuned on a curated image dataset (e.g., COCO) to learn the initial watermarking behavior; then, it is fine-tuned again on videos generated by the same model, to embed temporally consistent and uniquely traceable watermarks. This approach introduces no additional computational cost during video generation, allows flexible capacity control (each frame or group of frames may carry distinct messages), and supports practical use cases such as long-form video authentication and temporal tamper localization.

To evaluate the practicality and effectiveness of our watermarking method, we implemented VIDSTAMP on top of the Stable Video Diffusion (SVD) framework using a two-stage decoder fine-tuning process. Our model embeds 768 bits per video (48 bits per frame) with an average bit accuracy of 95.0%, while preserving video quality nearly identical to unwatermarked outputs (average perceptual score 0.836 vs. 0.838). Compared to state-of-the-art baselines, VIDSTAMP delivers a significantly stronger trade-off between capacity, quality, and detectability; it outperforms RivaGAN, VideoSeal, and VideoShield in log P-value, a statistical metric that measures how unlikely it is for the extracted watermark to be correct by chance alone. VIDSTAMP achieves a log P-value of -166.65, which indicates high statistical confidence in watermark presence even under distortion. Furthermore, unlike post-hoc watermarking methods, VIDSTAMP introduces no additional inference overhead, and uniquely supports frame-level tamper localization, attaining over 95% localization accuracy across various manipulation types including frame insertion, deletion, and reordering.

In summary, this paper makes the following contributions:

- A temporally-aware watermarking framework for video diffusion models. We present VIDSTAMP, a method that fine-tunes the decoder of a latent video diffusion model to embed per-frame or segment-level watermarks directly during generation. While our method does not modify the decoder architecture, it leverages the inherent temporally-aware components (e.g., 3D convolutions and temporal attention) already present in video diffusion decoders to enable tamper localization and ownership verification with no additional inference-time overhead. Additionally, segment-wise embedding offers flexible control over watermark capacity.

- A two-stage decoder fine-tuning pipeline. Our approach first fine-tunes the decoder on static images to promote spatial watermark separability, then adapts it to video synthesis to ensure temporal consistency. This training strategy enables high-capacity watermark embedding (768 bits per video) with strong robustness under distortion.

- State-of-the-art performance in quality, robustness, and tamper localization. VIDSTAMP achieves comparable or superior performance to prior baselines (RivaGAN, VideoSeal, VideoShield), while embedding significantly more bits and maintaining high video quality. It also supports frame-level tamper localization with over 95% accuracy under various manipulation types.

# II. Preliminaries

## Video Diffusion Models and Temporality

Recent advancements in generative modeling have extended diffusion models to video synthesis, enabling temporally coherent outputs. Ho et al. introduced video diffusion models by extending the standard image diffusion architecture to the spatio-temporal domain, training a 3D U-Net denoiser on both image and video data to improve fidelity and stability. To generate longer or higher-resolution videos, they proposed hierarchical sampling techniques, achieving promising results in text-conditional video generation. Subsequent works have focused on latent video diffusion to enhance efficiency and scalability. He et al. proposed generating videos in a low-dimensional 3D latent space, significantly reducing computation compared to pixel-space diffusion. Their Latent Video Diffusion Model employs a hierarchical approach that can produce videos with thousands of frames, using conditional latent perturbation and guidance to mitigate error accumulation over long durations. Similarly, Blattmann et al. transformed a pre-trained image diffusion model (Stable Diffusion) into a video generator by adding temporal layers to the latent diffusion model. By fine-tuning the decoder with 3D convolutions and temporal attention, they achieved high-resolution text-to-video synthesis while maintaining consistency across frames.

## Watermarking in Generative Models

Watermarking techniques for generative models have evolved across modalities, with early work in text embedding messages through lexical substitution, syntactic manipulation, or stylometry. More recently, decoding-based approaches skew token probabilities during generation, leaving imperceptible yet detectable traces, while neural methods encode bit strings through learned paraphrasing. Sentence-level approaches further leverage semantic similarity patterns to embed watermarks without modifying generation logits, improving robustness to paraphrasing attacks. In the image domain, early learning-based watermarking systems trained encoder-decoder CNNs to embed robust messages directly into images while resisting typical perturbations like cropping or JPEG compression. A major shift came with model-integrated watermarking in generative diffusion models—particularly the Stable Signature, which fine-tunes a latent diffusion model's decoder to embed persistent identifiers within the generation process itself. These in-model watermarks offer clear advantages: zero inference overhead, imperceptibility, and strong robustness even under heavy transformation. Follow-up methods further improve resilience by seeding watermark signals in the input noise, ensuring they propagate naturally through the diffusion process. Overall, integrated watermarking provides stronger tamper resistance and detection reliability than post-hoc methods, which, while easier to deploy, are more vulnerable to removal or degradation. Extending these ideas to video generation introduces new challenges—particularly maintaining temporal consistency—an issue our work addresses by leveraging temporally-aware modules in latent video diffusion decoders.

## Passive vs. Active Watermarking

Detection techniques for AI-generated content typically fall into two categories: passive forensics and active watermarking. Passive methods attempt to detect statistical or visual artifacts left by generative models. Early efforts focused on GANs, identifying frequency domain inconsistencies or unnatural textures. However, diffusion models have largely eliminated such artifacts, rendering many forensic detectors ineffective against modern image and video generation. In response, active watermarking has emerged as a more reliable strategy, embedding identifiers directly during generation. Post-hoc approaches apply a watermark after synthesis—such as through DCT, spatial overlays, or neural encoding—but they are often brittle and easily removed if the embedding strategy is known. For example, simple removal attacks can strip watermarks without visible degradation, while classical reviews highlight the vulnerability of post-generation schemes to estimation-based removal. In contrast, model-integrated methods embed signals during generation, making them more robust to adversarial tampering. Some recent works even propose theoretically undetectable watermarks by leveraging structured noise or optimal transport. As diffusion models continue to improve, these integrated methods offer stronger guarantees of persistence and provenance than passive or post-hoc approaches.

## Video Watermarking Techniques

Classical video watermarking has been studied for decades, yielding a spectrum of spatial-domain and transform-domain techniques. Spatial methods directly embed payload bits by modifying pixel intensities or colors in each frame, achieving high embedding capacity and simplicity at the cost of fragility against processing and compression. In contrast, transform-domain schemes hide information in frequency coefficients, leveraging the human visual system to preserve perceptual quality while improving robustness to compression and filtering. Many early approaches combine these strategies with error-correction coding and redundancy to resist desynchronization attacks; for instance, repeating or interleaving the watermark across frames can guard against frame dropping or temporal cropping, and applying cyclic error-correcting codes helps the decoder recover from bit errors introduced by noisy channels or re-encoding. Such designs allowed watermarks to survive common operations like recompression, scaling, and transcoding.

More recent learning-based methods employ deep neural networks to automatically optimize the imperceptible embedding of watermarks. These approaches typically train an encoder-decoder convolutional network to hide and extract a bit-string, often inserting a differentiable distortion layer simulating compression, scaling, or frame loss during training to enhance robustness. For example, some models train a multiscale video watermarking model that spreads the payload across spatial and temporal dimensions and employs adversarial discriminators to enforce invisibility and resilience. Such deep models significantly improve robustness against complex distortions while maintaining visual quality, although ensuring temporal consistency when applied frame-by-frame remains challenging.

The rise of GAN and diffusion-based video generators has spurred watermarking techniques tailored for AI-generated content. RivaGAN is an early deep watermarking architecture for video that introduces an adversarial training setup with an attacker network attempting to erase the watermark and a critic ensuring visual fidelity. Through an attention-based encoder-decoder, RivaGAN identifies perceptually significant regions to embed a robust invisible mark, achieving strong baseline robustness to compression and scaling attacks. However, as a post-processing method, it operates outside the generative model and does not explicitly address cross-frame coherence in long AI-generated videos.

More recent frameworks have improved on this. VideoSeal combines a trained neural embedder and extractor with a multi-stage training regimen: an initial image-domain pre-training, followed by video-domain fine-tuning with simulated video codecs and geometric transformations applied between the encoder and decoder to harden the watermark. VideoSeal also introduces temporal watermark propagation, wherein the watermark is carried over across frames without needing to embed it independently in each frame, thereby enhancing efficiency and temporal consistency. This yields state-of-the-art robustness under mixed distortions (e.g., recompression plus cropping) for high-definition videos. Nonetheless, VideoSeal remains an external module applied after video generation.

In parallel, VideoShield integrates watermarking into the diffusion sampling process for text-to-video models without requiring model retraining. VideoShield maps the hidden message bits to a set of template noise perturbations that are injected at each denoising step, so that the generated frames inherently contain a recoverable watermark; a reverse diffusion (e.g., DDIM inversion) is used to extract the watermark from the video, and the method can pinpoint tampered frames by checking consistency of the template bits across time. This approach achieves robust extraction and even enables spatial-temporal tamper localization in diffusion-generated videos, all with negligible impact on perceptual quality. While VideoShield offers a lightweight and inference-time-compatible solution, it relies on external perturbations and inversion steps.

In contrast, our proposed VIDSTAMP method embeds watermarks directly into the latent decoding process of the generative model. This tight integration ensures that the watermark is temporally coherent across frames and intrinsically bound to the content, offering greater flexibility in bit placement and message structure, higher capacity, and consistent robustness—without the need for post-hoc perturbation or inversion. By leveraging the model's existing temporal modules, VIDSTAMP maintains high visual fidelity while enabling reliable ownership verification and frame-level tamper detection.

# III. Methodology

## Overview

Inspired by the Stable Signature approach for image diffusion models—which showed that fine-tuning the generative model's decoder can embed a persistent invisible watermark into all outputs—we extend this idea to video generation. Figure 1 illustrates the overall architecture of our system. The key insight is that modern latent video diffusion models employ temporally-aware decoders (e.g., 3D convolutions and temporal attention layers), enabling the generative process itself to carry a watermark across time. By leveraging these temporal layers, we embed watermarks during generation, allowing each frame to carry a unique identifier and enabling distinct per-frame message decoding. This frame-specific encoding permits precise verification per frame, which facilitates localization of any temporal tampering in the video. For watermark extraction, we utilize a pre-trained decoder network from the HiDDeN deep watermarking framework to recover the embedded message from each generated frame.

## Two Stage Finetuning

Our training framework, illustrated in Figure 2, begins with a sequence of frames as the input data, which are passed through a variational autoencoder (VAE) to obtain latent representations. These latents are decoded frame-by-frame using a temporally-aware decoder—equipped with 3D convolutions and temporal attention—to generate video frames containing imperceptible embedded messages. Each frame is assigned a fixed message bit-string, and a pretrained message extractor—adapted from the HiDDeN architecture—is used to recover the message from each generated frame.

**Figure 2:** VIDSTAMP training pipeline. During training, a fixed set of messages is embedded into video frames through the decoder of a latent video diffusion model. The decoder is fine-tuned to maximize both message accuracy and perceptual quality, using a pretrained extractor for supervision. Message loss and perceptual loss are computed per frame to guide robust and imperceptible watermark learning.

The extraction is supervised using a binary cross-entropy (BCE) loss:

L_msg = (1/N) ∑ [y_i log(ŷ_i) + (1 - y_i) log(1 - ŷ_i)]

where y_i and ŷ_i denote the ground truth and predicted bits, respectively.

In parallel, we compute a Watson-VGG perceptual loss on a frame-by-frame basis to maintain the semantic integrity and visual quality of the generated outputs. This perceptual loss, denoted L_perc, compares deep feature activations between the original and reconstructed frames, emphasizing human-perceptible discrepancies.

The final training objective is a weighted sum of the message loss and perceptual loss:

L_total = α L_msg + β L_perc

where α and β are tunable hyperparameters that control the trade-off between bit accuracy and visual quality. During training, only the decoder is updated, while the encoder and message extractor remain frozen. This setup enables the decoder to learn effective message embedding strategies without compromising on generation fidelity.

Our training pipeline adopts a two-stage fine-tuning approach to equip the video diffusion model's decoder with the ability to embed robust, temporally-aware watermarks without degrading generation quality.

- In the first stage, we fine-tune the decoder using COCO image datasets. We treat a batch of independent images as a pseudo-video, where each image serves as a distinct frame. This strategy allows the decoder to perceive each frame as an independent unit, promoting diversity in learned representations across positions. Consequently, the model learns to associate distinct spatial features with different message embeddings, facilitating per-frame message separation. This stage is essential for initializing the decoder's ability to distinguish and encode frame-specific watermarks.

- In the second stage, we fine-tune the decoder using synthesized videos generated from the same diffusion model. This phase reinforces temporal consistency while preserving the model's ability to embed diverse frame-level messages. It also adapts the decoder to the statistical distribution of video data, improving fidelity and motion coherence in generated outputs. The dual-phase training balances frame-level watermark capacity with video-level coherence, enabling the model to embed traceable, tamper-localizable watermarks in temporally coherent output videos.

## Temporal Tamper Localization

To detect and localize frame-level tampering in watermarked videos, we propose a simple yet effective decoding-based algorithm that leverages the frame-wise embedded watermark messages. Given a set of known template messages corresponding to the original frame positions, we compare each decoded frame message in the potentially tampered video against all template keys using Hamming similarity. This allows us to identify which original frame each tampered frame most likely matches—or to flag it as an insertion if no match surpasses a similarity threshold.

The algorithm operates as follows: for each frame in the tampered video, we compute the similarity between its decoded message and each of the original reference keys used during generation. If the best match has a similarity score below a predefined threshold, the frame is classified as an inserted unauthentic frame. Otherwise, the frame is assigned to the most similar original key. The predicted sequence is then compared against the ground-truth frame mapping to calculate localization accuracy. This procedure enables identification of common temporal attacks such as frame insertion, deletion, and reordering. Our method can generalize to different key counts and message lengths, offering flexibility in watermarking granularity and tamper detection sensitivity.

Algorithm 1: Temporal Tamper Localization

1. Input: Template keys T (R^M×d), Tampered keys K (R^N×d), True sequence S (Z^N), Threshold θ
2. Output: Tamper localization accuracy
3. Initialize P ← empty list
4. For i = 1 to N:
    - Let k_i = K_i (current decoded key)
    - Compute Hamming similarity sim between k_i and all T_j
    - j* = argmax_j sim_j
    - If sim_{j*} < θ:
        - P.append(-1)  # Inserted frame
      Else:
        - P.append(j*)  # Best-matching original index
5. Compute accuracy as the proportion of frames correctly mapped: (1/N) ∑ [P_i == S_i]
6. Return accuracy

## Segment-wise Embedding

While per-frame watermarking offers high granularity and precise tamper localization, it can be sensitive to frame-level distortions and imposes high message embedding and extraction overhead. To balance capacity and efficiency, we introduce a segment-wise embedding strategy, as illustrated in Figure 3.

**Figure 3:** Segment-wise message embedding in VIDSTAMP. In addition to per-frame embedding, VIDSTAMP supports embedding watermark messages into fixed-length segments of k consecutive frames. The same message is repeated across each segment. Segment-wise embedding provides better control over the total bit capacity and simplifies message extraction in long-form videos.

Instead of assigning a unique message to each frame, we divide the video into fixed-length segments of k frames, and embed the same message across each segment. This approach provides greater flexibility in controlling the total number of embedded bits, which is particularly advantageous when dealing with longer videos. By reducing the number of unique message blocks relative to the total frame count, segment-wise embedding avoids unnecessary capacity that might otherwise increase susceptibility to quality degradation or overfitting. Furthermore, segment-level embedding aligns naturally with the temporal modeling behavior of latent video diffusion decoders, which utilize 3D convolutions and attention mechanisms across contiguous frame windows. As a result, our method achieves stronger temporal consistency, simplified extraction, and scalable watermark management, making it well-suited for high-resolution or long-form generative video content.

# IV. Experimental Setup

## Models and Baseline Methods

For our experiments, we build upon the Stable Video Diffusion (SVD) framework, a popular open-source image-to-video latent video generation model, which generates temporally coherent videos conditioned on a single image input. The model is configured to generate 16 frames per video at a frame rate of 16 frames per second (fps). While the decoder is fine-tuned at a spatial resolution of 256×256, inference is performed at 512×512 resolution to evaluate robustness and generalization under higher-fidelity outputs.

In our main experiments, we embed a fixed-length 48-bit message into each of the 16 frames, resulting in a total payload of 768 bits per video. Embedding is performed directly through fine-tuning the decoder, following our two-stage training pipeline. This allows the model to learn to encode unique messages at the frame level while maintaining perceptual and temporal quality. The resulting watermarks can be reliably extracted on a per-frame basis and used for applications such as ownership verification or temporal tamper localization.

Given the scarcity of end-to-end watermarking methods tailored for video generation, and the limitations of adapting image watermarking methods to the video domain, we compare VIDSTAMP with three representative and open-source baselines:

- RivaGAN: A post-hoc watermarking method for video data. We generate videos using the original SVD model and then apply RivaGAN to embed the watermark after generation.
- VideoSeal: Another post-hoc method that embeds watermark signals in the pixel space of pre-generated videos. We use the authors' released code and apply it to videos produced by SVD.
- VideoShield: A generation-integrated watermarking approach based on video diffusion. We generate videos using the authors' full pipeline, which embeds watermarks during the sampling process.

These baselines allow us to compare VIDSTAMP against both post-processing and integrated watermarking strategies, evaluating differences in capacity, quality preservation, and robustness.

## Metrics

We evaluate VIDSTAMP using both watermark accuracy metrics and video quality metrics, to capture a comprehensive view of performance.

- **Bit Accuracy:** Measures the proportion of correctly extracted bits from the watermarked video relative to the original message.

  Bit Accuracy = (1/n) ∑ [m_i == m̂_i]

  where m is the original bitstring and m̂ is the extracted bitstring.

- **Log P-Value:** Following VideoSeal, we also report the log P-value, which better reflects the statistical confidence in watermark detectability. Unlike bit accuracy, which may not capture the full capability of the watermark under varying capacities, the log P-value accounts for the probability of false positives, making it a more suitable metric when comparing models with different watermark capacities.

  For a watermark length L and observed bit accuracy a,

  log P = log10 [∑_{k=⌈aL⌉}^L (L choose k) * 0.5^L]

  A lower log P-value indicates a stronger, more detectable watermark.

- **Video Quality Metrics:** To evaluate whether watermark embedding affects the perceptual quality of the generated videos, we adopt a suite of standard video quality metrics inspired by VideoShield and measured using the VBench evaluation toolkit:

  - Subject Consistency: Measures whether the appearance of key subjects (e.g., people, animals, objects) remains consistent throughout the video.
  - Background Consistency: Evaluates the temporal coherence of background scenes.
  - Motion Smoothness: Assesses how physically plausible and continuous the motion is across frames.
  - Aesthetic Quality: Reflects the visual appeal of individual video frames.
  - Imaging Quality: Measures technical image fidelity, such as absence of noise, blur, or artifacts.

## Datasets

We use two datasets in our training pipeline, corresponding to the two-stage fine-tuning process described in Section III.

- For the first stage of fine-tuning—focused on learning spatially distinct message embeddings—we use the COCO dataset, a widely used benchmark for image understanding and generation. Each image is treated as an independent frame in a pseudo-video batch, allowing the decoder to learn to embed different messages into visually diverse content without relying on temporal cues.

- For the second stage, which adapts the model to temporal coherence and video distribution, we use the VBench test prompt set, which consists of 800 prompts spanning eight semantic categories (animal, architecture, food, human, lifestyle, plant, scenery, vehicles). To align with the input format required by our image-to-video Stable Video Diffusion (SVD) model, we first generate conditioning images for each prompt using Stable Diffusion 2.1. These images are then used as inputs to produce video samples with SVD. Out of the 800 prompts, 640 prompts (80%) are used to generate videos for the second stage of decoder fine-tuning. The remaining 160 prompts (20%) are reserved as an evaluation set. Videos generated from these prompts are used for testing watermark extraction accuracy, log P-value, and video quality, as well as for comparing VIDSTAMP against existing watermarking baselines such as RivaGAN, VideoSeal, and VideoShield.

This dataset split allows us to both train the decoder on a diverse range of video content and fairly assess performance across a broad set of semantically varied categories.

# V. Experimental Results

## Main Results

### Video Quality Preservation

Table I presents a detailed comparison of VIDSTAMP against prior watermarking approaches across both embedding performance and video quality. In terms of video quality, VIDSTAMP demonstrates the strongest balance between fidelity and watermark integration. It achieves an average quality score of 0.836, nearly identical to the unwatermarked output from Stable Video Diffusion (0.838), and comparable to or better than all competing methods. Notably, VIDSTAMP scores the highest or ties for best in Aesthetic Quality and Imaging Quality, indicating that our training-time integration approach effectively preserves both spatial and temporal coherence. Unlike post-processing watermarking methods such as RivaGAN and VideoSeal, which apply watermark signals after video generation and may degrade quality, VIDSTAMP embeds watermarks during the generation process itself by fine-tuning the decoder. This not only ensures better perceptual consistency but also introduces no additional computational overhead at inference time, as the watermark is inherently generated along with the video frames.

### Embedding Capacity and Bit Accuracy

In terms of bit accuracy, VIDSTAMP achieves a high value of 0.950. While this is marginally lower than VideoShield and VideoSeal, it is important to note that bit accuracy alone is insufficient for fair comparison across models with different capacities. VIDSTAMP embeds 768 bits per video (48 bits in each of 16 frames), which is significantly higher than other methods. Therefore, comparing bit accuracy without accounting for message length can misrepresent true performance.

### Statistical Detectability (Log P-Value)

To address this, we follow VideoSeal and report the log P-value, a metric that jointly considers both bit accuracy and bit length, offering a more reliable measure of watermark detectability. According to this metric, VIDSTAMP achieves a log P-value of -166.65, substantially lower (i.e., better) than VideoShield (-149.0), VideoSeal (-26.9), and RivaGAN (-9.6). This indicates that VIDSTAMP provides more statistically verifiable watermarks despite embedding significantly more information per video.

| Method      | Bit Length | Bit Accuracy | log10p   | Subject Consistency | Background Consistency | Motion Smoothness | Aesthetic Quality | Imaging Quality | Avg Quality |
|-------------|------------|--------------|----------|---------------------|-----------------------|-------------------|-------------------|-----------------|-------------|
| VIDSTAMP    | 768        | 0.950        | -166.65  | 0.959               | 0.955                 | 0.961             | 0.606             | 0.699           | 0.836       |
| VideoShield | 512        | 0.995        | -149.0   | 0.964               | 0.960                 | 0.963             | 0.587             | 0.6             |             |
| VideoSeal   | ...        | ...          | -26.9    | ...                 | ...                   | ...               | ...               | ...             | ...         |
| RivaGAN     | ...        | ...          | -9.6     | ...                 | ...                   | ...               | ...               | ...             | ...         |
| SVD (no WM) | 0          | -            | -        | 0.965               | 0.961                 | 0.964             | 0.604             | 0.703           | 0.838       |

**Table I:** Comparison of watermarking methods across embedding performance and video quality. VIDSTAMP embeds 768 bits per video (48 bits/frame, 16 frames), offering significantly higher capacity than prior work. The table reports bit accuracy, log P-value (lower is better), and five VBench-based quality metrics. The last row reports the output quality of the underlying Stable Video Diffusion model without watermarking, serving as a perceptual upper bound.

# Conclusion

VIDSTAMP presents a robust, high-capacity, and perceptually faithful watermarking framework for temporally-aware video diffusion models. By embedding watermarks directly in the latent decoding process and leveraging temporal model components, VIDSTAMP achieves strong resilience against video-specific attacks, supports frame-level tamper localization, and introduces no inference-time overhead. Experimental results demonstrate that VIDSTAMP outperforms prior watermarking methods in the key trade-offs of capacity, robustness, and perceptual quality, making it a practical solution for ownership verification and integrity assurance in AI-generated video content.
# Abstract

Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at https://github.com/CaraJ7/T2I-R1.

# Introduction

The emergence of advanced Large Language Models (LLMs), such as OpenAI and DeepSeek-R1, has demonstrated considerable reasoning capabilities across domains including mathematics and coding. Through reinforcement learning (RL), these models analyze problems progressively with a comprehensive Chain-of-Thought (CoT) before providing answers, significantly enhancing output accuracy. The CoT reasoning strategies have also been extended to the visual domain. Recent Large Multi-modal Models (LMMs) have adapted the paradigm to accommodate the visual understanding task. These advanced LMMs can jointly process images and their associated textual queries, performing step-by-step analyses of visual details and integrating them with reasoning steps to derive final answers.

Concurrently, CoT-like reasoning has been initially investigated in the visual generation task, particularly in autoregressive text-to-image generation. The pioneering work, Image Generation with CoT, regards the progressive generation of the image tokens as a kind of CoT analogous to that of the text tokens, and proposes to optimize this intermediate process to enhance the image quality.

Despite these advances, the exploration of CoT for image generation remains preliminary. Unlike image understanding, image generation requires the complex interpretation of cross-modal alignment and the synthesis of fine-grained visual details. To address these challenges, we identify two distinct levels of CoT reasoning that can be leveraged to enhance image generation, as illustrated in Fig. 1.

Semantic-level CoT is the textual reasoning about the image to generate, which is introduced prior to the image generation. The semantic-level CoT designs the global structure of the image, e.g., the appearance and location of each object. In case the prompt requires reasoning, the semantic-level CoT also helps to deduce the objects to generate. Optimizing the semantic-level CoT could explicitly manage the planning and reasoning of the prompt before the subsequent image tokens generation, making the generation easier.

Token-level CoT is the intermediate patch-by-patch generation process of the image. This process could be viewed as a form of CoT as it outputs each subsequent token conditioned on all previous tokens within a discrete space, similar to the textual CoT. Unlike semantic-level CoT, token-level CoT focuses on low-level details like pixel generation and maintaining visual coherence between adjacent patches. Optimizing the token-level CoT can enhance both the generation quality and the alignment between the prompt and the resulting images.

Despite recognizing these two levels of CoT, a critical question remains unaddressed: How can we enhance and coordinate them for text-to-image generation? Current mainstream generative models are trained exclusively on generation targets, lacking the explicit textual understanding required for semantic-level CoT reasoning. Although introducing a separate model (e.g., an LLM specifically for prompt interpretation) is technically feasible, this approach would significantly increase computational costs, complexity, and deployment challenges.

Recently, a trend has arisen to merge visual understanding and generation within a single model. Building upon LMMs, these unified LMMs (ULMs) could not only understand the visual inputs but also generate images from text prompts. However, their two capabilities are still decoupled, typically pre-trained in two independent stages, with no clear evidence that the understanding capabilities can benefit generation.

To fulfill our target, we introduce BiCoT-GRPO, an RL method to jointly optimize the two levels of CoT for ULM. We opt for RL instead of supervised fine-tuning for two reasons: (1) the ULM has possessed the fundamental ability needed for the semantic-level and token-level CoT; our goal is only to elicit the fusion of these two abilities by guiding the model's self-exploration; (2) RL methods have proven highly effective for enhancing reasoning capabilities, which are essential for both levels of CoT.

Specifically, we first instruct the ULM to imagine and plan the image based on the prompt to obtain the semantic-level CoT. Then, we feed it into the ULM as the condition for the subsequent image generation for token-level CoT. We simultaneously generate multiple images from each prompt and then compute group-relative reward to optimize both levels of CoT within the same iteration. Unlike understanding tasks, where clearly defined rules for rewards exist, image generation lacks such standardized rules. Therefore, we propose to utilize an ensemble of diverse vision experts as reward models. This reward design serves two critical purposes: it evaluates generated images from multiple dimensions to ensure reliable quality assessment, while also functioning as a regularization method to prevent the ULM from hacking a single reward model.

Through the proposed reasoning strategies, we obtain T2I-R1, the first reasoning-enhanced text-to-image model combining the semantic-level and token-level CoT. Empirical results show that our approach outperforms baseline models by 13% and 19% improvements on the T2I-CompBench and WISE benchmark, and even surpasses the previous state-of-the-art model FLUX.1. Qualitative analysis reveals that our method empowers the model to generate more human-aligned results by reasoning about the true intentions behind the prompt and demonstrates enhanced robustness when dealing with uncommon scenarios.

Our contributions are summarized as follows:

- We identify a dual-level reasoning process in the autoregressive image generation task by introducing the semantic-level and token-level CoT, which decouple high-level image planning from low-level pixel generation for more reliable generation.
- We develop BiCoT-GRPO, a new reinforcement learning framework that jointly optimizes both levels of CoT reasoning, seamlessly integrating the understanding capabilities of ULMs for image generation. For reward modeling, we investigate a robust reward system utilizing an ensemble of vision experts.
- Our resulting model, T2I-R1, that incorporates both levels of CoT using BiCoT-GRPO, demonstrates significant performance improvements and surpasses FLUX.1 across multiple established benchmarks.


# Related Work

## Unified Generation and Understanding LMM

Recently, the effort to unify image generation and understanding in a single LMM has attracted much attention. Building upon large language models (LLMs), it is natural for the LMMs to understand the image and output the text. However, the method of how to generate an image from an LMM is still under exploration. The image generation method diverges into different branches.

One line of the method relies on an exterior image generation model to complete generation. The generator often utilizes text-to-image diffusion models due to its powerful generation capability. To deliver the generation information, the LMM passes either the implicit conditional feature or the explicit image prompt to the generator. For example, EMU first trains the LMM to output CLIP image features identical to that input to the LMM. Then, a pretrained UNet of Stable Diffusion receives the output feature as the condition to generate an image.

Another line of the method seeks to train the LMM to generate discrete tokens produced by VQGAN to eliminate the need for an additional generator. Some works directly adopt the VQGAN encoder as the image tokenizer for LMM. However, the VQGAN encoder is only pretrained on the image reconstruction task and thereby generates visual tokens less helpful for image understanding. To improve the understanding capability, some propose to tackle the understanding and generation tasks with different vision encoders separately. The CLIP encoder deals with image input for understanding, while the VQGAN encoder is responsible for generation. Moreover, some works attempt to empower the vision encoder with both the understanding and the generation capability. VILA-U trains a vision encoder with both the contrastive loss for text-image understanding and reconstruction loss for image detail preserving. Thanks to the joint pretraining, the vision encoder could generate text-aligned discrete visual tokens. The LMM is then trained to receive the discrete tokens for image understanding and predict them for image generation.

## Reinforcement Learning for Large Reasoning Models

The emergence of OpenAI has gained tremendous attention in developing the reasoning capability of large language models. Later, DeepSeek-R1 proposes a rule-based reward and GRPO training method. The introduced method instructs the model to perform an extensive reasoning process before generating the final answer. The reward only focuses on the correctness of the final answer and the following of the pre-defined format. Recently, a number of works have applied this method to multi-modal large language models with task-specific rewards like correctness and IoU. This training paradigm largely helps various reasoning-intensive tasks like mathematical problem-solving and code generation.

# Method

## Preliminary

Recently, the employment of reinforcement learning has been the dominant approach to elicit the reasoning capability of large models. GRPO enhances PPO by eliminating the value function and estimating the advantage in a group-relative manner. For a specific prompt-answer pair, a group of individual responses is sampled from the old policy. Each response is then input to a reward function to obtain the individual reward. Then, the advantage of the i-th response is calculated by normalizing the rewards of the group. GRPO adopts a clipped objective similar to PPO. Besides, a KL penalty term between the current policy and the reference model is directly added in the loss function.

## Semantic-level and Token-level CoT

In the autoregressive text generation tasks of LLMs and LMMs, CoT occurs in the textual reasoning format. However, in autoregressive image generation tasks, we identify two distinct types of CoT that could enhance the image generation at different abstraction levels:

**Semantic-level CoT:** Semantic-level CoT is defined as the textual reasoning that precedes image generation, serving as an overall semantic planning stage for the intended image. This process mirrors human artistic creation: when given a brief prompt, an artist first thinks about the scene construction, considering object attributes, spatial relationships, and interactions. In addition to the planning for common prompts, we also observe the semantic-level CoT benefits two other scenarios. If the prompt does not directly depict the object to generate, the semantic-level CoT can reason about the true intention from the user's prompt, providing more aligned images. Additionally, the semantic-level CoT demonstrates importance when handling unusual or potentially ambiguous scenes. For example, when given the prompt "A pig on the bottom of a train," semantic-level CoT introduces the action "lying" for the pig, creating a more sensible scenario.

**Token-level CoT:** Unique to the image generation task, a token-level step-by-step thinking exists in the image generation process. The generation of image tokens much resembles a chain of thought: the image tokens are generated patch by patch, where the current patch is generated based on the previous ones. We define the sequential generation of image tokens as token-level CoT. This process parallels how an artist progressively fills a canvas, with the generated patches forming a visual reasoning chain that maintains coherence across the image. This chain of patches is later reshaped to a 2D grid and input to an image decoder to obtain the image. Unlike semantic-level CoT, which addresses global planning, token-level CoT focuses on local details and visual coherence across the image space.

## BiCoT-GRPO

GRPO has been proven to be highly effective for exploring the reasoning capability of the LLMs and LMMs. To accommodate both semantic-level and token-level CoT in image generation, we propose BiCoT-GRPO, where the model reasons twice in a single generation process. We instruct the model to first perform semantic-level CoT for global planning, and then dive into the local details by performing token-level CoT.

However, compared with the task of text generation, a great pipeline challenge is posed for incorporating two levels of CoT for image generation. Limited by the training paradigm, most current ULMs cannot generate interleaved images and text themselves. A manual signifier is often needed to instruct the model on which task to perform, either text generation or image generation. For Janus-Pro to generate an image, which is the ULM we use in this work, we need to manually concatenate an image start token to explicitly instruct the model to start generating image tokens.

To tackle this problem, we propose a novel pipeline to facilitate ULM in generating images with two levels of CoT, as shown in Fig. 3. Specifically, our pipeline is composed of a two-step generation process. The first step is to generate the semantic-level CoT. We input the image prompt and instruct the model to imagine and reason about the details of the image to generate semantic-level CoT. The second stage focuses on the token-level CoT generation. We input the image prompt, the generated semantic-level CoT in the first stage, and the image start token to the ULM for generating image tokens. Then, the image tokens are input to the image decoder to obtain the image.

Since there exist two types of CoT in our method, first the semantic-level CoT and then the token-level CoT, each response is composed of two parts. In this sense, the reward is converted to account for both semantic and token-level reasoning.

In practice, we incorporate the token-level policy gradient loss, where the loss term is normalized over all the generated tokens to balance the reward on overly long semantic-level CoT.

## Ensemble of Generation Rewards

Unlike DeepSeek-R1 with the rule-based reward, assessing the images based on pre-defined rules is infeasible. The assessment of the image includes various aspects, including the aesthetic appeal and objects existence, attributes, and relationships. Considering the complexity, we introduce an ensemble of vision experts to judge the generated image from multiple aspects. Meanwhile, the use of multiple reward functions also serves as a regularization method to prevent the ULM from hacking into a specific reward model.

As shown in Fig. 4, the ensemble contains the following experts:

- **Human Preference Model:** Human preference models (HPMs), such as HPS and ImageReward, are trained to simulate human aesthetic preferences. These models are developed using datasets of human rankings on synthetic images, where annotators evaluate and compare generated outputs. During inference, these models assess both the aesthetic quality and prompt alignment of a generated image, producing a composite human preference score. This expert provides a holistic reward signal from a general perspective.
- **Object Detector:** Another option of the reward model is an object detector, e.g., GroundingDINO and YOLO-world. These open-vocabulary detection models accept an image along with object queries as input and output both the spatial positions and confidence scores for detected objects. This kind of vision expert serves as an ideal tool to evaluate the objects existence and relationship concerning space and numbers. For implementation, we extract all objects from the training image prompts, then query the object detector to identify these objects within the generated image. For each object, we assign a binary existence score (1 if detected, 0 otherwise) and average these scores across all objects in the prompt. If the prompt contains a spatial relationship, we further leverage the detected location to validate its correctness. We calculate the relative distance and intersection over union (IoU) between the objects for the spatial score. If the number of the object is specifically pointed out in the prompt, we compare the number with the detected number of the object. The reward from the object detector is determined accordingly.
- **Visual Question Answering Model:** The visual question answering (VQA) models are trained to answer questions based on the image input. The VQA models include earlier models prior to LLM, e.g., BLIP and GIT, and LMMs like LLaVA. We leverage these models to judge the existence and attributes of the objects. For example, if the image prompt is "a red dog and a yellow cat," we first reformat each individual object with its attribute as a question to the VQA model, i.e., "a red dog?" and "a yellow cat?". Then, we record the probability for the model to answer "Yes" as the reward.
- **Output Reward Model:** Lastly, we also employ the output reward model (ORM) as a reward model. The ORM is fine-tuned from an LMM specifically for evaluating the alignment between the prompt and the image. The fine-tuning is to instruct the model to output "Yes" if the image perfectly aligns with the image and output "No" otherwise. Therefore, we calculate the ORM reward using the methodology similar to the VQA reward, except that we input the whole image prompt to the ORM instead of reformatting the prompt.

We can choose one or multiple reward functions illustrated above, and take the average as the final reward for a specific sample.

# Experiment

## Experimental Setup

**Training Settings:** Our training dataset comprises text prompts sourced from the training set of T2I-CompBench and others, totaling 6,786 prompts with no images. Prior to training, we use GPT-40 mini to extract the objects and their attributes from the prompts to facilitate computing the rewards. We use Janus-Pro-7B as the base model. We use a learning rate of 1e-6 and a beta of 0.01. For the reward model, we choose HPS as the human preference model, GroundingDINO as the object detector, and GIT as the VQA model. For the ORM, we finetune LLaVA-OneVision-7B in the same manner as previous work.

## T2I-CompBench Result

| Model | Attribute Binding | Object | Relationship | Complex | Color | Shape | Texture | Spatial | Non-Spatial |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| Structure Diffusion | 0.4990 | 0.4218 | 0.4900 | 0.1386 | 0.3111 | 0.3355 |  |  |  |
| Composable Diffusion | 0.4063 | 0.3299 | 0.3645 | 0.0800 | 0.2980 | 0.2898 |  |  |  |
| Attend-and-Excite | 0.6400 | 0.4517 | 0.5963 | 0.1455 | 0.3109 | 0.3401 |  |  |  |
| PixArt-a | 0.6690 | 0.4927 | 0.6477 | 0.2064 | 0.3197 | 0.3433 |  |  |  |
| CoMat | 0.7827 | 0.5329 | 0.6468 | 0.2428 | 0.3187 | 0.3680 |  |  |  |
| SD-v1.5 | 0.3758 | 0.3713 | 0.4186 | 0.1165 | 0.3112 | 0.3047 |  |  |  |
| SD-XL-base-1.0 | 0.5879 | 0.4687 | 0.5299 | 0.2131 | 0.3119 | 0.3237 |  |  |  |
| FLUX.1 | 0.7407 | 0.5718 | 0.6922 | 0.2863 | 0.3127 | 0.3703 |  |  |  |
| Show-o | 0.56 | 0.41 | 0.46 | 0.20 | 0.30 | 0.29 |  |  |  |
| Show-o PARM | 0.75 | 0.56 | 0.66 | 0.29 | 0.31 | 0.37 |  |  |  |
| EMU3 | 0.7544 | 0.5706 | 0.7164 |  |  |  |  |  |  |
| Janus-Pro-7B Baseline | 0.6359 | 0.3528 | 0.4936 | 0.2061 | 0.3085 | 0.3559 |  |  |  |
| **T2I-R1 (Ours)** | **0.8130** | 0.5852 | **0.7243** | 0.3378 | 0.3090 | 0.3993 |  |  |  |

## WISE Result

| Model | Cultural | Spatio-Temporal (Time) | Spatio-Temporal (Space) | Natural Science (Biology) | Natural Science (Physics) | Natural Science (Chemistry) | Overall |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| PixArt-Alpha | 0.45 | 0.50 | 0.48 | 0.49 | 0.56 | 0.34 | 0.47 |
| playground-v2.5 | 0.49 | 0.58 | 0.55 | 0.43 | 0.48 | 0.33 | 0.49 |
| SD-v1-5 | 0.34 | 0.35 | 0.32 | 0.28 | 0.29 | 0.21 | 0.32 |
| SD-XL-base-0.9 | 0.43 | 0.48 | 0.47 | 0.44 | 0.45 | 0.27 | 0.43 |
| FLUX.1-dev | 0.48 | 0.58 | 0.62 | 0.42 | 0.51 | 0.35 | 0.50 |
| Emu3 | 0.34 | 0.45 | 0.48 |  |  |  |  |

# Conclusion

We introduce T2I-R1, a novel text-to-image generation framework that unites semantic-level and token-level chain-of-thought reasoning within a unified large multi-modal model. Through the BiCoT-GRPO reinforcement learning approach and an ensemble of vision expert reward models, T2I-R1 achieves significant improvements over prior state-of-the-art models on both compositional and generalization benchmarks. Our results demonstrate that integrating explicit reasoning at both the semantic and token levels enables more robust, human-aligned, and interpretable image generation.

# Figures

**Figure 1:** The Illustration of CoT in Image Understand and Generation Tasks. In the image understanding task, the CoT is the textual reasoning process. In the autoregressive visual generation task, we identify two levels of CoT: the semantic-level and token-level CoT. The semantic-level CoT is the high-level planning prior to the image generation, in the form of text. The token-level CoT is the intermediate patch-by-patch generation process, focusing on the local pixel details within a patch, in the form of image tokens.

**Figure 2:** Visualization of the Image Generation Process of T2I-R1. All the prompts need reasoning or contain an uncommon scenario. We observe that T2I-R1 successfully deduces the true intention behind the prompt or provides a sensible imagination for the uncommon scenario highlighted in the text to produce a satisfying result compared with the baseline model, Janus-Pro.

**Figure 3:** Framework of BiCoT-GRPO. In step 1, we instruct the model to generate the semantic-level CoT based on the image prompt. In step 2, images are generated conditioned on both the image prompt and semantic-level CoT, with the intermediate generation process serving as token-level CoT. The resulting images are evaluated by an ensemble of vision experts to obtain rewards. We generate N images from each prompt to compute the group-relative reward and perform GRPO training.

**Figure 4:** Illustration of the Ensemble of Generation Rewards. We use GPT-40 mini to extract the objects and their attributes before training. Each specialized reward model receives customized information inputs for the reward calculation. We take the average of all the rewards as final reward.

**Figure 5:** Visualization Results. We provide the image generation results of the same prompt from four models: base model, the model with only semantic-level CoT optimized, the model with only token-level CoT optimized, and the model with both levels of CoT optimized.

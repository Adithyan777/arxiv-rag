# Abstract

This paper proposes a new watermarking method to embed ownership information into a deep neural network (DNN) that is robust to fine-tuning. Specifically, it proves that when the input feature of a convolutional layer contains only low-frequency components, specific frequency components of the convolutional filter remain unchanged by gradient descent during fine-tuning. A revised Fourier transform is introduced to extract frequency components from the convolutional filter. These frequency components are also shown to be equivariant to weight scaling and weight permutations. Based on this, a watermark module is designed to encode watermark information into specific frequency components of a convolutional filter. Preliminary experiments demonstrate the effectiveness of the method.

# 1. Introduction

Watermarking techniques have long been used to protect the copyright of digital content such as images, videos, and audio. Recently, these techniques have been extended to protect the intellectual property of neural networks by embedding ownership information implicitly into the network. This allows verification of the true origin if a network is stolen and further optimized.

Previous studies embedded ownership information in various ways, such as directly embedding into network parameters, using classification results on adversarial examples as backdoor watermarks, or adding soft watermarks to generated results. However, a core challenge remains: most watermarking techniques are not resistant to fine-tuning. When network parameters change during fine-tuning, embedded watermarks may be overwritten.

Although prior works have tested watermark resistance to fine-tuning, no theoretical framework guarantees robustness to fine-tuning. The key challenge is to identify invariant terms in the neural network that remain least affected by fine-tuning, such as certain parameters or properties of parameters.

This study aims to discover and prove such invariant terms. It builds on the finding that forward propagation through a convolutional layer can be reformulated as vector multiplication between frequency components of the convolutional filter and the input feature in the frequency domain. The paper proves that if the input feature contains only low-frequency components, specific frequency components of the convolutional filter remain stable during fine-tuning. These frequency components also exhibit equivariance to weight scaling and permutations. Thus, these components are used as robust watermarks.

To defend against overwriting attacks, an additional loss is introduced during training to ensure that overwriting the watermark significantly hurts model performance.

The contributions are:

- Discovery and theoretical proof that specific frequency components of convolutional filters remain invariant during training and are equivariant to weight scaling and permutations.

- Proposal to encode watermark information into these frequency components to ensure robustness to fine-tuning, weight scaling, and permutations.

- Preliminary experiments demonstrating the effectiveness of the method.

# 2. Related Work

Robustness of watermarks is a key issue in neural network watermarking, especially when embedded in network parameters. Attacks such as fine-tuning, weight scaling, weight permutations, pruning, and distillation can remove or hurt watermarks.

Weight scaling and permutations rearrange network parameters, changing watermarks without affecting model outputs. Some works found invariants to these attacks, embedding watermarks in such invariant terms.

Robustness to fine-tuning is more challenging. Existing methods defend fine-tuning attacks mostly by engineering approaches, such as selecting parameters that change little during fine-tuning or using classification accuracy on a trigger set as watermark. However, many watermarks can still be removed under certain fine-tuning settings.

Certified robustness methods prove a safe range of parameter changes during fine-tuning but do not propose intrinsically robust watermarks.

In contrast, this paper proves that specific frequency components of convolutional filters remain stable during fine-tuning and uses them as theoretically certified robust watermarks.

# 3. Method

## 3.1. Preliminaries: Reformulating Convolution in the Frequency Domain

Forward propagation through a convolutional filter can be reformulated in the frequency domain. Applying a discrete Fourier transform (DFT) to the input feature and a revised DFT to the convolutional filter yields frequency component vectors for both.

Let the convolutional filter have C channels and kernel size KÃ—K, parameterized by weights $$ W \in \mathbb{R}^{C \times K \times K} $$ and bias $$ b \in \mathbb{R} $$. The input feature is $$ X \in \mathbb{R}^{C \times M \times N} $$, and the output feature map is $$ Y \in \mathbb{R}^{M \times N} $$, computed as:

$$ Y = W * X + b \mathbf{1}_{M \times N} $$

where * denotes convolution and $$ \mathbf{1}_{M \times N} $$ is an all-ones matrix.

The frequency components of input and output features at frequency $$ (u,v) $$ are:

$$
G_{u,v}^c = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} X_{m,n}^c e^{-i \left( \frac{u m}{M} + \frac{v n}{N} \right) 2\pi}
$$

$$
H_{u,v} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} Y_{m,n} e^{-i \left( \frac{u m}{M} + \frac{v n}{N} \right) 2\pi}
$$

These can be organized into frequency spectrum matrices and vectors.

Theorem 3.1 states that forward propagation can be reformulated as a vector multiplication in the frequency domain:

$$
H_{u,v} = F_{W}^{u,v} \cdot F_{X}^{u,v} + b
$$

where $$ F_W^{u,v} $$ and $$ F_X^{u,v} $$ are frequency components of the filter and input feature respectively, and $$ \cdot $$ denotes scalar product.

The frequency component $$ F_W^{u,v} $$ of the convolutional filter is obtained by a revised discrete Fourier transform:

$$
F_W^{u,v} = T^{u,v}(W)
$$

where $$ T^{u,v} $$ is a transform defined over the filter weights.

## 3.2. Invariant Frequency Components of the Convolutional Filter

The paper proves that frequency components $$ F_W^{u,v} $$ at certain frequencies are stable during training (fine-tuning) and equivariant to weight scaling and permutations.

### Invariance to Fine-tuning

If the input feature contains only fundamental frequency components (low frequencies), then frequency components at specific frequencies $$ S $$ remain unchanged during gradient descent optimization.

Theorem 3.2 expresses the change in frequency components after one gradient descent step, showing it depends on the input frequency components. Corollary 3.3 states that if input frequencies other than the fundamental are zero, frequency components at set $$ S $$ remain invariant.

In practice, input features contain low-frequency components, so frequency components near $$ S $$ remain relatively stable.

### Equivariance to Weight Scaling

Weight scaling multiplies the filter weights by a constant $$ a $$. Theorem 3.5 shows frequency components scale accordingly:

$$
F_{aW}^{u,v} = a F_W^{u,v}
$$

### Equivariance to Weight Permutations

Permutation attacks rearrange filters and bias terms. Theorem 3.6 proves frequency components permute correspondingly, preserving the watermark information under permutations.

## 3.3. Using the Invariant Frequency Components as the Neural Network Watermark

Based on the above, the paper proposes to encode watermark information into the invariant frequency components.

### Watermark Module

A watermark module $$ X $$ is constructed in parallel to the network backbone, consisting of:

- A low-pass filter preserving low-frequency components.

- $$ D $$ convolutional filters $$ W_1, W_2, \ldots, W_D $$ with biases $$ b_1, b_2, \ldots, b_D $$.

The output is:

$$
Y_d = W_d * X + b_d \mathbf{1}_{M \times N}, \quad d=1,\ldots,D
$$

Low-pass filtering ensures the input to the watermark module contains only low-frequency components, guaranteeing stability of the watermark frequency components.

### Watermark Extraction

Frequency components $$ F_{W_d}^{u,v} $$ at frequencies in set $$ S $$ from each filter $$ W_d $$ form the watermark.

### Implementation Details

To avoid hurting the main network's performance, the watermark module is connected in parallel. Typical settings are kernel size $$ K=3 $$ and low-pass filter radius $$ r=1 $$.

### Visualization

Figure 4a shows the specific frequencies used as watermark; Figure 4b shows spatial domain feature maps of unit frequency components used as watermark.

## 3.4. Detecting the Watermark

Given a source watermarked DNN and a suspicious DNN, detection verifies if the suspicious DNN originates from the source via fine-tuning, weight scaling, or permutations.

Due to permutation attacks, detection considers matching between frequency components of filters in both networks, finding an optimal permutation.

The watermark detection rate (DR) is defined as the percentage of frequency components whose cosine similarity exceeds a threshold (set to 0.995).

High DR indicates the suspicious network originates from the source network.

## 3.5. Learning Towards the Overwriting Attack

Overwriting attacks replace watermark module parameters to remove watermarks.

To defend, the network is trained with an additional loss $$ L_{attack} $$ that forces the network to classify inputs into a pseudo category if the watermark is overwritten, significantly degrading performance.

The total loss is:

$$
L = L_{CE} + \lambda L_{attack}
$$

where $$ L_{CE} $$ is the cross-entropy loss and $$ \lambda $$ balances the two terms.

Random noise is added to watermark parameters during training to simulate overwriting.

### Ablation Studies

Experiments on AlexNet and ResNet18 with datasets CIFAR-10, CIFAR-100, Caltech-101, and Caltech-256 show that training with $$ L_{attack} $$ significantly reduces accuracy under overwriting attacks, demonstrating defense effectiveness.

| Dataset     | Baseline Accuracy | With $$L_{CE} + L_{attack}$$ Accuracy (No Attack) | With $$L_{CE} + L_{attack}$$ Accuracy (Under Attack) | With $$L_{CE}$$ Accuracy (No Attack) | With $$L_{CE}$$ Accuracy (Under Attack) |
|-------------|-------------------|-----------------------------------------------|--------------------------------------------------|---------------------------------|-------------------------------------|
| CIFAR-10    | 91.03 (AlexNet) / 94.83 (ResNet-18) | 90.28 / 43.55 | 92.17 / 72.26 | 91.12 / 91.12 | 94.89 / 94.89 |
| CIFAR-100   | 68.10 / 76.29     | 66.34 / 36.93                                 | 75.49 / 41.18                                    | 67.52 / 67.52                   | 76.53 / 76.53                       |
| Caltech-101| 66.46 / 70.10     | 62.15 / 32.53                                 | 67.14 / 41.33                                    | 67.84 / 67.84                   | 69.87 / 69.87                       |
| Caltech-256| 40.50 / 54.61     | 37.97 / 15.37                                 | 50.33 / 18.13                                    | 39.22 / 39.22                   | 53.86 / 53.86                       |

## 3.6. Verifying the Robustness of the Watermark

### Robustness to Fine-tuning

AlexNet and ResNet18 trained on CIFAR-100 were fine-tuned on CIFAR-10 and Caltech-101. The norm of frequency component changes was measured, showing stability of watermark components during fine-tuning.

Adding the watermark did not degrade fine-tuning performance.

| Source Dataset | Target Dataset | Baseline Accuracy | Watermarked Accuracy (Detection Rate %) |
|----------------|----------------|-------------------|-----------------------------------------|
| CIFAR-100      | CIFAR-10       | 88.90 (AlexNet) / 93.03 (ResNet-18) | 89.65 (100%) / 94.12 (100%)          |
| CIFAR-100      | Caltech-101    | 70.02 / 76.80     | 72.11 (100%) / 79.98 (100%)            |

### Robustness to Weight Scaling

Watermarked DNNs had their watermark module weights scaled by constants $$ a > 0 $$. Watermark detection rates remained 100%, confirming robustness.

| Scale $$ a $$ | CIFAR-10 | CIFAR-100 | Caltech-101 | Caltech-256 |
|---------------|----------|-----------|-------------|-------------|
| 1             | 100%     | 100%      | 100%        | 100%        |
| 10            | 100%     | 100%      | 100%        | 100%        |
| 100           | 100%     | 100%      | 100%        | 100%        |

### Robustness to Weight Permutations

Watermarked DNNs had their watermark module filters permuted randomly. Detection rates remained 100%, showing robustness.

| Permutation | CIFAR-10 | CIFAR-100 | Caltech-101 | Caltech-256 |
|-------------|----------|-----------|-------------|-------------|
| 1           | 100%     | 100%      | 100%        | 100%        |
| 2           | 100%     | 100%      | 100%        | 100%        |

# 4. Conclusion

This paper discovers and theoretically proves that specific frequency components of convolutional filters remain stable during training and are equivariant to weight scaling and permutations. Based on this, these frequency components are used as watermarks to embed ownership information, guaranteeing robustness to fine-tuning, weight scaling, and permutations.

To defend against overwriting attacks, an additional loss term is introduced during training to ensure significant performance degradation if the watermark is overwritten.

Preliminary experiments demonstrate the effectiveness of the proposed method.

# Figures

**Figure 1:** The framework of the proposed watermark. Specific frequency components of the convolutional filter obtained by a revised discrete Fourier transform remain stable during training and are used as robust watermarks.

**Figure 2:** Forward propagation in the frequency domain (a) and spatial domain (b). Convolution operation is equivalent to vector multiplication on frequency components.

**Figure 3:** Architecture of the watermark module connected in parallel to the backbone. The module contains a low-pass filter and convolution operations with multiple filters.

**Figure 4:** (a) Specific frequencies used as watermark. (b) Feature maps obtained by inverse discrete Fourier transform of unit frequency components used as watermark.

**Figure 5:** Heatmaps showing average norm of changes in frequency components before and after fine-tuning at different frequencies. Low frequencies are centered, high frequencies at corners.

# Tables

| Dataset     | Baseline Accuracy | With $$L_{CE} + L_{attack}$$ Accuracy (No Attack) | With $$L_{CE} + L_{attack}$$ Accuracy (Under Attack) | With $$L_{CE}$$ Accuracy (No Attack) | With $$L_{CE}$$ Accuracy (Under Attack) |
|-------------|-------------------|-----------------------------------------------|--------------------------------------------------|---------------------------------|-------------------------------------|
| CIFAR-10    | 91.03 / 94.83     | 90.28 / 43.55                                 | 92.17 / 72.26                                    | 91.12 / 91.12                   | 94.89 / 94.89                       |
| CIFAR-100   | 68.10 / 76.29     | 66.34 / 36.93                                 | 75.49 / 41.18                                    | 67.52 / 67.52                   | 76.53 / 76.53                       |
| Caltech-101 | 66.46 / 70.10     | 62.15 / 32.53                                 | 67.14 / 41.33                                    | 67.84 / 67.84                   | 69.87 / 69.87                       |
| Caltech-256 | 40.50 / 54.61     | 37.97 / 15.37                                 | 50.33 / 18.13                                    | 39.22 / 39.22                   | 53.86 / 53.86                       |

| Source Dataset | Target Dataset | Baseline Accuracy | Watermarked Accuracy (Detection Rate %) |
|----------------|----------------|-------------------|-----------------------------------------|
| CIFAR-100      | CIFAR-10       | 88.90 / 93.03     | 89.65 (100%) / 94.12 (100%)              |
| CIFAR-100      | Caltech-101    | 70.02 / 76.80     | 72.11 (100%) / 79.98 (100%)              |

| Scale $$ a $$ | CIFAR-10 | CIFAR-100 | Caltech-101 | Caltech-256 |
|---------------|----------|-----------|-------------|-------------|
| 1             | 100%     | 100%      | 100%        | 100%        |
| 10            | 100%     | 100%      | 100%        | 100%        |
| 100           | 100%     | 100%      | 100%        | 100%        |

| Permutation | CIFAR-10 | CIFAR-100 | Caltech-101 | Caltech-256 |
|-------------|----------|-----------|-------------|-------------|
| 1           | 100%     | 100%      | 100%        | 100%        |
| 2           | 100%     | 100%      | 100%        | 100%        |
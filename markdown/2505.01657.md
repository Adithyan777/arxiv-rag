# Abstract

Personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. Although effective, existing methods face two main issues. First, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. Disproportionately high weights for low-similarity items distort users visual preferences for the reference item. Second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. To address these issues, we propose Retrieval Augment Personalized Image GenerAtion guided by Recommendation (RAGAR). Our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users visual preferences for the reference item. Then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. Extensive experiments and human evaluations on three real-world datasets demonstrate that RAGAR achieves significant improvements in both personalization and semantic metrics compared to five baselines.

# Introduction

Personalized image generation has been widely applied in scenarios such as advertising systems and chat software. It aims to render reference images into preferred ones based on user visual preferences. Existing image generation methods often produce similar outputs for different users with similar input, failing to meet users' need for personalization. As a personalized generation method, some prior work extracts user preferences from user historical sequences by treating each item equally, ignoring the varying semantic similarities between the reference item and historical items, such as semantic differences in categories. This disproportionately amplifies the influence of irrelevant items, causing the extracted user preference to deviate from the true visual preferences on the reference item.

**Figure 1:** A comparative example shows the effect of retrieval and recommendation by contrasting generated results with and without them.

By incorporating semantic-based retrieval, more relevant visual preferences can be captured. We propose the assumption that filtering the items semantically related to the reference item enhances user preference modeling.

Another challenge lies in defining and evaluating personalization quantitatively. Current methods rely on either human evaluation or large multi-modal models, both of which are resource-intensive. Recent work evaluates and rectifies generated images heavily based on their pixel similarity to the reference image, resulting in limited personalization. Inspired by advances in recommendation systems, which excel in learning and assessing personalized preferences, we propose using multi-modal recommendation models to evaluate generated images.

Moreover, by combining models and metrics from the recommendation domain, we validate that:
1. Items selected through retrieval are more closely aligned with user preferences than randomly selected items.
2. Retrieving items semantically related to the reference item enhances the learning of user preferences compared to irrelevant items.

Hence, the paper proposes the Retrieval Augment Personalized Image GenerAtion guided by Recommendation (RAGAR), comprising three key modules:
- Retrieval Module: Contains a correlation unit to calculate the semantic similarity between historical and reference items and a fusion unit to fuse the filtered visual features with dynamic weight.
- Generation Module: Applies a balance calibrator to combine general user preferences derived from the fine-tuned LLM and the retrieval-augmented user preferences to produce images.
- Reflection Module: Calculates the loss to balance semantics and personalization, guided by the discriminator.

In summary, the key contributions are as follows:
- We are the first to emphasize the relationship between historical items and the reference item, supported by data analysis on real-world datasets.
- We propose a novel model RAGAR to generate personalized images, utilizing the retrieval assumption realized by calculating the semantic similarity between items to enhance semantic consistency. We design a discriminator to refine and assess the generated images, ensuring higher personalization quality.
- Experiments on three real-world datasets demonstrate that our proposed method can achieve significant improvements in both personalization and semantic alignment compared to five competing methods.

# Assumptions Validation

In this section, we validate our assumptions through data analysis on three real-world datasets: POG, ML-latest, and Stickers.

To evaluate the effectiveness of the retrieval method, we use the multi-modal recommendation model as the ranking model. We compare three types of user historical sequences:
- Ret: Contains the top-k items selected using the retrieval method.
- Exp-Ret (Expanded Retrieval): Contains the items ranked between k and 2k by the retrieval method.
- Random: Contains k randomly selected items.

To assess how well these sequences capture user preferences, we use Recall@10 and NDCG@10 as evaluation metrics.

**Figure 2:** Assumption validation results across three datasets.

Random has the lowest values in metrics, demonstrating that randomly selected items are less effective for preference modeling. Both Ret and Exp-Ret achieve higher metrics, suggesting that items related to the reference items are crucial for modeling user preferences. Moreover, both metrics of Ret are higher than those of Exp-Ret, which implies that retrieving higher ranked items enhances user preference modeling.

# Related Work

## Personalized Image Generation

Advances in diffusion models (DMs) have enabled high-quality image generation. However, personalized image generation, where user preferences guide the generation process, remains underexplored. Some methods rely on natural language instructions, while others combine user interactions and instructions to create personalized advertisements or product images. Recent works bridge DMs and large language models (LLMs), incorporating user historical sequences to extract personalized features. However, some methods rely solely on consistency loss for optimization, overfitting the reference images' features while neglecting personalized preferences. End-to-end multi-modal large language models (MLLMs) show promise in personalizing images from textual instructions but often overlook fine-grained preferences. Widely used evaluation metrics like FID, SSIM, LPIPS, and CLIP score do not align well with human judgment, making them inadequate for assessing personalization. Manual or MLLM-based evaluation is resource-intensive.

**Figure 3:** The overview framework of the proposed RAGAR model.

## Recommendation Systems

The goal of recommendation systems is to deliver personalized content that meets user preferences. These systems often rely on retrieval and recommendation techniques. Under the multi-modal setting, approaches integrate textual and visual features for enhanced commonsense reasoning and capture user-item relationships across modalities, enabling precise preference modeling for personalized recommendations.

# Method

In this section, we formulate our task, introduce key notation, and outline the RAGAR mainstream.

## Task Formulation

For the user u, let the historical sequence be denoted as Sᵤ = {I₁, I₂, ..., I_{N-1}}, and let I_N represent the reference item. Personalized image generation aims to produce an image that aligns with the user's personalized preferences and preserves semantic consistency with I_N. Under the multi-modal setting, each item I_j consists of an image and a text description.

## RAGAR

Our proposed method, RAGAR, is illustrated in Figure 3. To reduce the impact of irrelevant items, we introduce the retrieval module that calculates semantic similarity scores between the reference item and historical items. Then, the module fuses the visual features of the items with the weight corresponding to the scores to produce retrieval-augmented preference features. Next, the generation module employs a fine-tuned LLM to derive general preference features from multi-modal historical sequences, corrected by the retrieval-augmented preference. A diffusion-based generator then generates images conditioned by corrected preference. Finally, we design a reflection module to evaluate the generated images, balancing personalization and semantics to ensure high-quality results.

## Retrieval Module

The retrieval module aims to integrate visual features of historical items that are semantically relevant to the reference item, including the correlation unit and the fusion unit.

**Correlation Unit:** The correlation unit transforms image information from historical items into textual features to compute semantic similarity scores with the reference item. Given the historical sequence Sᵤ and reference item I_N for the user u, we extract the textual description from the item images using a caption model. The textual description is processed into high-dimensional semantic features with a text encoder. Then, we calculate the semantic similarity between historical items and the reference item using cosine similarity. The similarity scores are sorted in descending order, and the top-k items form the retrieval sequence Sᵤ^{ret}, capturing the most relevant items in the user's history.

**Fusion Unit:** The fusion unit integrates preference features from the retrieval sequence by computing a weighted sum of visual features based on their semantic similarity scores, emphasizing high-similarity items while filtering out low-similarity ones.

## Generation Module

The generation module bridges general user preferences with retrieval-augmented preferences to produce personalized images. It operates in three stages:
1. Extracting general preference features using an LLM.
2. Correcting the general preference features with the balance calibrator.
3. Generating personalized images using a diffusion-based generator.

**Prompt Construction:** To capture user preferences from interactions, we transform interacted items into structured textual descriptions suitable for LLM analysis. We construct prompts populated with item captions and text to extract concise keywords with an LLM. We filter out low-frequency keywords and retain the top-n keywords.

**General Preference Extraction:** Given the prompt, the output of the LLM can be divided into text-related and image-related embeddings. To bridge the gap between textual description and visual representation in LLMs, we adopt a Modal Mapper to align the image-related embedding with the image space. We concatenate the text-related embedding and the image-related embedding to obtain the multi-modal feature. To maximize keyword utilization, we use the text encoder to generate keyword features and employ a cross-attention layer to integrate features, obtaining the general preference feature.

**Balance Calibrator:** The retrieval-augmented preference focuses primarily on item features associated with the reference item. To narrow the gap between the retrieval-augmented preference feature and the global preference feature, we minimize the calibrator loss between them. This adjustment ensures that the general preference reflects both the general and retrieval-augmented preferences.

**Image Generation:** With the corrected preference, we utilize the diffusion-based generator to generate images. During training, we sample random noises and apply the reparameterization trick to enable gradient backpropagation through the sampling process. The generator then produces images using the policy gradient update strategy. Additionally, the keyword feature is used to generate a reference-like image, which serves as input to the ranking model for comparison with the personalized images during the reward computation.

## Reflection Module

As the diffusion-based generator, propagating gradients backwards from the generated images is challenging. To address this issue, we design a two-part reflection module that balances personalization and semantics in the generated images, ensuring both user alignment and semantic consistency.

**Personalization Reflection:** To provide personalized feedback on generated images, we leverage a pre-trained multimodal ranking model to evaluate images, reducing the reliance on expensive manual labeling. Given a set of images (reference, keyword-generated, general preference-generated), the ranking model assigns scores and rankings for each item. Images that align better with the user's personalization preference are expected to achieve higher scores and lower ranks. We then employ a reward function to guide the model toward generating images that reflect personalization preferences.

**Semantic Reflection:** To enhance the semantic consistency, we minimize the distance between the mappers output in the generation module and the semantic features of the reference image using the semantic loss.

**Joint Reflection:** We employ LoRA to update a limited set of parameters in LLM, the Modal Mapper, and the attention fusion components. The overall loss is defined as a weighted sum of the calibrator loss, rank loss, and semantic loss.

# Experimentation

We evaluate RAGAR on three different scenarios: commodity, movie poster, and sticker. Our experiments aim to answer the following research questions:
- RQ1: How does RAGAR perform compared to other generative methods in quantitative evaluation metrics?
- RQ2: Does RAGAR outperform baseline methods in human evaluation?
- RQ3: What is the impact of each module of RAGAR on performance? Specifically, how do the retrieval module and the reflection module contribute in the generation?
- RQ4: Can personalized images generated by RAGAR improve the performance in recommendation systems?

## Experimental Settings

**Datasets:** We utilize three real-world datasets to generate personalized images across different scenarios: POG, ML-latest, SER30K. POG is a multi-modal dataset of fashion clothing with user interaction history. ML-latest is a benchmark movie dataset with user ratings. SER30K is a large-scale sticker dataset where each sticker is categorized by theme and annotated with an associated emotion label. Dataset statistics are summarized below.

|        | POG   | ML-latest | SER30K |
|--------|-------|-----------|--------|
| Users  | 1000  | 4689      | 2230   |
| Items  | 19242 | 9742      | 30739  |

**Table 1:** The characteristics of experimental datasets.

**Comparison Methods:** We compare RAGAR with five generative baselines, including three DM-based models (Glide, SD, TI), and two LLM-based models (LaVIT, PMG).

**Evaluation Metrics:** To evaluate personalization, we calculate the CLIP Personalization Score (CPS) and the CLIP Personalization Image Score (CPIS), which measure the similarity between the generated images and the text description or images representing user preferences, respectively. We also calculate the LPIPS and SSIM to quantify the perceptual similarity. Furthermore, we compute the rank change (R) between the original and generated images. To evaluate semantic alignment, we calculate CLIP Score (CS), CLIP Image Score (CIS), LPIPS, and SSIM for reference images.

**Parameter Settings:** All baselines are tuned with a fixed learning rate of 1e-5 and stable diffusion 1.5 is used as the image generator. For RAGAR, the learning rate is set at 1e-5. The number of retrieval items is fixed to 5. The number of noise is set at 3. All experiments are conducted on a single NVIDIA-A100 GPU.

## Performance Comparison (RQ1)

| Datasets  | Methods | R     | CPS   | CPIS  | LPIPS | SSIM  | CS    | CIS   | LPIPS | SSIM  |
|-----------|---------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| POG       | GLIDE   | -54.02| 13.90 | 59.01 | 49.01 | 17.24 | 17.52 | 59.17 | 58.35 | 27.14 |
|           | SD-v1.5 | -25.88| 15.02 | 62.97 | 55.46 | 12.53 | 24.18 | 63.72 | 63.50 | 12.39 |
|           | TI      | -25.64| 15.64 | 63.79 | 53.68 | 15.67 | 24.14 | 67.83 | 62.40 | 15.23 |
|           | LaVIT   | -51.09| 15.40 | 63.46 | 48.77 | 22.65 | 24.07 | 72.04 | 56.42 | 24.79 |
|           | PMG     | -22.96| 14.82 | 56.74 | 55.76 | 5.19  | 18.48 | 57.00 | 63.89 | 5.02  |
|           | RAGAR   | -19.77| 15.79 | 66.88 | 55.93 | 17.99 | 24.28 | 74.55 | 59.29 | 19.08 |
| ML-latest | GLIDE   | -3.20 | 13.27 | 29.52 | 60.34 | 17.99 | 18.21 | 42.31 | 61.88 | 26.22 |
|           | SD-v1.5 | -1.97 | 12.85 | 31.07 | 59.74 | 14.02 | 18.38 | 53.59 | 58.58 | 14.51 |
|           | TI      | -2.58 | 14.42 | 32.85 | 59.33 | 14.29 | 25.25 | 54.17 | 59.35 | 14.79 |
|           | LaVIT   | -1.54 | 14.80 | 34.78 | 57.01 | 18.16 | 21.04 | 57.56 | 56.35 | 19.64 |
|           | PMG     | -0.11 | 15.07 | 41.80 | 53.83 | 7.34  | 14.30 | 43.85 | 53.51 | 7.29  |
|           | RAGAR   | 0.01  | 16.20 | 43.45 | 56.56 | 15.14 | 19.04 | 53.24 | 56.30 | 15.75 |
| Sticker   | GLIDE   | -0.86 | 12.27 | 49.52 | 59.78 | 18.00 | 16.47 | 48.97 | 67.88 | 28.23 |
|           | SD-v1.5 | -0.93 | 12.48 | 51.08 | 59.45 | 17.38 | 18.46 | 49.61 | 68.41 | 16.43 |
|           | TI      | -0.83 | 13.24 | 50.85 | 59.48 | 16.76 | 17.26 | 50.14 | 67.17 | 15.72 |
|           | LaVIT   | -0.75 | 12.64 | 53.05 | 57.82 | 24.41 | 21.12 | 58.99 | 62.27 | 24.43 |
|           | PMG     | -0.73 | 13.31 | 50.97 | 59.93 | 4.94  | 18.72 | 50.92 | 68.81 | 4.73  |
|           | RAGAR   | 0.99  | 14.97 | 55.25 | 56.71 | 25.61 | 17.69 | 64.41 | 61.38 | 27.48 |

**Table 2:** Quantitative performance comparison on three datasets in terms of personalization and semantic alignment.

Traditional DM-based methods perform poorly on personalization metrics. LLM-based methods demonstrate better performance, benefiting from their ability to understand and analyze text comprehensively. Our proposed method, RAGAR, achieves state-of-the-art performance due to the retrieval for sequence and the rank model for training across the three datasets.

## Human Evaluation (RQ2)

| Methods | POG (Per.) | POG (Sen.) | ML-latest (Per.) | ML-latest (Sen.) | Sticker (Per.) | Sticker (Sen.) |
|---------|------------|------------|------------------|------------------|----------------|----------------|
| ORI     | 2.26       | 2.16       | 2.18             |                  |                |                |
| PMG     | 2.08       | 1.62       | 2.10             | 1.94             | 2.24           | 1.88           |
| RAGAR   | 1.66       | 1.46       | 1.74             | 1.66             | 1.58           | 1.12           |

**Table 3:** Human evaluation comparison on both personalization and semantic alignment.

RAGAR outperforms the baselines on both metrics, indicating it better reflects user preference while preserving the semantics.

## Ablation Study (RQ3)

**Effect of retrieval module:** Excluding the retrieval module during training diminishes the model's ability to capture user preferences and interferes with semantics. The retrieval module selects items relevant to user preferences, thereby improving performance. Varying the retrieval number k during training shows that k=5 yields the best results when historical sequence length is 20.

**Effect of rank rewards:** Excluding rank rewards decreases performance. Adding the amount of sample noise boosts the model's performance, with r=5 striking the optimal balance between personalization and semantic alignment.

**Figure 4:** Ablation study of the retrieval module and the rank rewards.

## Case Study

We present cases of personalized images generated by RAGAR and PMG across three datasets. Figure 5 shows eight historical images, a reference image, and generated images for context. The orange dashed boxes highlight the relevant items retrieved by the RAGAR retrieval module, arranged together for clearer visualization. In various scenarios, RAGAR effectively captures user preferences and maintains semantic coherence, outperforming PMG.

**Figure 5:** Examples of image generation across three datasets.

## Auxiliary Generation (RQ4)

Personalized images generated by RAGAR not only excel in display quality but also enhance recommendation tasks. We validate this using the multi-modal recommendation model MICRO. During training of the rank model, reference images are replaced with those generated by RAGAR, PMG, and original images (ORI), respectively.

| Methods | POG R@10 | POG N@10 | ML-latest R@10 | ML-latest N@10 |
|---------|----------|----------|----------------|----------------|
| ORI     | 0.1765   | 0.1544   | 0.3229         | 0.2653         |
| PMG     | 0.1804   | 0.1550   | 0.3213         | 0.2682         |
| RAGAR   | 0.1927   | 0.1668   | 0.3299         | 0.2882         |

**Table 4:** Quantitative comparison of the recommendation performances.

Both PMG and RAGAR outperform ORI across all metrics, demonstrating the benefit of generative methods. Notably, RAGAR shows a stronger capability in modeling user preferences compared to PMG.

# Conclusions and Future Work

In this paper, we proposed RAGAR for personalized image generation that balances personalization and semantics. With retrieval-augmented preferences, multi-modal alignment, and reflection-based optimization, RAGAR generates high-quality images that enhance both user experience and downstream recommendation performance. Extensive experiments demonstrate our method outperforms five baselines. For future work, we intend to unify preferences and noise in the generation process to further enhance personalization.
# Abstract

The problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. However, its practical applicability remains limited, particularly in real-world outdoor scenarios. These environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3D lines, and varying lighting conditions, making the task notoriously difficult. Furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. In this study, we present a small dataset named ClearLines, and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3D line detection algorithms.

# Introduction

At the time of writing, the KITTI dataset was published 12 years ago. For the past three years, the best performing algorithm in its odometry/SLAM challenge has been the camera-based work by Cvii et al. Notably, they did not develop a new SLAM algorithm in this work but focused on finding a better set of camera calibration parameters, which highlights the importance of intrinsic camera calibration for visual SLAM and odometry algorithms. While an exceptionally well summarized theoretical framework for geometric computer vision exists, it took nearly a decade to identify improved camera parameters for one of the most renowned datasets. Interestingly, this refinement did not directly leverage the established theoretical framework. Instead, the dataset’s calibration images, IMU data, and image sequences together with a comprehensive SLAM pipeline, were employed in a labor and computationally expensive process tailored to the dataset, raising questions about the practical applicability and limitations of online camera calibration theories.

Our work addresses one of the areas where this theoretical-practical gap is particularly evident: calibration using straight lines. The underlying theory is simple—straight lines in 3D reproject to straight lines in 2D under the pinhole camera model. Deviations from this assumption reveal the non-perspective components of the camera model, namely the distortion parameters, which can then be estimated. The idea of utilizing 3D lines for calibration was already introduced more than half a century ago. Since then, numerous improvements regarding accuracy, computational efficiency, finding closed-form solutions, or adaptations to different camera and distortion models have been proposed. These works demonstrate excellent calibration results, but need to use synthetic images or images of calibration patterns, require human supervision or manual selection of line points. Other works also take overall robustness into account, which enables the usage of real imagery. Yet, the imagery still needs to display beneficial indoor environments with significant, continuous edges and otherwise mostly unstructured areas to limit the introduction of outliers.

The application of 3D-line based calibration is hindered in areas where taking controlled, indoor images is not feasible. For real outdoor applications, the task faces multiple difficulties, including variation in illumination, diverse scene content, as well as cluttered and non-continuous re-projection of straight 3D lines. These challenges are particularly relevant in the fields of Intelligent Transportation Systems, Autonomous Vehicles, and Platooning, and must be addressed in automated software stacks. Accurate calibration is essential for downstream tasks such as visual SLAM, localization, and map alignment, where distortion errors can accumulate and degrade system performance.

Furthermore, an application such as edge-segment detection, as defined below, is a multi-step process, where the strategy of selecting the best-performing algorithm for each stage is ineffective. The objectives of individual tasks can diverge drastically from the objectives of edge-segments detection. Consequently, results considered successful in their domain may fail to contribute to the overall goal. For example, edge detection, which typically serves as the initial stage in edge-segment pipelines, focuses on achieving a comprehensive set of precise edge pixels. While effective for general tasks such as image segmentation or object proposal generation, this objective differs from the goal of edge-segment detection in the context of camera calibration: identifying long, smooth, and continuous edge-segments that correspond to 3D lines.

Despite recent advances in edge detection and the availability of benchmark datasets for training and evaluating edge detection algorithms, no dedicated dataset exists for evaluating edge-segment detection in the context of camera calibration. This lack of datasets stems from the impracticality of manual labeling, as it requires accurately labeling hundreds of pixels to represent a single edge-segment. This high level of precision and effort makes creating ground truth references for straight edge-segments exceedingly difficult.

To address this gap, we present ClearLines, a small dataset designed specifically for edge-segment detection and provide practical guidance for implementing pipelines tailored to this purpose.

## Definitions and Notation

- **Edge:** An edge is a binary representation in an image where pixel values of 1 indicate the presence of edges, typically identified by intensity gradients. Although useful for general image analysis, raw edge images lack the structural organization required for higher-level tasks such as camera calibration.
- **Edge-Segment:** Edge-segments are refined representations of image edges, formed by grouping edge pixels into coherent structures. These structures are obtained by applying edge-detection algorithms and subsequent post-processing steps, such as edge-linking or edge-chaining. Edge-segments serve as the basis for fitting geometric primitives like lines, circles, and ellipses. However, when derived from outdoor scenes, most edge-segments do not correspond to straight lines in 3D, rendering them unsuitable for camera calibration.
- **Straight Edge-Segment:** A straight edge-segment is a specific type of edge-segment that corresponds to a straight line in 3D space. These segments are geometrically meaningful and can be used for camera calibration. In general, their projections in the image space do not appear as straight lines due to camera distortions. For brevity, we will refer to them simply as straight edge-segments throughout the paper.


## Contributions

- We introduce the compact ClearLines dataset for 3D lines in typical outdoor scenes, sampled from the KITTI dataset and the IAMCV dataset, aimed for the evaluation of camera calibration quality.
- We provide user-friendly evaluation scripts designed to measure performance metrics and validate results derived from this dataset.
- We offer practical guidance for implementing a pipeline to detect straight edge-segments effectively.

Paper Outline: Section II surveys existing literature. Section III details the design and setup of the dataset, including metrics and challenges encountered. Section IV provides an overview of our detection pipeline employed as a prelabeling step. It offers practical recommendations and best practices. Section V evaluates the utility and limitations of the proposed dataset. We address areas where the dataset could be improved or extended to better serve the research community. Finally, Section VI highlights key contributions and conclusions.

# Related Work

Bogdan et al. utilize publicly available panorama images from the Internet to generate synthetic images through a virtual camera. These synthetic images form a dataset used to train a neural network. The method is effective for applications where approximate undistortion is sufficient, as it employs only a single distortion parameter.

With regard to online camera calibration datasets, previous research focused mainly on sports-related applications, such as predicting the camera parameters using the re-projection error based on the landmarks of a football court. Other datasets focused on basketball and hockey-related data. These approaches take into account the structure of the courts, their definitive landmarks, as well as the camera locations to help with the camera calibration tasks. However, these approaches utilize clear, fixed structures that are barely occluded. For automated driving applications, these approaches cannot be utilized.

Automated data generation for line detection leverages fully calibrated and synchronized camera-LiDAR setups with precise ground truth poses. Assuming that the camera distortion model is accurate, such methods could serve as a foundation for straight edge-segment datasets, which is conceptually similar to our detection pipeline presented in Section IV, which is employed for pre-labeling.

Recent advances in edge detection have focused mainly on deep learning-based methods, achieving results that rival or surpass human capabilities. The performance of edge detection algorithms is typically evaluated using three main datasets: Multicue, NYUD, and the widely-used BSDS500 dataset. These datasets provide benchmarks for training and assessing the detection of general edges, with results ranked by their F-score—the harmonic mean of precision and recall. This metric is widely accepted as it aligns well with the requirements of related tasks, such as image segmentation and object proposal generation.

To the knowledge of the authors, no datasets exist for straight-line camera calibration, particularly in the context of automated driving.

# ClearLines Dataset

In order to create the dataset, images were taken from multiple datasets containing the vehicle view perspective, as well as being used in various automated driving applications, such as visual odometry. KITTI was selected as the first source of data as it is well established in the field with data that can be utilized in various automated driving modules. The data was collected in Karlsruhe, Germany, mostly in an urban environment. For the second source of data, the IAMCV dataset was used. This dataset was collected using the Johannes Kepler University, Intelligent Transport Systems research vehicle in real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany.

At the time of writing, 85 images from KITTI, 50 images from IAMCV downtown, and 21 images from IAMCV rural roundabout were selected and labeled.

**Figure 1:** Examples from the ClearLines dataset. First column: Original images from IAMCV dataset and KITTI dataset. Second column: Output of our edge-segment detection pipeline. Third column: Results after manual filtering. All images in black-white for visualization purpose.


| Dataset Source | Environment Type | Number of Images |
| :-- | :-- | :-- |
| KITTI | Urban Karlsruhe, Germany | 85 |
| IAMCV Downtown | Urban environments, Germany | 50 |
| IAMCV RuralRoundabout | Rural roads, roundabouts, highways | 21 |

**Table I:** Characteristics of the ClearLines Dataset

The process of manually labeling all straight edge-segments in a single image is a challenging, time-consuming task. This is apparent, as a single, useful edge-segment consists of hundreds of sub-pixel accurate image positions that need to be accurately labeled. Therefore, the selected data frames from the aforementioned datasets were processed with an edge-segment detection pipeline (see Section IV), which works well in arbitrary outdoor scenes and under significant camera distortion. The pipeline is tuned to have a high recall, which comes with an inevitable loss in precision. The images that did not retain a recall score close to 100% (assessed through visual inspection) were discarded. This process serves as a pre-labeling step. The minimum length of a usable edge-segment is set to 100 pixels, based on empirical observations of calibration reliability. The output of this pipeline is manually filtered to only retain edge-segments corresponding to straight 3D-lines.

Without context it is nearly impossible to distinguish edge-segments on cars and lane markings, which are close to straight in 3D, from true straight edge-segments.

## Metrics

As discussed in Section I, numerous studies have demonstrated that effective camera calibration can be achieved once a sufficiently large set of straight edge-segments is detected. The precise localization of edge-segments in the image is not a limiting factor, and we summarize a sub-pixel accurate approach in Section IV. Instead, two main challenges arise when detecting straight edge-segments in real-world outdoor data:

- **False Positives:** Many detected edge-segments do not correspond to actual straight edges in the scene. Traditional approaches struggle with high false positive rates that cannot be effectively handled.
- **Incomplete Straight Edge-Segments:** Due to factors such as occlusions by small foreground objects or lighting-induced discontinuities, straight edges often appear fragmented in the image. These incomplete segments reduce the signal-to-noise ratio, particularly for shorter segments, which are less effective for calibration.

To evaluate the performance of straight edge-segment detection under these conditions, we adopt the precision-recall framework, which is commonly applied in object detection tasks. These metrics are well-suited to addressing the aforementioned challenges.

- **Precision:** Measures the proportion of detected edge-segments that are true straight edge-segments. It is calculated as:

Precision = True Positives / (True Positives + False Positives)
- **Recall:** Measures the proportion of all true straight edge-segments that are successfully detected. It is calculated as:

Recall = True Positives / (True Positives + False Negatives)

**Figure 2:** Example output from the ClearLines evaluation framework. Green: True positives, Red: False positives, Orange: False negatives. Source: IAMCV dataset.

## Evaluation

To evaluate detection performance against our dataset, users can utilize the evaluation script provided in the repository. Besides calculating the metrics mentioned above, the framework visualizes the performance of the detection algorithm by highlighting true positives, false positives, and missed detections. This visualization provides a comprehensive analysis of the algorithm’s strengths and weaknesses.

# Pipeline Overview

This section describes the methodology of our straight edge-segment detection pipeline. Table II summarizes the precision and recall achieved by this pipeline. As highlighted in Section II, the absence of datasets for validating edge segment detection in camera calibration, coupled with the ineffectiveness of selecting domain-best algorithms for each stage, complicates the justification of methodological choices in traditional scientific terms. We cannot fully overcome this challenge. Part of this section adopts a meta-analytical approach which navigates through the vast body of literature available. We provide recommendations and best practices for each task involved. A high-level overview of the steps involved is presented in Figure 3, with a detailed description provided in the following text.


| Dataset | Precision | Recall |
| :-- | :-- | :-- |
| ClearLines - KITTI | 0.43 | 0.91 |
| ClearLines - IAMCV downtown | 0.46 | 0.98 |
| ClearLines - IAMCV roundabout | 0.24 | 0.98 |

**Table II:** Precision and recall achieved by our pre-labeling pipeline. The pipeline was tuned for high recall, and only images with near-perfect recall were retained, as removing false positives is more practical than adding missed detections.

## Image Preprocessing

In this step, Contrast Limited Adaptive Histogram Equalization (CLAHE) is applied. CLAHE is a technique used to enhance the local contrast of an image by dividing it into small tiles and applying histogram equalization to each region. This method not only enhances contrast but also ensures more similar contrast across different regions of the same image and across images captured under varying lighting conditions. As a result, it enables the use of a consistent set of parameters for subsequent processing steps, regardless of the input image’s lighting variations.

## Edge Detection

Our method is based on the Canny edge algorithm and tailored for straight edge-segment detection. The algorithm is applied twice: once for horizontal and once for vertical edges. This separation prevents undesired connections between edges of different orientations, simplifying subsequent steps.

Edge detection begins with applying a Gaussian filter to smooth the input image, followed by gradient computation using Sobel kernels in the horizontal (Gx) and vertical (Gy) directions. The gradient magnitude and orientation are computed as:

- G = sqrt(Gx^2 + Gy^2)
- θ = arctan(Gy / Gx)

Detected edges are refined through non-maximal suppression to thin the edges and eliminate spurious responses. Two thresholds (Thigh and Tlow) are applied during edge tracking to classify pixels as strong or weak edges. Weak edges are retained only if connected to strong edges, ensuring continuity. For our application, thresholds are set to Tlow = 40 and Thigh = 80.

1. **Gradient Magnitude and Orientation Check:**
    - B(x, y) = G(x, y) > max(G(x_nb1, y_nb1), G(x_nb2, y_nb2))
2. **Non-maximal Suppression:**
    - G(x, y) = G(x, y) if B(x, y), 0 otherwise.
3. **Edge Tracking and Thresholding:**
    - Edge(x, y) = Strong if G(x, y) > Thigh
    - Weak if Tlow < G(x, y) ≤ Thigh
    - None otherwise

## Edge Chaining

An approach based on Suzuki et al. is used to group edge pixels to continuous edge segments. This approach performs a raster scan on the input binary image. When an edge pixel is encountered, it follows connected edge pixels until it revisits the initial edge pixel of the segment, thereby efficiently grouping connected edge pixels to edge-segments. Every continuously connected set of edge pixels becomes an edge-segment. An efficient implementation can be found in the OpenCV library.

**Figure 3:** High-level overview of the edge-segment detection pipeline. Pink indicates input and output data, while blue represents processing steps. A detailed description is provided in the text.

## Subpixel Refinement

To extract meaningful results even from short edge-segments, subpixel accuracy is necessary. We recommend the work of Grompone et al., which is straightforward to implement and improves upon previous methods that occasionally suffer from oscillating artifacts.

## Merging

3D straight lines frequently appear discontinuous in images for various reasons. For instance, edge-segments of a series of identically mounted windows or poles with additional signs attached to them. To address this issue, we identify and merge edge-segments that are associated with the same 3D line. We fit a circle to each detected edge-segment, which presents a challenge as the edge-segments often represent only a small fraction of the associated virtual entity. In other words, the extent of the edge-segments is typically just a portion of the radius of the fitted circle. We employ Taubin’s approach, which demonstrates exceptional performance and robustness, converging even in cases of small, nearly straight segments. For each fitted circle, we calculate the residual to neighboring edge-segments. If it is below one pixel, the edge-segments are associated with the same circle and merged.

**Figure 4:** Illustration of 3D straight lines appearing interrupted in the image due to occlusion and discontinuous features. Examples from the KITTI dataset.

## Filtering

The filtering process is divided into the following stages:

- **Shape-Based Filtering:** Edge-segments are first filtered based on their shape. We impose a minimum length requirement of 100 pixels for an edge-segment to be accepted.
- **Distortion Parameter Estimation:** Robust distortion parameter estimation is then performed to identify edge-segments that conform to a linear approximation after undistortion. Edge-segments that deviate significantly from this approximation are discarded. To account for distortion, we use the widely recognized Brown-Conrady model, which incorporates both radial and tangential distortion effects. The undistorted image coordinates (u, v) are computed as follows:

u = ud - xc * (k1*r^2 + k2*r^4 + k3*r^6) + 2*p1*(ud - xc)*(vd - yc) + p2*(r^2 + 2*(ud - xc)^2)

v = vd - yc * (k1*r^2 + k2*r^4 + k3*r^6) + p1*(r^2 + 2*(vd - yc)^2) + 2*p2*(ud - xc)*(vd - yc)

where r = sqrt((ud - xc)^2 + (vd - yc)^2), k1, k2, and k3 are radial distortion coefficients, p1 and p2 are tangential distortion coefficients, and (xc, yc) is the center of distortion.

To handle outliers effectively, we employ a RANSAC-based scheme. The outlier threshold, which defines the distinction between inliers and outliers, plays a critical role in this process. We advocate for the use of MSAC, a variant that necessitates minimal adjustments to the traditional RANSAC framework. This approach offers a more stable and accurate model ranking, largely unaffected by the specific choice of the outlier-threshold, and consistently yields more precise models.

# Discussion and Future Work

While our ClearLines dataset represents a first step, it currently consists of roughly 150 labeled images. This is sufficient for evaluating algorithm performance but inadequate for training deep learning-based approaches. One promising direction for future work is the expansion of the dataset, particularly through the inclusion of synthetic data. Synthetic data offers the advantage of scalability, allowing for the generation of diverse and extensive datasets. However, it also introduces the issue of assuming a lens distortion model. Recent research highlights significant limitations in current distortion models, particularly in their ability to accurately represent real-world lens distortion.

Another critical area for future work is the evaluation of camera calibration within the context of SLAM (Simultaneous Localization and Mapping). While straight-line-based calibration methods are effective for estimating distortion parameters, they do not provide perspective parameters, such as focal length and principal point. Fortunately, methods for estimating these perspective parameters from undistorted images exist. Combining these methods to derive a complete set of calibration parameters and evaluating their effectiveness within a SLAM framework would be a valuable contribution to the field.

# Conclusion

This study presents the ClearLines dataset, tailored for intrinsic camera calibration using straight lines in real-world outdoor scenarios. Despite its compact size, the dataset addresses challenges in cluttered environments by combining data from KITTI, IAMCV, an edge-segment detection pipeline, and additional manual labeling. This comprehensive approach offers a starting point for intrinsic calibration evaluation. The provided methodological guidance and evaluation framework enable researchers to adapt and validate edge-segment detection pipelines, bridging the gap between theory and practical applications. The paper also discusses the dataset’s limitations and outlines future directions, including potential expansions and improvements, to support further advancements in online camera calibration.

# Acknowledgment

This work was funded by the Austrian Research Promotion Agency FFG, PDrive, project number 12451001.


# Abstract

Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA, a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.

# Introduction

Recent video Large Multi-modal Models (LMMs) have extended Large Language Models (LLMs) with video understanding capabilities. However, understanding and reasoning over the temporal relationships in long videos remains challenging for current models, particularly when analyzing events over time. Recent methods compress video tokens by consolidating key features from adjacent frames, which reduces computational and memory costs but leads to fine-grained temporal information loss. Other works construct synthetic datasets and develop training pipelines to improve temporal reasoning. For example, LLaVA-Video curates large-scale, high-quality video data for video-language instruction fine-tuning, and TPO uses contrast training pairs with preference learning to steer models toward contextually appropriate responses. However, these approaches still struggle to capture fine-grained event dependencies and achieve long-video temporal understanding.

To address these limitations, we introduce TEMPURA, a two-stage training pipeline that unifies dense event segmentation with masked event prediction to build robust video temporal understanding LMMs. In the first stage, TEMPURA enhances video reasoning by teaching the model to infer missing events and generate step-by-step causal explanations. Drawing inspiration from the Fill-in-the-Middle (FIM) paradigm, our training pipeline masks segments of dense video captions and leverages a strong LLM to predict pseudo-events and associated reasoning steps. This training objective maximizes the likelihood of reconstructing both the absent event and its causal narrative from the surrounding context, thereby aligning vision-based inference with language-based reasoning.

The second stage focuses on video segmentation and dense captioning, where the model learns to partition untrimmed videos into non-overlapping events with precise start and end timestamps, each enriched with detailed descriptions. This stage eliminates the need for auxiliary temporal encoders by directly grounding each event in its corresponding video segment.

To support TEMPURA's training pipeline, we introduce VER, a large-scale dataset constructed through a multi-step event annotation pipeline. The pipeline begins by filtering dynamic content from YT-1B and categorizing videos into 10 common categories using Llama-3-72B while discarding videos dominated by interviews, lectures, or speeches. We then applied GPT-40 to segment each video by sampling frames at 1 FPS and arranging them into chronological frame sequence images, which facilitates accurate event boundary detection and dense caption generation. A temporal coherence check further refines the data by filtering out events lacking causal relevance, and a masked event prediction subset reinforces the training signal for temporal inference.

The resulting VER dataset comprises 500K untrimmed videos spanning a total duration of 18K hours, providing dense, timestamp-aligned event captions and structured reasoning that capture fine-grained temporal dynamics across diverse video types.

Our experiments demonstrate the effectiveness of TEMPURA in video temporal understanding tasks. On the Charades-STA benchmark, TEMPURA achieves a mIoU of 39.2, outperforming the baseline by 6.3 points. On the QVHighlights dataset, it attains a HIT1 score of 51.7, surpassing the baseline by 6.9 points. Ablation studies reveal that sequentially applying masked event prediction followed by dense video captioning is crucial for unlocking fine-grained temporal reasoning, thereby enhancing the model's performance in video understanding.

In summary, TEMPURA advances video understanding by integrating dense video captioning with structured causal reasoning to capture fine-grained temporal dynamics in long videos. By decomposing videos into non-overlapping events with precise timestamps and enabling the model to infer missing events through masked prediction, TEMPURA goes beyond holistic processing to achieve robust temporal grounding and causal inference.

**Our contributions are twofold:**
- We develop TEMPURA, a novel training pipeline that leverages masked event prediction to reconstruct missing events with step-by-step causal explanations, and then refines temporal grounding via dense event segmentation and captioning.
- We curate VER, a large-scale dataset of 500K videos spanning 18K hours, annotated with diverse, timestamp-aligned event captions and structured reasoning across 10 common video categories.

# Related Work

## Video Large Multi-modal Models

Researchers have developed video Large Multi-modal Models (LMMs) that address a broad range of video understanding tasks and applications. Many models integrate vision foundation models with Large Language Models to enhance video question answering. Several approaches rely on token merging strategies to fuse visual tokens to enable long video question answering. Models such as LLaVA-OneVision and LLaVA-Next-Interleave, which extend the LLaVA architecture with a simple projector design, demonstrate strong performance across both image and video question answering. The Video-LLaMA series further incorporates an audio modality, supporting more fine-grained multi-modal video comprehension.

Recent works reveal, however, that many LMMs still struggle with temporal reasoning. The limited ability to capture the order of events arises from a shortage of temporally structured video training data and training methods that overlook time causality. To enhance temporal reasoning and understanding in LMMs, we propose masked temporal event learning in our training pipeline, which strengthens the model's ability to predict event order in videos.

## LLM with Reasoning

Recent advancements in LLMs have significantly improved their reasoning capabilities, enabling them to handle complex multi-step problems across various domains. Latest models like DeepSeek-R1 involve reinforcement learning during the training process and achieve state-of-the-art performance across various LLM evaluation benchmarks with strong reasoning ability. On the other hand, parallel efforts in LMMs have similarly advanced image-based reasoning, as demonstrated by studies training the multi-modal models to generate step-by-step solutions for math problems or perform chain-of-thought reasoning for object localization or visual reasoning. This emphasis on step-by-step reasoning in static domains naturally aligns with approaches like LLaVA-CoT, which leverages four sequential stages during model inference to guide models in systematically breaking down problems and delivering more accurate responses.

However, despite these advances, the application of such reasoning capabilities to the video domain, particularly for temporal understanding across dynamic sequences, remains largely unexplored, with few works developing large multi-modal models to address these challenges.

## Temporal Understanding with LMMs

Temporal understanding in videos is essential for comprehending event relationships and causal dependencies, enabling models to interpret actions, anticipate future occurrences, and infer missing visual events. Video LMMs have been developed to facilitate temporal grounding through timestamp-based event localization and video captioning. Models such as TimeMarker, VTimeLLM, and Momentor enhance video comprehension through adaptive token compression, segment-level event alignment, and fine-grained moment localization. Additionally, Trace, TimeSuite, and TimeChat introduce refined temporal modeling techniques, incorporating structured temporal embeddings and improved event localization.

However, these works primarily focus on timestamp retrieval and event segmentation, lacking the ability to infer missing events and reason about causal dependencies between actions. In this work, we address these limitations by incorporating masked event prediction and structured temporal reasoning, enhancing the coherence of event transitions and improving long video comprehension, thereby advancing the fine-grained temporal reasoning ability of video LMMs.

# Method

Understanding and reasoning about video content require the ability to segment a video into meaningful events, establish their temporal order, and infer relationships among them. We define video reasoning as the capability to:
1. Comprehend video progression by identifying distinct events and their temporal boundaries.
2. Analyze event relationships to infer missing or implicit information based on context and logical flow.

To develop a video LMM with robust video reasoning capabilities, we propose a structured training framework comprising two key stages: Masked Event Prediction Reasoning and Video Segmentation and Dense Captioning.

The first stage enables the model to infer missing events and reason about causality within the video context, while the second stage focuses on enhancing the video LMM's ability to decompose a video into temporally grounded event sequences. Together, these stages equip the video LMM with a structured understanding of video narratives, improving its generalization to downstream tasks such as temporal grounding and highlight detection.

## Masked Event Prediction

To enhance the video LMM's ability to reason from video input, we introduce Masked Event Prediction, a novel training stage that aims to enhance the model's understanding of event logical flow, causality, and inductive reasoning with that of a language model. Inspired by Fill-in-the-Middle (FIM), which is widely used in code and text infilling tasks, we extend this concept to the video domain. FIM typically trains a model to predict missing content based on preceding and succeeding contexts. Similarly, we formulate a video event infilling task where the video LMM learns to reconstruct masked video events through inferred text description.

To enable this capability, we leverage the strong reasoning ability of LLMs to generate pseudo-events and reasoning steps based on our dense video caption data. Specifically, we prompt the LLM to infer and construct plausible intermediate events that are masked within a video sequence, ensuring logical consistency with the surrounding context. As shown in Figure 2a, we apply segment-level masking to dense video captions and use the LLM to produce pseudo-events with step-by-step reasoning explanations for the missing segments. These generated pseudo-events and reasoning steps serve as supervised fine-tuning data for the video LMM, enabling it to align its video-based reasoning capability with the strong contextual understanding of LLMs. By training the video LMM on this curated data, we reinforce its ability to infer missing content and establish logical event progression solely from video input.

Formally, given a masked video input, the training objective is to maximize the likelihood of predicting the pseudo-event along with intermediate reasoning steps in a predefined structured format.

This stage bridges the gap between vision and language-based reasoning by aligning the strong logical filling ability of the LLM with the video understanding of the video LMM, making the model more effective on downstream tasks that require complex video comprehension.

## Video Segmentation and Dense Captioning

Dense video captioning is a crucial task for fine-grained video understanding. The resulting video events, grounded with timestamps, provide the necessary context for a language model to establish relationships between events, assisting it in extracting facts and reasoning in response to queries.

In the second training stage, Video Event Segmentation and Temporal Dense Captioning, we teach the model to break down a video into non-overlapping events and describe each event in detail. As illustrated in Figure 2b, we develop the video LMM's temporal awareness by learning to segment a video into non-overlapping events, each defined by its start and end timestamps. We design an instruction to guide the video LMM in transforming a video input into a structured event sequence, where each event is represented by its timestamp and caption.

Unlike previous approaches that utilize extra encoders to model time and saliency scores, we eliminate these components and instead train the model to ground all video segments using their enclosing timestamps. This is achieved by leveraging dense video captions from our VER dataset, which consists of 500K annotated videos. This design choice reduces the need for additional parameters, making the video LMM more versatile for various tasks while ensuring that it learns the structural and temporal progression of videos in this initial training stage.

# VER Data Pipeline

Our TEMPURA training pipeline equips the video LMM with three key capabilities:
1. Segmenting an untrimmed video into non-overlapping events while ensuring full video coverage.
2. Generating detailed descriptions for each segmented event.
3. Building a strong understanding of event logical flow, allowing the model to infer missing events in masked video segments based on contextual cues.

Existing datasets lack large-scale timestamp-aligned dense event captions and dense video coverage, where all events comprehensively describe the entire video. To support TEMPURA training, we construct Video Event Reasoning (VER), a large-scale dataset consisting of 500K untrimmed videos spanning a total duration of 18K hours. Our dataset provides non-overlapping video events with corresponding detailed descriptions. Compared to existing datasets, VER offers longer video hours, a diverse range of video types, and fine-grained event segmentation and captions. Additionally, our TEMPURA masked event prediction training leverages temporal event reasoning data generated from our dense event captions.

## Dataset Construction

The VER data pipeline begins by filtering and categorizing a large video pool. Static videos are removed to ensure a richer temporal structure. Videos are categorized into 10 predefined common video categories using Llama-3-72B to classify based on video captions.

To define event boundaries, GPT-40 is applied by sampling the video at 1 FPS and arranging the frames into frame sequence images. Each frame is indexed with a marker at the top-left corner, and frame sequence images are ordered chronologically. Event time boundaries are ensured to:
1. Not overlap,
2. Cover the entire video,
3. Fall within the video length range.

Once event boundaries are established, GPT-40 is further utilized to generate detailed event descriptions, compiling them into a structured narrative describing the video's progression and event sequences. After filtering and alignment, 500K videos with dense event captions are retained. Each annotated video contains a series of events, where each event includes an event ID, description, and start and end timestamps.

## Masked Event Prediction

To enhance video LMMs temporal reasoning and event inference, strong LLMs are leveraged for causal understanding and masked event prediction. Specifically, an event is randomly masked from the dense event caption and GPT-40 is employed to analyze the structured captions and predict the missing event within the masked time window. To ensure that masked events are logically inferable, videos with uncorrelated event captions are filtered out using GPT-40. This is achieved by prompting GPT-40 to determine whether a causal relationship exists between event captions, applying step-by-step reasoning to arrive at a binary decision. During training, LMMs reasoning capabilities are aligned with LLM event inference by fine-tuning on these structured reasoning processes.

# Experiments

## Implementation

Qwen2.5-VL is adopted as the base model and training is conducted on the collected data. The model is trained using DeepSpeed Zero2, with the global batch size set to 64. The LLM and MLP adapter are fine-tuned with a learning rate of 1e-5, while the vision encoder is trained with a lower learning rate of 2e-6. The original temporal encoding scheme of Qwen2.5-VL tends to misalign for fine-grained temporal grounding, especially in longer videos. To overcome this issue, two key modifications are introduced:
1. Overlay visual timestamps on the upper left corner of each sampled video frame to explicitly mark the temporal context.
2. Adjust the temporal encoding in M-ROPE by assigning a fixed position ID to every sampled frame, ensuring that the model reliably associates each frame with its corresponding timestamp.

Training is conducted on 8 NVIDIA H100 GPUs for 1 epoch in each training stage.

## Video Temporal Understanding

Evaluation focuses on video temporal understanding benchmarks, where the goal is to accurately localize temporal events within videos based on textual queries. Two specific tasks are evaluated: Video Temporal Grounding and Highlight Detection.

### Video Temporal Grounding

Video temporal grounding aims to localize specific moments in a video based on a language query. The model is evaluated on Charades-STA using mean Intersection over Union (mIoU) and Recall@1 at different IoU thresholds, assessing both temporal localization accuracy and recall.

| Method             | LLM Size | Charades-STA mIoU | R1 IoU0.3 | R1 IoU0.5 | R1 IoU0.7 | QVHighlight MAP | HIT1  |
|--------------------|----------|-------------------|-----------|-----------|-----------|-----------------|-------|
| Qwen-VL-2.5        | 3B       | 33.1              | 52.4      | 34.3      | 12.5      | 42.1            | 44.8  |
| TEMPURA (Ours)     | 3B       | 39.2              | 63.8      | 39.3      | 15.0      | 48.3            | 51.7  |

The model provides more granular descriptions of videos, including more detailed content descriptions and greater sensitivity to temporal intervals. Compared to baseline models' average of 15.53 events per video, TEMPURA achieves 27.49 events, demonstrating significantly more detailed temporal understanding and description capabilities.

### Highlight Detection

The goal of highlight detection is to identify relevant time windows within a video and predict saliency scores based on a given language query. The model is evaluated on QVHighlights, reporting mean Average Precision (mAP) and HIT1 as evaluation metrics. HIT1 measures whether the highest-ranked retrieved time window aligns with the ground truth.

TEMPURA improves the baseline model by 6.3 mIoU and either matches or exceeds the state-of-the-art in video temporal grounding—all without any target-task fine-tuning and with a smaller model size.

### Ablation Study

To study the effectiveness of each component in TEMPURA, ablation analysis is performed and mIoU and R1 IoU0.5 on Charades-STA are reported.

| Training Stages | mIoU | R1 IoU0.3 | R1 IoU0.5 |
|-----------------|------|-----------|-----------|
| S2              | 38.4 | 59.1      | 32.8      |
| S2→S1           | 34.0 | 55.6      | 32.6      |
| S1→S2           | 39.2 | 63.9      | 39.3      |

Using mask event prediction as the pre-training stage before dense captioning enhances the model's temporal understanding of videos. Training the model first on dense captioning and then fine-tuning on masked event prediction does not improve the model's ability to follow temporal grounding instructions.

# Supplementary Material

## VER Data Creation Pipeline and Statistics

After data filtering, video frames are uniformly sampled and arranged into frame sequence images. Timestamps are added in each image, and multiple frame sequence images are combined to obtain a grid-formatted composite image as the input. These inputs are first temporally segmented by GPT-4-0, and these segments are considered as events. Based on the generated time segments, GPT-4-0 then generates the descriptions of these events separately. Next, these temporally aligned event descriptions are used to construct masked event prediction data. Data with weak event correlations are filtered out. By inputting the event information into the model in text form, it determines whether each correlation is logically valid. After filtering, 200K reasoning data from 500K dense video captioning data are obtained. The dataset contains videos across 10 domains like travel, DIY, tech reviews, etc.

## Implementation Details

The 3B model from Qwen2.5-VL checkpoint is fully fine-tuned on two tasks, masked event prediction and event segmentation and temporal captioning, in two sequential stages. The training configuration includes:
- Global batch size 64, with 2 samples per device across 8 devices (gradient accumulation steps of 4)
- Learning rates: 1e-5 for the LLM and MLP adapter, 2e-6 for the vision encoder
- Weight decay: 0.1
- Warm-up ratio: 0.03 in the cosine learning rate schedule
- Gradient checkpoint enabled
- Liger kernel integration to reduce memory overhead

A uniform sampling rate at 1 frame per second and fixed frame size of 320×180 pixels are used.

## Qualitative Analysis

Comparisons show that TEMPURA consistently segments events accurately and assigns precise timestamps, outperforming baseline models in both long and short video temporal grounding tasks. TEMPURA produces more fine-grained event captions, as shown by the larger number of event captions produced by the model.

# Figures

**Figure 1:** Our model, TEMPURA, is trained using a two-stage process for video understanding. The model first infers event structures and causal relationships by filling in missing details and reasoning about event sequences, e.g., recognizing that shrimp must be battered before frying. Second, it is learned to partition video into non-overlapping events and describe them in detail. To achieve TEMPURA, we propose a new large-scale dataset consisting of 500k videos with dense event captions.

**Figure 2:** Overview of TEMPURA's two-stage training pipeline. (a) Masked Event Prediction Reasoning: The model infers missing events by analyzing the masked video context, generating both a textual description and step-by-step causal explanations. (b) Video Event Segmentation and Temporal Dense Captioning: The model partitions an untrimmed video into non-overlapping events, each aligned with precise start/end timestamps and enriched with detailed captions, thereby reinforcing a structured understanding of temporal progressions.

**Figure 3:** Structured Training Data for Masked Event Prediction and Dense Event Caption. Human Masked Event Prediction: Given the input video with the masked segment, please step-by-step reason and predict what might happen in the masked time segment based on the context of the video and give your answer in the following format: reasoning steps, description of the predicted event.

**Figure 4:** VER Data Pipeline. The pipeline begins by filtering and categorizing a large video pool. GPT-40 then generates event captions with start/end times, followed by a temporal coherence check that discards invalid events. For valid events, a subset is masked to form a fill-in-the-blank task, and GPT-40 infers the missing segments—ultimately creating a dataset for video temporal understanding.

**Figure 5:** Our model can segment videos into more fine-grained events, capturing subtle transitions and short-duration activities. In contrast, the baseline model QwenVL2.5 tends to generate coarser segments. This difference suggests that our approach is more adept at recognizing and differentiating fine-grained patterns within video sequences, leading to detailed and structured event representation.

# Conclusion

TEMPURA advances video temporal understanding by integrating dense video captioning with structured causal reasoning. By decomposing videos into non-overlapping events with precise timestamps and enabling the model to infer missing events through masked prediction, TEMPURA achieves robust temporal grounding and causal inference. The VER dataset supports this approach, providing comprehensive coverage for training and evaluation. Experiments demonstrate that TEMPURA outperforms strong baselines in both temporal grounding and highlight detection, establishing a new standard for fine-grained video understanding.
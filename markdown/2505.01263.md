# Abstract

Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. Existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. To address these issues, we propose a large language model (LLM) based flow matching architecture for dubbing, named FlowDubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the in-context sequence from movie scripts and reference audio. Then, the proposed semantic-aware learning focuses on capturing LLM semantic knowledge at the phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves acoustic quality in two aspects, which introduces an LLM-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. The demos are available at httpsgalaxycong.github.ioLLM-Flow-Dubber.

Keywords: Movie Dubbing, Visual Voice Cloning, Flow Matching

# Introduction

Movie Dubbing, also known as Visual Voice Cloning (V2C), aims to generate a vivid speech from scripts using a specified timbre conditioned by a single short reference audio while ensuring strict audio-visual synchronization with lip movement from silent video, as shown in Figure 1. It attracts great attention in the multimedia community and promises significant potential in real-world applications such as film post-production and personal speech AIGC.

Previous dubbing works achieve significant progress in improving pronunciation and are dedicated to reducing the word error rate (WER) of generated speech. They can be mainly divided into two groups. Since the dubbing resources are limited in scale (copyright issues) and are always accompanied by background sounds or environmental noise, one class of methods focuses primarily on leveraging external knowledge to improve pronunciation clarity by pre-training on clear large-scale text-to-speech corpus. For example, Speaker2Dubber proposes a two-stage dubbing architecture, which allows the model to first learn pronunciation via multi-task speaker pre-training on Libri-TTS 100 dataset and then optimize duration in stage two. Then, by pre-training on larger TTS corpus Libri-TTS 460 dataset, ProDubber proposes another novel two-stage dubbing method based on Style-TTS2 model, including prosody-enhanced pre-training and acoustic-disentangled prosody adapting. However, these pre-training methods rely too much on the TTS architecture and mainly adopt a Duration Predictor (DP) to produce rough duration without considering intrinsic relevance with lip motion, resulting in poor audio-visual sync.

The other family of methods do not care about pre-training, but try to decline WER by associating other related modality information that helps with pronunciation. For example, StyleDubber proposes a multi-modal style adaptor to learn pronunciation style from the reference audio and generate intermediate representations informed by the facial emotion presented in the video. However, due to the introduction of time stretching, StyleDubber can only keep the global time alignment (i.e., the total length of the synthesized dubbing is consistent with the target), which is still unsatisfactory in fine-grained matching with lip motion, bringing a bad audio-visual experience.

Except for the alignment issues mentioned above, the existing dubbing methods suffer from acoustic quality degradation, even in the advanced two-stage dubbing pre-training methods. For example, Speaker2Dubber freezes the text encoder in the second stage, which helps to maintain pronunciation. However, its use of a traditional FastSpeech2-based transformer fails to handle the complex and diverse spectrum changes, leading to subpar acoustic quality. In addition, the authoritative acoustic quality measurement predictor UTMOS reveals that the acoustic quality of current dubbing methods still requires improvement.

Recent advances in speech tokenization have revolutionized TTS synthesis by bridging the fundamental gap between continuous speech signals and token-based large language models (LLM). Due to LLM demonstrating excellent capability in sequential modeling and contextual understanding, these LLMs-based speech synthesis models achieve human-level expressive and naturalness. However, they are struggling to deal with dubbing task. Although some speed-controllable LLM speech models have been proposed, they still lack visual understanding capabilities, and the synthesized speech struggles to align with the lip motion changing in video. Besides, some speech models focus too much on the naturalness of speech, resulting in poor cloning ability, which makes it challenging to maintain speaker similarity in speech synthesis.

To address these issues, we propose an LLM-based flow matching architecture for dubbing, named FlowDubber, which guarantees high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving state-of-the-art acoustic quality via the proposed voice-enhanced flow matching as shown in Figure 1b.

Specifically, we first design an LLM-based Semantic-aware Learning (LLM-SL), which consists of pre-trained textual LLM Qwen2.5-0.5B to model the in-context sequence from movie scripts and reference audio and semantic-aware phoneme learning focuses on capturing the relevance between phoneme pronunciation unit and LLM semantic knowledge. Then, the proposed dual contrastive aligning (DCA) ensures mutual alignment between lip movement and phoneme sequence, reducing ambiguities where similar phonemes might be confused. Finally, we propose a novel Flow-based Voice Enhancing (FVE) module, which improves the acoustic quality from two sub-components: LLM-based acoustics flow matching guidance and style flow matching prediction. The LLM-based acoustics flow matching guidance focuses on improving the clarity of acoustics during recovering noise to mel-spectrograms by gradient vector field prediction conditioned on LLM.

The main contributions of the paper are as follows:

- We propose a powerful dubbing architecture FlowDubber, which incorporates LLM for semantic learning and flow matching for acoustic modeling to enable high-quality dubbing, including lip-sync, acoustic clarity, speaker similarity.
- We devise an LLM-based Semantic-aware Learning (LLM-SL) to absorb token-level semantic knowledge, which is convenient to achieve precisely lip-sync for dubbing by associating proposed dual contrastive aligning.
- We design a Flow-based Voice Enhancing mechanism to enhance the semantic information from LLM, refining the flow-matching generation process for high speech clarity.
- Extensive experimental results demonstrate the proposed FlowDubber performs favorably against state-of-the-art models on two dubbing benchmark datasets.

# Related Work

## Visual Voice Cloning

The V2C task requires generating high-quality dubbing speech (how a text might be said), but in step with the lip movements portrayed by video character, and in vocal style exemplified by reference audio. Some works focus primarily on improving the pronunciation clarity. For example, SOTA dubbing method ProDubber and Speaker2Dubber propose a pre-training framework to learn clear pronunciation representation from a large-scale text-to-speech corpus. However, they over-rely on the TTS architecture and use an inaccurate duration predictor to estimate the lip speaking time, without considering the intrinsic connection between visual movement and speech content. Besides, StyleDubber attempts to use time stretching to balance articulation and lip-sync. Although the overall length can remain consistent, this does not fundamentally achieve the audio-visual alignment in fine-grained lip-sync.

In this work, we propose FlowDubber, a novel dubbing architecture that combines LLM-based semantic-aware learning with dual contrastive alignment to achieve high-quality lip synchronization, and the proposed flow-matching enhancing mechanism delivers better acoustic quality than existing dubbing methods.

## Large Language Model and Speech Codec

The remarkable success of Large Language Models (LLMs) and the autoregressive (AR) model brings significant advancements in the field of speech synthesis. VALL-E first converts speech into neural codec tokens and treats the speech synthesis as a next-token prediction task. Subsequently, extensive research focuses on speech codecs and LLM-based speech generators to improve the synthesis performance. For example, DAC adopts the residual vector quantization and the multi-scale STFT discriminators to obtain higher-quality discrete speech tokens. Wavtokenizer and X-codec further improved the efficiency of codec and addressed the semantic shortcomings of previous codes. Besides, LLM-based speech synthesis systems combine the AR model with other components or rely on continuous acoustic features to achieve better performance. Recently, Llasa investigated the effects of training-time/inference-time scaling in LLM-based speech synthesis. However, they still lack visual understanding capability, and the generated speech struggles to align with the lip movement.

In this paper, we propose a powerful dubbing model that can achieve the best lip-sync and inherit the acoustic knowledge of LLM via flow-matching learning and phoneme learning, achieving advanced results against all dubbing methods.

## Speech Synthesis and Flow Matching

Flow Matching is a simulation-free approach to training continuous normalizing flow models, capable of modeling arbitrary probability paths and capturing the trajectories represented by diffusion processes. Due to the high quality and faster speed, flow matching has attracted significant attention in speech generation. For example, Matcha-TTS adopts the optimal transport conditional flow matching in single speaker TTS synthesis and Stable-VC adopts it in voice conversion field to improve fidelity. F5-TTS is another powerful TTS model to reconstruct high-quality mel-spectrograms by flow matching. Then, CosyVoice 2.0 has further proven its superior performance by combining flow matching with LLM. However, these methods are not suited to V2C dubbing task due to their inability to perceive proper pause in step with lip motion. Recently, EmoDub introduces classifier guidance in flow matching to control emotions via input labels and intensity.

In contrast, after integrating semantic-aware phoneme learning and lip-motion aligning, we focus on refining the flow-matching generation process to ensure clarity by introducing semantic knowledge from LLM via classifier-free guidance.

# Methods

## Overview

The target of the overall movie dubbing task is Y = FlowDubber(Wr, Tc, Vs), where the Vs represents the given a silent video clip, Wr is a reference waveform used for voice cloning, and Tc is current piece of text to convey speech content. The goal of FlowDubber is to generate a piece of high-quality speech that guarantees precise lip-sync with silent video, high speaker similarity, and clear pronunciation.

The main architecture of the proposed model is shown in Figure 2. Specifically, we introduce pre-trained textual LLM Qwen2.5-0.5B as the backbone of the speech language model to model the in-context sequence from movie scripts and reference audio by discretizing them. Then, the semantic knowledge of speech tokens is adapted to the phoneme level by semantic-aware phoneme learning. Next, the proposed Dual Contrastive Aligning (DCA) ensures the cross model alignment between lip-motion and phoneme level information from LLM. Finally, Flow-based Voice Enhancement (FVE) enhances the fused information from two aspects: Style Flow Matching Prediction aims to keep the speaker similarity and LLM-based Acoustics Flow Matching Guidance focuses on improving the acoustics clarity and suppressing noise.

## LLM-based Semantic-aware Learning

Different from the previous dubbing works, we introduce LLM-based semantic-aware learning to capture the phoneme level pronunciation via the powerful in-context learning capabilities of LLM Qwen2.5-0.5B between text token in movie script and semantic and identity token in reference audio.

**Speech Tokenization:**  
This module aims to transform the speech signal of reference audio Ra into a sequence of semantic tokens hq. It first utilizes a pre-trained self-supervised learning (SSL) model, wav2vec 2.0, to translate speech signals into a semantic embedding sequence. Then, the semantic encoder Sencoder, constructed with 12 ConvNeXt blocks and 2 downsampling blocks, is employed to process and down-sample the sequence further into an encoding sequence h = Hq = VQ(h), h = Sencoder(wav2vec2.0(Ra)), where the output Hq represents semantic tokens from h by Vector Quantization (VQ) layers. Specifically, the VQ adopts factorized codes manner and has an 8192 codebook size and 8 codebook dimensions. Different from CosyVocie 2.0, we also extract global tokens Gq from reference audios mel-spectrogram by Finite Scalar Quantization (FSQ) to keep the speaker characteristics.

**Speech Language Model:**  
Inspired by LLM successes, we employ the pre-trained textual LLM Qwen2.5-0.5B as the backbone of the speech language model. Specifically, we formulate GPT architecture as the next-token prediction paradigm, which adopts a decoder-only autoregressive transformer architecture:  
No = P(o1No | Tq, Hq, Gq, o1, ..., oi-1),  
where oi is the i-th generated speech token, and No is the length of generated speech tokens. The Tq represents text tokens by converting raw text Tc using a byte pair encoding (BPE)-based tokenizer. Hq are semantic tokens and Gq are global tokens from reference audio. By inputting the concatenation of Tq, Gq, Hq and previous special tokens o1, ..., oi-1, model can autoregressively generate current speech tokens oi with in-context semantic knowledge.

**Phoneme Level Semantic-aware Module:**  
Compared with zero-shot TTS, movie dubbing must be strictly matched with lip movements from silent video to achieve audio-visual synchronization. The proposed phoneme-level semantic-aware module aims to capture the semantic knowledge from the speech language model at the phoneme level, which helps preserve pronunciation and enables fine-grained alignment between phoneme unit and lip motion sequence. Specifically, the phoneme-level semantic-aware module consists of cross-modal transformers to calculate the relevance between textual phoneme embedding and LLM speech knowledge, which can be formulated as:  
S-P = P = f_LLM,mul(LN(Z), LN(Z)),  
where LN denotes the layer normalization in cross modal transformer, i = 1, ..., D denotes the number of feed-forwardly layers, and f_e is a position-wise feed-forward sublayer parametrized by θ. The multi-head attention is as follows:  
i,mul = LLMmul(softmax(S-P * Epho * G2P(Tc)), Silm - Silm),  
where G2P denotes the grapheme-to-phoneme to convert raw text T to a phoneme sequence, then the phoneme encoder Epho is used to obtain textual phoneme embedding. The Silm indicates the mapping speech feature from LLM tokens sequence o1N by codec decoder. In this case, the Silm is used as key and value and the textual phoneme embedding is used as query. Finally, we denote the last layer output of cross modal transformer as LLMp ∈ Rlp×dm, which represents the phoneme level semantic feature from LLM. The lp denotes the length of phoneme sequences and dm is the embedding size.

## Dual Contrastive Aligning for Dubbing

This module is designed to solve alignment problems in dubbing by introducing a Dual Contrastive Learning (DAL) between lip movement sequence and phoneme sequence.

**Lip-motion Feature Extractor:**  
To ensure fairness for measuring the alignment ability of DAL, we first use the same extractor to obtain lip motion features from silent videos Vs:  
zm = LipEncoder(LipCrop(Vs)),  
where zm ∈ RLv×dm denotes the output lip motion embedding, Lv indicates the length of lip sequence, and dm is embedding size. The LipCrop uses the face landmarks tool to crop mouth area and LipExtra consists of 3D convolution, ResNet-18, and 1D convolution to capture dynamic lip-motion representation.

**Dual Contrastive Learning:**  
We focus on learning the intrinsic correlation between phoneme-level pronunciation and lip movement to achieve reasonable alignment for movie dubbing. Following the contrastive learning manner, we introduce the InfoNCE loss to encourage the model to distinguish correct lip-phoneme pairs. Specifically, we first treat the lip motion features zm as queries and the phoneme embeddings zp as keys. To establish positive pairs, we align each lip motion frame with its corresponding phoneme based on ground-truth timing annotations by MFA and FPS. This ensures that each zim should be maximally similar to its temporally aligned zy, while being distinct from other phonemes:  
Lmp = -log (exp(zm * zp^T) / sum_j exp(zm * zp^T)),  
where i = 0, Lv - 1 represents the i-th frame of the lip sequence and j = 0, Lt - 1 represents the j-th textual phoneme from whole sequence. The means positive sample pairs, which are calculated in advance based on the ground-truth information during training. Conversely, we introduce a second contrastive loss by reversing the roles (treating phoneme features zp as queries and lip motion embeddings z as keys). In this case, each phoneme seeks to retrieve its temporally aligned lip feature while suppressing mismatched lip frames:  
Lpm = -log (exp(zp * zm^T) / sum_i exp(zp * zm^T)).  
Unlike single-directional contrastive learning, which only aligns one modality to the other, the proposed DCA ensures mutual alignment, reducing ambiguities where similar phonemes might be confused. Finally, we simply use their average as dual contrastive loss:  
Ldua = (Lmp + Lpm) / 2.

**Aligning Phoneme Level Feature:**  
The similarity matrix between textual phoneme embedding and lip movement embedding Sim(zm, zp) is constrained by dual contrastive learning, then the Sim(zm, zp) further guidance to the generation of aligning sequences, including:  
1. lip-related aligning sequences Clip  
2. phoneme-related aligning sequences  

Specifically, the Clip is obtained by multi-head attention module, in which the zp serves as key and value, and the zm is the query. The learnable Sim(zm, zp) is used as multi-head attention weight matrix to provide correct relevance. Next, by monotonic alignment search (MAS), the Sim(zm, zp) ∈ RLv×Lt is flat to mapping table tab ∈ RLt+1, which records the number of video frames corresponding to each phoneme unit. Finally, the tab, LLMp, zp, and Clip are associated to mel-spectrograms level prior conditions:  
FClip, Up(LLMp, zp, tab),  
where Up- is used to expand LLMp and zp to video level according to mapping tab. The F indicates the fusion module, which consists of two 2D upsampling convolutional layers and transformer-based mel-decoder. The output ∈ RLm×dm, where lm and dm represent the length and embedding size of mel-spectrogram.

## Flow-based Voice Enhancing

In this section, we introduce flow-based voice enhancing, including Style Flow Matching Prediction to inject speaker style into flow matching and LLM-based Acoustics Flow Matching Guidance to improve the clarity of generated speech by enhancing semantic information from the LLM.

**Style Flow Matching Prediction:**  
Flow matching generates mel-spectrograms M from Gaussian noise by a vector field. Given mel-spectrogram space with data M, where M ~ q(M). We aim to train a flow matching network to fit q(M) by predicting the probability density path given the vector field, which can be defined as pt(x). Here t ∈ [1], p0(x) = N(x0, I) and p1(x) = q(x). Flow matching can predict the probability density path, gradually transforming x0 ~ p0(x) into M ~ q(M). Our Flow matching prediction network is based on optimal-transport conditional flow matching (OT-CFM). OT-CFM uses a linear interpolation flow x(t) = (1 - t) x0 + t M, which satisfies the marginal condition 0(x) = x0 and 1(x) = M. The gradient field vector field of OT-CFM is ut(t, x) = M - x0. The training objective of Flow matching prediction network is to predict the gradient vector field vt(t, x|SATL, 0), which should be close to ut(t, x|M). Here SATL is style-enhanced mel-spectrograms level prior according to u in Eq. 10. To enhance speakers style, we introduced SATL in flow matching. Specifically, during the flow matching generation process, SATL introduces and enhances style information through affine transformation, which can be formulated as:  
P(SATL) = Y1 * x + Y2,  
where Y1, Y2 are parameters predicted by SATL based on style features. We train Style Flow Matching Prediction Network using condition SATL. We aim for the Flow Matching prediction network to generate the target mel-spectrogram M conditioned on a given SATL. During the inference process, Flow Matching prediction network solves the ODE d(x)/dt = vt(t, x|SATL, 0) from t = 0 to t = 1 to generate a mel-spectrogram M.

**LLM-based Acoustics Flow Matching Guidance:**  
To enhance the clarity of the generated audios, we enhanced the mel-spectrograms level prior conditions by LLM-based Acoustics Flow Matching Guidance. We observed that the generation process in LLM includes semantic tokens and text tokens, which introduce semantic knowledge. Specifically, we enhance LLMs information in flow matching process to improve speech clarity based on classifier-free guidance, which can be formulated as:  
x, vt(t, x|JSATL, xSATL, x),  
Here FClip, Up, zp, tab, refers to zero vector. By enhancing only the LLM information to improve speech clarity with classifier-free guidance, we can control the mel-spectrograms clarity by removing noise and boosting overall quality. As a result, the proposed guidance mechanism strengthens the semantic information accessible to the flow-matching prediction network, thereby refining the gradient vector field generation process to achieve higher speech clarity.

# Experimental Results

## Implementation Details

The semantic tokenizer consists of 12 ConvNeXt blocks and 2 downsampling blocks. The codebook size of VQ is 8192. The ECAPA-TDNN in the global tokenizer features an embedding dimension of 512, while the GE2E in style flow matching prediction is used to extract speaker embedding with dimension of 256. We follow the Qwen2.5 tokenizer to process raw text. The cross model transformer consists of 8 layers with 2 heads, and the dimension size is 256. The solver of conditional flow matching is euler, and we set the number of ODE steps is 10. In dual contrastive aligning, we use 4 heads for multi-head attention with 256 hidden sizes to obtain the attention similarity matrix. The temperature coefficient τ of Lpm and Lmp as both 0.1. We remove the vocoder bias denoiser in HiFiGAN. In data process, the video frames are sampled at 25 FPS and all audios are resampled to 16kHz. The lip region is resized to 96×96 and pre-trained on ResNet-18. The window length, frame size, and hop length in STFT are 640, 1,024, and 160, respectively. For LLM-based Voice Enhancement Guidance, the guidance scale is set between 0.0 and 0.8 empirically. We set the batch size to 16 on Chem dataset and 64 on GRID. Our model is implemented in PyTorch. Both training and inference are implemented with PyTorch on a GeForce RTX 4090 GPU.

## Quantitative Results

**Table 1: Compared with related Dubbing methods on Chem benchmark. For the Dub 1.0 setting, we use the ground truth audio as reference audio, for the Dub 2.0 setting, we use the non-ground truth audio from the same speaker within the dataset as the reference audio which is more aligned with practical usage in dubbing.**

| Dubbing Setting | Methods         | LSE-C | LSE-D | SIM-O | WER   | UTMOS | LSE-C | LSE-D | SIM-O | WER   | UTMOS |
|-----------------|----------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
|                 |                | Dub 1.0                                         | Dub 2.0                                         |
|                 | GT             | 8.12  | 6.59  | 0.927 | 3.85  | 4.18  | 8.12  | 6.59  | 0.927 | 3.85  | 4.18  |
|                 | Imagin         | 1.98  | 12.50 | 0.250 | 62.24 | 2.62  | 1.96  | 12.53 | 0.184 | 68.13 | 2.13  |
|                 | V2C-Net        | 1.97  | 12.17 | 0.154 | 90.47 | 1.81  | 1.82  | 12.09 | 0.087 | 94.59 | 1.76  |
|                 | HPMDubbing     | 7.85  | 7.19  | 0.536 | 16.05 | 2.16  | 3.98  | 9.50  | 0.187 | 29.82 | 2.01  |
|                 | StyleDubber    | 3.87  | 10.92 | 0.607 | 13.14 | 3.02  | 3.74  | 11.00 | 0.540 | 14.18 | 3.04  |
|                 | Speaker2Dubber | 3.76  | 10.56 | 0.663 | 16.98 | 3.61  | 3.45  | 11.17 | 0.583 | 18.10 | 3.64  |
|                 | M2CI-Dub       | 7.99  | 6.91  | 0.621 | 12.85 | 3.15  |       |       |       |       |       |
|                 | EmoDubber      | 8.11  | 6.92  | 0.718 | 11.72 | 3.82  | 8.09  | 6.96  | 0.625 | 12.81 | 3.75  |
|                 | Produbber      | 2.58  | 12.54 | 0.387 | 9.45  | 3.85  | 2.78  | 12.14 | 0.310 | 11.69 | 3.76  |
|                 | Ours           | 8.21  | 6.89  | 0.754 | 9.96  | 3.91  | 8.17  | 6.96  | 0.648 | 12.95 | 3.89  |

**Table 2: The zero shot results under Dub 3.0 setting, which use unseen speaker as reference audio.**

| Methods         | LSE-C | LSE-D | WER   | UTMOS |
|-----------------|-------|-------|-------|-------|
| StyleDubber     | 6.17  | 9.11  | 15.10 | 3.50  |
| Speaker2Dubber  | 4.83  | 10.39 | 15.91 | 3.53  |
| ProDubber       | 5.49  | 9.49  | 14.25 | 3.94  |
| Ours            | 7.43  | 6.64  | 13.96 | 3.98  |

# Figures

**Figure 1:**  
(a) Illustration of V2C task. (b) Illustration of the proposed FlowDubber. It brings a new level of high-quality lip-sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via voice-enhanced flow matching.

**Figure 2:**  
Overall framework of FlowDubber. It consists of LLM-based Semantic-aware Learning (LLM-SL), lip-phoneme Dual Contrastive Aligning (DCA), and Flow-based Voice Enhancing (FVE). Specifically, the LLM-SL includes Qwen2.5-0.5B speech language model and semantic-aware phoneme learning to keep pronunciation while aligning with DCA. The FVE consists of style flow matching prediction and LLM-based acoustics flow matching guidance to improve the acoustics quality.

# Conclusion

We propose FlowDubber, a novel movie dubbing architecture that leverages LLM-based semantic-aware learning, dual contrastive aligning for lip-phoneme synchronization, and flow-based voice enhancement for improved acoustic quality. Extensive experiments on benchmark datasets demonstrate that FlowDubber achieves state-of-the-art performance in terms of lip-sync, pronunciation, acoustic clarity, and speaker similarity, outperforming existing methods.
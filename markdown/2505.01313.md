# Abstract

This paper proposes a neural architecture search space using ResNet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. In addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. The experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the MNIST, Fashion-MNIST and CIFAR100 datasets.

# Introduction

Many studies on genetic algorithm-based neural architecture search (GA-NAS) algorithms focus solely on accuracy as the evaluation metric for selecting the best-performing neural network architectures. However, other metrics, such as loss, can also be valuable in identifying potentially good individuals for further training. In this paper, we propose to use the loss value on the validation set as an auxiliary indicator of DNN in the GA-NAS algorithm.

Using cooperative optimization in multi-objective genetic algorithms, we search for candidate network structures that perform well on both accuracy and loss metrics. This approach expands the search space and increases the possibility of discovering better neural network architectures. To support our search, we propose new genetic operators for variable-length gene codes based on the popular ResNet architecture. After training numerous neural network architectures, those located at the Pareto front undergo additional training to attain the final outcomes.

In our experiments, we compare the multi-objective algorithm with the single-objective algorithm by adjusting the scale factor of each objective value. This comparison provides insights into the potential benefits of using more than one metric in neural architecture search. Overall, our contributions include improved selection methods, novel genetic operators, and a comprehensive comparison of different optimization algorithms.

# Methodology

## Proposed Algorithm

Algorithm 1 shows the framework of our proposed algorithm. N equals the number of individuals in the population. T commonly manages to be the integer part of N/5, which is what we did for our experiments. The function gte in step 10 is the Chebyshev function, which takes the form:

gte(x, z) = max_i |f_i(x) - z_i|

In Algorithm 1, steps 3 and 7 take the longest time because evaluating individuals requires training the corresponding neural networks. The algorithm experiment is based on the BenchENAS platform, which randomly selects a GPU to train each individual neural network in the population and summarizes the results in terms of generations.

In the original version of the MOEAD algorithm, step 11 is placed before step 8, but it is not suitable for this experiment because the individuals in the population change during the training process, which is not conducive to parallelization. The implementation details of the proposed algorithm are presented later in this section. The training strategy of each individual neural network is shown in Appendix B.

**Algorithm 1: Framework of MO-ResNet**

- Input: stop rule of algorithm, M neural network evaluation metrics, N uniformly distributed weight vectors, number of neighbors T, epoch number for training n_ep_train, epoch number for full training n_ep_full.
- Output: set EP.

1. For each i = 1, ..., N:
    - Let B_i = {i_1, ..., i_T}, where i_1, ..., i_T are the nearest T vectors to i.
    - Initialize N individual ResNet architectures according to the genetic coding strategy and train them to obtain M evaluation indicators, let F_Vi = F(x_i).
    - Initialize z = (z_1, ..., z_m).
2. For i = 1 to N:
    - Randomly select two indexes k and l from B_i, apply crossover and mutation operators to generate new individual y_i from x_k, x_l.
    - Train individual y_i to obtain m evaluation metrics.
    - For each j = 1, ..., m, if z_j > f_j(y_i), let z_j = f_j(y_i).
    - Remove all vectors in EP that are dominated by F(y_i), and add F(y_i) to EP if none of the vectors in EP dominate F(y_i).
3. For i = 1 to N:
    - For each j in B_i, if gte(y_i, j, z) < gte(x_j, j, z), let x_j = y_i and F_Vj = F(y_i).
4. If the termination condition is not satisfied, back to line 5.
5. Continue to train the individuals in the EP set for n_ep_full more epochs.

## Search Space

**Figure 1:** Illustration of search space. The fixed part and the ResNet block consist of convolutional layers and pooling layers. In the process of initialization, the number of ResNet blocks N and the number of FFN layers M are randomly generated within a preset range. Then, the number of convolutional and pooling layers in each ResNet block is also randomly generated within another preset range. Last but not least, the hyperparameters of each conv/pool/FFN layer are also randomly selected within a predefined range.

Since for every individual, the fixed part will be the same, it does not need to be encoded. The encoding strategy of the individual could be recursively defined using pseudo regular expressions as follows:

gene_encode = m * resnet_block_encode + FFN_layer_encode^M

resnet_block_encode = m * conv_layer_encode + pool_layer_encode

conv_layer_encode = (conv_identify, filter_width, filter_height, stride_width, stride_height, output_channels)

pool_layer_encode = (pool_identify, filter_width, filter_height, stride_width, stride_height, pool_type)

FFN_layer_encode = m * (FFN_identify, output_neurons)

conv_identify = -1, pool_identify = -2, FFN_identify = -

The encoding position pool_type indicates maximum or average pooling. Note that in a ResNet block, when the input and the output have different lengths, widths, or number of channels, a 1x1 conv is required to downsample the input.

## Genetic Operators

In the representation information of neural network individuals, the number of convolutional layers, pooling layers and fully connected layers are encoded as integers, while the detailed information of each layer (e.g., filter width, output channels) is encoded as real numbers at crossover and mutation, and rounded down when used.

On a holistic level, genetic operators include:
- The mutation operation of adding a new layer when the number of its type is lower than the preset upper limit
- The mutation operation of randomly removing a layer when the number of its type is higher than the preset lower limit
- The mutation operation of changing internal parameters of a layer

When 1 and 2 are triggered, the information representing the number of a certain layer in the encoding changes accordingly. After all mutation operations are performed, if the number of units in an individual is greater than the original one, the additional units form a new ResNet block. Otherwise, units form ResNet blocks in the original way, however, the number of units contained in the last several ResNet blocks may be different. The mutation of FFN Layers does not affect how ResNet blocks are assembled.

On a detailed level, genetic operators include crossover and mutation operations encoded by each information in Figure 1. The crossover operation uses simulated binary crossover (SBX), and the mutation operation uses polynomial mutation (PM).

**Figure 2:** Illustration of genetic operators between individuals. (a)-(c) show the crossover, while (d)-(f) show the mutation.

# Experiments

## Description

The setup of our experiments is shown in Appendix D. Tables 1-3 record the performance of the MO-ResNet model on the MNIST, Fashion-MNIST, and CIFAR-100 datasets, respectively. Each best and average error rate was the result of six independent runs of the individual neural network on the EP set. Note that the first dimension of fitness vector is the error value and the second dimension is the k times the loss value in Algorithm 2. To show more valid numbers, the fitness vector column in Table 1 is shown as 1000 times its original value, and the fitness vector column in Tables 2 and 3 is shown as 100 times its original value.

In many literatures, NAS is represented as the following bi-level optimization problem:

maximize_A C_val(A, w*)  
subject to: w* = argmin_w L_train(w, A)

where A is the architecture, C_val is the accuracy on the validation set. The reporting of accuracy varies in different NAS literature. For the sake of fairness, the results of the validation set and the test set are reported separately in this paper. During our experiments, we found that these two metrics are similar after sufficient training.

### Table 1: Performance of our model on the MNIST dataset.

| model                | k   | EPset   | fitnessvector (1000) | Params | best      | mean      | SE        |
|----------------------|-----|---------|----------------------|--------|-----------|-----------|-----------|
| ScatNet-2            | 1   | -       | -                    | -      | 0.0127    | -         | -         |
| BackEISNN            | 22  | -       | -                    | -      | 0.0033    | 0.0042    | 0.0006    |
| EvoCNN               | 15  | -       | -                    | -      | 0.0118    | 0.0128    | -         |
| EvoAF                | 9   | -       | -                    | -      | 0.007     | -         | -         |
| MO-ResNet simplified | 0.0 | 1       | 6.5, -               | 2.02M  | 0.0038    | 0.0041    | 0.0004    |
| ...                  | ... | ...     | ...                  | ...    | ...       | ...       | ...       |

### Table 2: Performance of our model on the Fashion-MNIST dataset.

| model                | k   | EPset   | fitnessvector (100) | Params | best      | mean      | SE        |
|----------------------|-----|---------|---------------------|--------|-----------|-----------|-----------|
| GoogleNet            | 16  | -       | -                   | 5.98M  | 0.0786    | -         | -         |
| BackEISNN-E          | 22  | -       | -                   | -      | 0.073     | 0.0744    | 0.0008    |
| BackEISNN-D          | 22  | -       | -                   | -      | 0.0655    | 0.0696    | 0.0031    |
| MCNN15               | 12  | -       | -                   | -      | 0.0596    | -         | -         |
| EvoCNN               | 15  | -       | -                   | 6.52M  | 0.0547    | 0.0728    | -         |
| EvoAF                | 9   | -       | -                   | -      | 0.1025    | -         | -         |
| MO-ResNet            | 0.0 | 1       | 6.37, -             | 3.52M  | 0.0409    | 0.0424    | 0.0009    |
| ...                  | ... | ...     | ...                 | ...    | ...       | ...       | ...       |

### Table 3: Performance of our model on the CIFAR-100 dataset.

| model                | k   | EPset   | fitnessvector (100) | Params  | best      | mean      | SE        |
|----------------------|-----|---------|---------------------|---------|-----------|-----------|-----------|
| ResNet-18            | 6   | -       | -                   | 11.22M  | 0.2575    | -         | -         |
| ResNet-50            | 6   | -       | -                   | 23.71M  | 0.2709    | -         | -         |
| PRE-NAS              | 14  | -       | -                   | -       | 0.2651    | 0.2805    | 0.0121    |
| MO-ResNet            | 0.0 | 1       | 32.3, -             | 63.41M  | 0.2362    | 0.2406    | 0.0028    |
| ...                  | ... | ...     | ...                 | ...     | ...       | ...       | ...       |

## Overall Results and Analysis

Experimental outcomes were acquired by configuring various k values. Tables 1-3 display the best error rate and the average error rate in columns 6-7. Data before the slanted line indicates validation set results, and data after refers to test set values. When k=0, the algorithm solely explored the image recognition accuracy metric.

As demonstrated in the tables, MO-ResNet outperformed ScatNet in terms of recognition accuracy on the MNIST dataset when compared to a hand-designed neural network. However, it didn't improve upon BackEISNN concerning the best results, yet it surpassed it in average error rates when adjusting certain k values. The algorithm attained significantly superior model recognition accuracy compared to the GoogleNet, BackEISNN, and MCNN models on the Fashion-MNIST dataset. Moreover, MO-ResNet outperformed the two fundamental ResNet models on the CIFAR100 dataset.

Compared to other NAS techniques, MO-ResNet had better performance than EvoCNN and EvoAF on the MNIST and Fashion-MNIST datasets, and outperformed PRE-NAS on the CIFAR100 dataset. Notably, the best error rate for the MNIST dataset was not achieved when k=0. Furthermore, models with fewer parameters and a comparable or better accuracy than other models could be obtained on both the Fashion-MNIST and CIFAR100 datasets when k was not zero. These observations suggest that auxiliary evaluation metrics could increase the probability of discovering a competitive network that balances the trade-off between parameters and accuracy.

## Transfer Learning

We migrated the optimal network architecture searched on the CIFAR-100 dataset (starting with 7x7 convolution instead of 3x3), and then ran 150,000 steps on the ImageNet dataset, obtaining 39.8% Top-1 error and 18.5% Top-5 error on the training set, and on the test set yielded a Top-1 error of 35.4% and a Top-5 error of 14.2%, for a total of approximately 36 hours of training on 8 A100 GPU graphics cards. It may be possible to try to migrate more of the network structures obtained from training on the CIFAR-100 dataset in future work.

# Conclusion

In this paper, we propose a neural architecture search algorithm MO-ResNet based on the ResNet architecture, which is based on the MOEAD algorithm. After the search is completed, the individual neural networks on the EP set are further trained and the results are compared with the hand-designed neural networks and some other NAS search algorithms. Competitive results were achieved on the MNIST, Fashion-MNIST, and CIFAR-100 datasets. Future work could consider more neural network evaluation metrics and design more combinations of objective functions to find better neural network architectures.

# Acknowledgments

This research has been supported by Key Projects of the Ministry of Science and Technology of the People Republic of China (No.2020YFC0832405).

# Appendix

## A Related Work

EvoCNN algorithm first proposed a variable length encoding strategy in the development of GA-NAS algorithms. The search space of EvoCNN is a one-chain neural network structure. The search space of our work is an extension of EvoCNN. Without shortcut connections, the ResNet block will degenerate to a one-chain structure, so we could apply EvoCNN's genetic operator to each resnet block, then add a shortcut connection to it, and 1x1 convolutional layers for downsampling in the shortcut connection will depend on the case.

Our algorithm performed a multi-objective NAS on 3 datasets, including ablation experiments, with 4 adjustments of the target coefficients on each dataset, thus corresponding to a single-objective search on 12 datasets. Since our search space is based on ResNet, experiments were performed on CIFAR-100, a three-channel dataset, in addition to 2 datasets of the MNIST series for comparison with EvoCNN. Our experiments are based on the BenchENAS platform.

## B Training Strategy

Algorithm 2 describes the procedure for training individual neural networks and calculating the best fitness values. By setting batchsize, the loop body from step 2 to step 19 trains each individual neural network in the population and obtains the fitness vector.

In algorithm 2, stochastic gradient descent is used to train the neural network. Due to the learning rate, the error value is generally dominated by the direction of descent during the training process, but there are often cases when the error value rises instead of falling after one step of updating the neural network weights. To accurately evaluate each individual neural network, in addition to training a specific number of epochs, the current loss value and accuracy will be calculated after each epoch training and the historical optimal value of the individual neural network will be updated.

**Algorithm 2: Multi-objective fitness calculation**

- Input: Population Pt, training epoch number T, training set D_train, validation set D_val, mini-batch size batchsize, learning rate lr, fitness coefficient k, classifier
- Output: the fitness of the population Pt.

For each s in Pt:
- besterror <- 1
- bestloss <- trainstep <- D_train / batchsize
- evalstep <- D_val / batchsize
- Decode individual s into the original ResNet structure, let W = {w1, w2, ...}, initialize W
- For i = 1 to T:
    - For j = 1 to trainsteps:
        - W = W - lr * grad(L(W, D_train, j))
    - sumloss <- 0
    - correct <- 0
    - For j = 1 to evalsteps:
        - sumloss += L(W, D_val, j)
        - correct += correct_acc_j
    - If 1 - correct / D_val < besterror:
        - besterror = 1 - correct / D_val
    - If sumloss / D_val < bestloss:
        - bestloss = sumloss / D_val
    - s.fitness = (besterror, k * bestloss)

## C Figure of Search Space

**Figure 1:** Illustration of search space and encoding strategy.

**Figure 2:** Illustration of genetic operators between individuals.

## D Experimental Setup

### D.1 Environment Configuration

The experiment in this paper is based on BenchENAS platform, which can be deployed in a multi-machine and multi-card GPU environment with one central node server and multiple working nodes.

**Figure 3:** A brief illustration of BenchENAS platform deployment. The central node server is composed of the controller part and redis. After the individuals of each population are generated, the controller distributes all individual neural networks to the worker nodes. In the training process, the controller monitors the GPU status of the working node. When a GPU with sufficient video memory and suitable for training appears, a work node is ordered to create a process on the GPU to train an individual neural network. After the training is completed, the work node reports the result to the redis of the central node. After receiving the redis write event, the central node records the reported data in a disk file. According to the training results, the central node makes selection, crossover and mutation operations. When the training process of an individual ends, the GPU memory occupied will be released and allocated to the next individual. The central node server does not perform specific training tasks.

### D.2 Settings

The MNIST, Fashion-MNIST and CIFAR-100 datasets are used to test the effect of neural architecture search. For all datasets, the crossover probability is set 0.9, all mutation operators probability is set 0.2. In algorithm 1, M is set to 2, and M neural network evaluation metrics are the accuracy and loss value of the validation set, respectively. The optimizer used is SGD with momentum 0.9 and weight decay 0.0005.

The search space of the MNIST dataset contains only one ResNet block without shortcut connection plus fully connected layers, i.e., a one-chain neural network structure, because the MNIST dataset is easier to identify and the search space is simpler. For other datasets, the number of ResNet blocks is chosen randomly between 1 and 10. The fixed part, a 64-channel 3x3 convolution followed by a BatchNorm layer and a Relu layer, is included only for the Fashion-MNIST and CIFAR-100 datasets. All convolutional layers in the ResNet block are also followed by a BatchNorm layer and a Relu layer.

**Figure 4:** Figure of the change trend of error, loss and ratio (i.e., error/loss) values within 15 epochs.

In the MNIST dataset, both the number of generations and the number of individuals were set to 50, which is equal to EvoCNN. For each value of k, 2500 neural network individuals will be evaluated. Due to the longer training time and larger memory occupation of neural networks with shortcut connections, both counts were set to 20 in the Fashion-MNIST dataset and 35 in the CIFAR100 dataset. For each value of k, 400 individuals were trained on the Fashion-MNIST dataset and 1225 individuals on the CIFAR100 dataset.

Some other settings are shown in Table 4.

### Table 4: Some settings for each dataset.

| Parameter                              | MNIST | FashionMNIST | CIFAR-100 |
|-----------------------------------------|-------|--------------|-----------|
| batchsize in algorithm 2                | 64    | 100          | 100       |
| initial lr                              | 0.025 | 0.025        | 0.1       |
| n_ep_train                              | 10    | 12           | 45        |
| n_ep_full                               | 90    | 100          | 260       |
| # of conv layers per ResNet block       | 1-4   | 1-3          |           |
| # of pool layers in one full individual |       | 1-3          |           |
| # of FFN layers in one full individual  | 1-4   | 1-2          |           |
| range of filtersize for conv layers     | 2-20  | 1-5          |           |
| range of outputneurons for FFN layers   | 1000-2000 |        |           |
| available filtersize set for pool layers| 2,4   |              |           |
| range of channels for conv layers       | 3-50  | 3-128        |           |

## E Runtime Analysis

The algorithm needs to train n_ep_train epochs for each k value for each individual neural network in each generation. Assuming a neural network is trained on a dataset for 1 epoch as a time unit, a total of 50*50*10=25000 time units are run on the MNIST dataset, 20*20*12=4800 time units are run on the Fashion-MNIST dataset and 35*35*45=55125 time units are run on the CIFAR100 dataset. However, since we did not introduce residual connectivity on the MNIST dataset, the actual time required to train an individual neural network on the MNIST dataset for 1 time unit was shorter.

In practice, one graphics card can train multiple individual neural networks at the same time, and the NAS training process for multiple datasets can be carried out simultaneously without taking up full graphics memory. The use of multi-machine multi-card distributed training is often affected by network communication fluctuations, changes in the number of graphics card resources, host performance and other uncertainties. If GPU time is calculated based on the number of graphics cards' runtime, the total number of GPU days run for the MNIST dataset experiment is about 6-7, the total number of GPU days run for the Fashion-MNIST dataset experiment is about 4, and the total number of GPU days run for the CIFAR100 dataset experiment is about 37-39.

# Abstract

We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose: adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation.

# Introduction

In the era of generative AI, diffusion models have made remarkable advancements in synthesizing images. The outstanding capability of text-to-image diffusion models has been found to be useful not only for image generation but also for solving important inverse problems, image inpainting, image editing, and even further cross-modal conditional generation, such as depth-conditioned image generation and image-conditioned depth estimation. These works have demonstrated that utilizing the image prior of diffusion models is effective for modeling conditional distribution.

Recently, joint distribution modeling in image and depth modalities has shown that joint distribution modeling not only enables joint generation but also shows potential as a replaceable alternative to existing depth estimation methods and depth-conditional image generation methods within a single unified framework by treating them as special cases of joint modeling. It demonstrates that joint modeling can be easily generalized for various tasks including controllable and conditional generation and estimation.

While the joint modeling approaches have shown their rich versatility, the realism of the generation is limited. In this work, we propose JointDiT, a diffusion transformer designed for solid joint distribution modeling of image and depth. The high-fidelity images and geometrically accurate depth maps visually highlight the joint distribution capability of JointDiT, which has not been achieved before.

Furthermore, we design JointDiT to provide a replaceable alternative to conditional distribution models by constructing a joint distribution at all noise levels for each modality. For instance, the model performs joint generation when both the image and depth map are noise, depth estimation when only the image is clean, and depth-conditioned image generation when only the depth map is clean. To achieve this, we model the joint distribution by harnessing the strong image prior of state-of-the-art diffusion transformer and building a parallel depth branch through joint connection modules. By training on separate noise levels for each modality, JointDiT flexibly facilitates combinatorial tasks of image and depth, including the joint task, by simply controlling the timestep of each branch.

To enable separate noise level training, we also propose two simple yet effective techniques: adaptive scheduling weights and unbalanced timestep sampling strategy, designed for multi-modal diffusion training with separate noise levels. As shown in Figure 2, JointDiT produces smoother, more structured, and more plausible 3D lifting results compared to conditional generation models, which exhibit rough surfaces due to the uncertainty in depth estimation. JointDiT also achieves significantly superior joint generation results compared to previous joint generation methods while demonstrating comparable performance in conditional generation tasks, such as depth estimation and depth-conditioned image generation.

We summarize our contributions as follows:

- We present JointDiT, a model for solid joint distribution modeling between image and depth modalities across all noise levels by leveraging the strong image prior of diffusion transformers. It flexibly facilitates combinatorial tasks, such as joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch.
- We propose adaptive scheduling weights and the unbalanced timestep sampling strategy for separate noise level training in multi-modality, which significantly contribute to improving the performance of combinatorial tasks. Through these techniques, we demonstrate that joint distribution modeling can be the replaceable alternative of conditional generation.


# Preliminaries

**Flow Matching**

Flow matching is generative modeling that learns a time-dependent vector field, which transports one probability distribution to another. It is closely related to Continuous Normalizing Flows (CNFs), which model such transformations via differential equations. We adopt the notation from Lipman et al. to describe the flow matching formulation and objective.

Given the data points $x \in \mathbb{R}^d$ and probability density path $p_{0,1}$ that is a time-dependent probability density function satisfying $p_t(x) dx = 1$, the time-dependent vector field $v_{0,1}$ combines with a flow $\phi_{0,1}$, leading to the ordinary differential equation

$$
\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x)), \quad \phi_0(x) = x
$$

Through the push forward equation, the probability density function at time $t$, i.e., $p_t$, is transformed by $p_t = \phi_{t*} p_0$. The $\phi_{t*}$ and $p_0$ represent the push forward operator and simple prior (e.g., the standard normal distribution), and $p_1$ represents a data distribution.

The objective of flow matching is to estimate $v_t(x)$ using a learnable neural network $v_\theta(t, x)$ by minimizing

$$
L_{FM} = \mathbb{E}_{t, p_t(x)} \|v_\theta(t, x) - v_t(x)\|^2
$$

However, it is intractable to obtain the true vector field $v_t$. To address this, Conditional Flow Matching (CFM) introduces an extra condition by sampling the accessible data sample $x_1$ from unknown data distribution $q(x_1)$. By letting the true vector field be conditioned on $x_1$, that is $v_t(x|x_1)$, a tractable objective is obtained:

$$
L_{CFM} = \mathbb{E}_{t, q(x_1), p_t(x|x_1)} \|v_\theta(t, x) - v_t(x|x_1)\|^2
$$

# Method

Our goal is to develop a unified network that models the joint distributions between images and depth maps across all noise levels. This network can be applied to various tasks, including joint image-depth generation, depth estimation from an image, and depth-conditioned image generation by adjusting the noise levels of each modality.

Inspired by previous works that employ separate noise sampling for images and conditions, we extend the flow matching framework to learn a joint vector field $v_t(x, y|x_1, y_1)$ with two independent timesteps, $t_x$ and $t_y$. Here, $x$ and $y$ represent data points sampled from the RGB image and depth map distributions, respectively.

To estimate $v_t(x, y|x_1, y_1)$, we design a learnable neural network $v_\theta(t_x, t_y, x, y)$ and train it by minimizing the following Joint Conditional Flow Matching (JCFM) loss:

$$
L_{JCFM} = \mathbb{E}_{t_x, t_y, q(x_1, y_1), p_{t_x, t_y}(x, y|x_1, y_1)} \|v_\theta(t_x, t_y, x, y) - v_t(x, y|x_1, y_1)\|^2
$$

Once the network successfully learns to estimate the vector field, various tasks can be performed simply by adjusting $t_x$ and $t_y$:

- $t_x = 0, t_y = 0$: joint generation of both images and depth maps.
- $t_x = 1, t_y = 0$: depth estimation from a given image.
- $t_x = 0, t_y = 1$: depth-conditioned image generation.


## Joint Diffusion Transformer (JointDiT)

**Figure 3:** Overall pipeline of JointDiT. Building on Flux, we introduce a parallel depth branch with trainable LoRAs. The joint connection module facilitates the aligned joint generation. We propose adaptive scheduling weights and an unbalanced timestep sampling strategy for effective training using separate timesteps. MM-DiT and P-DiT represent the multi-modal diffusion transformer and parallel diffusion transformer.

JointDiT is built on Flux, an advanced diffusion transformer model that consists of multi-modal diffusion transformer (MM-DiT) and parallel diffusion transformer (P-DiT) blocks. To harness its strong image prior and the benefits of transformer architectures in dense prediction tasks, we extend it to joint image and depth distribution modeling by introducing a parallel depth branch alongside the pre-trained RGB branch. We add LoRAs to the MM-DiT and P-DiT blocks to process the depth domain, which has a different data distribution from images. Additionally, joint connection modules are introduced in each DiT block to model joint distribution by interchanging features between the RGB and depth branches. We train the LoRAs and joint connection modules while keeping the pre-trained backbone model frozen.

In the joint connection modules, feature exchange for joint distribution modeling occurs within the attention mechanism of each DiT block. We adopt the joint cross-attention module from prior work. This module facilitates joint distribution training by exchanging queries between the RGB and depth branches through attention mechanisms.

To further reinforce this, we propose adaptive scheduling weights, which encourage the joint model to follow the form and structure of the relatively cleaner domain between the RGB and depth branches. Specifically, we adaptively schedule the amount of information transferred between branches by joint cross attention according to the relative cleanliness of the given noisy image and the noisy depth. The adaptive scheduling weights are individually multiplied with the joint cross-attention outputs.

We also introduce the unbalanced timestep sampling strategy to enforce joint distribution modeling at any separate timesteps. The unbalanced timestep sampling strategy samples $t_x$ and $t_y$ independently from two unbalanced timestep distributions, with half probability during training. For the remaining half, the same timesteps are assigned to both $t_x$ and $t_y$. We experimentally validate that these two simple techniques are effective for building a solid joint distribution across all noise levels of images and depth maps, enhancing performance in joint generation, depth-conditioned image generation, and depth estimation.

# Experiments

**Implementation details**

To collect the training dataset, we randomly sample frames from a real-world internal video dataset, which allows us to acquire real-world images with a larger field of view easily. The sampled frames are resized while maintaining their aspect ratio, then center-cropped, to produce 512×512 images. The depth maps and text prompts are generated by Depth-Anything-v2 and LLaVa, respectively. We train our model on the collected dataset, which consists of 50k pairs, for 75k iterations with a batch size of 4 and a learning rate of 1e-5. We consistently use the LoRA rank of 64 and apply text drop with a probability of 10%. The training is conducted on a single NVIDIA H100 GPU for 3.5 days.

## Joint Generation

We demonstrate JointDiT's joint generation results by showing the jointly generated images, depth maps, and their 3D lifting results. To obtain the 3D lifting results, we apply an inverse projection to the generated image by utilizing the generated depth map. We compare our method with LDM3D and JointNet, as they provide raw depth maps that facilitate 3D lifting visualization.

**Figure 4:** 3D lifting results of LDM3D, JointNet, and our method. For each method, we lift the jointly generated image by using the jointly generated depth map. Our method generates highly plausible image-aligned 3D structures, surpassing previous joint generation methods in achieving superior consistency with real 3D space.

Compared to LDM3D and JointNet, our JointDiT shows high-fidelity images, fine-detailed depth maps, and geometrically accurate 3D lifting results. In contrast, the 3D lifting results of JointNet and LDM3D are geometrically inaccurate. We assume that this significant gap in geometric accuracy is caused by the differences in the image prior and the architecture between the baseline models, i.e., stable diffusion and Flux. The Flux model, which is built on the diffusion transformer architecture, demonstrates extraordinary image generation quality over stable diffusion that adopts the UNet-based architecture. In addition, the transformer architecture has been shown effective in depth estimation by several studies since it has the global receptive field different from the fully-convolutional networks.

## Depth Estimation

We assess the depth estimation capability of JointDiT, which can be regarded as joint generation with two extremely different time steps, i.e., $t_x = 1$ and $t_y = 0$. We compare our method with joint generation methods that support depth estimation, e.g., JointNet and UniCon. We also compare with discriminative depth estimation methods and generative depth estimation methods that utilize a diffusion model. Following the evaluation convention of prior work, we compare each method on the NYUv2, ScanNet, KITTI, DIODE, and ETH3D datasets. The evaluation metrics are Absolute Mean Relative Error (AbsRel) and δ1.


| Type | Method | NYUv2 AbsRel | NYUv2 δ1 | ScanNet AbsRel | ScanNet δ1 | KITTI AbsRel | KITTI δ1 | DIODE AbsRel | DIODE δ1 | ETH3D AbsRel | ETH3D δ1 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| Discriminative depth estimation | MiDaS | 11.1 | 88.5 | 12.1 | 84.6 | 23.6 | 63.0 | 33.2 | 71.5 | 18.4 | 75.2 |
|  | DPT | 9.8 | 90.3 | 8.2 | 93.4 | 10.0 | 90.1 | 18.2 | 75.8 | 7.8 | 94.6 |
|  | Depth-Anything-V2 | 4.4 | 97.9 | - | - | 7.5 | 94.8 | 6.5 | 95.4 | 13.2 | 86.2 |
| Generative depth estimation | Marigold | 5.5 | 96.4 | 6.4 | 95.1 | 9.9 | 91.6 | 30.8 | 77.3 | 6.5 | 96.0 |
|  | GeoWizard | 5.2 | 96.6 | 6.1 | 95.3 | 9.7 | 92.1 | 29.7 | 79.2 | 6.4 | 96.1 |
| Generative joint generation | JointNet | 13.7 | 81.9 | 14.7 | 79.5 | 20.9 | 66.7 | 35.0 | 58.5 | 27.1 | 73.5 |
|  | UniCon | 7.9 | 93.9 | 9.2 | 91.9 | - | - | - | - | - | - |
|  | Ours | 5.7 | 96.9 | 6.6 | 95.7 | 10.3 | 88.8 | 27.3 | 71.0 | 16.5 | 96.3 |
|  | Ours (ft) | 5.0 | 97.3 | 5.6 | 96.5 | 10.9 | 87.7 | 26.6 | 71.1 | 9.3 | 96.8 |

**Table 1:** Depth estimation results. Our approach significantly outperforms JointNet and UniCon. Additionally, it achieves comparable performance to generative depth estimation methods, except on the ETH3D dataset.

Compared to joint generation methods, our model achieves superior performance across all evaluation datasets. Figure 5 visualizes the depth estimation results of joint generative methods on the ScanNet dataset. Compared to JointNet and UniCon, our method captures sharp edges and fine details.

## Depth-Conditioned Image Generation

We validate the depth-conditioned image generation quality, another joint generation with different time steps, $t_x = 0$ and $t_y = 1$. We compare our method with ReadoutGuidance, ControlNet, and UniCon using the evaluation setup proposed by UniCon. The evaluation is based on the FID score between the generated and original images, as well as the consistency of depth estimation results, e.g., AbsRel.


| Method | OpenImages 6K FID | OpenImages 6K AbsRel |
| :-- | :-- | :-- |
| Readout-Guidance | 18.72 | 23.19 |
| ControlNet | 13.68 | 9.85 |
| UniCon | 13.21 | 9.26 |
| Ours | 12.62 | 6.99 |

**Table 2:** Depth-conditioned image generation results. Ours achieves the lowest FID and AbsRel.

The lower AbsRel indicates that the generated images accurately preserve the original images' geometry.

## Ablation Studies

We investigate the effectiveness of the proposed techniques for solid joint distribution modeling: adaptive scheduling weights dependent on the noise levels of each modality and unbalanced timestep sampling. For comparison, we train four models on our dataset for 75k iterations, varying the use of adaptive scheduling weights and unbalanced timestep sampling by either applying or omitting.


| Adaptive scheduling weights | Unbalanced timestep | ImageNet 6K FID | ImageNet 6K IS | ImageNet 6K CLIP | Pexel 6K FID | Pexel 6K IS | Pexel 6K CLIP | MSCOCO 30K FID | MSCOCO 30K IS | MSCOCO 30K CLIP |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  | 30.88 | 31.61 | 29.80 | 21.85 | 20.02 | 30.53 | 15.17 | 29.73 | 30.21 |
| ✓ |  | 29.37 | 33.36 | 29.89 | 22.01 | 19.68 | 30.15 | 13.76 | 30.73 | 30.43 |
|  | ✓ | 24.20 | 37.04 | 30.37 | 19.49 | 21.60 | 30.53 | 11.13 | 33.98 | 30.63 |
| ✓ | ✓ | 24.26 | 37.81 | 30.51 | 19.87 | 22.51 | 30.71 | 11.27 | 34.35 | 30.76 |

**Table 3:** Ablation studies on joint generation. Applying adaptive scheduling weights notably improves all evaluation metrics across all datasets. The unbalanced timestep sampling strategy enhances IS and CLIP scores when combined with adaptive scheduling weights.


| Adaptive scheduling weights | Unbalanced timestep | NYUv2 AbsRel | NYUv2 δ1 | Scannet AbsRel | Scannet δ1 | OpenImages 6K AbsRel | OpenImages 6K δ1 | FID | ImageReward Rank1 | Rank2 | Rank3 | Rank4 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  | 8.8 | 92.2 | 10.4 | 88.9 | 11.94 | 26.35 | 26.38 | 24.43 | 22.83 |  |  |
| ✓ |  | 7.8 | 93.9 | 8.7 | 92.4 | 12.51 | 21.77 | 24.82 | 25.48 | 27.93 |  |  |
|  | ✓ | 6.4 | 96.0 | 7.2 | 94.9 | 14.37 | 21.15 | 22.73 | 25.63 | 30.48 |  |  |
| ✓ | ✓ | 5.7 | 96.9 | 6.6 | 95.7 | 12.58 | 30.73 | 26.07 | 24.45 | 18.75 |  |  |

**Table 4:** Ablation studies on depth estimation and depth-conditioned image generation. Adaptive scheduling weights and unbalanced timestep sampling are effective for depth estimation. In depth-conditioned image generation, using both methods together achieved the best performance in ImageReward ranking, with the first rank highest and the last ranking lowest.

# Related Work

Text-to-image diffusion models have demonstrated the effectiveness of diffusion models for text-to-image generation, utilizing a forward and backward process formulated as a Markov chain. Score-based generative models provided a different perspective by modeling the diffusion process as learning the score, i.e., the gradient of the log probability density, from noisy data. This approach was further extended with Stochastic Differential Equations (SDEs), which unify the forward and backward processes in a continuous-time framework. More recently, Flow Matching has been introduced as an alternative to diffusion models, enabling exact likelihood training through Continuous Normalizing Flows (CNFs).

Stable Diffusion improved the efficiency of the diffusion process by operating in a latent space instead of the image space, allowing for a more compact and expressive representation. While early diffusion models primarily relied on U-Net architectures, recent studies have shown that the transformer-based architecture can also be highly effective for diffusion models. The diffusion transformer benefits from the global receptive field and scalability of the transformers, leading to improved generation quality. Models such as Flux and PixArt further demonstrate these advantages.

Joint and conditional diffusion models have also been explored. ControlNet introduced an additional zero-initialized network, enabling fine-grained control over conditional generation tasks such as depth-to-image and pose-to-image synthesis. LooseControl proposed a more relaxed conditioning approach, allowing for weaker or more flexible integration of conditional information. Meanwhile, prior works have shown that the image prior of pre-trained diffusion models can be beneficial for depth estimation. Models such as JointNet, LDM3D, and UniCon were designed to model the joint distribution between images and depth maps using diffusion models. However, these models are based on a U-Net based diffusion architecture, which has a limited receptive field. This is in contrast to recent findings suggesting that diffusion transformers provide a stronger image prior and a global receptive field, which is particularly useful for dense prediction tasks.

# Conclusion

We propose JointDiT, which models solid joint distribution. By harnessing the strong image prior and global receptive property of the state-of-the-art diffusion transformer, we build a unified model capturing multi-modal joint distribution at any separate noise levels. To achieve solid distribution, we present two simple yet effective techniques, called the adaptive scheduling weights dependent on the noise levels of modalities and unbalanced timestep sampling strategy. Through comprehensive experiments, we show that these two techniques notably improve the performance of joint generation, depth estimation, and depth-to-image generation. Our complete model generates images and depth maps, which form highly plausible and image-aligned 3D structures when lifted into 3D space. Furthermore, JointDiT achieves depth estimation performance comparable to that of diffusion-based depth estimation models, demonstrating that a joint distribution model can serve as a replacement for a conditional distribution model.

**Limitation:** To generate an image and its corresponding depth map, JointDiT requires an input batch size of 2 and additional parameters, increasing the network parameters by 19.8% and the sampling time for 20 steps from 1.6 s to 4.7 s. Exploring adaptations to a lightweight diffusion transformer model would be a promising research direction.

# Supplementary Material

## A. Implementation Details

### A.1. Experiment Setup

We consistently use 20 denoising steps and a guidance scale of 3.5 across all experiments.

**Joint generation:** We generate images and their corresponding depth maps by initially setting $t_x = 0$ and $t_y = 0$, i.e., sampling from a standard normal distribution. Joint generation occurs even without a text prompt. Despite being trained only on a 512×512 resolution dataset, JointDiT successfully operates at varying resolutions, such as 1024×1024.

**Depth estimation:** To estimate the depth map from a given image, we set $t_x = 1$ and $t_y = 0$ and provide an empty text prompt. We do not use the ensemble technique. JointDiT can operate at varying resolutions.

**Depth-conditioned image generation:** We generate depth-conditioned images from given text prompts by initially setting $t_x = 0$ and $t_y = 1$. The conditioning depth maps are obtained by Depth-Anything-V2.

### A.2. Data Preprocessing

We randomly sample RGB frames from the internal video dataset, resize so that the smaller dimension is 512 pixels, followed by a 512×512 center crop. Text prompts are obtained using LLaVA. Disparity maps are generated by Depth-Anything-V2 and normalized.

**Synthetic dataset:** For fine-tuning, we utilize the Hypersim, Replica, IRS, and MatrixCity datasets. We unify the ground-truth depth or disparity maps into disparity maps. Invalid regions are replaced with Depth-Anything-V2's estimation.

### A.3. Unbalanced Timestep Sampling Strategy

When applying the unbalanced timestep sampling strategy, the timesteps $t_x$ and $t_y$ are separately sampled from the timestep distributions $f_t$ and $g_t$, respectively, or vice versa. This is applied with a 50% probability during training, while for the remaining 50%, the same timestep sampled from $f_x$ is used for both $t_x$ and $t_y$.

### A.4. Architecture of Depth Branch

To build the depth branch, we add LoRAs to the original Flux architecture. LoRA is added to the components connected before and after the attention mechanisms of the MM-DiT and P-DiT blocks. We additionally add LoRAs to the input stage.


| Type | LoRA applied components |
| :-- | :-- |
| MM-DiT | imgmod.lin, imgattn.qkv, txtmod.lin, txtattn.qkv, imgattn.proj, txtattn.proj |
| P-DiT | linear1, modulation.lin |
| Input stage | vectorin.inlayer, vectorin.outlayer, txtin |

**Table 5:** LoRA-applied components.

## B. Additional Experiments

### B.1. Jointly Generated Image Quality

We quantitatively compare the quality of jointly generated images from JointNet, LDM3D, and our method. For evaluation, we use ImageNet 6K, Pexel 6K, and MSCOCO 30K. We measure the Inception Score (IS), Fréchet Inception Distance (FID), and CLIP similarity.


| Generation modality | Method | ImageNet 6K FID | ImageNet 6K IS | ImageNet 6K CLIP | Pexel 6K FID | Pexel 6K IS | Pexel 6K CLIP | MSCOCO 30K FID | MSCOCO 30K IS | MSCOCO 30K CLIP |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| Image | SD v2.1 | 23.13 | 40.49 | 31.16 | 20.53 | 24.73 | 31.37 | 15.00 | 37.13 | 31.37 |
|  | Flux | 25.96 | 46.12 | 30.90 | 24.71 | 25.32 | 31.09 | 22.85 | 41.40 | 30.77 |
| Image depth | JointNet | 25.92 | 37.23 | 30.50 | 20.28 | 24.94 | 30.72 | 12.62 | 35.88 | 30.80 |
|  | LDM3D | 37.72 | 31.73 | 30.45 | 32.50 | 20.26 | 30.52 | 25.58 | 29.36 | 30.81 |
|  | Ours | 24.26 | 37.81 | 30.51 | 19.87 | 22.51 | 30.71 | 11.27 | 34.35 | 30.76 |

**Table 6:** Quantitative evaluation on jointly generated images.

We also evaluate the human preference score using ImageReward, a trained model that estimates human preference for given text prompts and images.


| Method | ImageNet 6K Rank1 | Rank2 | Rank3 | Pexel 6K Rank1 | Rank2 | Rank3 | MSCOCO 30K Rank1 | Rank2 | Rank3 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| LDM3D | 27.56 | 35.90 | 36.54 | 26.21 | 33.42 | 40.37 | 27.74 | 34.04 | 38.22 |
| JointNet | 29.91 | 33.32 | 36.77 | 31.65 | 33.87 | 34.48 | 28.85 | 35.70 | 35.46 |
| Ours | 42.53 | 30.79 | 26.69 | 42.14 | 32.72 | 25.15 | 43.41 | 30.26 | 26.32 |

**Table 7:** Human preference evaluation on images jointly generated by joint generation methods and Ours.

### B.2. Ablation of the LoRAs Rank

We adopted a LoRA rank of 64 in our JointDiT model. To analyze the effect of the LoRA rank, we train our model with different LoRA ranks and evaluate depth estimation performance on the NYUv2 and ScanNet datasets.


| Lora rank | NYUv2 AbsRel | NYUv2 δ1 | ScanNet AbsRel | ScanNet δ1 |
| :-- | :-- | :-- | :-- | :-- |
| 16 | 9.1 | 90.6 | 9.8 | 89.7 |
| 32 | 6.6 | 95.7 | 8.5 | 92.4 |
| 64 (Ours) | 5.7 | 96.9 | 6.6 | 95.7 |

**Table 8:** Ablation studies of the rank of LoRA.

### B.3. Joint Panorama Generation

JointDiT can be used for RGB-D panorama generation as well. For panorama generation, we strictly follow the JointNet method combining whole and tile-based denoising strategies, to ensure a fair comparison. Figure 7 demonstrates the RGB-D panorama results. Compared to JointNet, JointDiT shows clear and structurally reasonable images along with sharp depth maps.

**Figure 7:** RGB-D panoramic generation results of JointNet and Ours. Our JointDiT generates more three-dimensional and sharper images and depth maps compared to JointNet.

## C. Additional Qualitative Results

**Joint generation:** Utilizing our JointDiT model, we generate images and corresponding depth maps. We visualize the images and depths with their 3D lifting results.

**Figure 8:** Our joint generation results are geometrically reasonable in 3D, with the surface characteristics of the images being well-preserved in the 3D space (e.g., smooth or rough textures).

**Depth estimation:** We visualize the depth estimation results of joint generation methods that support depth estimation, i.e., JointNet, UniCon, and Ours. Figure 9 illustrates the results. Compared to JointNet and UniCon, our method captures fine details in the depth and the shape of thin objects.

**Figure 9:** Depth estimation results of joint generation models. Our method shows sharp and fine-detailed depth visualization, which aligns with the trends observed in the qualitative results.

**Depth-conditioned image generation:** We visualize the depth-conditioned image generation results of JointNet, UniCon, and our method. Figure 10 demonstrates the results. JointNet and UniCon generally generate images that match the given depth and text prompts, but they sometimes do not fully understand the text prompt. In comparison, our JointDiT shows generation results that are well aligned with the given depth and text prompts, and we observe that it generates more realistic images than the other models.

**Figure 10:** Depth-conditioned image generation results of JointNet, UniCon, and Ours. Our JointDiT generates images that better reflect the text prompt and depth map, producing more realistic results compared to other methods.


# Abstract

3D part assembly aims to understand part relationships and predict their 6-DoF poses to construct realistic 3D shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. Existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. However, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. In this paper, we propose a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. Specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away strategy to address overlapping parts, thereby further enhancing the robustness of the method. To verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. The code has been released.

# Introduction

3D part assembly autonomously assembles unordered 3D pieces into a realistic, complete object by predicting the rotations and translations of each piece. This research topic has drawn great attention in the field of robotics in recent years, as it plays a crucial role in advancing robotic manipulation and automation. 3D part assembly is challenging because of the intricate geometries and various possible assembly combinations.

The existing approach to 3D part assembly relies on training machine learning models with extensive manually annotated data, including rotations and scalings. However, the high cost of data collection makes it impractical to create datasets for each task, limiting supervised methods to well-resourced domains like common datasets. This question drove us to search for new methods to reduce reliance on manual labeling.

Diffusion models are a recent class of likelihood-based generative models that model data distributions through an iterative noising and denoising process. Diffusion-based distillation models have demonstrated significant high-fidelity 3D content generation capabilities, highlighting both their theoretical robustness and practical applicability in generating complex 3D contents. Some studies have also demonstrated that a pre-trained diffusion model, leveraging its density estimates, can be transferred to handle various zero-shot tasks, including classification, semantic correspondence, segmentation, and open-vocabulary segmentation. These works further inspire us to explore how to distill the necessary pose transformations in assembly tasks using existing diffusion models.

In this paper, we propose a new algorithm for aligning density estimates to pose transformations. Specifically, we first introduce noise to a shape that has not been correctly positioned. This perturbed shape is then input into the diffusion model. The objective at this stage is to transform the disordered components into a distribution suitable for the diffusion model. By utilizing the denoising process, we can obtain a new point cloud that is closer to an accurate chair shape. However, this new point cloud does not represent a rigid transformation compared to the previous point cloud. To address this issue, we employ the ICP algorithm to align each part as closely as possible. By iteratively repeating this process, we can utilize the diffusion model to convert disordered parts into a complete shape, thereby accomplishing the entire assembly process. Since this process is performed explicitly, it allows us to apply direct pull-in or push-away operations for overlapping or distant parts, which is nearly impossible to achieve with other methods.

To validate our method, we employed four network architectures to predict rotational and translational transformations of parts. These baselines rely on the Shape Chamfer Distance (SCD) for supervised learning, aiming to approximate ground truth derived from reference samples generated by the diffusion model. Quantitative and qualitative results indicate that our method not only outperforms all baseline approaches in zero-shot settings but also surpasses some supervised techniques, underscoring its potential for practical applications.

**Contributions:**

- We propose the first zero-shot assembly method that utilizes density estimates from a diffusion model to achieve continuous and smooth transformations of parts, thereby coherently assembling multiple parts. Theoretical analysis within the paper supports the efficacy of this approach.
- We additionally introduce a push-away strategy to mitigate collisions between parts.
- Results show that our method outperforms all baselines in zero-shot settings and even some supervised approaches, highlighting its practical potential.


# Related Works

## 3D Assembly Modeling

Estimating object pose has been a key research focus for decades. Early research used visual sensors and neural networks for robotic assembly. Later, graph models were employed to capture semantic and geometric relationships among shape components, enabling advancements in assembly-based shape modeling, while a progressive strategy leveraging the recurrent graph learning framework was explored. To explore the diversity of assembly outcomes, several authors propose treating parts' poses as a distribution and achieving part assembly through a diffusion process involving noising and denoising. Furthermore, innovations in network architecture have been advancing concurrently, such as leveraging the Transformer framework to model structural relationships and introducing hierarchical assembly to tackle the challenges associated with managing numerous parts.

Unlike the aforementioned works that rely on manual annotations of each part's rotation and translation, our study aims to explore a novel approach to extracting the necessary pose transformations for assembly tasks. Specifically, we investigate how existing diffusion models can be leveraged to achieve this goal, thereby reducing the dependency on labor-intensive manual labeling.

## Diffusion Model

Diffusion models operate in two steps: adding noise to destroy data structure and reversing this noise to reconstruct it. This enables them to model target distributions and generate diverse content, including images, videos, 3D objects, and audio. Recent studies suggest that diffusion models encode semantic and grouping information, leading to two main research directions. The first leverages the internal representations of diffusion models for various discriminative tasks, requiring minimal additional training, such as zero-shot classification, label-efficient segmentation, and open-vocabulary segmentation. The second focuses on generative tasks, such as bridging 2D diffusion models and 3D generation through Score Distillation Sampling (SDS).

Our work builds upon the generative approach, introducing a theoretically sound and interpretable method to tackle the zero-shot assembly problem effectively.

# Methodology

In this section, we first provide a formal symbolic definition of diffusion models. Next, we introduce the zero-shot method proposed in this paper. Finally, based on our method, we present a new approach to mitigate part overlap.

## Diffusion Model Preliminaries

The diffusion model is a likelihood-based generative model, designed to learn the data distributions. Starting from an underlying data distribution, the model applies a forward process that progressively adds noise to a data sample, creating a sequence of latent variables governed by Gaussian transition kernels. At each time step, the marginal distribution is defined as a Gaussian, with the process converging to a Gaussian prior distribution as the time step approaches its maximum.

The reverse process, which corresponds to the generative process, is designed to reconstruct the original data from a sequence of noisy observations. The conditional distribution at each time step is modeled as a Gaussian with mean and covariance that typically decrease as the time step decreases, reflecting increased model confidence as it approaches the original data point.

The variance term may be parameterized or fixed according to a schedule that depends on the time step. In practice, this variance term may be simplified to depend only on the time step, for instance, by setting it proportional to the pre-defined noise scale. A linear noise schedule could be defined, ensuring that the model exhibits higher certainty in its predictions as it nears the reconstruction of the original data, thereby enhancing the stability and quality of the generated samples.

## Diffusion Based Iterative Zero-Shot Assembler

Denote the input point clouds as $P = \{P_i\}_{i=1}^N$, where $P_i \in \mathbb{R}^{d \times 3}$ corresponds to the i-th part of the 3D shape, consisting of d points in 3D space. In the zero-shot task, each part of the point cloud $P_i$ has a corresponding rigid transformation, described by a quaternion $quat_i \in \mathbb{R}^4$ and a translation vector $trans_i \in \mathbb{R}^3$, representing the rotation and translation of the part. The goal is to predict the pose parameters of the test samples without pose information during training.

To match the current shape to the diffusion model's requirements, we introduce Gaussian noise to the current shape. By utilizing the denoising process, we can obtain a new point cloud that is closer to the shape's distribution. Subsequently, to satisfy the requirements of rigid transformations, we employ ICP to obtain the vector of rotation and translation. We then apply the transformations to the input point cloud to obtain the updated poses, which are then utilized as the input for the next iteration.

By iterating the above process, we can utilize the diffusion model to convert disordered parts into a complete shape, thereby accomplishing the entire assembly process.

## Collision Detection and Handling

Given the explicit nature of our method, it facilitates the direct application of pull-in or push-away operations for either overlapping or distant parts. This strategy is very difficult to implement in existing methods due to their poses being implicitly generated by the model.

To describe the pushing behavior for reducing overlap, the overlap is quantified using a function that counts coincident points. The indicator function determines whether the overlap is below a predefined threshold. The centroids of the part and their intersection region are computed, and the displacement required to separate the part is calculated. This approach computes the necessary displacement direction and distance to reduce the overlap between parts.

**Figure 1:** The overall architecture of our algorithm. Given the misaligned input clouds, we introduce noise to the shape, which helps the diffusion model recognize the data. The diffusion process then refines the input, generating a point cloud closer to the target chair shape. To achieve rigid transformation, we apply the ICP method for alignment, producing updated pose vectors. By iterating this process, the algorithm effectively assembles the disordered parts into the final coherent structure.

# The Theory of Zero-Shot Assembly

As mentioned previously, part assembly seeks to optimize the rotation and translation of each part to transform the unordered input into a coherent realistic object. Let the transformations be represented by an optimizable matrix, then the assembly process can be formulated as an operation on the unordered point cloud.

For previously supervised part assemblers, the optimization is straightforward, where a distance function is minimized between the output and the ground truth. However, optimizing in our case, where no supervised data is available, is non-trivial. Instead of forcing the output to fit a determined object, we tend to make the generation of the output look like a realistic object, i.e., a sample from the distribution of real objects.

Inspired by prior work, we leverage a pre-trained diffusion model for 3D point cloud generation, which implicitly captures the distribution of point clouds in real-world objects. Then we optimize so that the output looks like a sample from this frozen diffusion model. This is achieved through a Score Distillation Sampling (SDS) loss.

In practice, we utilize the ICP algorithm to estimate the transformation between the current and generated point clouds. The smaller the change in the shape of each part, the more accurate the transformation obtained by ICP. We minimize the shape variation by controlling the magnitude of noise added and removed during the processes. By fixing the time step to a small value, we obtain more accurate ICP estimates.

**Figure 2:** Visual comparisons demonstrating our superior assembly performance over baseline methods on PartNet. The first column shows our input at the Excessive level, while the last column presents reference samples obtained through diffusion sampling.

**Figure 3:** Different time steps in our experiments.

# Experiments

## Datasets, Baselines, and Metrics

**Dataset:** We evaluate our method using assembly benchmark datasets PartNet, a large-scale shape dataset with fine-grained and hierarchical part segmentations, for both training and evaluation. We use its Chair subset and adopt the dataset's default train/test/validation splits. The number of parts ranges between 2 and 20. To assess robustness, we established four distinct noise levels: slight, moderate, substantial, and excessive.

**Comparison of Baselines:** We compared our approach with Complement, DGL, IET, HPA, and Simple. Among them, Simple is the baseline we designed, which utilizes seven trainable parameters to represent the rotational and translational transformations of parts.

**Evaluation Metrics:** We employ a set of metrics that include Part Accuracy (PA), Shape Chamfer Distance (SCD), Root Mean Squared Error for Rotation (RMSER), and Translation (RMSET). PA measures the precision of each part, SCD evaluates the overall shape quality, and RMSER and RMSET gauge the accuracy of rotation and translation predictions.

**Fair Part Accuracy (fPA):** Vanilla PA is determined by the Chamfer Distance between components with identical tensor indices. However, certain components, such as stool legs, are permitted to be positioned in regions with inconsistent indices. Therefore, we propose the concept of Fair Part Accuracy (fPA), which reassigns components based on minimum distance and evaluates accuracy accordingly.

## Experiments Results and Analysis

As demonstrated in Table 1, assembly performance declines with increasing noise intensity, thereby validating our noise level designations. The slight noise level evaluates the ability of our method to converge close to the ground truth, while the excessive noise level tests the extremes of performance. Our approach outperforms current methods in addressing the zero-shot challenge. All of our metrics outperform existing methods, except for SCD, as our method emphasizes density estimates from a diffusion model rather than sampling complete shapes.

**Table 1: Quantitative evaluation on zero-shot scenario.**


| Scenario | Methods | SCD | PA | fPA |
| :-- | :-- | :-- | :-- | :-- |
| Zero-Shot | Ours | 45.0 | 9.0 | 12.3 |
| Zero-Shot | Simple | 31.7 | 2.49 | 6.26 |
| Supervised | Complement | 24.1 | 8.78 | - |
| Supervised | DGL | 9.13 | 9.0 | - |

**Figure 4:** Different Views from Baseline-Simple and Ours. Baseline-Simple utilizes supervised learning on point clouds generated by a diffusion model, while our method employs density estimates. The results of Simple are similar to a set of point clouds from reference but do not correspond to a chair shape.

## Comparisons with Supervised Scenario

As indicated in Table 2, our work achieves comparable results to early supervised learning methods. This finding underscores that our method can deliver competitive outcomes even compared to pose-accessible supervised learning. While our work may not yet achieve the performance of existing well-designed supervised learning methods, we hope it offers insights that may contribute to future research in zero-shot learning.

## Analyzing Effectiveness and Failure Cases from a Diffusion Perspective

This subsection presents an experiment designed to illustrate both the effectiveness of the proposed method and its limitations on challenging samples. As shown in the last row of Figure 2, the assembled shape is incorrect (the cyan chair legs are misplaced at the same height as the chair back). We refer to this issue as severe misalignment, typically caused by the uncontrollable randomness in the 3D part initialization process. In such cases, the pretrained diffusion model, under a zero-shot setting, can only partially infer the correspondence between point cloud segments and part semantics, limiting its ability to resolve these errors.

To better understand this behavior, we analyze the method from the diffusion process perspective. Specifically, we take an incorrectly assembled shape with severe misalignment as input and apply the point cloud diffusion model for denoising. During this process, points within parts are allowed to move freely, not constrained to rigid transformations. Some parts are guided to more reasonable positions, resembling the effect of an ICP-like alignment. However, in some cases, parts remain significantly misaligned, and the diffusion model blends points into incorrect regions, revealing why the proposed method struggles with severe misalignments.

**Figure 5:** Ablation Study of Push Action. Based on our method, we can clearly separate overlapping parts, which helps reduce the overlap problem.

## Ablation Study

We propose an improved approach to address the collision issue that arises when identical parts are placed in the same position. To mitigate this issue, we introduce an explicit pushing-apart operation. Traditional model-based training methods struggle to achieve this directly, as the model generates the predicted poses of parts, and adjustments can only be made by tuning the model parameters, which limits operational flexibility. However, in this study, we innovatively incorporate an explicit pushing-apart operation into the original method. This operation effectively separates parts, thereby reducing collision issues.

## Application to Zero-Shot Airplane Assembly

The proposed method is not limited to zero-shot chair assembly; this section further evaluates its effectiveness on airplane models. A point cloud diffusion model pretrained on the airplane category of ShapeNet serves as the generative module. Airplane models are manually segmented into wings, fuselage, and tail, then perturbed with excessive noise before assembly using our method. The method successfully assembles disordered components into coherent airplane structures. In contrast, the baseline Simple tends to force component features to match those of a reference sample, often resulting in incorrect assemblies. Our method yields more plausible results due to its integration of a collision detection and resolution module, which effectively reduces component overlap. These results further demonstrate the generalizability and robustness of the proposed approach across diverse 3D assembly tasks.

**Figure 6:** Different noise levels in our experiments. Illustrations of input under various noise conditions, including slight, moderate, substantial, and excessive noise.

**Figure 7:** Performance of our method in assembly scenarios with different levels of complexity.

**Figure 8:** Visual results of 2D Image Reassembly. This figure showcases the effectiveness of the 2D diffusion model in reassembling fragmented images without simplifying the problem, achieving near-perfect reconstruction for 3×3 image puzzles.

**Figure 9:** The process of step-by-step assembly with 2D Stable Diffusion. By simplifying the experiment's complexity, the 3D part assembly can also be achieved using 2D stable diffusion.

**Figure 10:** Analyzing effectiveness and failure cases from a Diffusion perspective.

**Figure 11:** Visual results of zero-shot airplane assembly.

## Transfer to 2D Diffusion Model

As analyzed in Section 4, our method is not limited to specific types of diffusion models. A pretrained 2D image diffusion model, in theory, can also evaluate the quality of assembly results by differentiably rendering 3D components into 2D space and feeding them into the model. However, in practice, this approach is challenging due to the difficulty of propagating 2D signals back to 3D point clouds and pose parameters. To test this potential, we conducted a simplified experiment where all parts shared a common rotational perturbation. This perturbation was optimized using the SDS loss function. We utilized stable diffusion 2.1 with the prompt "a picture of colorful chair" to generate a realistic chair shape. These images are processed through microrendering, serving as inputs for the diffusion model to obtain SDS loss. This experiment demonstrates that 2D diffusion models have inherent assembly potential, though sophisticated methods are required to fully utilize it.

## Transfer to 2D Image Reassembly

We also examined the capability of 2D diffusion models in 2D image reassembly. Our model can almost perfectly reconstruct 3×3 image fragments without oversimplifying the challenge. 2D Image Reassembly involves reassembling cropped segments of a 2D image. In this experiment, no special architecture was designed; instead, we implemented a classifier utilizing two CNN layers. An MLP was implemented to predict the correct position of each sub-image within the whole picture. Training followed the method outlined earlier, using stable diffusion 2.1 with the prompt "a picture of chair." Our approach successfully handles 2D image reassembly challenges up to a complexity of 4×4.

# Conclusion

In this work, we introduced a novel zero-shot assembly method that leverages the inherent assembly capabilities of general-purpose diffusion models to generate continuous rigid transformations for object assembly without prior training on specific shapes or configurations. It uncovers the implicit assembly abilities of general models, enabling assembly tasks even with previously unseen data. Although there is a substantial amount of research in the field of diffusion models, there is relatively little research on how to apply these models to assembly tasks. Our approach aims to harness existing extensive work on diffusion models to achieve assembly at virtually no additional cost, which represents a meaningful and valuable contribution.

# Limitation and Future Work

The bottom row of Figure 2 exemplifies a failure case, revealing challenges in accurately placing overlapping parts. Especially when they are far from their ground truth positions, using the push operation cannot accurately place overlapping parts. Adjusting random inputs can mitigate this issue, but it is not robust enough. In the future, we plan to investigate an interpretable approach to reposition misplaced parts. This requires us to better explore the underutilized assembly knowledge inherent in general models and to be able to identify which positions have vacancies, thus allowing for the effective transfer of parts. Our goal is to improve the model's performance in assembly tasks without extensive additional costs or extensive supervised learning.

# Appendix

## Zero-Shot 2D Assembly with 2D Diffusion Model

In this experiment, we designed a model consisting of a two-layer Convolutional Neural Network (CNN) and a Multi-Layer Perceptron (MLP). We use trainable embedding as input to represent each sub-image. We used the softmax activation function to predict which sub-image should be chosen for each location, and we applied the SDS loss function to optimize the model. Specifically, we take the predicted values (logits) for each sub-image, multiply them by the sub-image pixels, and place them in their corresponding positions to create a larger image. To input this into the 2D stable diffusion model, we resize the generated image to 512×512 pixels. The entire process is differentiable, allowing the model parameters to be updated through backpropagation.

## Zero-Shot 3D Assembly with 2D Diffusion Model

In this experiment, we designed a model composed of a two-layer Transformer-Encoder and a Multi-Layer Perceptron (MLP), which is used to predict the rotation angles for each part. We still use trainable embedding as input to represent each part. We also used the SDS loss function here to optimize the model. The prompt we used is "a picture of a colorful chair," because each of our parts is colorful. To achieve a differentiable rendering of 3D point clouds, we utilized the FoV Orthographic Camera from the PyTorch3D library. The model can also update its parameters through backpropagation.

## Why the Supervised Method Fails on Zero-Shot Scenario

The translation component is supervised by Euclidean loss, which measures the distance between the predicted translation and ground-truth translation for each fracture. For rotation, we employ the Chamfer distance on the rotated point clouds of the parts. To ensure the overall quality of the assembled shape, we incorporate the Chamfer distance to evaluate the entire shape assembly process. The total loss, integrating these components, is defined as a weighted sum of translation, rotation, and shape losses. Among them, their target for fitting each part is much greater than other requirements. This is another reason why their methods cannot be correctly transferred to zero-shot scenarios.

## Reproducibility

In this work, we ensure that all results are reproducible. All proofs, code, and data will be provided upon acceptance. The theoretical contributions are clearly stated, with all assumptions and limitations outlined, and appropriate citations for the theoretical tools used. The datasets are publicly available, and the existing codes have been released.

**Figure 12:** More visual comparisons on PartNet.


# Abstract

Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent research leverages large language models (LLMs) by utilizing text-based inputs, which suffer severe information loss. To tackle these limitations, we propose a novel Intention-Conditioned Vision-Language (ICVL) model that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into an LLM for future action anticipation. Furthermore, we propose an effective example selection strategy that jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE datasets fully demonstrate the effectiveness and superiority of the proposed method.

# Introduction

Predicting future actions is a crucial task in fields such as human-computer interaction and robotic collaboration. This predictive capability enables systems to provide assistance or initiate interactions at appropriate moments, thereby enhancing both the naturalness and effectiveness of the interaction. For instance, in autonomous driving, accurately anticipating the intentions behind the movements of other vehicles enables the autonomous system to make proactive preparations, reducing potential hazards.

Unlike other video understanding tasks, action anticipation requires not only understanding the observed context but also predicting future actions based on the observation. This task is inherently challenging, as it requires both strong logical reasoning capabilities and the ability to manage the uncertainty of future actions.

To address the task of action anticipation, some approaches start by leveraging video data to learn visual features and model the temporal relationships between the features via neural networks. LSTM/RNN-based and Transformer-based models are employed to model the dependencies between actions as well as the object-action interaction relationships. Furthermore, hybrid architectures combining Transformers and GRUs have been proposed to make predictions. However, visual data is often redundant and low in information density. Methods relying solely on visual data lack prior knowledge, making it challenging to model the intrinsic evolution of actions and rendering them overly sensitive to visual variations.

After achieving significant success in natural language processing, large language models (LLMs) have been adapted to the vision domain, demonstrating remarkable adaptability. This success has motivated researchers to leverage the strong prior knowledge and reasoning capabilities of LLMs to address the challenge of action anticipation. An intuitive solution is to generate appropriate textual substitutes of the original video content, enabling LLMs to predict future actions through a question-answering paradigm. The simplest form of such substitutes is the observed action labels, generated by off-the-shelf action recognition models. Nevertheless, due to the limited accuracy of existing recognition models, these action labels often contain substantial noise and errors. Another approach involves using a Vision-Language Model (VLM) to generate more detailed textual captions. However, fully understanding video content and providing accurate descriptions is inherently challenging. Methods relying solely on textual inputs suffer from significant information loss, limiting the ability of LLMs to make precise and contextually informed predictions.

To fully preserve the visual content and extract crucial clues for long-term action anticipation (LTA), we propose a novel Intention-Conditioned Vision-Language (ICVL) model that integrates complementary visual and textual information with the commonsense prior knowledge of LLMs. This approach addresses the limitations of single-modality methods by combining rich visual features with high-level behavioral intentions to boost the performance of LLMs in LTA. On the one hand, visual data, such as the presence of objects like a bowl, offers valuable insights for future action prediction, even when these objects are not explicitly mentioned in textual substitutes. On the other hand, behavioral intentions, such as cleaning the kitchen, represent high-level semantic concepts that guide the evolution of actions over time. By capturing these intentions, we can better understand the progression of actions and gain critical insights for predicting future events.

Specifically, our ICVL model employs a Vision-Language Model (VLM) to infer behavioral intentions directly from video data by analyzing the entire temporal dynamics of the observed video. This allows the model to generate textual features that capture the high-level intentions behind the actions. We then introduce a novel fusion mechanism, Intention-Context Attention Fusion (ICAF), which integrates visual features with the inferred behavioral intentions to produce intention-enhanced visual embeddings. These embeddings are more discriminative, with reduced redundancy and higher information density, as they focus on the most relevant aspects of the visual data guided by behavioral intentions. Combined with carefully designed textual prompts, these enriched embeddings are fed into the LLM, which has been fine-tuned in a parameter-efficient manner to adapt to the specific task of action anticipation.

Additionally, to further improve the reasoning capabilities of LLMs, we propose an effective example selection mechanism that leverages both visual and textual modalities to identify the most relevant examples for in-context learning. This ensures that the LLM is provided with the most pertinent data, enhancing its ability to make informed and accurate predictions. Extensive experiments across three datasets demonstrate the effectiveness of our approach, validating the strength of combining vision, intention, and LLMs for long-term action anticipation.

Our key contributions can be summarized as follows:

- We propose a novel multimodal framework for long-term action anticipation that fully leverages both visual and textual information, integrating them with the prior knowledge and reasoning capabilities of LLMs.
- We introduce intention-enhanced visual features by fusing visual data with inferred behavioral intentions, addressing information loss and enriching the representations for more precise and reliable action predictions.
- We design an effective example selection mechanism that integrates both visual and textual modalities to identify the most relevant examples, improving long-term action anticipation via enhanced in-context learning.
- Extensive experiments demonstrate the effectiveness and superiority of our proposed method, achieving state-of-the-art performance on Ego4D, EPIC-Kitchens-55 and EGTEA GAZE datasets.

# Related Works

## Action Anticipation

Action anticipation aims at inferring future actions based on a period of observed video, and can be categorized into long-term and short-term anticipation tasks depending on the time to predict. Our work focuses on the long-term action anticipation. Previous methods mainly make predictions by modeling the temporal dynamics solely from the visual features. Rolling LSTM and Transformer-based architectures have been used to encode input and make recurrent predictions for future actions. Recently, approaches utilizing LLMs have become increasingly popular, substituting video content with observed action labels or using image captioning models to generate descriptions from multiple aspects of the video. However, these methods depend excessively on a single modality. Vision-based methods face information redundancy and lack prior knowledge, making it challenging to model long-term temporal relationships and make accurate predictions. Text-based methods suffer from severe information loss and noise, struggling to generate accurate action recognition results or detailed video descriptions, thereby impairing predictive accuracy.

**Figure 1:** Illustration of different action anticipation methods. (a) Vision-based methods. (b) Text-based methods. (c) Our proposed Intention-Conditioned Vision-Language (ICVL) model.

In contrast to the aforementioned methods, we leverage the contextual information from visual data, the intentional information from textual descriptions, and the commonsense reasoning capabilities of LLMs to enhance long-term action anticipation.

## Large Language Model

LLMs based on the Transformer architecture typically make predictions in an autoregressive manner. These models, often containing billions of parameters, are trained on vast amounts of data and have demonstrated remarkable performance in natural language processing. Notable examples include GPT-4 and LLAMA. To adapt these models more effectively to downstream tasks, some approaches propose to fine-tune part of the model parameters on specific datasets, while others attempt to leverage the LLMs' in-context learning ability by providing high quality examples. Both strategies can further enhance the performance of LLMs, yielding higher-quality responses. Moreover, LLMs exhibit a profound understanding of textual structure and semantics, as well as the ability to comprehend rich information from other modalities after alignment, such as visual and audio data. As a result, LLMs have been successfully applied to the visual domain, demonstrating significant performance.

Inspired by this approach and the unique nature of the LTA task to predict future actions, we creatively leverage high-level behavioral intentions to bridge past and future actions. By combining intentions with visual features, we generate intention-enhanced visual embeddings, which improve prediction by making visual features more discriminative and providing cues related to action evolution.

# The Proposed Method: ICVL

We introduce our proposed Intention-Conditioned Vision-Language (ICVL) model in this section, which combines LLM with intention-enhanced visual embeddings and carefully designed textual prompts to predict future actions.

**Figure 2:** Illustration of Intention-Conditioned Vision-Language (ICVL) model. Given a video, we use a VLM, a visual encoder, and an action recognition model to extract behavioral intention, original visual embeddings, and observed action labels respectively. The behavioral intention and visual embeddings are then integrated into the intention-enhanced visual embeddings through our proposed Intention-Context Attention Fusion (ICAF) module, in which visual features serve as the keys (K) and values (V), while textual intention features act as the queries (Q). Then we consider both visual similarity and textual similarity based on observed action labels to select examples from the training set for in-context learning. Finally, the textual prompt—composed of instructions, observed action labels, and selected examples—along with the intention-enhanced visual embeddings, are fed into the LLM to generate predictions for future action sequences.

## Action Recognition and Intention Inference

**Action labels.** Long-term action anticipation requires predicting future actions over an extended period, where upcoming actions are inferred from an observed video. The observed video can be divided into several segments, with each segment corresponding to an action label. These action labels are represented as verb-noun pairs, such as "put plant." We use the CLIP visual encoder to extract video features and get visual embeddings. Then we use a Transformer-based architecture as the action recognition model, which consists of a Transformer encoder to model the visual embeddings and two MLP heads to decode the verb and noun. The action recognition model is trained using the cross-entropy loss between predictions and ground-truth action labels.

**Intention Inference.** Human actions are inherently driven by high-level intentions, which guide the evolution of actions over time. Therefore, understanding an individual's intention is crucial for accurately predicting future actions. Instead of inferring intentions from noisy action labels, we leverage observed visual cues through a VLM to obtain more accurate intentions. We uniformly sample frames from an observed video and employ a pretrained VLM to sequentially infer behavioral intentions from each frame in chronological order, using the prompt "What does the person want to do?" The final behavioral intention is derived from the text generated by the VLM based on the last frame and its corresponding context.

## Intention-Context Attention Fusion

Multi-modality fusion has been proven effective in short-term action anticipation tasks. However, in the field of long-term action anticipation, this approach remains underexplored, particularly for LLM-based methods. We introduce our proposed Intention-Context Attention Fusion strategy.

**Visual and Intention embeddings.** For each video segment, we use a pretrained vision encoder to extract the original visual embeddings through uniformly sampled video frames. These visual embeddings serve as visual prompts, which are integrated with textual intention prompts as input for the LLM. To enhance the model's understanding of sequential information, we add 2D fixed positional encoding to the visual embeddings. Furthermore, to align the dimension of visual embeddings with the embedding space of the LLM, a linear project layer is used to get the final visual embeddings. For the intention embeddings, a pretrained text encoder is used to encode the behavioral intentions.

**Fusion strategy.** Our fusion strategy, based on cross-attention, integrates both visual and intention embeddings to obtain enhanced intention-enhanced visual embeddings. Visual embeddings serve as keys and values, intention embeddings act as queries. As intention is embedded in visual features and guides the evolution of actions, it can enhance the visual embeddings to be more discriminative. This fusion also enhances the LLM's ability to comprehend visual information and improves its interpretability, resulting in more discriminative and intention-consistent cross-modal representations.

## Example Selection

Augmenting LLMs with relevant demonstration examples can significantly enhance their generative capabilities. However, selecting appropriate examples for action anticipation tasks remains challenging due to the diversity of scenarios and the variability of actions. We propose an example selection mechanism that jointly considers both visual and textual modalities. This mechanism can provide more relevant and appropriate examples for in-context learning, thereby improving generalizability.

**Single-Modality Selection.** For the visual modality, after obtaining the original visual embeddings, we apply average pooling to derive the averaged visual embeddings as a global representation. We then utilize L2 distance to obtain the similarity scores between the query video and all the training videos, selecting the top-k examples based on the similarity scores. The example selection mechanism for the textual modality adheres to the same principles, where observed action labels are encoded to obtain textual embeddings.

**Prompting using In-Context Learning**

Instruction: Given a video and 8 observed actions, you are supposed to predict the next most possible 20 actions in the format of verb-noun pair in sequence that are consistent with logic and common sense.

Examples:
remove leaf, put plant, take grass, put grass, take trowel, put plant, dig plant, take plant - dig plant, remove plant, put plant, put plant, pull plant, take plant, put plant, take garbage, put garbage, dig plant, put plant, put plant, take plant, put plant, take garbage, take plant, put garbage, put trowel, take mask, move rope

Observed actions: put plant, put plant, put plant, put plant, put plant, put plant, put plant, cut plant

**Figure 3:** Illustration of prompt for LLMs using in-context learning. The prompt is composed of an instruction, selected examples based on multi-modality similarity, observed actions and intention-enhanced visual embeddings.

**Multi-Modality Selection.** After obtaining the similarity results of the visual and textual modalities, we adopt a weighted summation approach to comprehensively consider the similarities of both modalities. The comprehensive similarity score is then calculated using a weighting factor that reflects the balance between the two modalities. Based on the comprehensive similarity scores, the top-k examples are selected.

## Training

The visual and textual encoders in ICVL are frozen, while the ICAF module is fully trainable. Given the significant computational cost of fully training LLMs, we adopt Low-Rank Adaptation (LORA) for fine-tuning the LLM. All trainable parameters are optimized based on the text generated by the LLM. As the model is tasked with predicting a future action sequence, we employ the next-token prediction loss with negative log-likelihood to optimize the predicted tokens. We adopt an end-to-end training process so that both the ICAF module and the LORA Adapter module are fine-tuned simultaneously.

# Experiment

In this section, we first introduce the datasets and evaluation metrics, followed by providing implementation details. Subsequently, we compare ICVL with state-of-the-art methods under various popular benchmarks, and finally present ablation studies of the proposed strategies.

## Datasets and Evaluation Metrics

- **Ego4D:** A large-scale egocentric dataset encompassing hundreds of scenarios, such as home, outdoor, and workplace environments. Experiments are conducted on its Forecasting subset, which includes 243 hours of video and 3472 annotated clips. It has 117 verbs and 521 nouns for the LTA task.
- **EPIC-KITCHENS-55 (EK-55):** Contains 55 hours of egocentric videos centered around cooking scenarios, recorded by 32 participants in 32 different kitchens. It contains 125 verb categories and 352 noun categories.
- **EGTEA Gaze (EGTEA):** A first-person dataset containing 86 densely labeled cooking videos over 26 hours, with 19 verb categories and 51 noun categories.

**Evaluation Metrics:**  
For Ego4D, the default edit distance (ED) metric using the Damerau-Levenshtein distance is employed. ED is computed separately for verbs, nouns, and actions sequences. For EK-55 and EGTEA, mean average precision (mAP) for multi-label classification is used. The task involves observing the first P% of each video and predicting the actions that will occur in the remaining 100-P% of the video.

## Results and Analysis

### Ego4D Results

| Method            | Venue     | Visual Encoder | Noun   | Verb   | Action |
|-------------------|-----------|---------------|--------|--------|--------|
| PaMsEgoAI         | arXiv23   |               | 0.6291 | 0.6702 | 0.8753 |
| HAI-PUI           | arXiv24   |               | 0.6733 | 0.7721 | 0.9242 |
| AntGPT            | ICLR23    | CLIP          | 0.6755 | 0.6728 | 0.8931 |
| PlausiVL          | CVPR24    |               | 0.6466 | 0.6618 | 0.8771 |
| EgoVideo          | arXiv24   | EgoVideo-V    | 0.6264 | 0.6576 | 0.8619 |
| PALM              | ECCV24    | EgoVLP        | 0.6465 | 0.7111 | 0.8819 |
| ICVL (Ours)       |           | CLIP          | 0.6194 | 0.6516 | 0.8570 |

The results with bold and underline indicate the highest and second-highest values, respectively. Rows with gray shading represent LLM-based methods. Visual Encoder refers to the visual encoder of the action recognition model.

### EK-55 and EGTEA Results

| Method         | Venue     | ALL  | EK-55 FREQ | EGTEA RARE | ALL  | FREQ | RARE |
|----------------|-----------|------|------------|------------|------|------|------|
| Timeception    | CVPR19    | 35.6 | 55.9       | 26.1       | 74.1 | 79.7 | 59.7 |
| VideoGraph     | arXiv19   | 22.5 | 49.4       | 14.0       | 67.7 | 77.1 | 47.2 |
| EGO-TOPO       | CVPR20    | 38.0 | 56.9       | 29.2       | 73.5 | 80.7 | 54.7 |
| Anticipatr     | ECCV22    | 39.1 | 58.1       | 29.1       | 76.8 | 83.3 | 55.1 |
| AntGPT         | ICLR23    | 40.1 | 58.8       | 31.9       | 80.2 | 84.8 | 72.9 |
| PALM           | ECCV24    | 40.4 | 59.3       | 30.3       | 80.7 | 85.0 | 73.5 |
| ICVL (Ours)    |           | 43.3 | 61.6       | 33.8       | 81.0 | 85.2 | 73.7 |

The results with bold and underline indicate the highest and second-highest values, respectively. Rows with gray shading represent LLM-based method.

### Implementation Details

For action recognition, we utilize the frozen encoder CLIP ViT-L14 to extract visual features and then employ a Transformer encoder with 8 attention heads. For the ICAF module, we utilize BLIP2-OPT-2.7B as the frozen visual encoder, LLaMA 3.2-9B as the VLM to derive behavioral intentions, along with LLaMA 3-8B as the text encoder and the LLM for anticipation. The Adam optimizer is used for end-to-end training with a learning rate of 5e-5, over 8 epochs.

### Ablation Studies

**Effectiveness of the two proposed modules.** The results on the Ego4D dataset of ICAF and Example Selection modules are provided below. Both modules contribute to a significant overall improvement in model performance, with ICAF having the greatest impact.

| Method             | Noun   | Verb   | Action |
|--------------------|--------|--------|--------|
| Baseline           | 0.6469 | 0.6661 | 0.8773 |
| Visual features    | 0.6454 | 0.6580 | 0.8733 |
| Action labels      | 0.6383 | 0.6605 |        |
| VLM                |        |        |        |

**Figure 4:** Ablation study on the number of the Selected Examples.

| Method        | Noun   | Verb   | Action |
|---------------|--------|--------|--------|
| Concat        | 0.6329 | 0.6610 | 0.8676 |
| CrossAttn V   | 0.6285 | 0.6580 | 0.8656 |
| CrossAttn I   | 0.6287 | 0.6550 | 0.8643 |

| Method              | Noun   | Verb   | Action |
|---------------------|--------|--------|--------|
| Baseline            | 0.6927 | 0.6823 | 0.8944 |
| Baseline w/ ES      | 0.6549 | 0.6759 | 0.8813 |
| Baseline w/ ICAF    | 0.6287 | 0.6550 | 0.8643 |

# Conclusion

We propose a novel Intention-Conditioned Vision-Language (ICVL) model for long-term action anticipation, which fully leverages both visual and textual information, integrating them with the prior knowledge and reasoning capabilities of LLMs. By introducing intention-enhanced visual features and an effective example selection mechanism, our method achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness and robustness for long-term action anticipation tasks.
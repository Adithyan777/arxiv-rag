id,title,categories,abstract,doi,created,updated,authors
1909.01940,can we trust deep learning models diagnosis? the impact of domain shift   in chest radiograph classification,eess.iv cs.ai cs.cv cs.lg stat.ml,"while deep learning models become more widespread, their ability to handle unseen data and generalize for any scenario is yet to be challenged. in medical imaging, there is a high heterogeneity of distributions among images based on the equipment that generates them and their parametrization. this heterogeneity triggers a common issue in machine learning called domain shift, which represents the difference between the training data distribution and the distribution of where a model is employed. a high domain shift tends to implicate in a poor generalization performance from the models. in this work, we evaluate the extent of domain shift on four of the largest datasets of chest radiographs. we show how training and testing with different datasets (e.g., training in chestx-ray14 and testing in chexpert) drastically affects model performance, posing a big question over the reliability of deep learning models trained on public datasets. we also show that models trained on chexpert and mimic-cxr generalize better to other datasets.",10.1007/978-3-030-62469-9_7,2019-09-03,2020-06-22,"['eduardo h. p. pooch', 'pedro l. ballester', 'rodrigo c. barros']"
2008.06255,from attack to protection: leveraging watermarking attack network for   advanced add-on watermarking,cs.mm cs.cr cs.cv,"multi-bit watermarking (mw) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. however, these tools often fail to capitalize on the unique attributes of the targeted mw and typically neglect the aspect of visual quality, a critical factor in practical applications. to overcome these shortcomings, we introduce a watermarking attack network (wan), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within mw systems and induce watermark bit inversions, significantly diminishing watermark extractability. the proposed wan employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. our empirical results demonstrate that the wan effectively undermines various block-based mw systems while minimizing visual degradation caused by attacks. this is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. the wan functions not only as a benchmarking tool but also as an add-on watermarking (aow) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. extensive experimental results show that aow complements the performance of the targeted mw system by independently enhancing both imperceptibility and robustness.",,2020-08-14,2025-05-03,"['seung-hun nam', 'jihyeon kang', 'daesik kim', 'namhyuk ahn', 'wonhyuk ahn']"
2103.16074,pointba: towards backdoor attacks in 3d point cloud,cs.lg cs.cr cs.cv,"3d deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. however, recently several works raise the security issues of 3d deep models. although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3d deep learning systems but remains unexplored. we present the backdoor attacks in 3d point cloud with a unified framework that exploits the unique properties of 3d data and networks. in particular, we design two attack approaches on point cloud: the poison-label backdoor attack (pointpba) and the clean-label backdoor attack (pointcba). the first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. the attack algorithms are mainly motivated and developed by 1) the recent discovery of 3d adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. extensive experiments show the efficacy of the pointpba with over 95% success rate across various 3d datasets and models, and the more stealthy pointcba with around 50% success rate. our proposed backdoor attack in 3d point cloud is expected to perform as a baseline for improving the robustness of 3d deep models.",,2021-03-30,2025-05-08,"['xinke li', 'zhirui chen', 'yue zhao', 'zekun tong', 'yabang zhao', 'andrew lim', 'joey tianyi zhou']"
2109.05021,a deep learning-based unified framework for red lesions detection on   retinal fundus images,eess.iv cs.cv,"red-lesions, microaneurysms (mas) and hemorrhages (hms), are the early signs of diabetic retinopathy (dr). the automatic detection of mas and hms on retinal fundus images is a challenging task. most of the existing methods detect either only mas or only hms because of the difference in their texture, sizes, and morphology. though some methods detect both mas and hms, they suffer from the curse of dimensionality of shape and colors features and fail to detect all shape variations of hms such as flame-shaped. leveraging the progress in deep learning, we proposed a two-stream red lesions detection system dealing simultaneously with small and large red lesions. for this system, we introduced a new rois candidates generation method for large red lesions on fundus images; it is based on blood vessel segmentation and morphological operations, and reduces the computational complexity, and enhances the detection accuracy by generating a small number of potential candidates. for detection, we proposed a framework with two streams. we used pretrained vggnet as a backbone model and carried out several extensive experiments to tune it for vessels segmentation and candidates generation, and finally learning the appropriate mapping, which yields better detection of the red lesions comparing with the state-of-the-art methods. the experimental results validated the effectiveness of the system in the detection of both mas and hms; it yields higher performance for per lesion detection; its sensitivity equals 0.8589 and good froc score under 8 fpis on diaretdb1-ma reports froc=0.7518, and with sn=0.7552 and good froc score under 2,4and 8 fpis on diaretdb1-hm, and sn=0.8157 on e-ophtha with overall froc=0.4537 and on roch dataset with froc=0.3461 which is higher than the state-of-the art methods. for dr screening, the system performs well with good auc on diaretdb1-ma, diaretdb1-hm, and e-ophtha datasets.",,2021-09-09,2025-05-01,"['norah asiri', 'muhammad hussain', 'fadwa al adel']"
2202.03482,navigating neural space: revisiting concept activation vectors to   overcome directional divergence,cs.cv cs.ai cs.lg,"with a growing interest in understanding neural network prediction strategies, concept activation vectors (cavs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. commonly, cavs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. however, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. this discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. to address this, we introduce pattern-based cavs, solely focussing on concept signals, thereby providing more accurate concept directions. we evaluate various cav methods in terms of their alignment with the true concept direction and their impact on cav applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. we demonstrate the benefits of pattern-based cavs using the pediatric bone age, isic2019, and funnybirds datasets with vgg, resnet, rexnet, efficientnet, and vision transformer as model architectures.",,2022-02-07,2025-05-07,"['frederik pahde', 'maximilian dreyer', 'leander weber', 'moritz weckbecker', 'christopher j. anders', 'thomas wiegand', 'wojciech samek', 'sebastian lapuschkin']"
2202.09738,the loop game: quality assessment and optimization for low-light image   enhancement,eess.iv cs.cv,"there is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. with numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. in this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. in particular, we create a large-scale database for quality assessment of the enhanced low-light image (quote-lol), which serves as the foundation in studying and developing objective quality assessment measures. the objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. the superiority of the proposed scheme is validated based on various low-light scenes.",,2022-02-20,2025-05-03,"['danni huang', 'lingyu zhu', 'zihao lin', 'hanwei zhu', 'shiqi wang', 'baoliang chen']"
2208.03571,transformer-based assignment decision network for multiple object   tracking,cs.cv cs.ai,"data association is a crucial component for any multiple object tracking (mot) method that follows the tracking-by-detection paradigm. to generate complete trajectories such methods employ a data association process to establish assignments between detections and existing targets during each timestep. recent data association approaches try to solve either a multi-dimensional linear assignment task or a network flow minimization problem or tackle it via multiple hypotheses tracking. however, during inference an optimization step that computes optimal assignments is required for every sequence frame inducing additional complexity to any given solution. to this end, in the context of this work we introduce transformer-based assignment decision network (tadn) that tackles data association without the need of any explicit optimization during inference. in particular, tadn can directly infer assignment pairs between detections and active targets in a single forward pass of the network. we have integrated tadn in a rather simple mot framework, designed a novel training strategy for efficient end-to-end training and demonstrated the high potential of our approach for online visual tracking-by-detection mot on several popular benchmarks, i.e. mot17, mot20 and ua-detrac. our proposed approach demonstrates strong performance in most evaluation metrics despite its simple nature as a tracker lacking significant auxiliary components such as occlusion handling or re-identification. the implementation of our method is publicly available at https://github.com/psaltaath/tadn-mot.",,2022-08-06,2025-05-08,"['athena psalta', 'vasileios tsironis', 'konstantinos karantzalos']"
2210.11388,physics-informed deep diffusion mri reconstruction with synthetic data:   break training data bottleneck in artificial intelligence,eess.iv cs.cv,"diffusion magnetic resonance imaging (mri) is the only imaging modality for non-invasive movement detection of in vivo water molecules, with significant clinical and research applications. diffusion weighted imaging (dwi) mri acquired by multi-shot techniques can achieve higher resolution, better signal-to-noise ratio, and lower geometric distortion than single-shot, but suffers from inter-shot motion-induced artifacts. these artifacts cannot be removed prospectively, leading to the absence of artifact-free training labels. thus, the potential of deep learning in multi-shot dwi reconstruction remains largely untapped. to break the training data bottleneck, here, we propose a physics-informed deep dwi reconstruction method (pidd) to synthesize high-quality paired training data by leveraging the physical diffusion model (magnitude synthesis) and inter-shot motion-induced phase model (motion phase synthesis). the network is trained only once with 100,000 synthetic samples, achieving encouraging results on multiple realistic in vivo data reconstructions. advantages over conventional methods include: (a) better motion artifact suppression and reconstruction stability; (b) outstanding generalization to multi-scenario reconstructions, including multi-resolution, multi-b-value, multi-under-sampling, multi-vendor, and multi-center; (c) excellent clinical adaptability to patients with verifications by seven experienced doctors (p<0.001). in conclusion, pidd presents a novel deep learning framework by exploiting the power of mri physics, providing a cost-effective and explainable way to break the data bottleneck in deep learning medical imaging.",,2022-10-20,2025-05-03,"['chen qian', 'haoyu zhang', 'yuncheng gao', 'mingyang han', 'zi wang', 'dan ruan', 'yu shen', 'yaping wu', 'yirong zhou', 'chengyan wang', 'boyu jiang', 'ran tao', 'zhigang wu', 'jiazheng wang', 'liuhong zhu', 'yi guo', 'taishan kang', 'jianzhong lin', 'tao gong', 'chen yang', 'guoqiang fei', 'meijin lin', 'di guo', 'jianjun zhou', 'meiyun wang', 'xiaobo qu']"
2211.05781,demystify transformers & convolutions in modern image deep networks,cs.cv,"vision transformers have gained popularity recently, leading to the development of new vision backbones with improved features and consistent performance gains. however, these advancements are not solely attributable to novel feature transformation designs; certain benefits also arise from advanced network-level and block-level architectures. this paper aims to identify the real gains of popular convolution and attention operators through a detailed study. we find that the key difference among these feature transformation modules, such as attention or convolution, lies in their spatial feature aggregation approach, known as the ""spatial token mixer"" (stm). to facilitate an impartial comparison, we introduce a unified architecture to neutralize the impact of divergent network-level and block-level designs. subsequently, various stms are integrated into this unified framework for comprehensive comparative analysis. our experiments on various tasks and an analysis of inductive bias show a significant performance boost due to advanced network-level and block-level designs, but performance differences persist among different stms. our detailed analysis also reveals various findings about different stms, including effective receptive fields, invariance, and adversarial robustness tests.",10.1109/tpami.2024.3520508,2022-11-10,2024-12-18,"['xiaowei hu', 'min shi', 'weiyun wang', 'sitong wu', 'linjie xing', 'wenhai wang', 'xizhou zhu', 'lewei lu', 'jie zhou', 'xiaogang wang', 'yu qiao', 'jifeng dai']"
2211.06841,point-dae: denoising autoencoders for self-supervised point cloud   learning,cs.cv cs.ai,"masked autoencoder has demonstrated its effectiveness in self-supervised point cloud learning. considering that masking is a kind of corruption, in this work we explore a more general denoising autoencoder for point cloud learning (point-dae) by investigating more types of corruptions beyond masking. specifically, we degrade the point cloud with certain corruptions as input, and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. three corruption families (\ie, density/masking, noise, and affine transformation) and a total of fourteen corruption types are investigated with traditional non-transformer encoders. besides the popular masking corruption, we identify another effective corruption family, \ie, affine transformation. the affine transformation disturbs all points globally, which is complementary to the masking corruption where some local regions are dropped. we also validate the effectiveness of affine transformation corruption with the transformer backbones, where we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape, alleviating the position leakage problem in the reconstruction. extensive experiments on tasks of object classification, few-shot learning, robustness testing, part segmentation, and 3d object detection validate the effectiveness of the proposed method. the codes are available at \url{https://github.com/ybzh/point-dae}.",10.1109/tnnls.2025.3557055,2022-11-13,2024-07-25,"['yabin zhang', 'jiehong lin', 'ruihuang li', 'kui jia', 'lei zhang']"
2301.02008,expressive speech-driven facial animation with controllable emotions,cs.cv,"it is in high demand to generate facial animation with high realism, but it remains a challenging task. existing approaches of speech-driven facial animation can produce satisfactory mouth movement and lip synchronization, but show weakness in dramatic emotional expressions and flexibility in emotion control. this paper presents a novel deep learning-based approach for expressive facial animation generation from speech that can exhibit wide-spectrum facial expressions with controllable emotion type and intensity. we propose an emotion controller module to learn the relationship between the emotion variations (e.g., types and intensity) and the corresponding facial expression parameters. it enables emotion-controllable facial animation, where the target expression can be continuously adjusted as desired. the qualitative and quantitative evaluations show that the animation generated by our method is rich in facial emotional expressiveness while retaining accurate lip movement, outperforming other state-of-the-art methods.",10.1109/icmew59549.2023.00073,2023-01-05,2024-01-04,"['yutong chen', 'junhong zhao', 'wei-qiang zhang']"
2301.11564,learning 6-dof fine-grained grasp detection based on part affordance   grounding,cs.ro cs.cl cs.cv cs.hc,"robotic grasping is a fundamental ability for a robot to interact with the environment. current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. however, lacking a large part-wise 3d robotic dataset limits the development of part representation learning and downstream applications. in this paper, we propose a new large language-guided shape grasping dataset (named langshape) to promote 3d part-level affordance and grasping ability learning. from the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named langpartgpd), including a novel 3d part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (llms) could guide a robot to generate part-level 6-dof grasping pose with textual explanation. our method combines the advantages of human-robot collaboration and llms' planning ability using explicit language as a symbolic intermediate. to evaluate the effectiveness of our proposed method, we perform 3d part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. results show our method achieves competitive performance in 3d geometry fine-grained grounding, object affordance inference, and 3d part-aware grasping tasks. our dataset and code are available on our project website https://sites.google.com/view/lang-shape",,2023-01-27,2025-04-30,"['yaoxian song', 'penglei sun', 'piaopiao jin', 'yi ren', 'yu zheng', 'zhixu li', 'xiaowen chu', 'yue zhang', 'tiefeng li', 'jason gu']"
2302.06308,fine-tuning is a surprisingly effective domain adaptation baseline in   handwriting recognition,cs.cv,"in many machine learning tasks, a large general dataset and a small specialized dataset are available. in such situations, various domain adaptation methods can be used to adapt a general model to the target dataset. we show that in the case of neural networks trained for handwriting recognition using ctc, simple fine-tuning with data augmentation works surprisingly well in such scenarios and that it is resistant to overfitting even for very small target domain datasets. we evaluated the behavior of fine-tuning with respect to augmentation, training data size, and quality of the pre-trained network, both in writer-dependent and writer-independent settings. on a large real-world dataset, fine-tuning on new writers provided an average relative cer improvement of 25 % for 16 text lines and 50 % for 256 text lines.",,2023-02-13,2025-04-30,"['jan kohút', 'michal hradiš']"
2302.06318,towards writing style adaptation in handwriting recognition,cs.cv,"one of the challenges of handwriting recognition is to transcribe a large number of vastly different writing styles. state-of-the-art approaches do not explicitly use information about the writer's style, which may be limiting overall accuracy due to various ambiguities. we explore models with writer-dependent parameters which take the writer's identity as an additional input. the proposed models can be trained on datasets with partitions likely written by a single author (e.g. single letter, diary, or chronicle). we propose a writer style block (wsb), an adaptive instance normalization layer conditioned on learned embeddings of the partitions. we experimented with various placements and settings of wsb and contrastively pre-trained embeddings. we show that our approach outperforms a baseline with no wsb in a writer-dependent scenario and that it is possible to estimate embeddings for new writers. however, domain adaptation using simple fine-tuning in a writer-independent setting provides superior accuracy at a similar computational cost. the proposed approach should be further investigated in terms of training stability and embedding regularization to overcome such a baseline.",,2023-02-13,2025-04-30,"['jan kohút', 'michal hradiš', 'martin kišš']"
2303.08068,style feature extraction using contrastive conditioned variational   autoencoders with mutual information constraints,cs.cv cs.lg stat.ml,"extracting fine-grained features such as styles from unlabeled data is crucial for data analysis. unsupervised methods such as variational autoencoders (vaes) can extract styles that are usually mixed with other features. conditional vaes (cvaes) can isolate styles using class labels; however, there are no established methods to extract only styles using unlabeled data. in this paper, we propose a cvae-based method that extracts style features using only unlabeled data. the proposed model consists of a contrastive learning (cl) part that extracts style-independent features and a cvae part that extracts style features. the cl model learns representations independent of data augmentation, which can be viewed as a perturbation in styles, in a self-supervised manner. considering the style-independent features from the pretrained cl model as a condition, the cvae learns to extract only styles. additionally, we introduce a constraint based on mutual information between the cl and vae features to prevent the cvae from ignoring the condition. experiments conducted using two simple datasets, mnist and an original dataset based on google fonts, demonstrate that the proposed method can efficiently extract style features. further experiments using real-world natural image datasets were also conducted to illustrate the method's extendability.",10.1109/tkde.2025.3543383,2023-02-03,2023-03-16,"['suguru yasutomi', 'toshihisa tanaka']"
2303.12484,label-efficient deep learning in medical image analysis: challenges and   future directions,cs.cv cs.ai,"deep learning has significantly advanced medical imaging analysis (mia), achieving state-of-the-art performance across diverse clinical tasks. however, its success largely depends on large-scale, high-quality labeled datasets, which are costly and time-consuming to obtain due to the need for expert annotation. to mitigate this limitation, label-efficient deep learning methods have emerged to improve model performance under limited supervision by leveraging labeled, unlabeled, and weakly labeled data. in this survey, we systematically review over 350 peer-reviewed studies and present a comprehensive taxonomy of label-efficient learning methods in mia. these methods are categorized into four labeling paradigms: no label, insufficient label, inexact label, and label refinement. for each category, we analyze representative techniques across imaging modalities and clinical applications, highlighting shared methodological principles and task-specific adaptations. we also examine the growing role of health foundation models (hfms) in enabling label-efficient learning through large-scale pre-training and transfer learning, enhancing the use of limited annotations in downstream tasks. finally, we identify current challenges and future directions to facilitate the translation of label-efficient learning from research promise to everyday clinical care.",,2023-03-22,2025-05-08,"['cheng jin', 'zhengrui guo', 'yi lin', 'luyang luo', 'hao chen']"
2303.12675,vecfontsdf: learning to reconstruct and synthesize high-quality vector   fonts via signed distance functions,cs.cv,"font design is of vital importance in the digital content design and modern printing industry. developing algorithms capable of automatically synthesizing vector fonts can significantly facilitate the font design process. however, existing methods mainly concentrate on raster image generation, and only a few approaches can directly synthesize vector fonts. this paper proposes an end-to-end trainable method, vecfontsdf, to reconstruct and synthesize high-quality vector fonts using signed distance functions (sdfs). specifically, based on the proposed sdf-based implicit shape representation, vecfontsdf learns to model each glyph as shape primitives enclosed by several parabolic curves, which can be precisely converted to quadratic b\'ezier curves that are widely used in vector font products. in this manner, most image generation methods can be easily extended to synthesize vector fonts. qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality results on several tasks, including vector font reconstruction, interpolation, and few-shot vector font synthesis, markedly outperforming the state of the art. our code and trained models are available at https://xiazeqing.github.io/vecfontsdf.",,2023-03-22,2025-05-06,"['zeqing xia', 'bojun xiong', 'zhouhui lian']"
2305.00046,an automated end-to-end deep learning-based framework for lung cancer   diagnosis by detecting and classifying the lung nodules,eess.iv cs.ai cs.cv,"lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. the objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. the proposed framework consists of three stages: lung segmentation using a modified 3d u-net named 3d res-u-net, nodule detection using yolo-v5, and classification with a vision transformer-based architecture. we evaluated the proposed framework on a publicly available dataset, luna16. the proposed framework's performance was measured using the respective domain's evaluation matrices. the proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 map@50 from the segmented lung, at a low false-positive rate. the performance of both networks of the proposed framework was compared with other studies and found to outperform them regarding segmentation and detection accuracy. additionally, our proposed vision transformer network obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art networks. our proposed end-to-end deep learning-based framework can effectively segment lungs, and detect and classify lung nodules, specifically in low-resource settings with limited access to radiologists. the proposed framework outperforms existing studies regarding all the respective evaluation metrics. the proposed framework can potentially improve the accuracy and efficiency of lung cancer screening in low-resource settings, ultimately leading to better patient outcomes.",,2023-04-28,2025-05-07,"['samiul based shuvo', 'tasnia binte mamun']"
2305.13800,generalizable synthetic image detection via language-guided contrastive   learning,cs.cv,"the heightened realism of ai-generated images can be attributed to the rapid development of synthetic models, including generative adversarial networks (gans) and diffusion models (dms). the malevolent use of synthetic images, such as the dissemination of fake news or the creation of fake profiles, however, raises significant concerns regarding the authenticity of images. though many forensic algorithms have been developed for detecting synthetic images, their performance, especially the generalization capability, is still far from being adequate to cope with the increasing number of synthetic models. in this work, we propose a simple yet very effective synthetic image detection method via a language-guided contrastive learning. specifically, we augment the training images with carefully-designed textual labels, enabling us to use a joint visual-language contrastive supervision for learning a forensic feature space with better generalization. it is shown that our proposed language-guided synthesis detection (lasted) model achieves much improved generalizability to unseen image generation models and delivers promising performance that far exceeds state-of-the-art competitors over four datasets. the code is available at https://github.com/highwaywu/lasted.",,2023-05-23,2025-04-29,"['haiwei wu', 'jiantao zhou', 'shile zhang']"
2305.18708,infrared image deturbulence restoration using degradation   parameter-assisted wide & deep learning,cs.cv eess.iv,"infrared images captured under turbulent conditions are degraded by complex geometric distortions and blur. we address infrared deturbulence as an image restoration task, proposing dparnet, a parameter-assisted multi-frame network with a wide & deep architecture. dparnet learns a degradation prior (key parameter matrix) directly from degraded images without external knowledge. its wide & deep architecture uses these learned parameters to directly modulate restoration, achieving spatially and intensity adaptive results. evaluated on dedicated infrared deturbulence (49,744 images) and visible image denoising (109,536 images) datasets, dparnet significantly outperforms state-of-the-art (sota) methods in restoration performance and efficiency. notably, leveraging these parameters improves psnr by 0.6-1.1 db with less than 2% increase in model parameters and computational complexity. our work demonstrates that degraded images hide key degradation information that can be learned and utilized to boost adaptive image restoration.",,2023-05-29,2025-05-06,"['yi lu', 'yadong wang', 'xingbo jiang', 'xiangzhi bai']"
2306.03271,volumetric medical image segmentation through dual self-distillation in   u-shaped networks,eess.iv cs.cv,"u-shaped networks and its variants have demonstrated exceptional results for medical image segmentation. in this paper, we propose a novel dual self-distillation (dsd) framework in u-shaped networks for volumetric medical image segmentation. dsd distills knowledge from the ground-truth segmentation labels to the decoder layers. additionally, dsd also distills knowledge from the deepest decoder and encoder layer to the shallower decoder and encoder layers respectively of a single u-shaped network. dsd is a general training strategy that could be attached to the backbone architecture of any u-shaped network to further improve its segmentation performance. we attached dsd on several state-of-the-art u-shaped backbones, and extensive experiments on various public 3d medical image segmentation datasets (cardiac substructure, brain tumor and hippocampus) demonstrated significant improvement over the same backbones without dsd. on average, after attaching dsd to the u-shaped backbones, we observed an increase of 2.82\%, 4.53\% and 1.3\% in dice similarity score, a decrease of 7.15 mm, 6.48 mm and 0.76 mm in the hausdorff distance, for cardiac substructure, brain tumor and hippocampus segmentation, respectively. these improvements were achieved with negligible increase in the number of trainable parameters and training time. our proposed dsd framework also led to significant qualitative improvements for cardiac substructure, brain tumor and hippocampus segmentation over the u-shaped backbones. the source code is publicly available at https://github.com/soumbane/dualselfdistillation.",,2023-06-05,2025-05-01,"['soumyanil banerjee', 'nicholas summerfield', 'ming dong', 'carri glide-hurst']"
2306.07971,xraygpt: chest radiographs summarization using medical vision-language   models,cs.cv,"the latest breakthroughs in large vision-language models, such as bard and gpt-4, have showcased extraordinary abilities in performing a wide range of tasks. such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. however, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. on the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. in this paper, we introduce xraygpt, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. specifically, we align both medical visual encoder (medclip) with a fine-tuned large language model (vicuna), using a simple linear transformation. this alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. to enhance the performance of llms in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. these summaries serve to enhance the performance of llms through the fine-tuning process. our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/xraygpt.",,2023-06-13,2025-05-07,"['omkar thawakar', 'abdelrahman shaker', 'sahal shaji mullappilly', 'hisham cholakkal', 'rao muhammad anwer', 'salman khan', 'jorma laaksonen', 'fahad shahbaz khan']"
2306.11891,vitalvideos-europe: a dataset of face videos with ppg and blood pressure   ground truths,cs.cv,"we collected a large dataset consisting of 850 unique participants. for every participant we recorded two 30 second uncompressed videos, synchronized ppg waveforms and a single blood pressure measurement. gender, age and skin color were also registered for every participant. the dataset includes roughly equal numbers of males and females, as well as participants of all ages. while the skin color distribution could have been more balanced, the dataset contains individuals from every skin color. the data was collected in a diverse set of locations to ensure a wide variety of backgrounds and lighting conditions. in an effort to assist in the research and development of remote vital sign measurement we are now opening up access to this dataset.   vitalvideos.org for all datasets.",,2023-06-02,2025-05-01,['pieter-jan toye']
2306.16122,semantic positive pairs for enhancing visual representation learning of   instance discrimination methods,cs.cv cs.lg,"self-supervised learning algorithms (ssl) based on instance discrimination have shown promising results, performing competitively or even outperforming supervised learning counterparts in some downstream tasks. such approaches employ data augmentation to create two views of the same instance (i.e., positive pairs) and encourage the model to learn good representations by attracting these views closer in the embedding space without collapsing to the trivial solution. however, data augmentation is limited in representing positive pairs, and the repulsion process between the instances during contrastive learning may discard important features for instances that have similar categories. to address this issue, we propose an approach to identify those images with similar semantic content and treat them as positive instances, thereby reducing the chance of discarding important features during representation learning and increasing the richness of the latent representation. our approach is generic and could work with any self-supervised instance discrimination frameworks such as moco and simsiam. to evaluate our method, we run experiments on three benchmark datasets: imagenet, stl-10 and cifar-10 with different instance discrimination ssl approaches. the experimental results show that our approach consistently outperforms the baseline methods across all three datasets; for instance, we improve upon the vanilla moco-v2 by 4.1% on imagenet under a linear evaluation protocol over 800 epochs. we also report results on semi-supervised learning, transfer learning on downstream tasks, and object detection.",,2023-06-28,2025-04-30,"['mohammad alkhalefi', 'georgios leontidis', 'mingjun zhong']"
2307.11470,semi-supervised underwater image enhancement using a physics-aware   triple-stream network,cs.cv,"underwater images normally suffer from degradation due to the transmission medium of water bodies. both traditional prior-based approaches and deep learning-based methods have been used to address this problem. however, the inflexible assumption of the former often impairs their effectiveness in handling diverse underwater scenes, while the generalization of the latter to unseen images is usually weakened by insufficient data. in this study, we leverage both the physics-based image formation model (ifm) and deep learning techniques for underwater image enhancement (uie). to this end, we propose a novel physics-aware triple-stream underwater image enhancement network, i.e., pats-uienet, which comprises a direct signal transmission estimation steam (d-stream), a backscatter signal transmission estimation steam (b-stream) and an ambient light estimation stream (a-stream). this network fulfills the uie task by explicitly estimating the degradation parameters of a revised ifm. we also adopt an ifm-inspired semi-supervised learning framework, which exploits both the labeled and unlabeled images, to address the issue of insufficient data. to our knowledge, such a physics-aware deep network and the ifm-inspired semi-supervised learning framework have not been used for the uie task before. our method performs better than, or at least comparably to, sixteen baselines across six testing sets in the degradation estimation and uie tasks. these promising results should be due to the fact that the proposed method can not only model the degradation but also learn the characteristics of diverse underwater scenes.",,2023-07-21,2025-05-07,"['shixuan xu', 'hao qi', 'xinghui dong']"
2308.04369,sstformer: bridging spiking neural network and memory support   transformer for frame-event based recognition,cs.cv cs.mm cs.ne,"event camera-based pattern recognition is a newly arising research topic in recent years. current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. secondly, they adopt either spiking neural networks (snn) for energy-efficient recognition with suboptimal results, or artificial neural networks (ann) for energy-intensive, high-performance recognition. however, seldom of them consider achieving a balance between these two aspects. in this paper, we formally propose to recognize patterns by fusing rgb frames and event streams simultaneously and propose a new rgb frame-event recognition framework to address the aforementioned issues. the proposed method contains four main modules, i.e., memory support transformer network for rgb frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for rgb-event feature aggregation, and prediction head. due to the scarce of rgb-event based classification dataset, we also propose a large-scale pokerevent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a dvs346 event camera. extensive experiments on two rgb-event based classification datasets fully validated the effectiveness of our proposed framework. we hope this work will boost the development of pattern recognition by fusing rgb frames and event streams. both our dataset and source code of this work will be released at https://github.com/event-ahu/sstformer",,2023-08-08,2025-05-04,"['xiao wang', 'yao rong', 'zongzhen wu', 'lin zhu', 'bo jiang', 'jin tang', 'yonghong tian']"
2308.06057,illumination and shadows in head rotation: experiments with denoising   diffusion models,cs.cv,"accurately modeling the effects of illumination and shadows during head rotation is critical in computer vision for enhancing image realism and reducing artifacts. this study delves into the latent space of denoising diffusion models to identify compelling trajectories that can express continuous head rotation under varying lighting conditions. a key contribution of our work is the generation of additional labels from the celeba dataset,categorizing images into three groups based on prevalent illumination direction: left, center, and right. these labels play a crucial role in our approach, enabling more precise manipulations and improved handling of lighting variations. leveraging a recent embedding technique for denoising diffusion implicit models (ddim), our method achieves noteworthy manipulations, encompassing a wide rotation angle of $\pm 30$ degrees, while preserving individual distinct characteristics even under challenging illumination conditions. our methodology involves computing trajectories that approximate clouds of latent representations of dataset samples with different yaw rotations through linear regression. specific trajectories are obtained by analyzing subsets of data that share significant attributes with the source image, including light direction. notably, our approach does not require any specific training of the generative model for the task of rotation; we merely compute and follow specific trajectories in the latent space of a pre-trained face generation model. this article showcases the potential of our approach and its current limitations through a qualitative discussion of notable examples. this study contributes to the ongoing advancements in representation learning and the semantic investigation of the latent space of generative models.",,2023-08-11,2025-05-07,"['andrea asperti', 'gabriele colasuonno', 'antonio guerra']"
2308.13505,"joint modeling of feature, correspondence, and a compressed memory for   video object segmentation",cs.cv,"current prevailing video object segmentation methods follow the pipeline of extraction-then-matching, which first extracts features on current and reference frames independently, and then performs dense matching between them. this decoupled pipeline limits information propagation between frames to high-level features, hindering fine-grained details for matching. furthermore, the pixel-wise matching lacks holistic target understanding, making it prone to disturbance by similar distractors. to address these issues, we propose a unified vos framework, coined as jointformer, for jointly modeling feature extraction, correspondence matching, and a compressed memory. the core joint modeling block leverages attention to simultaneously extract and propagate the target information from the reference frame to the current frame and a compressed memory token. this joint scheme enables extensive multi-layer propagation beyond high-level feature space and facilitates robust instance-distinctive feature learning. to incorporate the long-term and holistic target information, we introduce a compressed memory token with a customized online updating mechanism, which aggregates target features and facilitates temporal information propagation in a frame-wise manner, enhancing global modeling consistency. our jointformer achieves a new state-of-the-art performance on the davis 2017 val/test-dev (89.7\% and 87.6\%) benchmarks and the youtube-vos 2018/2019 val (87.0\% and 87.0\%) benchmarks, outperforming the existing works. to demonstrate the generalizability of our model, it is further evaluated on four new benchmarks with various difficulties, including mose for complex scenes, visor for egocentric videos, vost for complex transformations, and lvos for long-term videos.",,2023-08-25,2025-04-30,"['jiaming zhang', 'yutao cui', 'gangshan wu', 'limin wang']"
2308.16082,signdiff: diffusion model for american sign language production,cs.cv,"in this paper, we propose a dual-condition diffusion pre-training model named signdiff that can generate human sign language speakers from a skeleton pose. signdiff has a novel frame reinforcement network called fr-net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. in addition, we propose a new method for american sign language production (aslp), which can generate asl skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. we propose the first baseline for asl production and report the scores of 17.19 and 12.85 on bleu-4 on the how2sign dev/test sets. we evaluated our model on the previous mainstream dataset phoenix14t, and the experiments achieved the sota results. in addition, our image quality far exceeds all previous results by 10 percentage points in terms of ssim.",,2023-08-30,2025-04-29,"['sen fang', 'chunyu sui', 'yanghao zhou', 'xuedong zhang', 'hongbin zhong', 'yapeng tian', 'chen chen']"
2309.06129,leyes: a lightweight framework for deep learning-based eye tracking   using synthetic eye images,cs.cv cs.ai cs.hc,"deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. this problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. while synthetic datasets can be a solution, their creation is both time and resource-intensive. to address this problem, we present a framework called light eyes or ""leyes"" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. leyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. we demonstrate that models trained using leyes are consistently on-par or outperform other state-of-the-art algorithms in terms of pupil and cr localization across well-known datasets. in addition, a leyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware. going forward, we are confident that leyes will revolutionize synthetic data generation for gaze estimation models, and lead to significant improvements of the next generation video-based eye trackers.",10.3758/s13428-025-02645-y,2023-09-12,2025-04-30,"['sean anthony byrne', 'virmarie maquiling', 'marcus nyström', 'enkelejda kasneci', 'diederick c. niehorster']"
2309.08035,interpretability-aware vision transformer,cs.cv,"vision transformers (vits) have become prominent models for solving various vision tasks. however, the interpretability of vits has not kept pace with their promising performance. while there has been a surge of interest in developing {\it post hoc} solutions to explain vits' outputs, these methods do not generalize to different downstream tasks and various transformer architectures. furthermore, if vits are not properly trained with the given data and do not prioritize the region of interest, the {\it post hoc} methods would be less effective. instead of developing another {\it post hoc} approach, we introduce a novel training procedure that inherently enhances model interpretability. our interpretability-aware vit (ia-vit) draws inspiration from a fresh insight: both the class patch and image patches consistently generate predicted distributions and attention maps. ia-vit is composed of a feature extractor, a predictor, and an interpreter, which are trained jointly with an interpretability-aware training objective. consequently, the interpreter simulates the behavior of the predictor and provides a faithful explanation through its single-head self-attention mechanism. our comprehensive experimental results demonstrate the effectiveness of ia-vit in several image classification tasks, with both qualitative and quantitative evaluations of model performance and interpretability. source code is available from: https://github.com/qiangyao1988/ia-vit.",,2023-09-14,2025-05-01,"['yao qiang', 'chengyin li', 'prashant khanduri', 'dongxiao zhu']"
2309.14630,free discontinuity regression: with an application to the economic   effects of internet shutdowns,econ.em cs.cv math.st stat.ap stat.me stat.th,"sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. we therefore introduce free discontinuity regression (fdr), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. by extending a convex relaxation of the mumford-shah functional to random spatial sampling and correlated noise, fdr overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. this yields the first identification and uniform consistency results for multivariate jump surfaces: under mild sbv regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. hyperparameters are selected automatically from the data using stein's unbiased risk estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. applying fdr to an internet shutdown in india reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. by unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, fdr turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data.",,2023-09-25,2025-05-08,"['florian gunsilius', 'david van dijcke']"
2310.04306,towards a robust group-level emotion recognition via uncertainty-aware   learning,cs.cv cs.ai,"group-level emotion recognition (ger) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. however, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. in this paper, we propose an uncertainty-aware learning (ual) method to extract more robust representations for ger. by explicitly modeling the uncertainty of each individual, we utilize stochastic embedding drawn from a gaussian distribution instead of deterministic point embedding. this representation captures the probabilities of different emotions and generates diverse predictions through this stochasticity during the inference stage. furthermore, uncertainty-sensitive scores are adaptively assigned as the fusion weights of individuals' face within each group. moreover, we develop an image enhancement module to enhance the model's robustness against severe noise. the overall three-branch model, encompassing face, object, and scene component, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.",,2023-10-06,2025-05-06,"['qing zhu', 'qirong mao', 'jialin zhang', 'xiaohua huang', 'wenming zheng']"
2310.05829,ustep: spatio-temporal predictive learning under a unified view,cs.cv,"spatio-temporal predictive learning plays a crucial role in self-supervised learning, with wide-ranging applications across a diverse range of fields. previous approaches for temporal modeling fall into two categories: recurrent-based and recurrent-free methods. the former, while meticulously processing frames one by one, neglect short-term spatio-temporal information redundancies, leading to inefficiencies. the latter naively stack frames sequentially, overlooking the inherent temporal dependencies. in this paper, we re-examine the two dominant temporal modeling approaches within the realm of spatio-temporal predictive learning, offering a unified perspective. building upon this analysis, we introduce ustep (unified spatio-temporal predictive learning), an innovative framework that reconciles the recurrent-based and recurrent-free methods by integrating both micro-temporal and macro-temporal scales. extensive experiments on a wide range of spatio-temporal predictive learning demonstrate that ustep achieves significant improvements over existing temporal modeling approaches, thereby establishing it as a robust solution for a wide range of spatio-temporal applications.",,2023-10-09,2025-05-08,"['cheng tan', 'jue wang', 'zhangyang gao', 'siyuan li', 'stan z. li']"
2310.08387,aligning data selection with performance: performance-driven   reinforcement learning for active learning in object detection,cs.cv cs.lg,"active learning strategies aim to train high-performance models with minimal labeled data by selecting the most informative instances for labeling. however, existing methods for assessing data informativeness often fail to align directly with task model performance metrics, such as mean average precision (map) in object detection. this paper introduces mean-ap guided reinforced active learning for object detection (mgral), a novel approach that leverages the concept of expected model output changes as informativeness for deep detection networks, directly optimizing the sampling strategy using map. mgral employs a reinforcement learning agent based on lstm architecture to efficiently navigate the combinatorial challenge of batch sample selection and the non-differentiable nature between performance and selected batches. the agent optimizes selection using policy gradient with map improvement as the reward signal. to address the computational intensity of map estimation with unlabeled samples, we implement fast look-up tables, ensuring real-world feasibility. we evaluate mgral on pascal voc and ms coco benchmarks across various backbone architectures. our approach demonstrates strong performance, establishing a new paradigm in reinforcement learning-based active learning for object detection.",,2023-10-12,2025-05-06,"['zhixuan liang', 'xingyu zeng', 'rui zhao', 'ping luo']"
2310.15402,towards contrast-agnostic soft segmentation of the spinal cord,eess.iv cs.cv,"spinal cord segmentation is clinically relevant and is notably used to compute spinal cord cross-sectional area (csa) for the diagnosis and monitoring of cord compression or neurodegenerative diseases such as multiple sclerosis. while several semi and automatic methods exist, one key limitation remains: the segmentation depends on the mri contrast, resulting in different csa across contrasts. this is partly due to the varying appearance of the boundary between the spinal cord and the cerebrospinal fluid that depends on the sequence and acquisition parameters. this contrast-sensitive csa adds variability in multi-center studies where protocols can vary, reducing the sensitivity to detect subtle atrophies. moreover, existing methods enhance the csa variability by training one model per contrast, while also producing binary masks that do not account for partial volume effects. in this work, we present a deep learning-based method that produces soft segmentations of the spinal cord. using the spine generic public database of healthy participants ($\text{n}=267$; $\text{contrasts}=6$), we first generated participant-wise soft ground truth (gt) by averaging the binary segmentations across all 6 contrasts. these soft gt, along with aggressive data augmentation and a regression-based loss function, were used to train a u-net model for spinal cord segmentation. we evaluated our model against state-of-the-art methods and performed ablation studies involving different loss functions and domain generalization methods. our results show that using the soft segmentations along with a regression loss function reduces csa variability ($p < 0.05$, wilcoxon signed-rank test). the proposed spinal cord segmentation model generalizes better than the state-of-the-art methods amongst unseen datasets, vendors, contrasts, and pathologies (compression, lesions), while accounting for partial volume effects.",10.1016/j.media.2025.103473,2023-10-23,2024-07-23,"['sandrine bédard', 'enamundram naga karthik', 'charidimos tsagkas', 'emanuele pravatà', 'cristina granziera', 'andrew smith', 'kenneth arnold weber', 'julien cohen-adad']"
2311.08786,disentangle before anonymize: a two-stage framework for   attribute-preserved and occlusion-robust de-identification,cs.cv,"in an era where personal photos are easily leaked and collected, face de-identification is a crucial method for protecting identity privacy. however, current face de-identification techniques face challenges in preserving attribute details and often produce anonymized results with reduced authenticity. these shortcomings are particularly evident when handling occlusions,frequently resulting in noticeable editing artifacts. our primary finding in this work is that simultaneous training of identity disentanglement and anonymization hinders their respective effectiveness.therefore, we propose ""disentangle before anonymize"",a novel two-stage framework(dbaf)designed for attributepreserved and occlusion-robust de-identification. this framework includes a contrastive identity disentanglement (cid) module and a key-authorized reversible identity anonymization (kria) module, achieving faithful attribute preservation and high-quality identity anonymization edits. additionally, we introduce a multiscale attentional attribute retention (maar) module to address the issue of reduced anonymization quality under occlusions.extensive experiments demonstrate that our method outperforms state-of-the-art de-identification approaches, delivering superior quality, enhanced detail fidelity, improved attribute preservation performance, and greater robustness to occlusions.",,2023-11-15,2025-05-01,"['mingrui zhu', 'dongxin chen', 'xin wei', 'nannan wang', 'xinbo gao']"
2311.12421,two views are better than one: monocular 3d pose estimation with   multiview consistency,cs.cv,"deducing a 3d human pose from a single 2d image is inherently challenging because multiple 3d poses can correspond to the same 2d representation. 3d data can resolve this pose ambiguity, but it is expensive to record and requires an intricate setup that is often restricted to controlled lab environments. we propose a method that improves the performance of deep learning-based monocular 3d human pose estimation models by using multiview data only during training, but not during inference. we introduce a novel loss function, consistency loss, which operates on two synchronized views. this approach is simpler than previous models that require 3d ground truth or intrinsic and extrinsic camera parameters. our consistency loss penalizes differences in two pose sequences after rigid alignment. we also demonstrate that our consistency loss substantially improves performance for fine-tuning without requiring 3d data. furthermore, we show that using our consistency loss can yield state-of-the-art performance when training models from scratch in a semi-supervised manner. our findings provide a simple way to capture new data, e.g in a new domain. this data can be added using off-the-shelf cameras with no calibration requirements. we make all our code and data publicly available.",,2023-11-21,2025-05-08,"['christian keilstrup ingwersen', 'rasmus tirsgaard', 'rasmus nylander', 'janus nørtoft jensen', 'anders bjorholm dahl', 'morten rieger hannemose']"
2311.14090,class uncertainty: a measure to mitigate class imbalance,cs.lg cs.cv,"class-wise characteristics of training examples affect the performance of deep classifiers. a well-studied example is when the number of training examples of classes follows a long-tailed distribution, a situation that is likely to yield sub-optimal performance for under-represented classes. this class imbalance problem is conventionally addressed by approaches relying on the class-wise cardinality of training examples, such as data resampling. in this paper, we demonstrate that considering solely the cardinality of classes does not cover all issues causing class imbalance. to measure class imbalance, we propose ""class uncertainty"" as the average predictive uncertainty of the training examples, and we show that this novel measure captures the differences across classes better than cardinality. we also curate svci-20 as a novel dataset in which the classes have equal number of training examples but they differ in terms of their hardness; thereby causing a type of class imbalance which cannot be addressed by the approaches relying on cardinality. we incorporate our ""class uncertainty"" measure into a diverse set of ten class imbalance mitigation methods to demonstrate its effectiveness on long-tailed datasets as well as on our svci-20. code and datasets will be made available.",,2023-11-23,2025-04-30,"['z. s. baltaci', 'k. oksuz', 's. kuzucu', 'k. tezoren', 'b. k. konar', 'a. ozkan', 'e. akbas', 's. kalkan']"
2311.14284,paragraph-to-image generation with information-enriched diffusion model,cs.cv,"text-to-image (t2i) models have recently experienced rapid development, achieving astonishing performance in terms of fidelity and textual alignment capabilities. however, given a long paragraph (up to 512 words), these generation models still struggle to achieve strong alignment and are unable to generate images depicting complex scenes. in this paper, we introduce an information-enriched diffusion model for paragraph-to-image generation task, termed paradiffusion, which delves into the transference of the extensive semantic comprehension capabilities of large language models to the task of image generation. at its core is using a large language model (e.g., llama v2) to encode long-form text, followed by fine-tuning with lora to alignthe text-image feature spaces in the generation task. to facilitate the training of long-text semantic alignment, we also curated a high-quality paragraph-image pair dataset, namely paraimage. this dataset contains a small amount of high-quality, meticulously annotated data, and a large-scale synthetic dataset with long text descriptions being generated using a vision-language model. experiments demonstrate that paradiffusion outperforms state-of-the-art models (sd xl, deepfloyd if) on vilg-300 and paraprompts, achieving up to 15% and 45% human voting rate improvements for visual appeal and text faithfulness, respectively. the code and dataset will be released to foster community research on long-text alignment.",,2023-11-24,2025-05-06,"['weijia wu', 'zhuang li', 'yefei he', 'mike zheng shou', 'chunhua shen', 'lele cheng', 'yan li', 'tingting gao', 'di zhang']"
2311.18681,radialog: a large vision-language model for radiology report generation   and conversational assistance,cs.cv cs.cl,"conversational ai tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. towards this goal, we introduce radialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. radialog effectively integrates visual image features and structured pathology findings with a large language model (llm) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. to keep the conversational abilities of the underlying llm, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest x-ray radiology tasks. by training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. our code is available on github: https://github.com/chantalmp/radialog.",,2023-11-30,2025-05-07,"['chantal pellegrini', 'ege özsoy', 'benjamin busam', 'nassir navab', 'matthias keicher']"
2312.01581,plum: improving inference efficiency by leveraging repetition-sparsity   trade-off,cs.lg cs.ai cs.cv,"efficient inference of deep neural networks (dnns) on resource-constrained edge devices is essential. quantization and sparsity are key techniques that translate to repetition and sparsity within tensors at the hardware-software interface. this paper introduces the concept of repetition-sparsity trade-off that helps explain computational efficiency during inference. we propose plum, a unified co-design framework that integrates dnn inference systems and quantization (forward and backward pass) to leverage the repetition-sparsity trade-off to improve inference efficiency. our results demonstrate that plum's quantization method is more accurate than binary quantization with the same number of non-zero weights. detailed analysis indicates that signed binarization generates a smaller distribution of effectual (non-zero) parameters nested within a larger distribution of total parameters of latent full-precision weights for a dnn block. finally, the proposed plum framework achieves a 26% speedup on real hardware, doubles energy efficiency, and reduces density by 2.8x compared to binary methods while retaining top-1 accuracy when compared to prior-art methods for resnets on imagenet (by achieving 66.2% top-1 accuracy), presenting an alternative solution for deploying efficient models in resource-limited environments.",,2023-12-03,2025-05-05,"['sachit kuhar', 'yash jain', 'alexey tumanov']"
2312.02312,visual encoders for data-efficient imitation learning in modern video   games,cs.lg cs.ai cs.cv,"video games have served as useful benchmarks for the decision-making community, but going beyond atari games towards modern games has been prohibitively expensive for the vast majority of the research community. prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. an alternative approach is to train agents using imitation learning to play video games purely from images. however, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? to answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in minecraft, counter-strike: global offensive, and minecraft dungeons. our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as dinov2 depending on the game. in addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.",,2023-12-04,2025-04-30,"['lukas schäfer', 'logan jones', 'anssi kanervisto', 'yuhan cao', 'tabish rashid', 'raluca georgescu', 'dave bignell', 'siddhartha sen', 'andrea treviño gavito', 'sam devlin']"
2312.04106,do not deepfake me: privacy-preserving neural 3d head reconstruction   without sensitive images,cs.cv,"while 3d head reconstruction is widely used for modeling, existing neural reconstruction approaches rely on high-resolution multi-view images, posing notable privacy issues. individuals are particularly sensitive to facial features, and facial image leakage can lead to many malicious activities, such as unauthorized tracking and deepfake. in contrast, geometric data is less susceptible to misuse due to its complex processing requirements, and absence of facial texture features. in this paper, we propose a novel two-stage 3d facial reconstruction method aimed at avoiding exposure to sensitive facial information while preserving detailed geometric accuracy. our approach first uses non-sensitive rear-head images for initial geometry and then refines this geometry using processed privacy-removed gradient images. extensive experiments show that the resulting geometry is comparable to methods using full images, while the process is resistant to deepfake applications and facial recognition (fr) systems, thereby proving its effectiveness in privacy protection.",10.1609/aaai.v39i4.32461,2023-12-07,2024-12-14,"['jiayi kong', 'xurui song', 'shuo huai', 'baixin xu', 'jun luo', 'ying he']"
2312.12028,eyepreserve: identity-preserving iris synthesis,cs.cv cs.lg,"synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to the intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. this paper presents the first method of fully data-driven, identity-preserving, pupil size-varying synthesis of iris images. this approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities, as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. iris recognition experiments suggest that the proposed deformation model both preserves the identity when changing the pupil size, and offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation models. two immediate applications of the proposed approach are: (a) synthesis of, or enhancement of the existing biometric datasets for iris recognition, mimicking those acquired with iris sensors, and (b) helping forensic human experts examine iris image pairs with significant differences in pupil dilation. images considered in this work conform to selected iso/iec 29794-6 quality metrics to make them applicable in biometric systems. the source codes and model weights are offered with this paper.",,2023-12-19,2025-04-30,"['siamul karim khan', 'patrick tinsley', 'mahsa mitcheff', 'patrick flynn', 'kevin w. bowyer', 'adam czajka']"
2312.12419,scene-conditional 3d object stylization and composition,cs.cv,"recently, 3d generative models have made impressive progress, enabling the generation of almost arbitrary 3d assets from text or image inputs. however, these approaches generate objects in isolation without any consideration for the scene where they will eventually be placed. in this paper, we propose a framework that allows for the stylization of an existing 3d asset to fit into a given 2d scene, and additionally produce a photorealistic composition as if the asset was placed within the environment. this not only opens up a new level of control for object stylization, for example, the same assets can be stylized to reflect changes in the environment, such as summer to winter or fantasy versus futuristic settings-but also makes the object-scene composition more controllable. we achieve this by combining modeling and optimizing the object's texture and environmental lighting through differentiable ray tracing with image priors from pre-trained text-to-image diffusion models. we demonstrate that our method is applicable to a wide variety of indoor and outdoor scenes and arbitrary objects. project page: https://jensenzhoujh.github.io/scene-cond-3d/.",,2023-12-19,2025-05-01,"['jinghao zhou', 'tomas jakab', 'philip torr', 'christian rupprecht']"
2401.03048,latte: latent diffusion transformer for video generation,cs.cv,"we propose latte, a novel latent diffusion transformer for video generation. latte first extracts spatio-temporal tokens from input videos and then adopts a series of transformer blocks to model video distribution in the latent space. in order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. to improve the quality of generated videos, we determine the best practices of latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. our comprehensive evaluation demonstrates that latte achieves state-of-the-art performance across four standard video generation datasets, i.e., faceforensics, skytimelapse, ucf101, and taichi-hd. in addition, we extend latte to the text-to-video generation (t2v) task, where latte achieves results that are competitive with recent t2v models. we strongly believe that latte provides valuable insights for future research on incorporating transformers into diffusion models for video generation.",,2024-01-05,2025-05-01,"['xin ma', 'yaohui wang', 'xinyuan chen', 'gengyun jia', 'ziwei liu', 'yuan-fang li', 'cunjian chen', 'yu qiao']"
2401.09736,ddm: a metric for comparing 3d shapes using directional distance fields,cs.cv,"qualifying the discrepancy between 3d geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. in this paper, we propose ddm, an efficient, effective, robust, and differentiable distance metric for 3d geometry data. specifically, we construct ddm based on the proposed implicit representation of 3d models, namely directional distance field (ddf), which defines the directional distances of 3d points to a model to capture its local surface geometry. we then transfer the discrepancy between two 3d geometric models as the discrepancy between their ddfs defined on an identical domain, naturally establishing model correspondence. to demonstrate the advantage of our ddm, we explore various distance metric-driven 3d geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. extensive experiments show that our ddm achieves significantly higher accuracy under all tasks. as a generic distance metric, ddm has the potential to advance the field of 3d geometric modeling. the source code is available at https://github.com/rsy6318/ddm.",,2024-01-18,2025-04-29,"['siyu ren', 'junhui hou', 'xiaodong chen', 'hongkai xiong', 'wenping wang']"
2401.12033,momentum-sam: sharpness aware minimization without computational   overhead,cs.lg cs.cv,"the recently proposed optimization algorithm for deep neural networks sharpness aware minimization (sam) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. while significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making sam unfeasible in case of limited computationally capacities. motivated by nesterov accelerated gradient (nag) we propose momentum-sam (msam), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over sgd or adam. we evaluate msam in detail and reveal insights on separable mechanisms of nag, sam and msam regarding training optimization and generalization. code is available at https://github.com/marlonbecker/msam.",,2024-01-22,2025-05-05,"['marlon becker', 'frederick altrock', 'benjamin risse']"
2402.04168,informed reinforcement learning for situation-aware traffic rule   exceptions,cs.lg cs.cv cs.ro,"reinforcement learning is a highly active research field with promising advancements. in the field of autonomous driving, however, often very simple scenarios are being examined. common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. in this work, we introduce informed reinforcement learning, where a structured rulebook is integrated as a knowledge source. we learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. our method is applicable to arbitrary rl models. we successfully demonstrate high completion rates of complex scenarios with recent model-based agents.",10.1109/icra57147.2024.10610842,2024-02-06,2024-06-12,"['daniel bogdoll', 'jing qin', 'moritz nekolla', 'ahmed abouelazm', 'tim joseph', 'j. marius zöllner']"
2402.11487,visual concept-driven image generation with text-to-image diffusion   model,cs.cv,"text-to-image (tti) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. however, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. in this work, we propose a concept-driven tti personalization framework that addresses these core challenges. we build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the tti model. however, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. we do so by introducing an expectation maximization (em)-like optimization procedure where we alternate between learning the custom tokens and estimating (latent) masks encompassing corresponding concepts in user-supplied images. we obtain these masks based on cross-attention, from within the u-net parameterized latent diffusion model and subsequent densecrf optimization. we illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a by-product, latent masks. we illustrate the benefits of the proposed approach qualitatively and quantitatively with several examples and use cases that can combine three or more entangled concepts.",,2024-02-18,2025-05-01,"['tanzila rahman', 'shweta mahajan', 'hsin-ying lee', 'jian ren', 'sergey tulyakov', 'leonid sigal']"
2402.17767,opening articulated structures in the real world,cs.ro cs.ai cs.cv cs.lg,"what does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? this work answers this question using opening of articulated structures as a mobile manipulation testbed. specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. we first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. videos, code, and models are available on the project website: https://arjung128.github.io/opening-articulated-structures/",,2024-02-27,2025-05-06,"['arjun gupta', 'michelle zhang', 'rishik sathua', 'saurabh gupta']"
2403.02241,neural redshift: random networks are not random functions,cs.lg cs.ai cs.cv,"our understanding of the generalization capabilities of neural networks (nns) is still incomplete. prevailing explanations are based on implicit biases of gradient descent (gd) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. this paper seeks other sources of generalization in nns.   findings. to understand the inductive biases provided by architectures independently from gd, we examine untrained, random-weight networks. even simple mlps show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. but unlike common wisdom, nns do not have an inherent ""simplicity bias"". this property depends on components such as relus, residual connections, and layer normalizations. alternative architectures can be built with a bias for any level of complexity. transformers also inherit all these properties from their building blocks.   implications. we provide a fresh explanation for the success of deep learning independent from gradient-based training. it points at promising avenues for controlling the solutions implemented by trained models.",,2024-03-04,2025-04-29,"['damien teney', 'armand nicolicioiu', 'valentin hartmann', 'ehsan abbasnejad']"
2403.02411,ninformer: a network in network transformer with token mixing generated   gating function,cs.cv cs.lg,"the attention mechanism is the primary component of the transformer architecture; it has led to significant advancements in deep learning spanning many domains and covering multiple tasks. in computer vision, the attention mechanism was first incorporated in the vision transformer vit, and then its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. while the attention mechanism is very expressive and capable, it comes with the disadvantage of being computationally expensive and requiring datasets of considerable size for effective optimization. to address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. examples of such attempts in the vision domain are the mlp-mixer, the conv-mixer, the perciver-io, and many more attempts with different sets of advantages and disadvantages. this paper introduces a new computational block as an alternative to the standard vit block. the newly proposed block reduces the computational requirements by replacing the normal attention layers with a network in network structure, therefore enhancing the static approach of the mlp-mixer with a dynamic learning of element-wise gating function generated by a token mixing process. extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the vision domain.",10.1007/s00521-025-11226-1,2024-03-04,2025-05-01,"['abdullah nazhat abdullah', 'tarkan aydin']"
2403.06869,impact of noisy supervision in foundation model learning,cs.lg cs.ai cs.cl cs.cv,"foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. however, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. this paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy imagenet-1k, yfcc15m, and cc12m datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (id) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (ood) performance, where training and testing distributions are significantly different. these observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. we empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. we then propose a tuning method (nmtune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. we additionally conduct extensive experiments on popular vision and language models, including apis, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as noisy model learning.",,2024-03-11,2025-05-04,"['hao chen', 'zihan wang', 'ran tao', 'hongxin wei', 'xing xie', 'masashi sugiyama', 'bhiksha raj', 'jindong wang']"
2403.07569,exploring challenges in deep learning of single-station ground motion   records,eess.sp cs.cv cs.lg,"contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. these models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. however, the extent to which these models truly extract meaningful patterns from these complex time-series signals remains underexplored. in this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. our experimental results reveal a strong dependence on the highly correlated primary (p) and secondary (s) phase arrival times. these findings expose a critical gap in the current research landscape, highlighting the lack of robust methodologies for deep learning from single-station ground motion recordings that do not rely on auxiliary inputs.",,2024-03-12,2025-05-04,"['ümit mert çağlar', 'baris yilmaz', 'melek türkmen', 'erdem akagündüz', 'salih tileylioglu']"
2403.08142,fieldnet: efficient real-time shadow removal for enhanced vision in   field robotics,cs.cv,"shadows significantly hinder computer vision tasks in outdoor environments, particularly in field robotics, where varying lighting conditions complicate object detection and localisation. we present fieldnet, a novel deep learning framework for real-time shadow removal, optimised for resource-constrained hardware. fieldnet introduces a probabilistic enhancement module and a novel loss function to address challenges of inconsistent shadow boundary supervision and artefact generation, achieving enhanced accuracy and simplicity without requiring shadow masks during inference. trained on a dataset of 10,000 natural images augmented with synthetic shadows, fieldnet outperforms state-of-the-art methods on benchmark datasets (istd, istd+, srd), with up to $9$x speed improvements (66 fps on nvidia 2080ti) and superior shadow removal quality (psnr: 38.67, ssim: 0.991). real-world case studies in precision agriculture robotics demonstrate the practical impact of fieldnet in enhancing weed detection accuracy. these advancements establish fieldnet as a robust, efficient solution for real-time vision tasks in field robotics and beyond.",10.1016/j.eswa.2025.127442,2024-03-12,2025-05-07,"['alzayat saleh', 'alex olsen', 'jake wood', 'bronson philippa', 'mostafa rahimi azghadi']"
2403.08256,ig-fiqa: improving face image quality assessment through intra-class   variance guidance robust to inaccurate pseudo-labels,cs.cv,"in the realm of face image quality assesment (fiqa), method based on sample relative classification have shown impressive performance. however, the quality scores used as pseudo-labels assigned from images of classes with low intra-class variance could be unrelated to the actual quality in this method. to address this issue, we present ig-fiqa, a novel approach to guide fiqa training, introducing a weight parameter to alleviate the adverse impact of these classes. this method involves estimating sample intra-class variance at each iteration during training, ensuring minimal computational overhead and straightforward implementation. furthermore, this paper proposes an on-the-fly data augmentation methodology for improved generalization performance in fiqa. on various benchmark datasets, our proposed method, ig-fiqa, achieved novel state-of-the-art (sota) performance.",10.1109/access.2025.3562654,2024-03-13,,"['minsoo kim', 'gi pyo nam', 'haksub kim', 'haesol park', 'ig-jae kim']"
2403.10635,medslip: medical dual-stream language-image pre-training with   pathology-anatomy semantic alignment,cs.cv,"pathology and anatomy are two essential groups of semantics in medical data. pathology describes what the diseases are, while anatomy explains where the diseases occur. they describe diseases from different perspectives, providing complementary insights into diseases. thus, properly understanding these semantics and their relationships can enhance medical vision-language models (vlms). however, pathology and anatomy semantics are usually entangled in medical data, hindering vlms from explicitly modeling these semantics and their relationships. to address this challenge, we propose medslip, a novel medical dual-stream language-image pre-training pipeline, to disentangle pathology and anatomy semantics and model the relationships between them. we introduce a dual-stream mechanism in medslip to explicitly disentangle medical semantics into pathology-relevant and anatomy-relevant streams and align visual and textual information within each stream. furthermore, we propose an interaction modeling module with prototypical contrastive learning loss and intra-image contrastive learning loss to regularize the relationships between pathology and anatomy semantics. we apply medslip to chest x-ray analysis and conduct comprehensive evaluations with four benchmark datasets: nih cxr14, rsna pneumonia, siim-acr pneumothorax, and covidx cxr-4. the results demonstrate medslip's superior generalizability and transferability across different scenarios. the code is available at https://github.com/shef-aire/medslip, and the pre-trained model is released at https://huggingface.co/pykale/medslip.",,2024-03-15,2025-04-30,"['wenrui fan', 'mohammod n. i. suvon', 'shuo zhou', 'xianyuan liu', 'samer alabed', 'venet osmani', 'andrew j. swift', 'chen chen', 'haiping lu']"
2403.12431,geometric constraints in deep learning frameworks: a survey,cs.cv cs.ai,"stereophotogrammetry is an emerging technique of scene understanding. its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. since then, thousands of approaches have been explored. the classic geometric techniques of shape from stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. more recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. in this survey, we explore the overlap for geometric-based and deep learning-based frameworks. we compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. we present a new taxonomy for prevalent geometry enforcing constraints used in modern deep learning frameworks. we also present insightful observations and potential future research directions.",10.1145/3729221,2024-03-19,,"['vibhas k vats', 'david j crandall']"
2403.16677,fool: addressing the downlink bottleneck in satellite computing with   neural feature compression,cs.lg cs.cv cs.dc cs.ni eess.iv,"nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for earth observation. as constellation sizes increase, network contention poses a downlink bottleneck. orbital edge computing (oec) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. however, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. this work presents fool, an oec-native and task-agnostic feature compression method that preserves prediction performance. fool partitions high-resolution satellite imagery to maximize throughput. further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. while fool is a feature compressor, it can recover images with competitive scores on quality measures at lower bitrates. we extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. lastly, we test the feasibility of our system for standardized nanosatellite form factors. we demonstrate that fool permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.",10.1109/tmc.2025.3544516,2024-03-25,2025-05-01,"['alireza furutanpey', 'qiyang zhang', 'philipp raith', 'tobias pfandzelter', 'shangguang wang', 'schahram dustdar']"
2403.18816,garment3dgen: 3d garment stylization and texture generation,cs.cv,"we introduce garment3dgen a new method to synthesize 3d garment assets from a base mesh given a single input image as guidance. our proposed approach allows users to generate 3d textured clothes based on both real and synthetic images, such as those generated by text prompts. the generated assets can be directly draped and simulated on human bodies. we leverage the recent progress of image-to-3d diffusion methods to generate 3d garment geometries. however, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3d target. carefully designed losses allow the base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. finally, we generate high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3d assets. with garment3dgen users can generate the simulation-ready 3d garment of their choice without the need of artist intervention. we present a plethora of quantitative and qualitative comparisons on various assets and demonstrate that garment3dgen unlocks key applications ranging from sketch-to-simulated garments or interacting with the garments in vr. code is publicly available.",,2024-03-27,2025-04-30,"['nikolaos sarafianos', 'tuur stuyck', 'xiaoyu xiang', 'yilei li', 'jovan popovic', 'rakesh ranjan']"
2404.01249,fireants: adaptive riemannian optimization for multi-scale diffeomorphic   matching,cs.cv,"the paper proposes fireants, the first multi-scale adaptive riemannian optimization algorithm for dense diffeomorphic image matching. one of the most critical and understudied aspects of diffeomorphic image matching algorithms are its highly ill-conditioned nature. we quantitatively capture the extent of ill-conditioning in a typical mri matching task, motivating the need for an adaptive optimization algorithm for diffeomorphic matching. to this end, fireants generalizes the concept of momentum and adaptive estimates of the hessian to mitigate this ill-conditioning in the non-euclidean space of diffeomorphisms. unlike common non-euclidean manifolds, we also formalize considerations for multi-scale optimization of diffeomorphisms. our rigorous mathematical results and operational contributions lead to a state-of-the-art dense matching algorithm that can be applied to generic image data with remarkable accuracy and robustness. we demonstrate consistent improvements in image matching performance across a spectrum of community-standard medical and biological correspondence matching challenges spanning a wide variety of image modalities, anatomies, resolutions, acquisition protocols, and preprocessing pipelines. this improvement is supplemented by 300x to 3200x speedup over existing cpu-based state-of-the-art algorithms. for the first time, we perform diffeomorphic matching of sub-micron mouse isocortex volumes at native resolution, and generate a 25{\mu}m mouse brain atlas in under 25 minutes. our fast implementation also enables hyperparameter studies that were intractable with existing correspondence matching algorithms.",,2024-04-01,2025-05-03,"['rohit jena', 'pratik chaudhari', 'james c. gee']"
2404.01330,p-hologen: an end-to-end generative framework for phase-only holograms,cs.cv cs.gr cs.lg eess.iv,"holography stands at the forefront of visual technology, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. although generative models have been extensively explored in the image domain, their application to holograms remains relatively underexplored due to the inherent complexity of phase learning. exploiting generative models for holograms offers exciting opportunities for advancing innovation and creativity, such as semantic-aware hologram generation and editing. currently, the most viable approach for utilizing generative models in the hologram domain involves integrating an image-based generative model with an image-to-hologram conversion model, which comes at the cost of increased computational complexity and inefficiency. to tackle this problem, we introduce p-hologen, the first end-to-end generative framework designed for phase-only holograms (pohs). p-hologen employs vector quantized variational autoencoders to capture the complex distributions of pohs. it also integrates the angular spectrum method into the training process, constructing latent spaces for complex phase data using strategies from the image processing domain. extensive experiments demonstrate that p-hologen achieves superior quality and computational efficiency compared to the existing methods. furthermore, our model generates high-quality unseen, diverse holographic content from its learned latent space without requiring pre-existing images. our work paves the way for new applications and methodologies in holographic content creation, opening a new era in the exploration of generative holographic content. the code for our paper is publicly available on https://github.com/james0223/p-hologen.",10.1111/cgf.15244,2024-03-29,2025-05-02,"['joohyun park', 'yujin jeon', 'huiyong kim', 'seunghwan baek', 'hyeongyeop kang']"
2404.02225,chosen: contrastive hypothesis selection for multi-view depth refinement,cs.cv cs.ai,"we propose chosen, a simple yet flexible, robust and effective multi-view depth refinement framework. it can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. given an initial depth estimation, chosen iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. the key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. integrated in a simple baseline multi-view stereo pipeline, chosen delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.",,2024-04-02,2025-05-05,"['di qiu', 'yinda zhang', 'thabo beeler', 'vladimir tankovich', 'christian häne', 'sean fanello', 'christoph rhemann', 'sergio orts escolano']"
2404.05046,fgaif: aligning large vision-language models with fine-grained ai   feedback,cs.cv cs.cl,"large vision-language models (lvlms) have demonstrated proficiency in tackling a variety of visual-language tasks. however, current lvlms suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. to tackle this issue, existing methods mainly utilize reinforcement learning (rl) to align modalities in lvlms. however, they still suffer from three main limitations: (1) general feedback can not indicate the hallucination type contained in the response; (2) sparse rewards only give the sequence-level reward for the whole response; and (3)annotation cost is time-consuming and labor-intensive. to handle these limitations, we propose an innovative method to align modalities in lvlms through fine-grained artificial intelligence feedback (fgaif), which mainly consists of three steps: ai-based feedback collection, fine-grained reward model training, and reinforcement learning with fine-grained reward. specifically, we first utilize ai tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. finally, a novel fine-grained feedback module is integrated into the proximal policy optimization (ppo) algorithm. extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. notably, compared with previous models trained with the rl-based aligning method, our proposed method is effective even with fewer parameters.",,2024-04-07,2025-05-06,"['liqiang jing', 'xinya du']"
2404.10716,mowa: multiple-in-one image warping model,cs.cv,"while recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations. to address diverse types of warping in practice, we propose a multiple-in-one image warping model (named mowa) in this work. specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level. to further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for more accurate estimation. to our knowledge, this is the first work that solves multiple practical warping tasks in one single model. extensive experiments demonstrate that our mowa, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks. moreover, mowa also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations. the code and more visual results can be found on the project page: https://kangliao929.github.io/projects/mowa/.",,2024-04-16,2025-05-03,"['kang liao', 'zongsheng yue', 'zhonghua wu', 'chen change loy']"
2404.18624,do vision & language decoders use images and text equally? how   self-consistent are their explanations?,cs.cl cs.ai cs.cv cs.lg,"vision and language model (vlm) decoders are currently the best-performing architectures on multimodal tasks. next to answers, they are able to produce natural language explanations, either in post-hoc or cot settings. however, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. in this work, we investigate if vlms rely on their input modalities differently when they produce explanations as opposed to answers. we also evaluate the self-consistency of vlm decoders in both post-hoc and cot explanation settings, by extending existing unimodal tests and measures to vlm decoders. we find that most tested vlms are less self-consistent than llms. text contributions in all tested vl decoders are more important than image contributions in all examined tasks. however, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. this difference is even larger in cot compared to post-hoc explanations. lastly, we provide an up-to-date benchmarking of state-of-the-art vl decoders on the valse benchmark, which before was restricted to vl encoders. we find that the tested vl decoders still struggle with most phenomena tested by valse.",,2024-04-29,2025-05-01,"['letitia parcalabescu', 'anette frank']"
2405.00318,covariant spatio-temporal receptive fields for spiking neural networks,cs.ne cs.cv cs.lg,"biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. there are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. we use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. this work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control.",,2024-05-01,2025-05-04,"['jens egholm pedersen', 'jörg conradt', 'tony lindeberg']"
2405.00507,f2m-reg: unsupervised rgb-d point cloud registration with frame-to-model   optimization,cs.cv,"this work studies the problem of unsupervised rgb-d point cloud registration, which aims at training a robust registration model without ground-truth pose supervision. existing methods usually leverages unposed rgb-d sequences and adopt a frame-to-frame framework based on differentiable rendering to train the registration model, which enforces the photometric and geometric consistency between the two frames for supervision. however, this frame-to-frame framework is vulnerable to inconsistent factors between different frames, e.g., lighting changes, geometry occlusion, and reflective materials, which leads to suboptimal convergence of the registration model. in this paper, we propose a novel frame-to-model optimization framework named f2m-reg for unsupervised rgb-d point cloud registration. we leverage the neural implicit field as a global model of the scene and optimize the estimated poses of the frames by registering them to the global model, and the registration model is subsequently trained with the optimized poses. thanks to the global encoding capability of neural implicit field, our frame-to-model framework is significantly more robust to inconsistent factors between different frames and thus can provide better supervision for the registration model. besides, we demonstrate that f2m-reg can be further enhanced by a simplistic synthetic warming-up strategy. to this end, we construct a photorealistic synthetic dataset named sim-rgbd to initialize the registration model for the frame-to-model optimization on real-world rgb-d sequences. extensive experiments on four challenging benchmarks have shown that our method surpasses the previous state-of-the-art counterparts by a large margin, especially under scenarios with severe lighting changes and low overlap. our code and models are available at https://github.com/mrisland/f2m_reg.",,2024-05-01,2025-05-01,"['zhinan yu', 'zheng qin', 'yijie tang', 'yongjun wang', 'renjiao yi', 'chenyang zhu', 'kai xu']"
2405.00998,part-aware shape generation with latent 3d diffusion of neural voxel   fields,cs.cv,"this paper presents a novel latent 3d diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures. compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation. on one hand, we introduce a latent 3d diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details. on the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results. through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data. the results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods.",,2024-05-02,2025-05-01,"['yuhang huang', 'shilong zou', 'xinwang liu', 'kai xu']"
2405.01101,enhancing person re-identification via uncertainty feature fusion method   and auto-weighted measure combination,cs.cv,"person re-identification (re-id) is a challenging task that involves identifying the same person across different camera views in surveillance systems. current methods usually rely on features from single-camera views, which can be limiting when dealing with multiple cameras and challenges such as changing viewpoints and occlusions. in this paper, a new approach is introduced that enhances the capability of reid models through the uncertain feature fusion method (uffm) and auto-weighted measure combination (amc). uffm generates multi-view features using features extracted independently from multiple images to mitigate view bias. however, relying only on similarity based on multi-view features is limited because these features ignore the details represented in single-view features. therefore, we propose the amc method to generate a more robust similarity measure by combining various measures. our method significantly improves rank@1 accuracy and mean average precision (map) when evaluated on person re-identification datasets. combined with the bot baseline on challenging datasets, we achieve impressive results, with a 7.9% improvement in rank@1 and a 12.1% improvement in map on the msmt17 dataset. on the occluded-dukemtmc dataset, our method increases rank@1 by 22.0% and map by 18.4%. code is available: https://github.com/chequanghuy/enhancing-person-re-identification-via-uffm-and-amc",10.1016/j.knosys.2024.112737,2024-05-02,2025-05-05,"['quang-huy che', 'le-chuong nguyen', 'duc-tuan luu', 'vinh-tiep nguyen']"
2405.04489,s3former: self-supervised high-resolution transformer for solar pv   profiling,cs.cv,"as the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident. renewable energies have emerged as a viable solution for users, with photovoltaic energy being a favored choice for small installations due to its reliability and efficiency. accurate mapping of pv installations is crucial for understanding the extension of its adoption and informing energy policy. to meet this need, we introduce s3former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid. solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, ground sampling distance variations and lack of appropriate initialization weights for optimized training. to tackle these complexities, s3former features a masked attention mask transformer incorporating a self-supervised learning pretrained backbone. specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the transformer architecture to enhance the localization of solar pv installations. we introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of s3former. we evaluated s3former using diverse datasets, demonstrate improvement state-of-the-art models.",,2024-05-07,2025-04-30,"['minh tran', 'adrian de luis', 'haitao liao', 'ying huang', 'roy mccann', 'alan mantooth', 'jack cothren', 'ngan le']"
2405.10718,signllm: sign language production large language models,cs.cv cs.cl,"in this paper, we propose signllm, a multilingual sign language production (slp) large language model, which includes two novel multilingual slp modes mlsf and prompt2langgloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. both modes can use a new rl loss based on reinforcement learning and a new rl module named priority learning channel. these rl components can accelerate the training by enhancing the model's capability to sample high-quality data. to train signllm, we introduce prompt2sign, a comprehensive multilingual sign language dataset, which builds from public data, including american sign language (asl) and seven others. this dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. we extensively evaluate signllm, demonstrating that our model achieves state-of-the-art performance on slp tasks across eight sign languages.",,2024-05-17,2025-04-29,"['sen fang', 'chen chen', 'lei wang', 'ce zheng', 'chunyu sui', 'yapeng tian']"
2405.11536,robmot: robust 3d multi-object tracking by observational noise and state   estimation drift mitigation on lidar pointcloud,cs.cv cs.ro,"this paper addresses limitations in 3d tracking-by-detection methods, particularly in identifying legitimate trajectories and reducing state estimation drift in kalman filters. existing methods often use threshold-based filtering for detection scores, which can fail for distant and occluded objects, leading to false positives. to tackle this, we propose a novel track validity mechanism and multi-stage observational gating process, significantly reducing ghost tracks and enhancing tracking performance. our method achieves a $29.47\%$ improvement in multi-object tracking accuracy (mota) on the kitti validation dataset with the second detector. additionally, a refined kalman filter term reduces localization noise, improving higher-order tracking accuracy (hota) by $4.8\%$. the online framework, robmot, outperforms state-of-the-art methods across multiple detectors, with hota improvements of up to $3.92\%$ on the kitti testing dataset and $8.7\%$ on the validation dataset, while achieving low identity switch scores. robmot excels in challenging scenarios, tracking distant objects and prolonged occlusions, with a $1.77\%$ mota improvement on the waymo open dataset, and operates at a remarkable 3221 fps on a single cpu, proving its efficiency for real-time multi-object tracking.",,2024-05-19,2025-05-01,"['mohamed nagy', 'naoufel werghi', 'bilal hassan', 'jorge dias', 'majid khonji']"
2405.18416,3d streetunveiler with semantic-aware 2dgs -- a simple baseline,cs.cv,"unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. however, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. unlike object-centric 3d inpainting, which relies on thorough observation in a small scene, street scene cases involve long trajectories that differ from previous 3d inpainting tasks. the camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. to address these obstacles, we introduce streetunveiler to reconstruct an empty street. streetunveiler learns a 3d representation of the empty street from crowded observations. our representation is based on the hard-label semantic 2d gaussian splatting (2dgs) for its scalability and ability to identify gaussians to be removed. we inpaint rendered image after removing unwanted gaussians to provide pseudo-labels and subsequently re-optimize the 2dgs. given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. this decomposition helps us to minimize the regions that need to be inpainted. to enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. our experiments conducted on the street scene dataset successfully reconstructed a 3d representation of the empty street. the mesh representation of the empty street can be extracted for further applications. the project page and more visualizations can be found at: https://streetunveiler.github.io",,2024-05-28,2025-04-30,"['jingwei xu', 'yikai wang', 'yiqun zhao', 'yanwei fu', 'shenghua gao']"
2405.20152,uncovering bias in large vision-language models at scale with   counterfactuals,cs.cv,"with the advent of large language models (llms) possessing increasingly impressive capabilities, a number of large vision-language models (lvlms) have been proposed to augment llms with visual inputs. such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. while prior studies have examined the social biases contained in text generated by llms, this topic has been relatively unexplored in lvlms. examining social biases in lvlms is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. to address this challenging problem, we conduct a large-scale study of text generated by different lvlms under counterfactual changes to input images, producing over 57 million responses from popular models. our multi-dimensional bias evaluation framework reveals that social attributes such as perceived race, gender, and physical characteristics depicted in images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of individuals.",,2024-05-30,2025-04-30,"['phillip howard', 'kathleen c. fraser', 'anahita bhiwandiwalla', 'svetlana kiritchenko']"
2406.00834,end-to-end hybrid refractive-diffractive lens design with differentiable   ray-wave model,cs.gr cs.cv physics.optics,"hybrid refractive-diffractive lenses combine the light efficiency of refractive lenses with the information encoding power of diffractive optical elements (doe), showing great potential as the next generation of imaging systems. however, accurately simulating such hybrid designs is generally difficult, and in particular, there are no existing differentiable image formation models for hybrid lenses with sufficient accuracy.   in this work, we propose a new hybrid ray-tracing and wave-propagation (ray-wave) model for accurate simulation of both optical aberrations and diffractive phase modulation, where the doe is placed between the last refractive surface and the image sensor, i.e. away from the fourier plane that is often used as a doe position. the proposed ray-wave model is fully differentiable, enabling gradient back-propagation for end-to-end co-design of refractive-diffractive lens optimization and the image reconstruction network. we validate the accuracy of the proposed model by comparing the simulated point spread functions (psfs) with theoretical results, as well as simulation experiments that show our model to be more accurate than solutions implemented in commercial software packages like zemax. we demonstrate the effectiveness of the proposed model through real-world experiments and show significant improvements in both aberration correction and extended depth-of-field (edof) imaging. we believe the proposed model will motivate further investigation into a wide range of applications in computational imaging, computational photography, and advanced optical design. code will be released upon publication.",10.1145/3680528.3687640,2024-06-02,,"['xinge yang', 'matheus souza', 'kunyi wang', 'praneeth chakravarthula', 'qiang fu', 'wolfgang heidrich']"
2406.00985,paralleledits: efficient multi-object image editing,cs.cv,"text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. a major challenge is making simultaneous edits across multiple objects or attributes. applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. in this paper, we address these challenges with significant contributions. our main contribution is the development of paralleledits, a method that seamlessly manages simultaneous edits across multiple attributes. in contrast to previous approaches, paralleledits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. this is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. additionally, we introduce the pie-bench++ dataset, an expansion of the original pie-bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. this dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios.",,2024-06-03,2025-05-04,"['mingzhen huang', 'jialing cai', 'shan jia', 'vishnu suresh lokhande', 'siwei lyu']"
2406.01494,robust classification by coupling data mollification with label   smoothing,cs.cv cs.lg stat.ml,"introducing training-time augmentations is a key technique to enhance generalization and prepare deep neural networks against test-time corruptions. inspired by the success of generative diffusion models, we propose a novel approach of coupling data mollification, in the form of image noising and blurring, with label smoothing to align predicted label confidences with image degradation. the method is simple to implement, introduces negligible overheads, and can be combined with existing augmentations. we demonstrate improved robustness and uncertainty quantification on the corrupted image benchmarks of cifar, tinyimagenet and imagenet datasets.",,2024-06-03,2025-05-01,"['markus heinonen', 'ba-hien tran', 'michael kampffmeyer', 'maurizio filippone']"
2406.02720,3d-hgs: 3d half-gaussian splatting,cs.cv cs.gr,"photo-realistic image rendering from 3d scene reconstruction has advanced significantly with neural rendering techniques. among these, 3d gaussian splatting (3d-gs) outperforms neural radiance fields (nerfs) in quality and speed but struggles with shape and color discontinuities. we propose 3d half-gaussian (3d-hgs) kernels as a plug-and-play solution to address these limitations. our experiments show that 3d-hgs enhances existing 3d-gs methods, achieving state-of-the-art rendering quality without compromising speed.",,2024-06-04,2025-05-04,"['haolin li', 'jinyang liu', 'mario sznaier', 'octavia camps']"
2406.03184,ouroboros3d: image-to-3d generation via 3d-aware recursive diffusion,cs.cv,"existing single image-to-3d creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3d reconstruction. however, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. we introduce a unified 3d generation framework, named ouroboros3d, which integrates diffusion-based multi-view image generation and 3d reconstruction into a recursive diffusion process. in our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. during the multi-view denoising process, the multi-view diffusion model uses the 3d-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. the recursive diffusion framework with 3d-aware feedback unites the entire process and improves geometric consistency.experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. project page: https://costwen.github.io/ouroboros3d/",,2024-06-05,2025-05-01,"['hao wen', 'zehuan huang', 'yaohui wang', 'xinyuan chen', 'lu sheng']"
2406.04321,vidmuse: a simple video-to-music generation framework with   long-short-term modeling,cs.cv cs.lg cs.mm cs.sd,"in this work, we systematically study music generation conditioned solely on the video. first, we present a large-scale dataset comprising 360k video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. furthermore, we propose vidmuse, a simple framework for generating music aligned with video inputs. vidmuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. by incorporating local and global visual cues, vidmuse enables the creation of musically coherent audio tracks that consistently match the video content through long-short-term modeling. through extensive experiments, vidmuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. the code and datasets are available at https://vidmuse.github.io/.",,2024-06-06,2025-05-07,"['zeyue tian', 'zhaoyang liu', 'ruibin yuan', 'jiahao pan', 'qifeng liu', 'xu tan', 'qifeng chen', 'wei xue', 'yike guo']"
2406.06050,generalizable human gaussians from single-view image,cs.cv,"in this work, we tackle the task of learning 3d human gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. we introduce a single-view generalizable human gaussian model (hgm), which employs a novel generate-then-refine pipeline with the guidance from human body prior and diffusion prior. our approach uses a controlnet to refine rendered back-view images from coarse predicted human gaussians, then uses the refined image along with the input image to reconstruct refined human gaussians. to mitigate the potential generation of unrealistic human poses and shapes, we incorporate human priors from the smpl-x model as a dual branch, propagating image features from the smpl-x volume to the image gaussians using sparse convolution and attention mechanisms. given that the initial smpl-x estimation might be inaccurate, we gradually refine it with our hgm model. we validate our approach on several publicly available datasets. our method surpasses previous methods in both novel view synthesis and surface reconstruction. our approach also exhibits strong generalization for cross-dataset evaluation and in-the-wild images.",,2024-06-10,2025-05-08,"['jinnan chen', 'chen li', 'jianfeng zhang', 'lingting zhu', 'buzhen huang', 'hanlin chen', 'gim hee lee']"
2406.07113,beyond bare queries: open-vocabulary object grounding with 3d scene   graph,cs.cv cs.ai,"locating objects described in natural language presents a significant challenge for autonomous agents. existing clip-based open-vocabulary methods successfully perform 3d object grounding with simple (bare) queries, but cannot cope with ambiguous descriptions that demand an understanding of object relations. to tackle this problem, we propose a modular approach called bbq (beyond bare queries), which constructs 3d scene graph representation with metric and semantic spatial edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. bbq employs robust dino-powered associations to construct 3d object-centric map and an advanced raycasting algorithm with a 2d vision-language model to describe them as graph nodes. on the replica and scannet datasets, we have demonstrated that bbq takes a leading place in open-vocabulary 3d semantic segmentation compared to other zero-shot methods. also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. on challenging sr3d+, nr3d and scanrefer benchmarks, our deductive approach demonstrates a significant improvement, enabling objects grounding by complex queries compared to other state-of-the-art methods. the combination of our design choices and software implementation has resulted in significant data processing speed in experiments on the robot on-board computer. this promising performance enables the application of our approach in intelligent robotics projects. we made the code publicly available at https://linukc.github.io/beyondbarequeries/.",,2024-06-11,2025-05-06,"['sergey linok', 'tatiana zemskova', 'svetlana ladanova', 'roman titkov', 'dmitry yudin', 'maxim monastyrny', 'aleksei valenkov']"
2406.07361,deep implicit optimization enables robust learnable features for   deformable image registration,cs.cv cs.lg eess.iv,"deep learning in image registration (dlir) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. however, existing dlir methods forego many of the benefits and invariances of optimization methods. the lack of a task-specific inductive bias in dlir methods leads to suboptimal performance, especially in the presence of domain shift. our method aims to bridge this gap between statistical learning and optimization by explicitly incorporating optimization as a layer in a deep network. a deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. this optimal warp is then used to minimize image and label alignment errors. by implicitly differentiating end-to-end through an iterative optimization solver, we explicitly exploit invariances of the correspondence matching problem induced by the optimization, while learning registration and label-aware features, and guaranteeing the warp functions to be a local minima of the registration objective in the feature space. our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. for the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. end-to-end feature learning also facilitates interpretability of features and arbitrary test-time regularization, which is not possible with existing dlir methods.",10.1016/j.media.2025.103577,2024-06-11,2025-05-03,"['rohit jena', 'pratik chaudhari', 'james c. gee']"
2406.07880,a comprehensive survey on machine learning driven material defect   detection,cs.cv eess.iv,"material defects (md) represent a primary challenge affecting product performance and giving rise to safety issues in related products. the rapid and accurate identification and localization of md constitute crucial research endeavors in addressing contemporary challenges associated with md. in recent years, propelled by the swift advancement of machine learning (ml) technologies, particularly exemplified by deep learning, ml has swiftly emerged as the core technology and a prominent research direction for material defect detection (mdd). through a comprehensive review of the latest literature, we systematically survey the ml techniques applied in mdd into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. we provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. finally, the survey explores potential future directions in mdd utilizing ml technologies. this survey consolidates ml-based mdd literature and provides a foundation for future research and practice.",10.1145/3730576,2024-06-12,2025-05-02,"['jun bai', 'di wu', 'tristan shelley', 'peter schubel', 'david twine', 'john russell', 'xuesen zeng', 'ji zhang']"
2406.13896,smore: simultaneous map and object reconstruction,cs.cv,"we present a method for dynamic surface reconstruction of large-scale urban scenes from lidar. depth-based reconstructions tend to focus on small-scale objects or large-scale slam reconstructions that treat moving objects as outliers. we take a holistic perspective and optimize a compositional model of a dynamic scene that decomposes the world into rigidly-moving objects and the background. to achieve this, we take inspiration from recent novel view synthesis methods and frame the reconstruction problem as a global optimization over neural surfaces, ego poses, and object poses, which minimizes the error between composed spacetime surfaces and input lidar scans. in contrast to view synthesis methods, which typically minimize 2d errors with gradient descent, we minimize a 3d point-to-surface error by coordinate descent, which we decompose into registration and surface reconstruction steps. each step can be handled well by off-the-shelf methods without any re-training. we analyze the surface reconstruction step for rolling-shutter lidars, and show that deskewing operations common in continuous time slam can be applied to dynamic objects as well, improving results over prior art by an order of magnitude. beyond pursuing dynamic reconstruction as a goal in and of itself, we propose that such a system can be used to auto-label partially annotated sequences and produce ground truth annotation for hard-to-label problems such as depth completion and scene flow. please see https://anishmadan23.github.io/smore/ for more visual results.",,2024-06-19,2025-05-06,"['nathaniel chodosh', 'anish madan', 'simon lucey', 'deva ramanan']"
2406.17774,uncertainty for svbrdf acquisition using frequency analysis,cs.cv cs.gr,"this paper aims to quantify uncertainty for svbrdf acquisition in multi-view captures. under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. we study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. the result is a method that computes a map of uncertainty over an entire object within a millisecond. we find that the frequency model allows us to recover svbrdf parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. we then show that the uncertainty map can be applied to improve svbrdf acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. our code is available at https://github.com/rubenwiersma/svbrdf_uncertainty.",10.1145/3721238.3730592,2024-06-25,2025-05-07,"['ruben wiersma', 'julien philip', 'miloš hašan', 'krishna mullia', 'fujun luan', 'elmar eisemann', 'valentin deschaintre']"
2406.18360,xld: a cross-lane dataset for benchmarking novel driving view synthesis,cs.cv,"comprehensive testing of autonomous systems through simulation is essential to ensure the safety of autonomous driving vehicles. this requires the generation of safety-critical scenarios that extend beyond the limitations of real-world data collection, as many of these scenarios are rare or rarely encountered on public roads. however, evaluating most existing novel view synthesis (nvs) methods relies on sporadic sampling of image frames from the training data, comparing the rendered images with ground-truth images. unfortunately, this evaluation protocol falls short of meeting the actual requirements in closed-loop simulations. specifically, the true application demands the capability to render novel views that extend beyond the original trajectory (such as cross-lane views), which are challenging to capture in the real world. to address this, this paper presents a synthetic dataset for novel driving view synthesis evaluation, which is specifically designed for autonomous driving simulations. this unique dataset includes testing images captured by deviating from the training trajectory by $1-4$ meters. it comprises six sequences that cover various times and weather conditions. each sequence contains $450$ training images, $120$ testing images, and their corresponding camera poses and intrinsic parameters. leveraging this novel dataset, we establish the first realistic benchmark for evaluating existing nvs approaches under front-only and multicamera settings. the experimental findings underscore the significant gap in current approaches, revealing their inadequate ability to fulfill the demanding prerequisites of cross-lane or closed-loop simulation.",,2024-06-26,2025-05-07,"['hao li', 'chenming wu', 'ming yuan', 'yan zhang', 'chen zhao', 'chunyu song', 'haocheng feng', 'errui ding', 'dingwen zhang', 'jingdong wang']"
2407.01866,image-gs: content-adaptive image representation via 2d gaussians,cs.cv cs.gr,"neural image representations have emerged as a promising approach for encoding and rendering visual data. combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications.   inspired by recent advancements in radiance field rendering, we introduce image-gs, a content-adaptive image representation based on 2d gaussians. leveraging a custom differentiable renderer, image-gs reconstructs images by adaptively allocating and progressively optimizing a group of anisotropic, colored 2d gaussians. it achieves a favorable balance between visual fidelity and memory efficiency across a variety of stylized images frequently seen in graphics workflows, especially for those showing non-uniformly distributed features and in low-bitrate regimes. moreover, it supports hardware-friendly rapid random access for real-time usage, requiring only 0.3k macs to decode a pixel. through error-guided progressive optimization, image-gs naturally constructs a smooth level-of-detail hierarchy. we demonstrate its versatility with several applications, including texture compression, semantics-aware compression, and joint image compression and restoration.",10.1145/3721238.3730596,2024-07-01,2025-05-07,"['yunxiang zhang', 'bingxuan li', 'alexandr kuznetsov', 'akshay jindal', 'stavros diolatzis', 'kenneth chen', 'anton sochenov', 'anton kaplanyan', 'qi sun']"
2407.02329,migc++: advanced multi-instance generation controller for image   synthesis,cs.cv,"we introduce the multi-instance generation (mig) task, which focuses on generating multiple instances within a single image, each accurately placed at predefined positions with attributes such as category, color, and shape, strictly following user specifications. mig faces three main challenges: avoiding attribute leakage between instances, supporting diverse instance descriptions, and maintaining consistency in iterative generation. to address attribute leakage, we propose the multi-instance generation controller (migc). migc generates multiple instances through a divide-and-conquer strategy, breaking down multi-instance shading into single-instance tasks with singular attributes, later integrated. to provide more types of instance descriptions, we developed migc++. migc++ allows attribute control through text \& images and position control through boxes \& masks. lastly, we introduced the consistent-mig algorithm to enhance the iterative mig ability of migc and migc++. this algorithm ensures consistency in unmodified regions during the addition, deletion, or modification of instances, and preserves the identity of instances when their attributes are changed. we introduce the coco-mig and multimodal-mig benchmarks to evaluate these methods. extensive experiments on these benchmarks, along with the coco-position benchmark and drawbench, demonstrate that our methods substantially outperform existing techniques, maintaining precise control over aspects including position, attribute, and quantity. project page: https://github.com/limuloo/migc.",,2024-07-02,2025-05-04,"['dewei zhou', 'you li', 'fan ma', 'zongxin yang', 'yi yang']"
2407.05576,care-ego: contact-aware relationship modeling for egocentric interactive   hand-object segmentation,cs.cv,"egocentric interactive hand-object segmentation (egoihos) requires the segmentation of hands and interacting objects in egocentric images, which is crucial for understanding human behavior in assistive systems. previous methods typically recognize hands and interacting objects as distinct semantic categories based solely on visual features, or simply use hand predictions as auxiliary cues for object segmentation. despite the promising progress achieved by these methods, they fail to adequately model the interactive relationships between hands and objects while ignoring the coupled physical relationships among object categories, ultimately constraining their segmentation performance. to make up for the shortcomings of existing methods, we propose a novel method called care-ego that achieves state-of-the-art performance by emphasizing the contact between hands and objects from two aspects. first, we introduce a hand-guided object feature enhancer (hofe) to establish the hand-object interactive relationships to extract more contact-relevant and discriminative object features. second, we design the contact-centric object decoupling strategy (cods) to explicitly model and disentangle coupling relationships among object categories, thereby emphasizing contact-aware feature learning. experiments on various in-domain and out-of-domain test sets show that care-ego significantly outperforms existing methods with robust generalization capability. codes are publicly available at https://github.com/yuggiehk/care-ego/.",,2024-07-07,2025-05-05,"['yuejiao su', 'yi wang', 'lap-pui chau']"
2407.05679,bevworld: a multimodal world simulator for autonomous driving via   scene-level bev latents,cs.cv cs.ai,"world models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. in this paper, we propose bevworld, a novel framework that transforms multimodal sensor inputs into a unified and compact bird's eye view (bev) latent space for holistic environment modeling. the proposed world model consists of two main components: a multi-modal tokenizer and a latent bev sequence diffusion model. the multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent bev tokens into lidar and surround-view image observations via ray-casting rendering in a self-supervised manner. this enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. on top of this, the latent bev sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. extensive experiments demonstrate the effectiveness of bevworld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.",,2024-07-08,2025-04-30,"['yumeng zhang', 'shi gong', 'kaixin xiong', 'xiaoqing ye', 'xiaofan li', 'xiao tan', 'fan wang', 'jizhou huang', 'hua wu', 'haifeng wang']"
2407.06606,tailored design of audio-visual speech recognition models using   branchformers,cs.cv cs.cl,"recent advances in audio-visual speech recognition (avsr) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. in most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. however, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. in this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the branchformer, in the design of parameter-efficient avsr systems. to be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. extensive experiments on english and spanish avsr benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. even when trained on a moderate scale of data, our models achieve competitive word error rates (wer) of approximately 2.5\% for english and surpass existing approaches for spanish, establishing a new benchmark with an average wer of around 9.1\%. these results reflect how our tailored avsr system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr.",10.1016/j.csl.2025.101811.,2024-07-09,2025-05-06,"['david gimeno-gómez', 'carlos-d. martínez-hinarejos']"
2407.08277,stixelnext: toward monocular low-weight perception for object   segmentation and free space detection,cs.cv,"in this work, we present a novel approach for general object segmentation from a monocular image, eliminating the need for manually labeled training data and enabling rapid, straightforward training and adaptation with minimal data. our model initially learns from lidar during the training process, which is subsequently removed from the system, allowing it to function solely on monocular imagery. this study leverages the concept of the stixel-world to recognize a medium level representation of its surroundings. our network directly predicts a 2d multi-layer stixel-world and is capable of recognizing and locating multiple, superimposed objects within an image. due to the scarcity of comparable works, we have divided the capabilities into modules and present a free space detection in our experiments section. furthermore, we introduce an improved method for generating stixels from lidar data, which we use as ground truth for our network.",10.1109/iv55156.2024.10588680,2024-07-11,,"['marcel vosshans', 'omar ait-aider', 'youcef mezouar', 'markus enzweiler']"
2407.08364,scalar function topology divergence: comparing topology of 3d objects,cs.cv cs.lg math.at math.mg,"we propose a new topological tool for computer vision - scalar function topology divergence (sftd), which measures the dissimilarity of multi-scale topology between sublevel sets of two functions having a common domain. functions can be defined on an undirected graph or euclidean space of any dimensionality. most of the existing methods for comparing topology are based on wasserstein distance between persistence barcodes and they don't take into account the localization of topological features. the minimization of sftd ensures that the corresponding topological features of scalar functions are located in the same places. the proposed tool provides useful visualizations depicting areas where functions have topological dissimilarities. we provide applications of the proposed method to 3d computer vision. in particular, experiments demonstrate that sftd as an additional loss improves the reconstruction of cellular 3d shapes from 2d fluorescence microscopy images, and helps to identify topological errors in 3d segmentation. additionally, we show that sftd outperforms betti matching loss in 2d segmentation problems.",10.1007/978-3-031-73223-2_16,2024-07-11,2024-11-12,"['ilya trofimov', 'daria voronkova', 'eduard tulchinskii', 'evgeny burnaev', 'serguei barannikov']"
2407.12772,lmms-eval: reality check on the evaluation of large multimodal models,cs.cl cs.cv,"the advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of large multi-modal models (lmms) remain limited. in this work, we introduce lmms-eval, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. although lmms-eval offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. to approach this evaluation trilemma, we further introduce lmms-eval lite, a pruned evaluation toolkit that emphasizes both coverage and efficiency. additionally, we present multimodal livebench that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. in summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of lmms. we opensource our codebase and maintain leaderboard of livebench at https://github.com/evolvinglmms-lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/livebench.",,2024-07-17,2025-05-05,"['kaichen zhang', 'bo li', 'peiyuan zhang', 'fanyi pu', 'joshua adrian cahyono', 'kairui hu', 'shuai liu', 'yuanhan zhang', 'jingkang yang', 'chunyuan li', 'ziwei liu']"
2407.17265,scisegv2: a universal tool for segmentation of intramedullary lesions in   spinal cord injury,cs.cv cs.ai,"spinal cord injury (sci) is a devastating incidence leading to permanent paralysis and loss of sensory-motor functions potentially resulting in the formation of lesions within the spinal cord. imaging biomarkers obtained from magnetic resonance imaging (mri) scans can predict the functional recovery of individuals with sci and help choose the optimal treatment strategy. currently, most studies employ manual quantification of these mri-derived biomarkers, which is a subjective and tedious task. in this work, we propose (i) a universal tool for the automatic segmentation of intramedullary sci lesions, dubbed \texttt{scisegv2}, and (ii) a method to automatically compute the width of the tissue bridges from the segmented lesion. tissue bridges represent the spared spinal tissue adjacent to the lesion, which is associated with functional recovery in sci patients. the tool was trained and validated on a heterogeneous dataset from 7 sites comprising patients from different sci phases (acute, sub-acute, and chronic) and etiologies (traumatic sci, ischemic sci, and degenerative cervical myelopathy). tissue bridges quantified automatically did not significantly differ from those computed manually, suggesting that the proposed automatic tool can be used to derive relevant mri biomarkers. \texttt{scisegv2} and the automatic tissue bridges computation are open-source and available in spinal cord toolbox (v6.4 and above) via the \texttt{sct\_deepseg -task seg\_sc\_lesion\_t2w\_sci} and \texttt{sct\_analyze\_lesion} functions, respectively.",10.1007/978-3-031-82007-6_19,2024-07-24,,"['enamundram naga karthik', 'jan valošek', 'lynn farner', 'dario pfyffer', 'simon schading-sassenhausen', 'anna lebret', 'gergely david', 'andrew c. smith', 'kenneth a. weber', 'maryam seif', 'rhscir network imaging group', 'patrick freund', 'julien cohen-adad']"
2407.19282,a self-supervised and adversarial approach to hyperspectral demosaicking   and rgb reconstruction in surgical imaging,eess.iv cs.cv,"hyperspectral imaging holds promises in surgical imaging by offering biological tissue differentiation capabilities with detailed information that is invisible to the naked eye. for intra-operative guidance, real-time spectral data capture and display is mandated. snapshot mosaic hyperspectral cameras are currently seen as the most suitable technology given this requirement. however, snapshot mosaic imaging requires a demosaicking algorithm to fully restore the spatial and spectral details in the images. modern demosaicking approaches typically rely on synthetic datasets to develop supervised learning methods, as it is practically impossible to simultaneously capture both snapshot and high-resolution spectral images of the exact same surgical scene. in this work, we present a self-supervised demosaicking and rgb reconstruction method that does not depend on paired high-resolution data as ground truth. we leverage unpaired standard high-resolution surgical microscopy images, which only provide rgb data but can be collected during routine surgeries. adversarial learning complemented by self-supervised approaches are used to drive our hyperspectral-based rgb reconstruction into resembling surgical microscopy images and increasing the spatial resolution of our demosaicking. the spatial and spectral fidelity of the reconstructed hyperspectral images have been evaluated quantitatively. moreover, a user study was conducted to evaluate the rgb visualisation generated from these spectral images. both spatial detail and colour accuracy were assessed by neurosurgical experts. our proposed self-supervised demosaicking method demonstrates improved results compared to existing methods, demonstrating its potential for seamless integration into intra-operative workflows.",,2024-07-27,,"['peichao li', 'oscar maccormac', 'jonathan shapey', 'tom vercauteren']"
2407.19894,"cardiosyntax: end-to-end syntax score prediction -- dataset, benchmark   and method",cs.cv eess.iv,"the syntax score has become a widely used measure of coronary disease severity, crucial in selecting the optimal mode of the revascularization procedure. this paper introduces a new medical regression and classification problem - automatically estimating syntax score from coronary angiography. our study presents a comprehensive cardiosyntax dataset of 3,018 patients for the syntax score estimation and coronary dominance classification. the dataset features a balanced distribution of individuals with zero and non-zero scores. this dataset includes a first-of-its-kind, complete coronary angiography samples captured through a multi-view x-ray video, allowing one to observe coronary arteries from multiple perspectives. furthermore, we present a novel, fully automatic end-to-end method for estimating the syntax. for such a difficult task, we have achieved a solid coefficient of determination r2 of 0.51 in score value prediction and 77.3% accuracy for zero score classification.",10.1109/wacv61041.2025.00573,2024-07-29,2025-05-06,"['alexander ponomarchuk', 'ivan kruzhilov', 'galina zubkova', 'artem shadrin', 'ruslan utegenov', 'ivan bessonov', 'pavel blinov']"
2408.05411,how does audio influence visual attention in omnidirectional videos?   database and model,cs.cv,"understanding and predicting viewer attention in omnidirectional videos (odvs) is crucial for enhancing user engagement in virtual and augmented reality applications. although both audio and visual modalities are essential for saliency prediction in odvs, the joint exploitation of these two modalities has been limited, primarily due to the absence of large-scale audio-visual saliency databases and comprehensive analyses. this paper comprehensively investigates audio-visual attention in odvs from both subjective and objective perspectives. specifically, we first introduce a new audio-visual saliency database for omnidirectional videos, termed avs-odv database, containing 162 odvs and corresponding eye movement data collected from 60 subjects under three audio modes including mute, mono, and ambisonics. based on the constructed avs-odv database, we perform an in-depth analysis of how audio influences visual attention in odvs. to advance the research on audio-visual saliency prediction for odvs, we further establish a new benchmark based on the avs-odv database by testing numerous state-of-the-art saliency models, including visual-only models and audio-visual models. in addition, given the limitations of current models, we propose an innovative omnidirectional audio-visual saliency prediction network (omniavs), which is built based on the u-net architecture, and hierarchically fuses audio and visual features from the multimodal aligned embedding space. extensive experimental results demonstrate that the proposed omniavs model outperforms other state-of-the-art models on both odv avs prediction and traditional avs predcition tasks. the avs-odv database and omniavs model will be released to facilitate future research.",,2024-08-09,2025-05-05,"['yuxin zhu', 'huiyu duan', 'kaiwei zhang', 'yucheng zhu', 'xilei zhu', 'long teng', 'xiongkuo min', 'guangtao zhai']"
2408.05956,boosting adverse weather crowd counting via multi-queue contrastive   learning,cs.cv,"currently, most crowd counting methods have outstanding performance under normal weather conditions. however, our experimental validation reveals two key obstacles limiting the accuracy improvement of crowd counting models: 1) the domain gap between the adverse weather and the normal weather images; 2) the weather class imbalance in the training set. to address the problems, we propose a two-stage crowd counting method named multi-queue contrastive learning (mqcl). specifically, in the first stage, our target is to equip the backbone network with weather-awareness capabilities. in this process, a contrastive learning method named multi-queue moco designed by us is employed to enable representation learning under weather class imbalance. after the first stage is completed, the backbone model is ""mature"" enough to extract weather-related representations. on this basis, we proceed to the second stage, in which we propose to refine the representations under the guidance of contrastive learning, enabling the conversion of the weather-aware representations to the normal weather domain. through such representation and conversion, the model achieves robust counting performance under both normal and adverse weather conditions. extensive experimental results show that, compared to the baseline, mqcl reduces the counting error under adverse weather conditions by 22%, while introducing only about 13% increase in computational burden, which achieves state-of-the-art performance.",,2024-08-12,2025-05-07,"['tianhang pan', 'xiuyi jia']"
2408.06502,prompt recovery for image generation models: a comparative study of   discrete optimizers,cs.cv cs.lg,"recovering natural language prompts for image generation models, solely based on the generated images is a difficult discrete optimization problem. in this work, we present the first head-to-head comparison of recent discrete optimization techniques for the problem of prompt inversion. we evaluate greedy coordinate gradients (gcg), pez , random search, autodan and blip2's image captioner across various evaluation metrics related to the quality of inverted prompts and the quality of the images generated by the inverted prompts. we find that focusing on the clip similarity between the inverted prompts and the ground truth image acts as a poor proxy for the similarity between ground truth image and the image generated by the inverted prompts. while the discrete optimizers effectively minimize their objectives, simply using responses from a well-trained captioner often leads to generated images that more closely resemble those produced by the original prompts.",,2024-08-12,2025-04-29,"['joshua nathaniel williams', 'avi schwarzschild', 'yutong he', 'j. zico kolter']"
2408.08518,visual-friendly concept protection via selective adversarial   perturbations,cs.cv,"personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. researchers attempt to prevent malicious personalization using adversarial perturbations. however, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. they utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. in this work, we propose the visual-friendly concept protection (vcpro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. to ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the lagrangian multiplier method. qualitative and quantitative experiments validate that vcpro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.",,2024-08-16,2025-05-01,"['xiaoyue mi', 'fan tang', 'juan cao', 'peng li', 'yang liu']"
2408.10453,kubrick: multimodal agent collaborations for synthetic video generation,cs.cv cs.gr cs.mm,"text-to-video generation has been dominated by diffusion-based or autoregressive models. these novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. the film industry relies on manually-edited computer-generated imagery (cgi) using 3d modeling software. human-directed 3d synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3d rendering experts. we introduce an automatic synthetic video generation pipeline based on vision large language model (vlm) agent collaborations. given a language description of a video, multiple vlm agents direct various processes of the generation pipeline. they cooperate to create blender scripts which render a video following the given description. augmented with blender-based movie making knowledge, the director agent decomposes the text-based video description into sub-processes. for each sub-process, the programmer agent produces python-based blender scripts based on function composing and api calling. the reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the programmer agent. the programmer agent iteratively improves scripts to yield the best video outcome. our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. our framework outperforms other approaches in a user study on quality, consistency, and rationality.",,2024-08-19,2025-05-05,"['liu he', 'yizhi song', 'hejun huang', 'pinxin liu', 'yunlong tang', 'daniel aliaga', 'xin zhou']"
2408.13117,end-to-end surface optimization for light control,cs.gr cs.cv,"designing a freeform surface to reflect or refract light to achieve a target distribution is a challenging inverse problem. in this paper, we propose an end-to-end optimization strategy for an optical surface mesh. our formulation leverages a novel differentiable rendering model, and is directly driven by the difference between the resulting light distribution and the target distribution. we also enforce geometric constraints related to fabrication requirements, to facilitate cnc milling and polishing of the designed surface. to address the issue of local minima, we formulate a face-based optimal transport problem between the current mesh and the target distribution, which makes effective large changes to the surface shape. the combination of our optimal transport update and rendering-guided optimization produces an optical surface design with a resulting image closely resembling the target, while the geometric constraints in our optimization help to ensure consistency between the rendering model and the final physical results. the effectiveness of our algorithm is demonstrated on a variety of target images using both simulated rendering and physical prototypes.",10.1145/3732284,2024-08-23,2025-05-07,"['yuou sun', 'bailin deng', 'juyong zhang']"
2408.13431,face clustering via early stopping and edge recall,cs.cv,"large-scale face clustering has achieved significant progress, with many efforts dedicated to learning to cluster large-scale faces with supervised-learning. however, complex model design and tedious clustering processes are typical in existing methods. such limitations result in infeasible clustering in real-world applications. reasonable and efficient model design and training need to be taken into account. besides, developing unsupervised face clustering algorithms is crucial, which are more realistic in real-world applications. in this paper, we propose a novel unsupervised face clustering algorithm fc-es and a novel supervised face clustering algorithm fc-eser to address these issues. an efficient and effective neighbor-based edge probability and a novel early stopping strategy are proposed in fc-es, guaranteeing the accuracy and recall of large-scale face clustering simultaneously. furthermore, to take advantage of supervised learning, a novel edge recall strategy is proposed in fc-eser to further recall the edge connections that are not connected in fc-es. extensive experiments on multiple benchmarks for face, person, and vehicle clustering show that our proposed fc-es and fc-eser significantly outperform previous state-of-the-art methods. our code will be available at https://github.com/jumptoliujj/fc-eser.",,2024-08-23,2025-05-04,['junjie liu']
2408.14348,deep learning-based ecological analysis of camera trap images is   impacted by training data quality and quantity,cs.cv,"large image collections generated from camera traps offer valuable insights into species richness, occupancy, and activity patterns, significantly aiding biodiversity monitoring. however, the manual processing of these datasets is time-consuming, hindering analytical processes. to address this, deep neural networks have been adopted to automate image labelling, but the impact of classification error on ecological metrics remains unclear. here, we analyse data from camera trap collections in an african savannah (82,300 images, 47 species) and an asian sub-tropical dry forest (40,308 images, 29 species) to compare ecological metrics derived from expert-generated species identifications with those generated by deep learning classification models. we specifically assess the impact of deep learning model architecture, the proportion of label noise in the training data, and the size of the training dataset on three ecological metrics: species richness, occupancy, and activity patterns. overall, ecological metrics derived from deep neural networks closely match those calculated from expert labels and remain robust to manipulations in the training pipeline. we found that the choice of deep learning model architecture does not impact ecological metrics, and ecological metrics related to the overall community (species richness, community occupancy) were resilient to up to 10% noise in the training dataset and a 50% reduction in the training dataset size. however, we caution that less common species are disproportionately affected by a reduction in deep neural network accuracy, and this has consequences for species-specific metrics (occupancy, diel activity patterns). to ensure the reliability of their findings, practitioners should prioritize creating large, clean training sets with balanced representation across species over exploring numerous deep learning model architectures.",,2024-08-26,2025-05-07,"['peggy a. bevan', 'omiros pantazis', 'holly pringle', 'guilherme braga ferreira', 'daniel j. ingram', 'emily madsen', 'liam thomas', 'dol raj thanet', 'thakur silwal', 'santosh rayamajhi', 'gabriel brostow', 'oisin mac aodha', 'kate e. jones']"
2408.15994,perceive-ir: learning to perceive degradation better for all-in-one   image restoration,cs.cv,"existing all-in-one image restoration methods often fail to perceive degradation types and severity levels simultaneously, overlooking the importance of fine-grained quality perception. moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. to address these limitations, we propose perceive-ir, a novel backbone-agnostic all-in-one image restoration framework designed for fine-grained quality control across various degradation types and severity levels. its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. specifically, perceive-ir operates in two key stages: 1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern three tier quality levels by optimizing the alignment between prompts and images within the clip perception space. this stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; 2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. this strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while pulling it away from both low- and medium-quality samples.",,2024-08-28,2025-05-07,"['xu zhang', 'jiaqi ma', 'guoli wang', 'qian zhang', 'huan zhang', 'lefei zhang']"
2408.16859,evaluating deep learning models for breast cancer classification: a   comparative study,eess.iv cs.cv,"this study evaluates the effectiveness of deep learning models in classifying histopathological images for early and accurate detection of breast cancer. eight advanced models, including resnet-50, densenet-121, resnext-50, vision transformer (vit), googlenet (inception v3), efficientnet, mobilenet, and squeezenet, were compared using a dataset of 277,524 image patches. the vision transformer (vit) model, with its attention-based mechanisms, achieved the highest validation accuracy of 94%, outperforming conventional cnns. the study demonstrates the potential of advanced machine learning methods to enhance precision and efficiency in breast cancer diagnosis in clinical settings.",10.1117/12.3047441,2024-08-29,2025-05-08,"['sania eskandari', 'ali eslamian', 'nusrat munia', 'amjad alqarni', 'qiang cheng']"
2408.17090,fissionvae: federated non-iid image generation with latent space and   decoder decomposition,cs.lg cs.ai cs.cv,"federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. while considerable research has focused on federated image generation, particularly generative adversarial networks, variational autoencoders have received less attention. in this paper, we address the challenges of non-iid (independently and identically distributed) data environments featuring multiple groups of images of different types. non-iid data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. we thereby introduce fissionvae that decouples the latent space and constructs decoder branches tailored to individual client groups. this method allows for customized learning that aligns with the unique data distributions of each group. additionally, we incorporate hierarchical vaes and demonstrate the use of heterogeneous decoder architectures within fissionvae. we also explore strategies for setting the latent prior distributions to enhance the decoupling process. to evaluate our approach, we assemble two composite datasets: the first combines mnist and fashionmnist; the second comprises rgb datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images. our experiments demonstrate that fissionvae greatly improves generation quality on these datasets compared to baseline federated vae models.",,2024-08-30,2025-05-05,"['chen hu', 'hanchi ren', 'jingjing deng', 'xianghua xie', 'xiaoke ma']"
2408.17237,a nonlinear elasticity model in computer vision,math.ap cs.cv,"the purpose of this paper is to analyze a nonlinear elasticity model introduced by the authors for comparing two images, regarded as bounded open subsets of $\r^n$ together with associated vector-valued intensity maps. optimal transformations between the images are sought as minimisers of an integral functional among orientation-preserving homeomorphisms. the existence of minimisers is proved under natural coercivity and polyconvexity conditions, assuming only that the intensity functions are bounded measurable. variants of the existence theorem are also proved, first under the constraint that finite sets of landmark points in the two images are mapped one to the other, and second when one image is to be compared to an unknown part of another.   the question is studied as to whether for images related by an affine mapping the unique minimiser is given by that affine mapping. for a natural class of functional integrands an example is given guaranteeing that this property holds for pairs of images in which the second is a scaling of the first by a constant factor. however for the property to hold for arbitrary pairs of affinely related images it is shown that the integrand has to depend on the gradient of the transformation as a convex function of its determinant alone. this suggests a new model in which the integrand depends also on second derivatives of the transformation, and an example is given for which both existence of minimisers is assured and the above property holds for all pairs of affinely related images.",,2024-08-30,2025-05-08,"['john m. ball', 'christopher l. horner']"
2409.00042,glyph-based uncertainty visualization and analysis of time-varying   vector fields,cs.hc cs.cv cs.gr,"uncertainty is inherent to most data, including vector field data, yet it is often omitted in visualizations and representations. effective uncertainty visualization can enhance the understanding and interpretability of vector field data. for instance, in the context of severe weather events such as hurricanes and wildfires, effective uncertainty visualization can provide crucial insights about fire spread or hurricane behavior and aid in resource management and risk mitigation. glyphs are commonly used for representing vector uncertainty but are often limited to 2d. in this work, we present a glyph-based technique for accurately representing 3d vector uncertainty and a comprehensive framework for visualization, exploration, and analysis using our new glyphs. we employ hurricane and wildfire examples to demonstrate the efficacy of our glyph design and visualization tool in conveying vector field uncertainty.",10.1109/uncertaintyvisualization63963.2024.00014,2024-08-18,,"['timbwaoga a. j. ouermi', 'jixian li', 'zachary morrow', 'bart van bloemen waanders', 'chris r. johnson']"
2409.00313,training-free sketch-guided diffusion with latent optimization,cs.cv,"based on recent advanced diffusion models, text-to-image (t2i) generation models have demonstrated their capabilities to generate diverse and high-quality images. however, leveraging their potential for real-world content creation, particularly in providing users with precise control over the image generation result, poses a significant challenge. in this paper, we propose an innovative training-free pipeline that extends existing text-to-image generation models to incorporate a sketch as an additional condition. to generate new images with a layout and structure closely resembling the input sketch, we find that these core features of a sketch can be tracked with the cross-attention maps of diffusion models. we introduce latent optimization, a method that refines the noisy latent at each intermediate step of the generation process using cross-attention maps to ensure that the generated images adhere closely to the desired structure outlined in the reference sketch. through latent optimization, our method enhances the accuracy of image generation, offering users greater control and customization options in content creation.",,2024-08-30,2025-05-07,"['sandra zhang ding', 'jiafeng mao', 'kiyoharu aizawa']"
2409.00362,udgs-slam : unidepth assisted gaussian splatting for monocular slam,cs.cv cs.ro,"recent advancements in monocular neural depth estimation, particularly those achieved by the unidepth network, have prompted the investigation of integrating unidepth within a gaussian splatting framework for monocular slam. this study presents udgs-slam, a novel approach that eliminates the necessity of rgb-d sensors for depth estimation within gaussian splatting framework. udgs-slam employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and gaussian scene representation parameters. the proposed method achieves high-fidelity rendered images and low atermse of the camera trajectory. the performance of udgs-slam is rigorously evaluated using the tum rgb-d dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.",10.1016/j.array.2025.100400,2024-08-31,2025-05-02,"['mostafa mansour', 'ahmed abdelsalam', 'ari happonen', 'jari porras', 'esa rahtu']"
2409.01341,enhancing test time adaptation with few-shot guidance,cs.cv,"deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. to address this issue, test time adaptation (tta) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. in response, we develop few-shot test time adaptation (fs-tta), a novel and practical setting that utilizes a few-shot support set on top of tta. adhering to the principle of few inputs, big gains, fs-tta reduces blind exploration in unseen target domains. furthermore, we propose a two-stage framework to tackle fs-tta, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our fs-tta and framework.",,2024-09-02,2025-05-07,"['siqi luo', 'yi xin', 'yuntao du', 'zhongwei wan', 'tao tan', 'guangtao zhai', 'xiaohong liu']"
2409.03757,lexicon3d: probing visual foundation models for complex 3d scene   understanding,cs.cv cs.ai cs.cl cs.lg cs.ro,"complex 3d scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. however, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. to address this issue, we present a comprehensive study that probes various visual encoding models for 3d scene understanding, identifying the strengths and limitations of each model across different scenarios. our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3d foundation models. we evaluate these models in four tasks: vision-language scene reasoning, visual grounding, segmentation, and registration, each focusing on different aspects of scene understanding. our evaluations yield key findings: dinov2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. these insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. code: https://github.com/yunzeman/lexicon3d",,2024-09-05,2025-05-08,"['yunze man', 'shuhong zheng', 'zhipeng bao', 'martial hebert', 'liang-yan gui', 'yu-xiong wang']"
2409.03766,comparison of kinematics and kinetics between opencap and a marker-based   motion capture system in cycling,cs.cv,"this study evaluates the agreement of marker-based and markerless (opencap) motion capture systems in assessing joint kinematics and kinetics during cycling. markerless systems, such as opencap, offer the advantage of capturing natural movements without physical markers, making them more practical for real-world applications. however, the agreement of opencap with a marker-based system, particularly in cycling, remains underexplored. ten participants cycled at varying speeds and resistances while motion data were recorded using both systems. key metrics, including joint angles, moments, and joint reaction loads, were computed using opensim and compared using root mean squared error (rmse) per trial across participants, pearson correlation coefficients (r) per trial across participants and repeated measures bland-altman to control trials dependency within subject. results revealed very strong agreement (r gt 0.9) for hip (flexion/extension), knee (flexion/extension), and ankle (dorsiflexion/plantarflexion) joint angles.",,2024-08-20,2025-04-30,"['reza kakavand', 'reza ahmadi', 'atousa parsaei', 'w. brent edwards', 'amin komeili']"
2409.07012,towards predicting temporal changes in a patient's chest x-ray images   based on electronic health records,eess.iv cs.ai cs.cv,"chest x-ray (cxr) is an important diagnostic tool widely used in hospitals to assess patient conditions and monitor changes over time. recently, generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic cxrs. however, these models mainly focus on conditional generation using single-time-point data, i.e., generating cxrs conditioned on their corresponding reports from a specific time. this limits their clinical utility, particularly for capturing temporal changes. to address this limitation, we propose a novel framework, ehrxdiff, which predicts future cxr images by integrating previous cxrs with subsequent medical events, e.g., prescriptions, lab measures, etc. our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous cxr image and a history of medical events. we comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. results show that our framework generates high-quality, realistic future images that effectively capture potential temporal changes. this suggests that our framework could be further developed to support clinical decision-making and provide valuable insights for patient monitoring and treatment planning in the medical field. the code is available at https://github.com/dek924/ehrxdiff.",,2024-09-11,2025-05-05,"['daeun kyung', 'junu kim', 'tackeun kim', 'edward choi']"
2409.07271,cfcpalsy: facial image synthesis with cross-fusion cycle diffusion model   for facial paralysis individuals,cs.cv,"currently, the diagnosis of facial paralysis remains a challenging task, often relying heavily on the subjective judgment and experience of clinicians, which can introduce variability and uncertainty in the assessment process. one promising application in real-life situations is the automatic estimation of facial paralysis. however, the scarcity of facial paralysis datasets limits the development of robust machine learning models for automated diagnosis and therapeutic interventions. to this end, this study aims to synthesize a high-quality facial paralysis dataset to address this gap, enabling more accurate and efficient algorithm training. specifically, a novel cross-fusion cycle palsy expression generative model (cfcpalsy) based on the diffusion model is proposed to combine different features of facial information and enhance the visual details of facial appearance and texture in facial regions, thus creating synthetic facial images that accurately represent various degrees and types of facial paralysis. we have qualitatively and quantitatively evaluated the proposed method on the commonly used public clinical datasets of facial paralysis to demonstrate its effectiveness. experimental results indicate that the proposed method surpasses state-of-the-art methods, generating more realistic facial images and maintaining identity consistency.",,2024-09-11,2025-05-03,"['weixiang gao', 'yating zhang', 'yifan xia']"
2409.07284,tld-ready: traffic light detection -- relevance estimation and   deployment analysis,cs.cv cs.lg,"effective traffic light detection is a critical component of the perception stack in autonomous vehicles. this work introduces a novel deep-learning detection system while addressing the challenges of previous work. utilizing a comprehensive dataset amalgamation, including the bosch small traffic lights dataset, lisa, the driveu traffic light dataset, and a proprietary dataset from karlsruhe, we ensure a robust evaluation across varied scenarios. furthermore, we propose a relevance estimation system that innovatively uses directional arrow markings on the road, eliminating the need for prior map creation. on the driveu dataset, this approach results in 96% accuracy in relevance estimation. finally, a real-world evaluation is performed to evaluate the deployment and generalizing abilities of these models. for reproducibility and to facilitate further research, we provide the model weights and code: https://github.com/kastel-mobilitylab/traffic-light-detection.",10.1109/itsc58415.2024.10919699,2024-09-11,,"['nikolai polley', 'svetlana pavlitska', 'yacin boualili', 'patrick rohrbeck', 'paul stiller', 'ashok kumar bangaru', 'j. marius zöllner']"
2409.08091,ezigen: enhancing zero-shot personalized image generation with precise   subject encoding and decoupled guidance,cs.cv,"zero-shot personalized image generation models aim to produce images that align with both a given text prompt and subject image, requiring the model to incorporate both sources of guidance. existing methods often struggle to capture fine-grained subject details and frequently prioritize one form of guidance over the other, resulting in suboptimal subject encoding and imbalanced generation. in this study, we uncover key insights into overcoming such drawbacks, notably that 1) the choice of the subject image encoder critically influences subject identity preservation and training efficiency, and 2) the text and subject guidance should take effect at different denoising stages. building on these insights, we introduce a new approach, ezigen, that employs two main components: leveraging a fixed pre-trained diffusion unet itself as subject encoder, following a process that balances the two guidances by separating their dominance stage and revisiting certain time steps to bootstrap subject transfer quality. through these two components, ezigen, initially built upon sd2.1-base, achieved state-of-the-art performances on multiple personalized generation benchmarks with a unified model, while using 100 times less training data. moreover, by further migrating our design to sdxl, ezigen is proven to be a versatile model-agnostic solution for personalized generation. demo page: zichengduan.github.io/pages/ezigen/index.html",,2024-09-12,2025-05-01,"['zicheng duan', 'yuxuan ding', 'chenhui gou', 'ziqin zhou', 'ethan smith', 'lingqiao liu']"
2409.08215,lt3sd: latent trees for 3d scene diffusion,cs.cv cs.ai,"we present lt3sd, a novel latent diffusion model for large-scale 3d scene generation. recent advances in diffusion models have shown impressive results in 3d object generation, but are limited in spatial extent and quality when extended to 3d scenes. to generate complex and diverse 3d scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. we can then learn a generative diffusion process in this latent 3d scene space, modeling the latent components of a scene at each resolution level. to synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3d scenes through shared diffusion generation across multiple scene patches. through extensive experiments, we demonstrate the efficacy and benefits of lt3sd for large-scale, high-quality unconditional 3d scene generation and for probabilistic completion for partial scene observations.",,2024-09-12,2025-05-01,"['quan meng', 'lei li', 'matthias nießner', 'angela dai']"
2409.08474,rethinking meta-learning from a learning lens,cs.lg cs.cv,"meta-learning seeks to learn a well-generalized model initialization from training tasks to solve unseen tasks. from the ""learning to learn"" perspective, the quality of the initialization is modeled with one-step gradient decent in the inner loop. however, contrary to theoretical expectations, our empirical analysis reveals that this may expose meta-learning to underfitting. to bridge the gap between theoretical understanding and practical implementation, we reconsider meta-learning from the ""learning"" lens. we propose that the meta-learning model comprises two interrelated components: parameters for model initialization and a meta-layer for task-specific fine-tuning. these components will lead to the risks of overfitting and underfitting depending on tasks, and their solutions, fewer parameters vs. more meta-layer, are often in conflict. to address this, we aim to regulate the task information the model receives without modifying the data or model structure. our theoretical analysis indicates that models adapted to different tasks can mutually reinforce each other, highlighting the effective information. based on this insight, we propose trlearner, a plug-and-play method that leverages task relation to calibrate meta-learning. it first extracts task relation matrices and then applies relation-aware consistency regularization to guide optimization. extensive theoretical and empirical evaluations demonstrate its effectiveness.",,2024-09-12,2025-05-06,"['jingyao wang', 'wenwen qiang', 'changwen zheng', 'hui xiong', 'gang hua']"
2409.09085,hesso: towards automatic efficient and user friendly any neural network   training and pruning,cs.lg cs.cv eess.iv,"structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (dnns) into compact sub-networks while retaining performance. the existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. the only-train-once (oto) series has been recently proposed to resolve the many pain points by streamlining the workflow by automatically conducting (i) search space generation, (ii) structured sparse optimization, and (iii) sub-network construction. however, the built-in sparse optimizers in the oto series, i.e., the half-space projected gradient (hspg) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. to address such limitations, we propose a hybrid efficient structured sparse optimizer (hesso). hesso could automatically and efficiently train a dnn to produce a high-performing subnetwork. meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. to address another common issue of irreversible performance collapse observed in pruning dnns, we further propose a corrective redundant identification cycle (cric) for reliably identifying indispensable structures. we numerically demonstrate the efficacy of hesso and its enhanced version hesso-cric on a variety of applications ranging from computer vision to natural language processing, including large language model. the numerical results showcase that hesso can achieve competitive even superior performance to varying state-of-the-arts and support most dnn architectures. meanwhile, cric can effectively prevent the irreversible performance collapse and further enhance the performance of hesso on certain applications.",,2024-09-11,2025-05-07,"['tianyi chen', 'xiaoyi qu', 'david aponte', 'colby banbury', 'jongwoo ko', 'tianyu ding', 'yong ma', 'vladimir lyapunov', 'ilya zharkov', 'luming liang']"
2409.09724,mfclip: multi-modal fine-grained clip for generalizable diffusion face   forgery detection,cs.cv,"the rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (ffd) techniques. although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. in addition, most ffd methods tend to identify facial images generated by gan, but struggle to detect unseen diffusion-synthesized ones. to address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (clip), to achieve generalizable diffusion face forgery detection (dffd). in this paper, we propose a novel multi-modal fine-grained clip (mfclip) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of dffd. specifically, we devise a fine-grained language encoder (fle) that extracts fine global language features from hierarchical text prompts. we design a multi-modal vision encoder (mve) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. moreover, we build an innovative plug-and-play sample pair attention (spa) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.",,2024-09-15,2025-05-02,"['yaning zhang', 'tianyi wang', 'zitong yu', 'zan gao', 'linlin shen', 'shengyong chen']"
2409.09779,underwater image enhancement via dehazing and color restoration,cs.cv eess.iv,"underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. to overcome this limitation, we propose a vision transformer (vit)-based network (referred to as waterformer) to improve underwater image quality. waterformer contains three major components: a dehazing block (dehazeformer block) to capture the self-correlated haze features and extract deep-level features, a color restoration block (crb) to capture self-correlated color cast features, and a channel fusion block (cfb) that dynamically integrates these decoupled features to achieve comprehensive enhancement. to ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. further, a chromatic consistency loss and sobel color loss are designed to respectively preserve color fidelity and enhance structural details during network training. comprehensive experimental results demonstrate that waterformer outperforms other state-of-the-art methods in enhancing underwater images.",,2024-09-15,2025-05-04,"['chengqin wu', 'shuai yu', 'tuyan luo', 'qiuhua rao', 'qingson hu', 'jingxiang xu', 'lijun zhang']"
2409.11686,automated detection of underdiagnosed medical conditions via   opportunistic imaging,cs.cv cs.ai cs.lg,"abdominal computed tomography (ct) scans are frequently performed in clinical settings. opportunistic ct involves repurposing routine ct images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. this study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. we analyze 2,674 inpatient ct scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic ct scans) and their corresponding documentation in radiology reports and icd coding. through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were icd-coded. our findings demonstrate opportunistic ct's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.",,2024-09-17,2025-05-08,"['asad aali', 'andrew johnston', 'louis blankemeier', 'dave van veen', 'laura t derry', 'david svec', 'jason hom', 'robert d. boutin', 'akshay s. chaudhari']"
2409.12002,towards global localization using multi-modal object-instance   re-identification,cs.ro cs.cv,"re-identification (reid) is a critical challenge in computer vision, predominantly studied in the context of pedestrians and vehicles. however, robust object-instance reid, which has significant implications for tasks such as autonomous exploration, long-term perception, and scene understanding, remains underexplored. in this work, we address this gap by proposing a novel dual-path object-instance re-identification transformer architecture that integrates multimodal rgb and depth information. by leveraging depth data, we demonstrate improvements in reid across scenes that are cluttered or have varying illumination conditions. additionally, we develop a reid-based localization framework that enables accurate camera localization and pose identification across different viewpoints. we validate our methods using two custom-built rgb-d datasets, as well as multiple sequences from the open-source tum rgb-d datasets. our approach demonstrates significant improvements in both object instance reid (map of 75.18) and localization accuracy (success rate of 83% on tum-rgbd), highlighting the essential role of object reid in advancing robotic perception. our models, frameworks, and datasets have been made publicly available.",,2024-09-18,2025-05-01,"['aneesh chavan', 'vaibhav agrawal', 'vineeth bhat', 'sarthak chittawar', 'siddharth srivastava', 'chetan arora', 'k madhava krishna']"
2409.16111,cloudtrack: scalable uav tracking with cloud semantics,cs.ro cs.cv,"nowadays, unmanned aerial vehicles (uavs) are commonly used in search and rescue scenarios to gather information in the search area. the automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. in this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of uav hardware. our approach has several advantages. it can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. our experimental results demonstrate the versatility and efficacy of our approach.",,2024-09-24,2025-05-08,"['yannik blei', 'michael krawez', 'nisarga nilavadi', 'tanja katharina kaiser', 'wolfram burgard']"
2409.16663,mitigating covariate shift in imitation learning for autonomous vehicles   using latent space generative world models,cs.ro cs.cv cs.lg cs.sy eess.sy,"we propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. a world model is a neural network capable of predicting an agent's next state given past states and actions. by leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. during end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. we present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the carla simulator, as well as showing the ability to handle perturbations in both carla and nvidia's drive sim.",,2024-09-25,2025-04-30,"['alexander popov', 'alperen degirmenci', 'david wehr', 'shashank hegde', 'ryan oldja', 'alexey kamenev', 'bertrand douillard', 'david nistér', 'urs muller', 'ruchi bhargava', 'stan birchfield', 'nikolai smolyanskiy']"
2409.19911,replace anyone in videos,cs.cv,"the field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. however, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. in this work, we present the replaceanyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. to prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. replaceanyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. the proposed replaceanyone can be seamlessly applied not only to traditional 3d-unet base models but also to dit-based video models such as wan2.1. the code will be available at https://github.com/ali-vilab/unianimate-dit.",,2024-09-29,2025-05-07,"['xiang wang', 'shiwei zhang', 'haonan qiu', 'ruihang chu', 'zekun li', 'yingya zhang', 'changxin gao', 'yuehuan wang', 'chunhua shen', 'nong sang']"
2410.02768,uncertainty-guided self-questioning and answering for video-language   alignment,cs.cv cs.ai,"the development of multi-modal models has been rapidly advancing, with some demonstrating remarkable capabilities. however, annotating video-text pairs remains expensive and insufficient. take video question answering (videoqa) tasks as an example, human annotated questions and answers often cover only part of the video, since the corresponding text is often short and monotonous, leading to underutilization of video. to address this, we propose a bootstrapping video-language alignment framework (bovila), a self-training method that augments question samples during training process through llm-based self-questioning and answering, which help model exploit video information and the internal knowledge of llms more thoroughly to improve modality alignment. however, low-quality self-generated questions may instead contaminate the performance, especially in the early stages of training, as we have observed in our experiments. to filter bad self-generated questions, we introduce evidential deep learning (edl) to estimate uncertainty and assess the quality of self-generated questions by evaluating the modality alignment within the context. to the best of our knowledge, this work is the first to explore llm-based self-training frameworks for modality alignment. we evaluate bovila on five strong videoqa benchmarks, where it outperforms several state-of-the-art methods and demonstrate its effectiveness and generality. additionally, we provide extensive analyses of the self-training framework and the edl-based uncertainty filtering mechanism. the code will be made available.",,2024-09-17,2025-05-06,"['jin chen', 'kaijing ma', 'haojian huang', 'han fang', 'hao sun', 'mehdi hosseinzadeh', 'zhe liu']"
2410.03359,an enhanced harmonic densely connected hybrid transformer network   architecture for chronic wound segmentation utilising multi-colour space   tensor merging,eess.iv cs.ai cs.cv,"chronic wounds and associated complications present ever growing burdens for clinics and hospitals world wide. venous, arterial, diabetic, and pressure wounds are becoming increasingly common globally. these conditions can result in highly debilitating repercussions for those affected, with limb amputations and increased mortality risk resulting from infection becoming more common. new methods to assist clinicians in chronic wound care are therefore vital to maintain high quality care standards. this paper presents an improved hardnet segmentation architecture which integrates a contrast-eliminating component in the initial layers of the network to enhance feature learning. we also utilise a multi-colour space tensor merging process and adjust the harmonic shape of the convolution blocks to facilitate these additional features. we train our proposed model using wound images from light-skinned patients and test the model on two test sets (one set with ground truth, and one without) comprising only darker-skinned cases. subjective ratings are obtained from clinical wound experts with intraclass correlation coefficient used to determine inter-rater reliability. for the dark-skin tone test set with ground truth, we demonstrate improvements in terms of dice similarity coefficient (+0.1221) and intersection over union (+0.1274). qualitative analysis showed high expert ratings, with improvements of >3% demonstrated when comparing the baseline model with the proposed model. this paper presents the first study to focus on darker-skin tones for chronic wound segmentation using models trained only on wound images exhibiting lighter skin. diabetes is highly prevalent in countries where patients have darker skin tones, highlighting the need for a greater focus on such cases. additionally, we conduct the largest qualitative study to date for chronic wound segmentation.",10.1016/j.compbiomed.2025.110172,2024-10-04,,"['bill cassidy', 'christian mcbride', 'connah kendrick', 'neil d. reeves', 'joseph m. pappachan', 'cornelius j. fernandez', 'elias chacko', 'raphael brüngel', 'christoph m. friedrich', 'metib alotaibi', 'abdullah abdulaziz alwabel', 'mohammad alderwish', 'kuan-ying lai', 'moi hoon yap']"
2410.03577,look twice before you answer: memory-space visual retracing for   hallucination mitigation in multimodal large language models,cs.cv,"despite their impressive capabilities, multimodal large language models (mllms) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. unlike in llms, hallucinations in mllms often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to ""amnesia"" about visual information. to address this issue, we propose memvr, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the mllm through feed forward network (ffn) as ""key-value memory"" at the middle trigger layer. this ""look-twice"" mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. comprehensive experimental evaluations demonstrate that memvr significantly mitigates hallucination across various mllms and excels in general benchmarks without incurring additional time overhead. the implementation is available from https://github.com/1zhou-wang/memvr",,2024-10-04,2025-05-08,"['xin zou', 'yizhou wang', 'yibo yan', 'yuanhuiyi lyu', 'kening zheng', 'sirui huang', 'junkai chen', 'peijie jiang', 'jia liu', 'chang tang', 'xuming hu']"
2410.03825,monst3r: a simple approach for estimating geometry in the presence of   motion,cs.cv,"estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. in this paper, we present motion dust3r (monst3r), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt dust3r's representation, previously only used for static scenes, to dynamic scenes. however, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. moreover, monst3r shows promising results for primarily feed-forward 4d reconstruction.",,2024-10-04,2025-05-08,"['junyi zhang', 'charles herrmann', 'junhwa hur', 'varun jampani', 'trevor darrell', 'forrester cole', 'deqing sun', 'ming-hsuan yang']"
2410.04634,is what you ask for what you get? investigating concept associations in   text-to-image models,cs.cv,"text-to-image (t2i) models are increasingly used in impactful real-life applications. as such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. however, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. to address this, we propose concept2concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. this characterization allows us to use our framework to audit models and prompt-datasets. to demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. lastly, we implement concept2concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. a demo is available at https://tinyurl.com/concept2conceptdemo.",,2024-10-06,2025-05-07,"['salma s. abdel magid', 'weiwei pan', 'simon warchol', 'grace guo', 'junsik kim', 'mahia rahman', 'hanspeter pfister']"
2410.06126,$x^2$-dfd: a framework for explainable and extendable deepfake detection,cs.cv,"detecting deepfakes has become an important task. most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. recent studies leveraging mllms for deepfake detection have shown improvements in explainability. however, the performance of pre-trained mllms (e.g., llava) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. in this work, we empirically assess the strengths and weaknesses of mllms specifically in deepfake detection via forgery features analysis. building on these assessments, we propose a novel framework called ${x}^2$-dfd, consisting of three core modules. the first module, model feature assessment (mfa), measures the detection capabilities of forgery features intrinsic to mllms, and gives a descending ranking of these features. the second module, strong feature strengthening (sfs), enhances the detection and explanation capabilities by fine-tuning the mllm on a dataset constructed based on the top-ranked features. the third module, weak feature supplementing (wfs), improves the fine-tuned mllm's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. to verify the effectiveness of this framework, we further present a practical implementation, where an automated forgery features generation, evaluation, and ranking procedure is designed for mfa module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for sfs model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained mllm, is integrated for wfs module. experiments show that our approach enhances both detection and explanation performance.",,2024-10-08,2025-05-02,"['yize chen', 'zhiyuan yan', 'siwei lyu', 'baoyuan wu']"
2410.07577,3d vision-language gaussian splatting,cs.cv,"recent advancements in 3d reconstruction methods and vision-language models have propelled the development of multi-modal 3d scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. however, current multi-modal scene understanding approaches have naively embedded semantic representations into 3d reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. to alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3d vision-language gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. we propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. we also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.",,2024-10-09,2025-05-05,"['qucheng peng', 'benjamin planche', 'zhongpai gao', 'meng zheng', 'anwesa choudhuri', 'terrence chen', 'chen chen', 'ziyan wu']"
2410.09049,scenecraft: layout-guided 3d scene generation,cs.cv,"the creation of complex 3d scenes tailored to user specifications has been a tedious and challenging task with traditional 3d modeling tools. although some pioneering methods have achieved automatic text-to-3d generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. we introduce scenecraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. central to our method is a rendering-based technique, which converts 3d semantic layouts into multi-view 2d proxy maps. furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (nerf) as the final scene representation. without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. code and more results are available at: https://orangesodahub.github.io/scenecraft",,2024-10-11,2025-05-08,"['xiuyu yang', 'yunze man', 'jun-kun chen', 'yu-xiong wang']"
2410.10821,tex4d: zero-shot 4d scene texturing with video diffusion models,cs.cv,"3d meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, ar, and vr. however, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. on the other hand, while video diffusion models excel at text-driven video generation, they often lack 3d geometry awareness and struggle with achieving multi-view consistent texturing for 3d meshes. in this work, we present tex4d, a zero-shot approach that integrates inherent 3d geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4d textures. given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the uv space. to ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. however, straightforwardly combining the video diffusion model and the uv texture aggregation leads to blurry results. we analyze the underlying causes and propose a simple yet effective modification to the ddim sampling process to address this issue. additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. to the best of our knowledge, tex4d is the first method specifically designed for 4d scene texturing. extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences.",,2024-10-14,2025-05-03,"['jingzhi bao', 'xueting li', 'ming-hsuan yang']"
2410.12705,worldcuisines: a massive-scale benchmark for multilingual and   multicultural visual question answering on global cuisines,cs.cl cs.ai cs.cv,"vision language models (vlms) often struggle with culture-specific knowledge, particularly in languages other than english and in underrepresented cultural contexts. to evaluate their understanding of such knowledge, we introduce worldcuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. this benchmark includes a visual question answering (vqa) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural vqa benchmark to date. it includes tasks for identifying dish names and their origins. we provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). our findings show that while vlms perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. to support future research, we release a knowledge base with annotated food entries and images along with the vqa data.",,2024-10-16,2025-05-08,"['genta indra winata', 'frederikus hudi', 'patrick amadeus irawan', 'david anugraha', 'rifki afina putri', 'yutong wang', 'adam nohejl', 'ubaidillah ariq prathama', 'nedjma ousidhoum', 'afifa amriani', 'anar rzayev', 'anirban das', 'ashmari pramodya', 'aulia adila', 'bryan wilie', 'candy olivia mawalim', 'ching lam cheng', 'daud abolade', 'emmanuele chersoni', 'enrico santus', 'fariz ikhwantri', 'garry kuwanto', 'hanyang zhao', 'haryo akbarianto wibowo', 'holy lovenia', 'jan christian blaise cruz', 'jan wira gotama putra', 'junho myung', 'lucky susanto', 'maria angelica riera machin', 'marina zhukova', 'michael anugraha', 'muhammad farid adilazuarda', 'natasha santosa', 'peerat limkonchotiwat', 'raj dabre', 'rio alexander audino', 'samuel cahyawijaya', 'shi-xiong zhang', 'stephanie yulia salim', 'yi zhou', 'yinxuan gui', 'david ifeoluwa adelani', 'en-shiun annie lee', 'shogo okada', 'ayu purwarianti', 'alham fikri aji', 'taro watanabe', 'derry tanti wijaya', 'alice oh', 'chong-wah ngo']"
2410.16296,large scale mri collection and segmentation of cirrhotic liver,eess.iv cs.cv,"liver cirrhosis represents the end stage of chronic liver disease, characterized by extensive fibrosis and nodular regeneration that significantly increases mortality risk. while magnetic resonance imaging (mri) offers a non-invasive assessment, accurately segmenting cirrhotic livers presents substantial challenges due to morphological alterations and heterogeneous signal characteristics. deep learning approaches show promise for automating these tasks, but progress has been limited by the absence of large-scale, annotated datasets. here, we present cirrmri600+, the first comprehensive dataset comprising 628 high-resolution abdominal mri scans (310 t1-weighted and 318 t2-weighted sequences, totaling nearly 40,000 annotated slices) with expert-validated segmentation labels for cirrhotic livers. the dataset includes demographic information, clinical parameters, and histopathological validation where available. additionally, we provide benchmark results from 11 state-of-the-art deep learning experiments to establish performance standards. cirrmri600+ enables the development and validation of advanced computational methods for cirrhotic liver analysis, potentially accelerating progress toward automated cirrhosis visual staging and personalized treatment planning.",,2024-10-06,2025-05-07,"['debesh jha', 'onkar kishor susladkar', 'vandan gorade', 'elif keles', 'matthew antalek', 'deniz seyithanoglu', 'timurhan cebeci', 'halil ertugrul aktas', 'gulbiz dagoglu kartal', 'sabahattin kaymakoglu', 'sukru mehmet erturk', 'yuri velichko', 'daniela ladner', 'amir a. borhani', 'alpay medetalibeyoglu', 'gorkem durak', 'ulas bagci']"
2410.17262,emogene: audio-driven emotional 3d talking-head generation,cs.cv cs.ai cs.hc cs.lg,"audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. while recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. in this paper, we introduce emogene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. our approach employs a variational autoencoder (vae)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. these landmarks drive a neural radiance fields (nerf)-based emotion-to-video module to render realistic emotional talking-head videos. additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. extensive experiments demonstrate that emogene outperforms previous methods in generating high-fidelity emotional talking-head videos.",,2024-10-07,2025-05-01,"['wenqing wang', 'yun fu']"
2410.18794,warp-lca: efficient convolutional sparse coding with locally competitive   algorithm,cs.cv cs.lg eess.iv,"the locally competitive algorithm (lca) can solve sparse coding problems across a wide range of use cases. recently, convolution-based lca approaches have been shown to be highly effective for enhancing robustness for image recognition tasks in vision pipelines. to additionally maximize representational sparsity, lca with hard-thresholding can be applied. while this combination often yields very good solutions satisfying an $\ell_0$ sparsity criterion, it comes with significant drawbacks for practical application: (i) lca is very inefficient, typically requiring hundreds of optimization cycles for convergence; (ii) the use of hard-thresholding results in a non-convex loss function, which might lead to suboptimal minima. to address these issues, we propose the locally competitive algorithm with state warm-up via predictive priming (warp-lca), which leverages a predictor network to provide a suitable initial guess of the lca state based on the current input. our approach significantly improves both convergence speed and the quality of solutions, while maintaining and even enhancing the overall strengths of lca. we demonstrate that warp-lca converges faster by orders of magnitude and reaches better minima compared to conventional lca. moreover, the learned representations are more sparse and exhibit superior properties in terms of reconstruction and denoising quality as well as robustness when applied in deep recognition pipelines. furthermore, we apply warp-lca to image denoising tasks, showcasing its robustness and practical effectiveness. our findings confirm that the naive use of lca with hard-thresholding results in suboptimal minima, whereas initializing lca with a predictive guess results in better outcomes. this research advances the field of biologically inspired deep learning by providing a novel approach to convolutional sparse coding.",10.1016/j.neucom.2025.130291,2024-10-24,2025-04-30,"['geoffrey kasenbacher', 'felix ehret', 'gerrit ecke', 'sebastian otte']"
2410.19816,"divshift: exploring domain-specific distribution shifts in large-scale,   volunteer-collected biodiversity datasets",cs.cv,"large-scale, volunteer-collected datasets of community-identified natural world imagery like inaturalist have enabled marked performance gains for fine-grained visual classification of species using machine learning methods. however, such data -- sometimes referred to as citizen science data -- are opportunistic and lack a structured sampling strategy. this volunteer-collected biodiversity data contains geographic, temporal, taxonomic, observers, and sociopolitical biases that can have significant effects on biodiversity model performance, but whose impacts are unclear for fine-grained species recognition performance. here we introduce diversity shift (divshift), a framework for quantifying the effects of domain-specific distribution shifts on machine learning model performance. to diagnose the performance effects of biases specific to volunteer-collected biodiversity data, we also introduce divshift - north american west coast (divshift-nawc), a curated dataset of almost 7.5 million inaturalist images across the western coast of north america partitioned across five types of expert-verified bias. we compare species recognition performance across these bias partitions using a diverse variety of species- and ecosystem-focused accuracy metrics. we observe that these biases confound model performance less than expected from the underlying label distribution shift, and that more data leads to better model performance but the magnitude of these improvements are bias-specific. these findings imply that while the structure within natural world images provides generalization improvements for biodiversity monitoring tasks, the biases present in volunteer-collected biodiversity data can also affect model performance; thus these models should be used with caution in downstream biodiversity monitoring tasks.",10.1609/aaai.v39i27.35060,2024-10-17,2025-05-01,"['elena sierra', 'lauren e. gillespie', 'salim soltani', 'moises exposito-alonso', 'teja kattenborn']"
2410.22330,vision-language models create cross-modal task representations,cs.cv cs.cl cs.lg,"autoregressive vision-language models (vlms) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. we find that vlms align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify vlm processing. we measure this alignment via cross-modal transfer -- the ability of a task vector derived in one modality to trigger the correct generation in another -- on a range of tasks and model architectures. although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. taken together, our findings shed light on how vlms internally process task information, and how they map different modalities into common semantic representations. project page: https://vlm-cross-modal-reps.github.io.",,2024-10-29,2025-05-07,"['grace luo', 'trevor darrell', 'amir bar']"
2410.22530,adaptive aggregation weights for federated segmentation of pancreas mri,eess.iv cs.cv cs.dc,"federated learning (fl) enables collaborative model training across institutions without sharing sensitive data, making it an attractive solution for medical imaging tasks. however, traditional fl methods, such as federated averaging (fedavg), face difficulties in generalizing across domains due to variations in imaging protocols and patient demographics across institutions. this challenge is particularly evident in pancreas mri segmentation, where anatomical variability and imaging artifacts significantly impact performance. in this paper, we conduct a comprehensive evaluation of fl algorithms for pancreas mri segmentation and introduce a novel approach that incorporates adaptive aggregation weights. by dynamically adjusting the contribution of each client during model aggregation, our method accounts for domain-specific differences and improves generalization across heterogeneous datasets. experimental results demonstrate that our approach enhances segmentation accuracy and reduces the impact of domain shift compared to conventional fl methods while maintaining privacy-preserving capabilities. significant performance improvements are observed across multiple hospitals (centers).",,2024-10-29,2025-05-06,"['hongyi pan', 'gorkem durak', 'zheyuan zhang', 'yavuz taktak', 'elif keles', 'halil ertugrul aktas', 'alpay medetalibeyoglu', 'yury velichko', 'concetto spampinato', 'ivo schoots', 'marco j. bruno', 'rajesh n. keswani', 'pallavi tiwari', 'candice bolan', 'tamas gonda', 'michael g. goggins', 'michael b. wallace', 'ziyue xu', 'ulas bagci']"
2411.01624,precm: the padding-based rotation equivariant convolution mode for   semantic segmentation,cs.cv,"semantic segmentation is an important branch of image processing and computer vision. with the popularity of deep learning, various convolutional neural networks have been proposed for pixel-level classification and segmentation tasks. in practical scenarios, however, imaging angles are often arbitrary, encompassing instances such as water body images from remote sensing and capillary and polyp images in the medical domain, where prior orientation information is typically unavailable to guide these networks to extract more effective features. in this case, learning features from objects with diverse orientation information poses a significant challenge, as the majority of cnn-based semantic segmentation networks lack rotation equivariance to resist the disturbance from orientation information. to address this challenge, this paper first constructs a universal convolution-group framework aimed at more fully utilizing orientation information and equipping the network with rotation equivariance. subsequently, we mathematically design a padding-based rotation equivariant convolution mode (precm), which is not only applicable to multi-scale images and convolutional kernels but can also serve as a replacement component for various types of convolutions, such as dilated convolutions, transposed convolutions, and asymmetric convolution. to quantitatively assess the impact of image rotation in semantic segmentation tasks, we also propose a new evaluation metric, rotation difference (rd). the replacement experiments related to six existing semantic segmentation networks on three datasets show that, the average intersection over union (iou) of their precm-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%, 8.33% compared to their original versions in terms of random angle rotation. and the average rd values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%, 3.43% respectively.",10.1109/tip.2025.3558425,2024-11-03,2025-04-30,"['xinyu xu', 'huazhen liu', 'tao zhang', 'huilin xiong', 'wenxian yu']"
2411.01742,learning from convolution-based unlearnable datasets,cs.lg cs.cv,"the construction of large datasets for deep learning has raised concerns regarding unauthorized use of online data, leading to increased interest in protecting data from third-parties who want to use it for training. the convolution-based unlearnable dataset (cuda) method aims to make data unlearnable by applying class-wise blurs to every image in the dataset so that neural networks learn relations between blur kernels and labels, as opposed to informative features for classifying clean data. in this work, we evaluate whether cuda data remains unlearnable after image sharpening and frequency filtering, finding that this combination of simple transforms improves the utility of cuda data for training. in particular, we observe a substantial increase in test accuracy over adversarial training for models trained with cuda unlearnable data from cifar-10, cifar-100, and imagenet-100. in training models to high accuracy using unlearnable data, we underscore the need for ongoing refinement in data poisoning techniques to ensure data privacy. our method opens new avenues for enhancing the robustness of unlearnable datasets by highlighting that simple methods such as sharpening and frequency filtering are capable of breaking convolution-based unlearnable datasets.",,2024-11-03,2025-05-07,"['dohyun kim', 'pedro sandoval-segura']"
2411.02116,advancements and limitations of llms in replicating human color-word   associations,cs.cl cs.cv cs.gr cs.hc,"color-word associations play a fundamental role in human cognition and design applications. large language models (llms) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. however, their ability to replicate human color-word associations remains understudied. we compared multiple generations of llms (from gpt-3 to gpt-4o) against human color-word associations using data collected from over 10,000 japanese participants, involving 17 colors and 80 words (10 word from eight categories) in japanese. our findings reveal a clear progression in llm performance across generations, with gpt-4o achieving the highest accuracy in predicting the best voted word for each color and category. however, the highest median performance was approximately 50% even for gpt-4o with visual inputs (chance level of 10%). moreover, we found performance variations across word categories and colors: while llms tended to excel in categories such as rhythm and landscape, they struggled with categories such as emotions. interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. thus, despite reasonable alignment in basic color discrimination, humans and llms still diverge systematically in the words they assign to those colors. our study highlights both the advancements in llm capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and llms in representing color-word associations.",,2024-11-04,2025-05-07,"['makoto fukushima', 'shusuke eshita', 'hiroshige fukuhara']"
2411.02179,clear: robust context-guided generative lighting estimation for mobile   augmented reality,cs.cv cs.gr cs.hc,"high-quality environment lighting is essential for creating immersive mobile augmented reality (ar) experiences. however, achieving visually coherent estimation for mobile ar is challenging due to several key limitations in ar device sensing capabilities, including low camera fov and limited pixel dynamic ranges. recent advancements in generative ai, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. in this work, we design and implement a generative lighting estimation system called clear that can produce high-quality, diverse environment maps in the format of 360{\deg} hdr images. specifically, we design a two-step generation pipeline guided by ar environment context data to ensure the output aligns with the physical environment's visual context and color appearance. to improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on ar devices. through a combination of quantitative and qualitative evaluations, we show that clear outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. for example, clear achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. clear produces lighting estimates of comparable or better quality in just 3.2 seconds -- over 110x faster than state-of-the-art methods.",,2024-11-04,2025-05-05,"['yiqin zhao', 'mallesham dasari', 'tian guo']"
2411.02979,cad-nerf: learning nerfs from uncalibrated few-view images by cad model   retrieval,cs.cv,"reconstructing from multi-view images is a longstanding problem in 3d vision, where neural radiance fields (nerfs) have shown great potential and get realistic rendered images of novel views. currently, most nerf methods either require accurate camera poses or a large number of input images, or even both. reconstructing nerf from few-view images without poses is challenging and highly ill-posed. to address this problem, we propose cad-nerf, a method reconstructed from less than 10 images without any known poses. specifically, we build a mini library of several cad models from shapenet and render them from many random views. given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated nerf methods. then, the geometry of the object is trained by the cad guidance. the deformation of the density field and camera poses are optimized jointly. then texture and density are trained and fine-tuned as well. all training phases are in self-supervised manners. comprehensive evaluations of synthetic and real images show that cad-nerf successfully learns accurate densities with a large deformation from retrieved cad models, showing the generalization abilities.",10.1007/s11704-024-40417-7,2024-11-05,2025-05-04,"['xin wen', 'xuening zhu', 'renjiao yi', 'zhifeng wang', 'chenyang zhu', 'kai xu']"
2411.03239,decoupling fine detail and global geometry for compressed depth map   super-resolution,cs.cv,"recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. however, current methods still suffer from two challenges. first, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. to address these challenges, we propose a novel framework, termed geometry-decoupled network (gdnet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. to be specific, we propose the fine geometry detail encoder (fgde), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. in addition, we develop the global geometry encoder (gge) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. we conduct experiments on multiple benchmark datasets, demonstrating that our gdnet significantly outperforms current methods in terms of geometric consistency and detail recovery. in the eccv 2024 aim compressed depth upsampling challenge, our solution won the 1st place award. our codes are available at: https://github.com/ian0926/gdnet.",,2024-11-05,2025-05-06,"['huan zheng', 'wencheng han', 'jianbing shen']"
2411.04997,llm2clip: powerful language model unlocks richer visual representation,cs.cv cs.cl,"clip is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. its effectiveness primarily stems from the use of natural language as rich supervision. motivated by the remarkable advancements in large language models (llms), this work explores how llms' superior text understanding and extensive open-world knowledge can enhance clip's capability, especially for processing longer and more complex image captions. we propose an efficient post-training strategy that integrates llms into pretrained clip. to address the challenge posed by the autoregressive nature of llms, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of llm outputs. extensive experiments demonstrate that our approach outperforms lora-based methods, achieving nearly fourfold faster training with superior performance. furthermore, we validate substantial improvements over state-of-the-art models such as clip, eva02, and siglip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.",,2024-11-07,2025-05-07,"['weiquan huang', 'aoqi wu', 'yifan yang', 'xufang luo', 'yuqing yang', 'liang hu', 'qi dai', 'chunyu wang', 'xiyang dai', 'dongdong chen', 'chong luo', 'lili qiu']"
2411.05261,cyclic vision-language manipulator: towards reliable and fine-grained   image interpretation for automated report generation,cs.cv cs.ai cs.cl cs.lg,"despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. this paper introduces a novel approach to identify specific image features in x-ray images that influence the outputs of report generation models. specifically, we propose cyclic vision-language manipulator cvlm, a module to generate a manipulated x-ray from an original x-ray and its report from a designated report generator. the essence of cvlm is that cycling manipulated x-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for x-ray generation, achieving the term ""cyclic manipulation"". this process allows direct comparison between original and manipulated x-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. empirical evaluations demonstrate that cvlm can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of ai-generated reports.",,2024-11-07,2025-05-06,"['yingying fang', 'zihao jin', 'shaojie guo', 'jinda liu', 'zhiling yue', 'yijian gao', 'junzhi ning', 'zhi li', 'simon walsh', 'guang yang']"
2411.06685,high-frequency enhanced hybrid neural representation for video   compression,cs.cv cs.ai eess.iv,"neural representations for videos (nerv) have simplified the video codec process and achieved swift decoding speeds by encoding video content into a neural network, presenting a promising solution for video compression. however, existing work overlooks the crucial issue that videos reconstructed by these methods lack high-frequency details. to address this problem, this paper introduces a high-frequency enhanced hybrid neural representation network. our method focuses on leveraging high-frequency information to improve the synthesis of fine details by the network. specifically, we design a wavelet high-frequency encoder that incorporates wavelet frequency decomposer (wfd) blocks to generate high-frequency feature embeddings. next, we design the high-frequency feature modulation (hfm) block, which leverages the extracted high-frequency embeddings to enhance the fitting process of the decoder. finally, with the refined harmonic decoder block and a dynamic weighted frequency loss, we further reduce the potential loss of high-frequency information. experiments on the bunny and uvg datasets demonstrate that our method outperforms other methods, showing notable improvements in detail preservation and compression performance.",,2024-11-10,2025-04-29,"['li yu', 'zhihui li', 'jimin xiao', 'moncef gabbouj']"
2411.06911,gaussian process emulators for few-shot segmentation in cardiac mri,cs.cv cs.ai cs.lg,"segmentation of cardiac magnetic resonance images (mri) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. most recent techniques rely on deep learning and usually require an extensive amount of labeled data. to overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. in this work, we introduce a new method that merges few-shot learning with a u-net architecture and gaussian process emulators (gpes), enhancing data integration from a support set for improved performance. gpes are trained to learn the relation between the support images and the corresponding masks in latent space, facilitating the segmentation of unseen query images given only a small labeled support set at inference. we test our model with the m&ms-2 public dataset to assess its ability to segment the heart in cardiac magnetic resonance imaging from different orientations, and compare it with state-of-the-art unsupervised and few-shot methods. our architecture shows higher dice coefficients compared to these methods, especially in the more challenging setups where the size of the support set is considerably small.",10.1007/978-3-031-87756-8_26,2024-11-11,2024-11-12,"['bruno viti', 'franz thaler', 'kathrin lisa kapper', 'martin urschler', 'martin holler', 'elias karabelas']"
2411.07848,zero-shot object-centric instruction following: integrating foundation   models with traditional navigation,cs.ro cs.cv,"large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3d graph of landmarks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. in this work, we propose language-inferred factor graph for instruction following (lifgif), a zero-shot method to ground natural language instructions in such a map. lifgif also includes a policy for following natural language navigation instructions in a novel environment while the map is constructed, enabling robust navigation performance in the physical world. to evaluate lifgif, we present a new dataset, object-centric vln (oc-vln), in order to evaluate grounding of object-centric natural language navigation instructions. we compare to two state-of-the-art zero-shot baselines from related tasks, object goal navigation and vision language navigation, to demonstrate that lifgif outperforms them across all our evaluation metrics on ocvln. finally, we successfully demonstrate the effectiveness of lifgif for performing zero-shot object-centric instruction following in the real world on a boston dynamics spot robot.",,2024-11-12,2025-05-07,"['sonia raychaudhuri', 'duy ta', 'katrina ashton', 'angel x. chang', 'jiuguang wang', 'bernadette bucher']"
2411.08777,ludo: low-latency understanding of deformable objects using point cloud   occupancy functions,cs.ro cs.cv,"accurately determining the shape of objects and the location of their internal structures within deformable objects is crucial for medical tasks that require precise targeting, such as robotic biopsies. we introduce ludo, a method for accurate low-latency understanding of deformable objects. ludo reconstructs objects in their deformed state, including their internal structures, from a single-view point cloud observation in under 30 ms using occupancy networks. ludo provides uncertainty estimates for its predictions. additionally, it provides explainability by highlighting key features in its input observations. both uncertainty and explainability are important for safety-critical applications such as surgical interventions. we demonstrate ludo's abilities for autonomous targeting of internal regions of interest (rois) in deformable objects. we evaluate ludo in real-world robotic experiments, achieving a success rate of 98.9% for puncturing various rois inside deformable objects. ludo demonstrates the potential to interact with deformable objects without the need for deformable registration methods.",,2024-11-13,2025-05-08,"['pit henrich', 'franziska mathis-ullrich', 'paul maria scheikl']"
2411.09986,unlocking transfer learning for open-world few-shot recognition,cs.cv cs.ai,"few-shot open-set recognition (fsosr) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. to unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. in the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. during the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. the proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniimagenet and tieredimagenet, with only a 1.5\% increase in training effort. our work demonstrates the effectiveness of transfer learning in fsosr.",,2024-11-15,2025-05-03,"['byeonggeun kim', 'juntae lee', 'kyuhong shim', 'simyung chang']"
2411.10232,coloredit: training-free image-guided color editing with diffusion model,cs.cv cs.ai,"text-to-image (t2i) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. however, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. in this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. we observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. lastly, we present a benchmark dataset called colorbench, the first benchmark to evaluate the performance of color change methods. extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.",,2024-11-15,2025-04-30,"['xingxi yin', 'zhi li', 'jingfeng zhang', 'chenglin li', 'yin zhang']"
2411.12286,glover: generalizable open-vocabulary affordance reasoning for   task-oriented grasping,cs.ro cs.cv,"inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3d radiance modeling, restricting real-time, open-vocabulary interactions with objects. to address these limitations, we propose glover, a unified generalizable open-vocabulary affordance reasoning framework, which fine-tunes the large language models (llms) to predict the visual affordance of graspable object parts within rgb feature space. we compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. glover inherits world knowledge and common-sense reasoning from llms, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. to enable effective real-world deployment, we present affordance-aware grasping estimation (age), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. in evaluations across 30 table-top real-world scenes, glover achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. we also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.",,2024-11-19,2025-05-01,"['teli ma', 'zifan wang', 'jiaming zhou', 'mengmeng wang', 'junwei liang']"
2411.12516,modular autonomous virtualization system for two-dimensional   semiconductor quantum dot arrays,cond-mat.mes-hall cs.cv cs.et cs.lg quant-ph,"arrays of gate-defined semiconductor quantum dots are among the leading candidates for building scalable quantum processors. high-fidelity initialization, control, and readout of spin qubit registers require exquisite and targeted control over key hamiltonian parameters that define the electrostatic environment. however, due to the tight gate pitch, capacitive crosstalk between gates hinders independent tuning of chemical potentials and interdot couplings. while virtual gates offer a practical solution, determining all the required cross-capacitance matrices accurately and efficiently in large quantum dot registers is an open challenge. here, we establish a modular automated virtualization system (mavis) -- a general and modular framework for autonomously constructing a complete stack of multilayer virtual gates in real time. our method employs machine learning techniques to rapidly extract features from two-dimensional charge stability diagrams. we then utilize computer vision and regression models to self-consistently determine all relative capacitive couplings necessary for virtualizing plunger and barrier gates in both low- and high-tunnel-coupling regimes. using mavis, we successfully demonstrate accurate virtualization of a dense two-dimensional array comprising ten quantum dots defined in a high-quality ge/sige heterostructure. our work offers an elegant and practical solution for the efficient control of large-scale semiconductor quantum dot systems.",10.1103/physrevx.15.021034,2024-11-19,2025-05-06,"['anantha s. rao', 'donovan buterakos', 'barnaby van straaten', 'valentin john', 'cécile x. yu', 'stefan d. oosterhout', 'lucas stehouwer', 'giordano scappucci', 'menno veldhorst', 'francesco borsoi', 'justyna p. zwolak']"
2411.14412,adversarial data poisoning attacks on quantum machine learning in the   nisq era,quant-ph cs.cr cs.cv,"with the growing interest in quantum machine learning (qml) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with qml has become an urgent priority. one key concern in the qml domain is the threat of data poisoning attacks in the current quantum cloud setting. adversarial access to training data could severely compromise the integrity and availability of qml models. classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for qml models in the noisy intermediate scale quantum (nisq) era. in this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ess) by analyzing the outputs of encoding circuits. leveraging this approach, we introduce a \underline{qu}antum \underline{i}ndiscriminate \underline{d}ata poisoning attack, quid. through extensive experiments conducted in both noiseless and noisy environments (e.g., ibm\_brisbane's noise), across various architectures and datasets, quid achieves up to $92\%$ accuracy degradation in model performance compared to baseline models and up to $75\%$ accuracy degradation compared to random label-flipping. we also tested quid against state-of-the-art classical defenses, with accuracy degradation still exceeding $50\%$, demonstrating its effectiveness. this work represents the first attempt to reevaluate data poisoning attacks in the context of qml.",,2024-11-21,2025-04-30,"['satwik kundu', 'swaroop ghosh']"
2411.14423,physflow: unleashing the potential of multi-modal foundation models and   video diffusion for 4d dynamic physical scene simulation,cs.cv,"realistic simulation of dynamic scenes requires accurately capturing diverse material properties and modeling complex object interactions grounded in physical principles. however, existing methods are constrained to basic material types with limited predictable parameters, making them insufficient to represent the complexity of real-world materials. we introduce physflow, a novel approach that leverages multi-modal foundation models and video diffusion to achieve enhanced 4d dynamic scene simulation. our method utilizes multi-modal models to identify material types and initialize material parameters through image queries, while simultaneously inferring 3d gaussian splats for detailed scene representation. we further refine these material parameters using video diffusion with a differentiable material point method (mpm) and optical flow guidance rather than render loss or score distillation sampling (sds) loss. this integrated framework enables accurate prediction and realistic simulation of dynamic interactions in real-world scenarios, advancing both accuracy and flexibility in physics-based simulations.",,2024-11-21,2025-05-08,"['zhuoman liu', 'weicai ye', 'yan luximon', 'pengfei wan', 'di zhang']"
2411.14432,insight-v: exploring long-chain visual reasoning with multimodal large   language models,cs.cv,"large language models (llms) demonstrate enhanced capabilities and reliability by reasoning more, evolving from chain-of-thought prompting to product-level solutions like openai o1. despite various efforts to improve llm reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. in this paper, we present insight-v, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (mllms). specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. we observe that directly supervising mllms with such long and complex reasoning data will not yield ideal reasoning ability. to tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. we further incorporate an iterative dpo algorithm to enhance the reasoning agent's generation stability and quality. based on the popular llava-next model and our stronger base mllm, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. benefiting from our multi-agent system, insight-v can also easily maintain or improve performance on perception-focused multi-modal tasks.",,2024-11-21,2025-05-02,"['yuhao dong', 'zuyan liu', 'hai-long sun', 'jingkang yang', 'winston hu', 'yongming rao', 'ziwei liu']"
2411.15106,"about time: advances, challenges, and outlooks of action understanding",cs.cv cs.ai cs.lg,"we have witnessed impressive advances in video action understanding. increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. this survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. we focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. we broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). this division allows us to identify specific action modeling and video representation challenges. finally, we outline future directions to address current shortcomings.",,2024-11-22,2025-05-06,"['alexandros stergiou', 'ronald poppe']"
2411.15255,osmamba: omnidirectional spectral mamba with dual-domain prior generator   for exposure correction,eess.iv cs.cv cs.lg,"exposure correction is a fundamental problem in computer vision and image processing. recently, frequency domain-based methods have achieved impressive improvement, yet they still struggle with complex real-world scenarios under extreme exposure conditions. this is due to the local convolutional receptive fields failing to model long-range dependencies in the spectrum, and the non-generative learning paradigm being inadequate for retrieving lost details from severely degraded regions. in this paper, we propose omnidirectional spectral mamba (osmamba), a novel exposure correction network that incorporates the advantages of state space models and generative diffusion models to address these limitations. specifically, osmamba introduces an omnidirectional spectral scanning mechanism that adapts mamba to the frequency domain to capture comprehensive long-range dependencies in both the amplitude and phase spectra of deep image features, hence enhancing illumination correction and structure recovery. furthermore, we develop a dual-domain prior generator that learns from well-exposed images to generate a degradation-free diffusion prior containing correct information about severely under- and over-exposed regions for better detail restoration. extensive experiments on multiple-exposure and mixed-exposure datasets demonstrate that the proposed osmamba achieves state-of-the-art performance both quantitatively and qualitatively.",,2024-11-22,2025-05-06,"['gehui li', 'bin chen', 'chen zhao', 'lei zhang', 'jian zhang']"
2411.15388,a contrast-agnostic method for ultra-high resolution claustrum   segmentation,cs.cv cs.lg eess.iv,"the claustrum is a band-like gray matter structure located between putamen and insula whose exact functions are still actively researched. its sheet-like structure makes it barely visible in in vivo magnetic resonance imaging (mri) scans at typical resolutions and neuroimaging tools for its study, including methods for automatic segmentation, are currently very limited. in this paper, we propose a contrast- and resolution-agnostic method for claustrum segmentation at ultra-high resolution (0.35 mm isotropic); the method is based on the synthseg segmentation framework (billot et al., 2023), which leverages the use of synthetic training intensity images to achieve excellent generalization. in particular, synthseg requires only label maps to be trained, since corresponding intensity images are synthesized on the fly with random contrast and resolution. we trained a deep learning network for automatic claustrum segmentation, using claustrum manual labels obtained from 18 ultra-high resolution mri scans (mostly ex vivo). we demonstrated the method to work on these 18 high resolution cases (dice score = 0.632, mean surface distance = 0.458 mm, and volumetric similarity = 0.867 using 6-fold cross validation (cv)), and also on in vivo t1-weighted mri scans at typical resolutions (~1 mm isotropic). we also demonstrated that the method is robust in a test-retest setting and when applied to multimodal imaging (t2-weighted, proton density and quantitative t1 scans). to the best of our knowledge this is the first accurate method for automatic ultra-high resolution claustrum segmentation, which is robust against changes in contrast and resolution. the method is released at https://github.com/chiara-mauri/claustrum_segmentation and as part of the neuroimaging package freesurfer (fischl, 2012).",,2024-11-22,2025-04-29,"['chiara mauri', 'ryan fritz', 'jocelyn mora', 'benjamin billot', 'juan eugenio iglesias', 'koen van leemput', 'jean augustinack', 'douglas n greve']"
2411.15539,large language model with region-guided referring and grounding for ct   report generation,cs.cv cs.ai,"computed tomography (ct) report generation is crucial to assist radiologists in interpreting ct volumes, which can be time-consuming and labor-intensive. existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. to address this issue, we propose reg2rg, the first region-guided referring and grounding framework for ct report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. a local feature decoupling (lfd) strategy is proposed to preserve the local high-resolution details with little computational overhead. then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. moreover, we propose a novel region-report alignment (rra) training strategy. it leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. a large language model (llm) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. extensive experiments on two large-scale chest ct-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. the code is available at https://github.com/zhi-xuan-chen/reg2rg.",,2024-11-23,2025-05-04,"['zhixuan chen', 'yequan bie', 'haibo jin', 'hao chen']"
2411.15702,editable-deepsc: reliable cross-modal semantic communications for facial   editing,cs.it cs.cv cs.ni math.it,"real-time computer vision (cv) plays a crucial role in various real-world applications, whose performance is highly dependent on communication networks. nonetheless, the data-oriented characteristics of conventional communications often do not align with the special needs of real-time cv tasks. to alleviate this issue, the recently emerged semantic communications only transmit task-related semantic information and exhibit a promising landscape to address this problem. however, the communication challenges associated with semantic facial editing, one of the most important real-time cv applications on social media, still remain largely unexplored. in this paper, we fill this gap by proposing editable-deepsc, a novel cross-modal semantic communication approach for facial editing. firstly, we theoretically discuss different transmission schemes that separately handle communications and editings, and emphasize the necessity of joint editing-channel coding (jecc) via iterative attributes matching, which integrates editings into the communication chain to preserve more semantic mutual information. to compactly represent the high-dimensional data, we leverage inversion methods via pre-trained stylegan priors for semantic coding. to tackle the dynamic channel noise conditions, we propose snr-aware channel coding via model fine-tuning. extensive experiments indicate that editable-deepsc can achieve superior editings while significantly saving the transmission bandwidth, even under high-resolution and out-of-distribution (ood) settings.",,2024-11-23,2025-05-06,"['bin chen', 'wenbo yu', 'qinshan zhang', 'tianqu zhuang', 'yong jiang', 'shu-tao xia']"
2411.15923,deep learning for automated multi-scale functional field boundaries   extraction using multi-date sentinel-2 and planetscope imagery: case study of   netherlands and pakistan,cs.cv cs.ai cs.lg,"this study explores the effectiveness of multi-temporal satellite imagery for better functional field boundary delineation using deep learning semantic segmentation architecture on two distinct geographical and multi-scale farming systems of netherlands and pakistan. multidate images of april, august and october 2022 were acquired for planetscope and sentinel-2 in sub regions of netherlands and november 2022, february and march 2023 for selected area of dunyapur in pakistan. for netherlands, basic registration crop parcels (brp) vector layer was used as labeled training data. while self-crafted field boundary vector data were utilized for pakistan. four deep learning models with unet architecture were evaluated using different combinations of multi-date images and ndvi stacks in the netherlands subregions. a comparative analysis of iou scores assessed the effectiveness of the proposed multi-date ndvi stack approach. these findings were then applied for transfer learning, using pre-trained models from the netherlands on the selected area in pakistan. additionally, separate models were trained using self-crafted field boundary data for pakistan, and combined models were developed using data from both the netherlands and pakistan. results indicate that multi-date ndvi stacks provide additional temporal context, reflecting crop growth over different times of the season. the study underscores the critical role of multi-scale ground information from diverse geographical areas in developing robust and universally applicable models for field boundary delineation. the results also highlight the importance of fine spatial resolution for extraction of field boundaries in regions with small scale framing. the findings can be extended to multi-scale implementations for improved automatic field boundary delineation in heterogeneous agricultural environments.",,2024-11-24,,"['saba zahid', 'sajid ghuffar', 'n/a obaid-ur-rehman', 'syed roshaan ali shah']"
2411.16508,all languages matter: evaluating lmms on culturally diverse 100   languages,cs.cv cs.cl,"existing large multimodal models (lmms) generally focus on only a few regions and languages. as lmms continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. in pursuit of culturally diverse global multimodal models, our proposed all languages matter benchmark (alm-bench) represents the largest and most comprehensive effort to date for evaluating lmms across 100 languages. alm-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in lmm research. the benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. alm-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. to capture the rich tapestry of global cultures, alm-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. through this, alm-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source lmms but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. our benchmark is publicly available.",,2024-11-25,2025-04-30,"['ashmal vayani', 'dinura dissanayake', 'hasindri watawana', 'noor ahsan', 'nevasini sasikumar', 'omkar thawakar', 'henok biadglign ademtew', 'yahya hmaiti', 'amandeep kumar', 'kartik kuckreja', 'mykola maslych', 'wafa al ghallabi', 'mihail mihaylov', 'chao qin', 'abdelrahman m shaker', 'mike zhang', 'mahardika krisna ihsani', 'amiel esplana', 'monil gokani', 'shachar mirkin', 'harsh singh', 'ashay srivastava', 'endre hamerlik', 'fathinah asma izzati', 'fadillah adamsyah maani', 'sebastian cavada', 'jenny chim', 'rohit gupta', 'sanjay manjunath', 'kamila zhumakhanova', 'feno heriniaina rabevohitra', 'azril amirudin', 'muhammad ridzuan', 'daniya kareem', 'ketan more', 'kunyang li', 'pramesh shakya', 'muhammad saad', 'amirpouya ghasemaghaei', 'amirbek djanibekov', 'dilshod azizov', 'branislava jankovic', 'naman bhatia', 'alvaro cabrera', 'johan obando-ceron', 'olympiah otieno', 'fabian farestam', 'muztoba rabbani', 'sanoojan baliah', 'santosh sanjeev', 'abduragim shtanchaev', 'maheen fatima', 'thao nguyen', 'amrin kareem', 'toluwani aremu', 'nathan xavier', 'amit bhatkal', 'hawau toyin', 'aman chadha', 'hisham cholakkal', 'rao muhammad anwer', 'michael felsberg', 'jorma laaksonen', 'thamar solorio', 'monojit choudhury', 'ivan laptev', 'mubarak shah', 'salman khan', 'fahad khan']"
2411.16721,steering away from harm: an adaptive approach to defending vision   language model against jailbreaks,cs.cv cs.ai,"vision language models (vlms) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. existing defenses, such as input preprocessing, adversarial training, and response evaluation-based methods, are often impractical for real-world deployment due to their high costs. to address this challenge, we propose astra, an efficient and effective defense by adaptively steering models away from adversarial feature directions to resist vlm attacks. our key procedures involve finding transferable steering vectors representing the direction of harmful response and applying adaptive activation steering to remove these directions at inference time. to create effective steering vectors, we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks. these tokens are then used to construct steering vectors. during inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs. extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency in mitigating jailbreak risks. additionally, astra exhibits good transferability, defending against unseen attacks (i.e., structured-based attack, perturbation-based attack with project gradient descent variants, and text-only attack). our code is available at \url{https://github.com/astral-group/astra}.",,2024-11-22,2025-05-01,"['han wang', 'gang wang', 'huan zhang']"
2411.16926,context-aware input orchestration for video inpainting,cs.cv,"traditional neural network-driven inpainting methods struggle to deliver high-quality results within the constraints of mobile device processing power and memory. our research introduces an innovative approach to optimize memory usage by altering the composition of input data. typically, video inpainting relies on a predetermined set of input frames, such as neighboring and reference frames, often limited to five-frame sets. our focus is to examine how varying the proportion of these input frames impacts the quality of the inpainted video. by dynamically adjusting the input frame composition based on optical flow and changes of the mask, we have observed an improvement in various contents including rapid visual context changes.",,2024-11-25,2025-05-05,"['hoyoung kim', 'azimbek khudoyberdiev', 'seonghwan jeong', 'jihoon ryoo']"
2411.17251,interpretable dynamic graph neural networks for small occluded object   detection and tracking,cs.cv cs.lg,"the detection and tracking of small, occluded objects such as pedestrians, cyclists, and motorbikes pose significant challenges for traffic surveillance systems because of their erratic movement, frequent occlusion, and poor visibility in dynamic urban environments. traditional methods like yolo11, while proficient in spatial feature extraction for precise detection, often struggle with these small and dynamically moving objects, particularly in handling real-time data updates and resource efficiency. this paper introduces dgnn-yolo, a novel framework that integrates dynamic graph neural networks (dgnns) with yolo11 to address these limitations. unlike standard gnns, dgnns are chosen for their superior ability to dynamically update graph structures in real-time, which enables adaptive and robust tracking of objects in highly variable urban traffic scenarios. this framework constructs and regularly updates its graph representations, capturing objects as nodes and their interactions as edges, thus effectively responding to rapidly changing conditions. additionally, dgnn-yolo incorporates grad-cam, grad-cam++, and eigen-cam visualization techniques to enhance interpretability and foster trust, offering insights into the model's decision-making process. extensive experiments validate the framework's performance, achieving a precision of 0.8382, recall of 0.6875, and map@0.5:0.95 of 0.6476, significantly outperforming existing methods. this study offers a scalable and interpretable solution for real-time traffic surveillance and significantly advances intelligent transportation systems' capabilities by addressing the critical challenge of detecting and tracking small, occluded objects.",,2024-11-26,2025-05-05,"['shahriar soudeep', 'md abrar jahin', 'm. f. mridha']"
2411.17662,robopepp: vision-based robot pose and joint angle estimation through   embedding predictive pre-training,cs.ro cs.cv,"vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. while images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. to address this, we introduce robopepp, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. the pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.",,2024-11-26,2025-05-02,"['raktim gautam goswami', 'prashanth krishnamurthy', 'yann lecun', 'farshad khorrami']"
2411.18159,type-r: automatically retouching typos for text-to-image generation,cs.cv,"while recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image. in this paper, we propose to retouch erroneous text renderings in the post-processing pipeline. our approach, called type-r, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words. through extensive experiments, we show that type-r, in combination with the latest text-to-image models such as stable diffusion or flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.",,2024-11-27,2025-04-30,"['wataru shimoda', 'naoto inoue', 'daichi haraguchi', 'hayato mitani', 'seiichi uchida', 'kota yamaguchi']"
2411.18673,ac3d: analyzing and improving 3d camera control in video diffusion   transformers,cs.cv,"numerous works have recently integrated 3d camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. in this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3d camera manipulation without compromising synthesis quality. first, we determine that motion induced by camera movements in videos is low-frequency in nature. this motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. this suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to a 4x reduction of training parameters, improved training speed, and 10% higher visual quality. finally, we complement the typical dataset for camera control learning with a curated dataset of 20k diverse, dynamic videos with stationary cameras. this helps the model distinguish between camera and scene motion and improves the dynamics of generated pose-conditioned videos. we compound these findings to design the advanced 3d camera control (ac3d) architecture, the new state-of-the-art model for generative video modeling with camera control.",,2024-11-27,2025-05-06,"['sherwin bahmani', 'ivan skorokhodov', 'guocheng qian', 'aliaksandr siarohin', 'willi menapace', 'andrea tagliasacchi', 'david b. lindell', 'sergey tulyakov']"
2411.18674,active data curation effectively distills large-scale multimodal models,cs.cv cs.lg,"knowledge distillation (kd) is the de facto standard for compressing large-scale models into smaller ones. prior works have explored ever more complex kd strategies involving different objective functions, teacher-ensembles, and weight inheritance. in this work we explore an alternative, yet simple approach -- active data curation as effective distillation for contrastive multimodal pretraining. our simple online batch selection method, acid, outperforms strong kd baselines across various model-, data- and compute-configurations. further, we find such an active data curation strategy to in fact be complementary to standard kd, and can be effectively combined to train highly performant inference-efficient models. our simple and scalable pretraining framework, aced, achieves state-of-the-art results across 27 zero-shot classification and retrieval tasks with upto 11% less inference flops. we further demonstrate that our aced models yield strong vision-encoders for training generative multimodal models in the lit-decoder setting, outperforming larger vision encoders for image-captioning and visual question-answering tasks.",,2024-11-27,2025-05-05,"['vishaal udandarao', 'nikhil parthasarathy', 'muhammad ferjad naeem', 'talfan evans', 'samuel albanie', 'federico tombari', 'yongqin xian', 'alessio tonioni', 'olivier j. hénaff']"
2411.19167,hot3d: hand and object tracking in 3d from egocentric multi-view videos,cs.cv cs.ai cs.ro,"we introduce hot3d, a publicly available dataset for egocentric hand and object tracking in 3d. the dataset offers over 833 minutes (3.7m+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. in addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. the recordings include multiple synchronized data streams containing egocentric multi-view rgb/monochrome images, eye gaze signal, scene point clouds, and 3d poses of cameras, hands, and objects. the dataset is recorded with two headsets from meta: project aria, which is a research prototype of ai glasses, and quest 3, a virtual-reality headset that has shipped millions of units. ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. hand annotations are provided in the umetrack and mano formats, and objects are represented by 3d meshes with pbr materials obtained by an in-house scanner. in our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3d hand tracking, model-based 6dof object pose estimation, and 3d lifting of unknown in-hand objects. the evaluated multi-view methods, whose benchmarking is uniquely enabled by hot3d, significantly outperform their single-view counterparts.",,2024-11-28,2025-04-30,"['prithviraj banerjee', 'sindi shkodrani', 'pierre moulon', 'shreyas hampali', 'shangchen han', 'fan zhang', 'linguang zhang', 'jade fountain', 'edward miller', 'selen basol', 'richard newcombe', 'robert wang', 'jakob julian engel', 'tomas hodan']"
2411.19415,amo sampler: enhancing text rendering with overshooting,cs.cv cs.ai cs.lg,"achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. sate-of-the-art models like stable diffusion 3 (sd3), flux, and auraflow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. we introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. specifically, we introduce an overshooting sampler for pretrained rectified flow (rf) models, by alternating between over-simulating the learned ordinary differential equation (ode) and reintroducing noise. compared to the euler sampler, the overshooting sampler effectively introduces an extra langevin dynamics term that can help correct the compounding error from successive euler steps and therefore improve the text rendering. however, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. to address this issue, we propose an attention modulated overshooting sampler (amo), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. amo demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on sd3 and flux without compromising overall image quality or increasing inference cost. code available at: https://github.com/hxixixh/amo-release.",,2024-11-28,2025-05-02,"['xixi hu', 'keyang xu', 'bo liu', 'qiang liu', 'hongliang fei']"
2411.19509,ditto: motion-space diffusion for controllable realtime talking head   synthesis,cs.cv cs.lg cs.sd eess.as,"recent advances in diffusion models have endowed talking head synthesis with subtle expressions and vivid head movements, but have also led to slow inference speed and insufficient control over generated results. to address these issues, we propose ditto, a diffusion-based talking head framework that enables fine-grained controls and real-time inference. specifically, we utilize an off-the-shelf motion extractor and devise a diffusion transformer to generate representations in a specific motion space. we optimize the model architecture and training strategy to address the issues in generating motion representations, including insufficient disentanglement between motion and identity, and large internal discrepancies within the representation. besides, we employ diverse conditional signals while establishing a mapping between motion representation and facial semantics, enabling control over the generation process and correction of the results. moreover, we jointly optimize the holistic framework to enable streaming processing, real-time inference, and low first-frame delay, offering functionalities crucial for interactive applications such as ai assistants. extensive experimental results demonstrate that ditto generates compelling talking head videos and exhibits superiority in both controllability and real-time performance.",,2024-11-29,2025-04-30,"['tianqi li', 'ruobing zheng', 'minghui yang', 'jingdong chen', 'ming yang']"
2412.00065,dyrect computed tomography: dynamic reconstruction of events on a   continuous timescale,eess.iv cs.cv,"time-resolved high-resolution x-ray computed tomography (4d $\mu$ct) is an imaging technique that offers insight into the evolution of dynamic processes inside materials that are opaque to visible light. conventional tomographic reconstruction techniques are based on recording a sequence of 3d images that represent the sample state at different moments in time. this frame-based approach limits the temporal resolution compared to dynamic radiography experiments due to the time needed to make ct scans. moreover, it leads to an inflation of the amount of data and thus to costly post-processing computations to quantify the dynamic behaviour from the sequence of time frames, hereby often ignoring the temporal correlations of the sample structure. our proposed 4d $\mu$ct reconstruction technique, named dyrect, estimates individual attenuation evolution profiles for each position in the sample. this leads to a novel memory-efficient event-based representation of the sample, using as little as three image volumes: its initial attenuation, its final attenuation and the transition times. this third volume represents local events on a continuous timescale instead of the discrete global time frames. we propose a method to iteratively reconstruct the transition times and the attenuation volumes. the dynamic reconstruction technique was validated on synthetic ground truth data and experimental data, and was found to effectively pinpoint the transition times in the synthetic dataset with a time resolution corresponding to less than a tenth of the amount of projections required to reconstruct traditional $\mu$ct time frames.",10.1109/tci.2025.3566241,2024-11-15,,"['wannes goethals', 'tom bultreys', 'steffen berg', 'matthieu n. boone', 'jan aelterman']"
2412.03093,expanding event modality applications through a robust clip-based   encoder,cs.cv,"this paper introduces a powerful encoder that transfers clip`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains. while large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality. to address this challenge, we adapt clip`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving text alignment while mitigating catastrophic forgetting. our encoder achieves strong performance in object recognition, with competitive results in zero-shot and few-shot learning tasks. notably, it generalizes effectively to events extracted from video data without requiring additional training, highlighting its versatility. additionally, we integrate this encoder within a cross-modality framework that facilitates interaction across five modalities-image, event, text, sound, and depth-expanding the possibilities for cross-modal applications. overall, this work underscores the transformative potential of a robust event encoder, broadening the scope and utility of event-based data across various fields.",,2024-12-04,2025-05-08,"['sungheon jeong', 'hanning chen', 'sanggeon yun', 'suhyeon cho', 'wenjun huang', 'xiangjian liu', 'mohsen imani']"
2412.03118,objectfinder: an open-vocabulary assistive system for interactive object   search by blind people,cs.hc cs.cv,"searching for objects in unfamiliar scenarios is a challenging task for blind people. it involves specifying the target object, detecting it, and then gathering detailed information according to the user's intent. however, existing description- and detection-based assistive technologies do not sufficiently support the multifaceted nature of interactive object search tasks. we present objectfinder, an open-vocabulary wearable assistive system for interactive object search by blind people. objectfinder allows users to query target objects using flexible wording. once the target object is detected, it provides egocentric localization information in real-time, including distance and direction. users can then initiate different branches to gather detailed information based on their intent towards the target object, such as navigating to it or perceiving its surroundings. objectfinder is powered by a seamless combination of open-vocabulary models, namely an open-vocabulary object detector and a multimodal large language model. the objectfinder design concept and its development were carried out in collaboration with a blind co-designer. to evaluate objectfinder, we conducted an exploratory user study with eight blind participants. we compared objectfinder to bemyai and google lookout, popular description- and detection-based assistive applications. our findings indicate that most participants felt more independent with objectfinder and preferred it for object search, as it enhanced scene context gathering and navigation, and allowed for active target identification. finally, we discuss the implications for future assistive systems to support interactive object search.",,2024-12-04,2025-04-30,"['ruiping liu', 'jiaming zhang', 'angela schön', 'karin müller', 'junwei zheng', 'kailun yang', 'anhong guo', 'kathrin gerling', 'rainer stiefelhagen']"
2412.03255,dynamiccontrol: adaptive condition selection for improved text-to-image   generation,cs.cv,"to enhance the controllability of text-to-image diffusion models, current controlnet-like models have explored various control signals to dictate image attributes. however, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. this underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. to address this issue, we propose a novel framework, dynamiccontrol, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. this controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. then, we integrate a multimodal large language model (mllm) to build an efficient condition evaluator. this evaluator optimizes the ordering of conditions based on the double-cycle controller's score ranking. our method jointly optimizes mllms and diffusion models, utilizing mllms' reasoning capabilities to facilitate multi-condition text-to-image (t2i) tasks. the final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate controlnet, thereby enhancing control over generated images. through both quantitative and qualitative comparisons, dynamiccontrol demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls.",,2024-12-04,2025-05-06,"['qingdong he', 'jinlong peng', 'pengcheng xu', 'boyuan jiang', 'xiaobin hu', 'donghao luo', 'yong liu', 'yabiao wang', 'chengjie wang', 'xiangtai li', 'jiangning zhang']"
2412.03413,deep learning for sea surface temperature reconstruction under cloud   occlusion,cs.cv,"sea surface temperature (sst) reconstructions from satellite images affected by cloud gaps have been extensively documented in the past three decades. here we describe several machine learning models to fill the cloud-occluded areas starting from modis aqua nighttime l3 images. to tackle this challenge, we employed a type of convolutional neural network model (u-net) to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas. we demonstrate the outstanding precision of u-net with respect to available products done using oi interpolation algorithms. our best-performing architecture show 50% lower root mean square errors over established gap-filling methods.",,2024-12-04,2025-05-07,"['andrea asperti', 'ali aydogdu', 'angelo greco', 'fabio merizzi', 'pietro miraglio', 'beniamino tartufoli', 'alessandro testa', 'nadia pinardi', 'paolo oddo']"
2412.04204,pangaea: a global and inclusive benchmark for geospatial foundation   models,cs.cv,"geospatial foundation models (gfms) have emerged as powerful tools for extracting representations from earth observation data, but their evaluation remains inconsistent and narrow. existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of gfms. additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of gfm performance. in particular, most existing benchmarks are geographically biased towards north america and europe, questioning the global applicability of gfms. to overcome these challenges, we introduce pangaea, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. it establishes a robust and widely applicable benchmark for gfms. we evaluate the most popular gfms openly available on this benchmark and analyze their performance across several domains. in particular, we compare these models to supervised baselines (e.g. unet and vanilla vit), and assess their effectiveness when faced with limited labeled data. our findings highlight the limitations of gfms, under different scenarios, showing that they do not consistently outperform supervised models. pangaea is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. by releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. the code is available at https://github.com/vmarsocci/pangaea-bench.",,2024-12-05,2025-04-30,"['valerio marsocci', 'yuru jia', 'georges le bellier', 'david kerekes', 'liang zeng', 'sebastian hafner', 'sebastian gerard', 'eric brune', 'ritu yadav', 'ali shibli', 'heng fang', 'yifang ban', 'maarten vergauwen', 'nicolas audebert', 'andrea nascetti']"
2412.04280,humanedit: a high-quality human-rewarded dataset for instruction-based   image editing,cs.cv cs.gr,"we present humanedit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. humanedit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. with meticulously curation, humanedit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. the dataset includes six distinct types of editing instructions: action, add, counting, relation, remove, and replace, encompassing a broad spectrum of real-world scenarios. all images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. furthermore, humanedit offers comprehensive diversity and high-resolution $1024 \times 1024$ content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. with the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release humanedit at https://huggingface.co/datasets/bryanw/humanedit.",,2024-12-05,2025-05-06,"['jinbin bai', 'wei chow', 'ling yang', 'xiangtai li', 'juncheng li', 'hanwang zhang', 'shuicheng yan']"
2412.04472,stereo anywhere: robust zero-shot deep stereo matching even where either   stereo or mono fail,cs.cv,"we introduce stereo anywhere, a novel stereo-matching framework that combines geometric constraints with robust priors from monocular depth vision foundation models (vfms). by elegantly coupling these complementary worlds through a dual-branch architecture, we seamlessly integrate stereo matching with learned contextual cues. following this design, our framework introduces novel cost volume fusion mechanisms that effectively handle critical challenges such as textureless regions, occlusions, and non-lambertian surfaces. through our novel optical illusion dataset, monotrap, and extensive evaluation across multiple benchmarks, we demonstrate that our synthetic-only trained model achieves state-of-the-art results in zero-shot generalization, significantly outperforming existing solutions while showing remarkable robustness to challenging cases such as mirrors and transparencies.",,2024-12-05,2025-05-07,"['luca bartolomei', 'fabio tosi', 'matteo poggi', 'stefano mattoccia']"
2412.05538,not just text: uncovering vision modality typographic threats in image   generation models,cs.cv cs.pf,"current image generation models can effortlessly produce high-quality, highly realistic images, but this also increases the risk of misuse. in various text-to-image or image-to-image tasks, attackers can generate a series of images containing inappropriate content by simply editing the language modality input. to mitigate this security concern, numerous guarding or defensive strategies have been proposed, with a particular emphasis on safeguarding language modality. however, in practical applications, threats in the vision modality, particularly in tasks involving the editing of real-world images, present heightened security risks as they can easily infringe upon the rights of the image owner. therefore, this paper employs a method named typographic attack to reveal that various image generation models are also susceptible to threats within the vision modality. furthermore, we also evaluate the defense performance of various existing methods when facing threats in the vision modality and uncover their ineffectiveness. finally, we propose the vision modal threats in image generation models (vmt-igms) dataset, which would serve as a baseline for evaluating the vision modality vulnerability of various image generation models.",,2024-12-06,2025-04-29,"['hao cheng', 'erjia xiao', 'jiayan yang', 'jiahang cao', 'qiang zhang', 'jize zhang', 'kaidi xu', 'jindong gu', 'renjing xu']"
2412.06204,you kan do it in a single shot: plug-and-play methods with   single-instance priors,cs.cv,"the use of plug-and-play (pnp) methods has become a central approach for solving inverse problems, with denoisers serving as regularising priors that guide optimisation towards a clean solution. in this work, we introduce kan-pnp, an optimisation framework that incorporates kolmogorov-arnold networks (kans) as denoisers within the plug-and-play (pnp) paradigm. kan-pnp is specifically designed to solve inverse problems with single-instance priors, where only a single noisy observation is available, eliminating the need for large datasets typically required by traditional denoising methods. we show that kans, based on the kolmogorov-arnold representation theorem, serve effectively as priors in such settings, providing a robust approach to denoising. we prove that the kan denoiser is lipschitz continuous, ensuring stability and convergence in optimisation algorithms like pnp-admm, even in the context of single-shot learning. additionally, we provide theoretical guarantees for kan-pnp, demonstrating its convergence under key conditions: the convexity of the data fidelity term, lipschitz continuity of the denoiser, and boundedness of the regularisation functional. these conditions are crucial for stable and reliable optimisation. our experimental results show, on super-resolution and joint optimisation, that kan-pnp outperforms exiting methods, delivering superior performance in single-shot learning with minimal data. the method exhibits strong convergence properties, achieving high accuracy with fewer iterations.",,2024-12-08,2025-05-02,"['yanqi cheng', 'carola-bibiane schönlieb', 'angelica i aviles-rivero']"
2412.06314,cad-unet: a capsule network-enhanced unet architecture for accurate   segmentation of covid-19 lung infections from ct images,eess.iv cs.ai cs.cv,"since the outbreak of the covid-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing covid-19 pneumonia. in clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of covid-19. segmentation of covid-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. to address these challenges, this paper introduces a novel deep network architecture, called cad-unet, for segmenting covid-19 lung infections. in this architecture, capsule networks are incorporated into the existing unet framework. capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. they utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. this design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. the experimental results demonstrate the superior segmentation performance of the proposed model. the code has been released at: https://github.com/amanotooko-jie/cad-unet.",,2024-12-09,2025-04-29,"['yijie dang', 'weijun ma', 'xiaohu luo', 'huaizhu wang']"
2412.06491,ppt: pretraining with pseudo-labeled trajectories for motion forecasting,cs.cv cs.ro,"accurately predicting how agents move in dynamic scenes is essential for safe autonomous driving. state-of-the-art motion forecasting models rely on large curated datasets with manually annotated or heavily post-processed trajectories. however, building these datasets is costly, generally manual, hard to scale, and lacks reproducibility. they also introduce domain gaps that limit generalization across environments. we introduce ppt (pretraining with pseudo-labeled trajectories), a simple and scalable alternative that uses unprocessed and diverse trajectories automatically generated from off-the-shelf 3d detectors and tracking. unlike traditional pipelines aiming for clean, single-label annotations, ppt embraces noise and diversity as useful signals for learning robust representations. with optional finetuning on a small amount of labeled data, models pretrained with ppt achieve strong performance across standard benchmarks particularly in low-data regimes, and in cross-domain, end-to-end and multi-class settings. ppt is easy to implement and improves generalization in motion forecasting. code and data will be released upon acceptance.",,2024-12-09,2025-04-30,"['yihong xu', 'yuan yin', 'éloi zablocki', 'tuan-hung vu', 'alexandre boulch', 'matthieu cord']"
2412.06661,efficiency meets fidelity: a novel quantization framework for stable   diffusion,cs.cv,"text-to-image generation via stable diffusion models (sdm) have demonstrated remarkable capabilities. however, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. while recent studies have explored post-training quantization (ptq) and quantization-aware training (qat) methods to compress diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. this consistency is paramount for professional applications where both efficiency and output reliability are essential. to ensure that quantized sdm generates high-quality and consistent images, we propose an efficient quantization framework for sdm. our framework introduces a serial-to-parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.   through comprehensive evaluation across multiple stable diffusion variants (v1-4, v2-1, xl 1.0, and v3), our method demonstrates superior performance over state-of-the-art approaches with shorter training times. under w4a8 quantization settings, we achieve significant improvements in both distribution similarity and visual fidelity, while preserving a high image quality.",,2024-12-09,2025-05-07,"['shuaiting li', 'juncan deng', 'zeyu wang', 'kedong xu', 'rongtao deng', 'hong gu', 'haibin shen', 'kejie huang']"
2412.06690,fedsynthct-brain: a federated learning framework for multi-institutional   brain mri-to-ct synthesis,eess.iv cs.cv,"the generation of synthetic computed tomography (sct) images has become a pivotal methodology in modern clinical practice, particularly in the context of radiotherapy (rt) treatment planning. the use of sct enables the calculation of doses, pushing towards magnetic resonance imaging (mri) guided radiotherapy treatments. deep learning methods for mri-to-sct have shown promising results, but their reliance on single-centre training dataset limits generalisation capabilities to diverse clinical settings. moreover, creating centralised multi-centre datasets may pose privacy concerns. to address the aforementioned issues, we introduced fedsynthct-brain, an approach based on the federated learning (fl) paradigm for mri-to-sct in brain imaging. this is among the first applications of fl for mri-to-sct, employing a cross-silo horizontal fl approach that allows multiple centres to collaboratively train a u-net-based deep learning model. we validated our method using real multicentre data from four european and american centres, simulating heterogeneous scanner types and acquisition modalities, and tested its performance on an independent dataset from a centre outside the federation. in the case of the unseen centre, the federated model achieved a median mean absolute error (mae) of $102.0$ hu across 23 patients, with an interquartile range of $96.7-110.5$ hu. the median (interquartile range) for the structural similarity index (ssim) and the peak signal to noise ratio (pnsr) were $0.89 (0.86-0.89)$ and $26.58 (25.52-27.42)$, respectively. the analysis of the results showed acceptable performances of the federated approach, thus highlighting the potential of fl to enhance mri-to-sct to improve generalisability and advancing safe and equitable clinical applications while fostering collaboration and preserving data privacy.",10.1016/j.compbiomed.2025.110160,2024-12-09,2025-05-06,"['ciro benito raggio', 'mathias krohmer zabaleta', 'nils skupien', 'oliver blanck', 'francesco cicone', 'giuseppe lucio cascini', 'paolo zaffino', 'lucia migliorelli', 'maria francesca spadea']"
2412.06806,a physics-inspired deep learning framework with polar coordinate   attention for ptychographic imaging,physics.optics cs.cv,"ptychographic imaging confronts inherent challenges in applying deep learning for phase retrieval from diffraction patterns. conventional neural architectures, both convolutional neural networks and transformer-based methods, are optimized for natural images with euclidean spatial neighborhood-based inductive biases that exhibit geometric mismatch with the concentric coherent patterns characteristic of diffraction data in reciprocal space. in this paper, we present ppn, a physics-inspired deep learning network with polar coordinate attention (poca) for ptychographic imaging, that aligns neural inductive biases with diffraction physics through a dual-branch architecture separating local feature extraction from non-local coherence modeling. it consists of a poca mechanism that replaces euclidean spatial priors with physically consistent radial-angular correlations. ppn outperforms existing end-to-end models, with spectral and spatial analysis confirming its greater preservation of high-frequency details. notably, ppn maintains robust performance compared to iterative methods even at low overlap ratios, making it well suited for high-throughput imaging in real-world acquisition scenarios for samples with consistent structural characteristics.",,2024-11-25,2025-05-02,"['han yue', 'jun cheng', 'yu-xuan ren', 'chien-chun chen', 'grant a. van riessen', 'philip heng wai leong', 'steve feng shu']"
2412.07825,3dsrbench: a comprehensive 3d spatial reasoning benchmark,cs.cv,"3d spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3d space. this allows models to develop a comprehensive understanding of the 3d scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and ar/vr. while large multi-modal models (lmms) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3d spatial reasoning on diverse natural images are less studied. in this work we present the first comprehensive 3d spatial reasoning benchmark, 3dsrbench, with 2,772 manually annotated visual question-answer pairs across 12 question types. we conduct robust and thorough evaluation of 3d spatial reasoning capabilities by balancing the data distribution and adopting a novel flipeval strategy. to further study the robustness of 3d spatial reasoning w.r.t. camera 3d viewpoints, our 3dsrbench includes two subsets with 3d spatial reasoning questions on paired images with common and uncommon viewpoints. we benchmark a wide range of open-sourced and proprietary lmms, uncovering their limitations in various aspects of 3d awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. our 3dsrbench provide valuable findings and insights about the future development of lmms with strong 3d reasoning capabilities. our project page and dataset is available https://3dsrbench.github.io.",,2024-12-10,2025-05-08,"['wufei ma', 'haoyu chen', 'guofeng zhang', 'yu-cheng chou', 'celso m de melo', 'alan yuille']"
2412.08378,fila: fine-grained vision language models,cs.cv cs.ai,"recently, there has been growing interest in the capability of multimodal large language models (mllms) to process high-resolution images. a common approach currently involves dynamically cropping the original high-resolution image into smaller sub-images, which are then fed into a vision encoder that was pre-trained on lower-resolution images. however, this cropping approach often truncates objects and connected areas in the original image, causing semantic breaks. to address this limitation, we introduce hyvilm, designed to process images of any resolution while retaining the overall context during encoding. specifically, we: (i) design a new visual encoder called hybrid encoder that not only encodes individual sub-images but also interacts with detailed global visual features, significantly improving the model's ability to encode high-resolution images. (ii) propose an optimal feature fusion strategy for the dynamic cropping approach, effectively leveraging information from different layers of the vision encoder. compared with the state-of-the-art mllms under the same setting, our hyvilm outperforms existing mllms in nine out of ten tasks. specifically, hyvilm achieves a 9.6% improvement in performance on the textvqa task and a 6.9% enhancement on the docvqa task.",,2024-12-11,2025-04-30,"['shiding zhu', 'wenhui dong', 'jun song', 'yingbo wang', 'yanan guo', 'bo zheng']"
2412.09507,vision transformers for efficient indoor pathloss radio map prediction,cs.cv cs.ai cs.ni,"indoor pathloss prediction is a fundamental task in wireless network planning, yet it remains challenging due to environmental complexity and data scarcity. in this work, we propose a deep learning-based approach utilizing a vision transformer (vit) architecture with dino-v2 pretrained weights to model indoor radio propagation. our method processes a floor map with additional features of the walls to generate indoor pathloss maps. we systematically evaluate the effects of architectural choices, data augmentation strategies, and feature engineering techniques. our findings indicate that extensive augmentation significantly improves generalization, while feature engineering is crucial in low-data regimes. through comprehensive experiments, we demonstrate the robustness of our model across different generalization scenarios.",10.3390/electronics14101905,2024-12-12,2025-05-08,"['rafayel mkrtchyan', 'edvard ghukasyan', 'khoren petrosyan', 'hrant khachatrian', 'theofanis p. raptis']"
2412.09621,stereo4d: learning how things move in 3d from internet stereo videos,cs.cv,"learning to understand dynamic 3d scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3d motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. we present a system for mining high-quality 4d reconstructions from internet stereoscopic, wide-angle videos. our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3d reconstructions. we use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3d point clouds with long-term motion trajectories. we demonstrate the utility of this data by training a variant of dust3r to predict structure and 3d motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. project page and data at: https://stereo4d.github.io",,2024-12-12,2025-04-30,"['linyi jin', 'richard tucker', 'zhengqi li', 'david fouhey', 'noah snavely', 'aleksander holynski']"
2412.10316,brushedit: all-in-one image inpainting and editing,cs.cv cs.ai,"image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. however, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. to address these limitations, we propose brushedit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (mllms) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. specifically, we devise a system enabling free-form instruction editing by integrating mllms and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. extensive experiments show that our framework effectively combines mllms and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.",,2024-12-13,2025-05-05,"['yaowei li', 'yuxuan bian', 'xuan ju', 'zhaoyang zhang', 'junhao zhuang', 'ying shan', 'yuexian zou', 'qiang xu']"
2412.11026,scenellm: implicit language reasoning in llm for dynamic scene graph   generation,cs.cv cs.ai,"dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, uavs, and autonomous driving systems to make informed decisions. parsing these scenes into semantic triplets <subject-predicate-object> for accurate scene graph generation (sgg) is highly challenging due to the fluctuating spatio-temporal complexity. inspired by the reasoning capabilities of large language models (llms), we propose scenellm, a novel framework that leverages llms as powerful scene analyzers for dynamic sgg. our framework introduces a video-to-language (v2l) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for llms. to better encode spatial information, we devise a spatial information aggregation (sia) scheme, inspired by the structure of chinese characters, which encodes spatial data into tokens. using optimal transport (ot), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. to further improve the llm's ability to process this implicit linguistic input, we apply low-rank adaptation (lora) to fine-tune the model. finally, we use a transformer-based sgg predictor to decode the llm's reasoning and predict semantic triplets. our method achieves state-of-the-art results on the action genome (ag) benchmark, and extensive experiments show the effectiveness of scenellm in understanding and generating accurate dynamic scene graphs.",,2024-12-14,2025-05-06,"['hang zhang', 'zhuoling li', 'jun liu']"
2412.11039,airmorph: topology-preserving deep learning for pulmonary airway   analysis,eess.iv cs.cv,"accurate anatomical labeling and analysis of the pulmonary structure and its surrounding anatomy from thoracic ct is getting increasingly important for understanding the etilogy of abnormalities or supporting targetted therapy and early interventions. whilst lung and airway cell atlases have been attempted, there is a lack of fine-grained morphological atlases that are clinically deployable. in this work, we introduce airmorph, a robust, end-to-end deep learning pipeline enabling fully automatic and comprehensive airway anatomical labeling at lobar, segmental, and subsegmental resolutions that can be used to create digital atlases of the lung. evaluated across large-scale multi-center datasets comprising diverse pulmonary conditions, the airmorph consistently outperformed existing segmentation and labeling methods in terms of accuracy, topological consistency, and completeness. to simplify clinical interpretation, we further introduce a compact anatomical signature quantifying critical morphological airway features, including stenosis, ectasia, tortuosity, divergence, length, and complexity. when applied to various pulmonary diseases such as pulmonary fibrosis, emphysema, atelectasis, consolidation, and reticular opacities, it demonstrates strong discriminative power, revealing disease-specific morphological patterns with high interpretability and explainability. additionally, airmorph supports efficient automated branching pattern analysis, potentially enhancing bronchoscopic navigation planning and procedural safety, offering a valuable clinical tool for improved diagnosis, targeted treatment, and personalized patient care.",,2024-12-14,2025-05-08,"['minghui zhang', 'chenyu li', 'fangfang xie', 'yaoyu liu', 'hanxiao zhang', 'junyang wu', 'chunxi zhang', 'jie yang', 'jiayuan sun', 'guang-zhong yang', 'yun gu']"
2412.11815,colorflow: retrieval-augmented image sequence colorization,cs.cv,"automatic black-and-white image sequence colorization while preserving character and object identity (id) is a complex task with significant market demand, such as in cartoon or comic series colorization. despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.to address this, we propose colorflow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. unlike existing methods that require per-id finetuning or explicit id embedding extraction, we propose a novel robust and generalizable retrieval augmented colorization pipeline for colorizing images with relevant color references. our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. we utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. to evaluate our model, we introduce colorflow-bench, a comprehensive benchmark for reference-based colorization. results show that colorflow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. we release our codes and models on our project page: https://zhuang2002.github.io/colorflow/.",,2024-12-16,2025-05-04,"['junhao zhuang', 'xuan ju', 'zhaoyang zhang', 'yong liu', 'shiyi zhang', 'chun yuan', 'ying shan']"
2412.12126,seamless optical cloud computing across edge-metro network for   generative ai,cs.dc cs.cv cs.lg eess.iv eess.sp,"the rapid advancement of generative artificial intelligence (ai) in recent years has profoundly reshaped modern lifestyles, necessitating a revolutionary architecture to support the growing demands for computational power. cloud computing has become the driving force behind this transformation. however, it consumes significant power and faces computation security risks due to the reliance on extensive data centers and servers in the cloud. reducing power consumption while enhancing computational scale remains persistent challenges in cloud computing. here, we propose and experimentally demonstrate an optical cloud computing system that can be seamlessly deployed across edge-metro network. by modulating inputs and models into light, a wide range of edge nodes can directly access the optical computing center via the edge-metro network. the experimental validations show an energy efficiency of 118.6 mw/tops (tera operations per second), reducing energy consumption by two orders of magnitude compared to traditional electronic-based cloud computing solutions. furthermore, it is experimentally validated that this architecture can perform various complex generative ai models through parallel computing to achieve image generation tasks.",,2024-12-04,2025-05-01,"['sizhe xing', 'aolong sun', 'chengxi wang', 'yizhi wang', 'boyu dong', 'junhui hu', 'xuyu deng', 'an yan', 'yingjun liu', 'fangchen hu', 'zhongya li', 'ouhan huang', 'junhao zhao', 'yingjun zhou', 'ziwei li', 'jianyang shi', 'xi xiao', 'richard penty', 'qixiang cheng', 'nan chi', 'junwen zhang']"
2412.13695,optical aberrations in autonomous driving: physics-informed   parameterized temperature scaling for neural network uncertainty calibration,cs.cv,"'a trustworthy representation of uncertainty is desirable and should be considered as a key feature of any machine learning method' (huellermeier and waegeman, 2021). this conclusion of huellermeier et al. underpins the importance of calibrated uncertainties. since ai-based algorithms are heavily impacted by dataset shifts, the automotive industry needs to safeguard its system against all possible contingencies. one important but often neglected dataset shift is caused by optical aberrations induced by the windshield. for the verification of the perception system performance, requirements on the ai performance need to be translated into optical metrics by a bijective mapping. given this bijective mapping it is evident that the optical system characteristics add additional information about the magnitude of the dataset shift. as a consequence, we propose to incorporate a physical inductive bias into the neural network calibration architecture to enhance the robustness and the trustworthiness of the ai target application, which we demonstrate by using a semantic segmentation task as an example. by utilizing the zernike coefficient vector of the optical system as a physical prior we can significantly reduce the mean expected calibration error in case of optical aberrations. as a result, we pave the way for a trustworthy uncertainty representation and for a holistic verification strategy of the perception chain.",,2024-12-18,2025-04-30,"['dominik werner wolf', 'alexander braun', 'markus ulrich']"
2412.14415,drivegpt: scaling autoregressive behavior models for driving,cs.lg cs.ai cs.cv cs.ro,"we present drivegpt, a scalable behavior model for autonomous driving. we model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. we scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. we evaluate drivegpt across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. in a separate prediction task, drivegpt outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.",,2024-12-18,2025-05-01,"['xin huang', 'eric m. wolff', 'paul vernaza', 'tung phan-minh', 'hongge chen', 'david s. hayden', 'mark edmonds', 'brian pierce', 'xinxin chen', 'pratik elias jacob', 'xiaobai chen', 'chingiz tairbekov', 'pratik agarwal', 'tianshi gao', 'yuning chai', 'siddhartha srinivasa']"
2412.14803,video prediction policy: a generalist robot policy with predictive   visual representations,cs.cv cs.ro,"visual representations play a crucial role in developing generalist robotic policies. previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. recently, video diffusion models (vdms) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. we hypothesize that vdms inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. based on this hypothesis, we propose the video prediction policy (vpp), which learns implicit inverse dynamics model conditioned on predicted future representations inside vdms. to predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. in experiments, vpp achieves a 18.6\% relative improvement on the calvin abc-d generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\% increase in success rates for complex real-world dexterous manipulation tasks. project page at https://video-prediction-policy.github.io",,2024-12-19,2025-05-04,"['yucheng hu', 'yanjiang guo', 'pengchao wang', 'xiaoyu chen', 'yen-jen wang', 'jianke zhang', 'koushil sreenath', 'chaochao lu', 'jianyu chen']"
2412.15023,folai: synchronized foley sound generation with semantic and temporal   alignment,cs.sd cs.cv cs.lg cs.mm eess.as,"traditional sound design workflows rely on manual alignment of audio events to visual cues, as in foley sound design, where everyday actions like footsteps or object interactions are recreated to match the on-screen motion. this process is time-consuming, difficult to scale, and lacks automation tools that preserve creative intent. despite recent advances in vision-to-audio generation, producing temporally coherent and semantically controllable sound effects from video remains a major challenge. to address these limitations, we introduce folai, a two-stage generative framework that decouples the when and the what of sound synthesis, i.e., the temporal structure extraction and the semantically guided generation, respectively. in the first stage, we estimate a smooth control signal from the video that captures the motion intensity and rhythmic structure over time, serving as a temporal scaffold for the audio. in the second stage, a diffusion-based generative model produces sound effects conditioned both on this temporal envelope and on high-level semantic embeddings, provided by the user, that define the desired auditory content (e.g., material or action type). this modular design enables precise control over both timing and timbre, streamlining repetitive tasks while preserving creative flexibility in professional foley workflows. results on diverse visual contexts, such as footstep generation and action-specific sonorization, demonstrate that our model reliably produces audio that is temporally aligned with visual motion, semantically consistent with user intent, and perceptually realistic. these findings highlight the potential of folai as a controllable and modular solution for scalable, high-quality foley sound synthesis in professional and interactive settings. supplementary materials are accessible on our dedicated demo page at https://ispamm.github.io/folai.",,2024-12-19,2025-05-05,"['riccardo fosco gramaccioni', 'christian marinoni', 'emilian postolache', 'marco comunità', 'luca cosmo', 'joshua d. reiss', 'danilo comminiello']"
2412.16698,"interact with me: joint egocentric forecasting of intent to interact,   attitude and social actions",cs.cv cs.hc,"for efficient human-agent interaction, an agent should proactively recognize their target user and prepare for upcoming interactions. we formulate this challenging problem as the novel task of jointly forecasting a person's intent to interact with the agent, their attitude towards the agent and the action they will perform, from the agent's (egocentric) perspective. so we propose \emph{socialegonet} - a graph-based spatiotemporal framework that exploits task dependencies through a hierarchical multitask learning approach. socialegonet uses whole-body skeletons (keypoints from face, hands and body) extracted from only 1 second of video input for high inference speed. for evaluation, we augment an existing egocentric human-agent interaction dataset with new class labels and bounding box annotations. extensive experiments on this augmented dataset, named jpl-social, demonstrate \emph{real-time} inference and superior performance (average accuracy across all tasks: 83.15\%) of our model outperforming several competitive baselines. the additional annotations and code will be available upon acceptance.",,2024-12-21,2025-05-08,"['tongfei bian', 'yiming ma', 'mathieu chollet', 'victor sanchez', 'tanaya guha']"
2412.17378,balanced 3dgs: gaussian-wise parallelism rendering with fine-grained   tiling,cs.cv,"3d gaussian splatting (3dgs) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. however, training a 3dgs model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and gaussian spheres causes poor rendercuda kernel performance. we introduce balanced 3dgs, a gaussian-wise parallelism rendering with fine-grained tiling approach in 3dgs training process, perfectly solving load-imbalance issues. first, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to streaming multiprocessor(sm) resources within a single gpu dynamically, which constitutes the foundation of load balancing. second, we are the first to propose the gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all sms, which boosts the forward rendercuda kernel performance by up to 7.52x. besides, we present a self-adaptive render kernel selection strategy during the 3dgs training process based on different load-balance situations, which effectively improves training efficiency.",,2024-12-23,2025-05-08,"['hao gui', 'lin hu', 'rui chen', 'mingxiao huang', 'yuxin yin', 'jin yang', 'yong wu', 'chen liu', 'zhongxu sun', 'xueyang zhang', 'kun zhan']"
2412.18165,parallel neural computing for scene understanding from lidar perception   in autonomous racing,cs.cv,"autonomous driving in high-speed racing, as opposed to urban environments, presents significant challenges in scene understanding due to rapid changes in the track environment. traditional sequential network approaches may struggle to meet the real-time knowledge and decision-making demands of an autonomous agent covering large displacements in a short time. this paper proposes a novel baseline architecture for developing sophisticated models capable of true hardware-enabled parallelism, achieving neural processing speeds that mirror the agent's high velocity. the proposed model (parallel perception network (ppn)) consists of two independent neural networks, segmentation and reconstruction networks, running parallelly on separate accelerated hardware. the model takes raw 3d point cloud data from the lidar sensor as input and converts it into a 2d bird's eye view map on both devices. each network independently extracts its input features along space and time dimensions and produces outputs parallelly. the proposed method's model is trained on a system with two nvidia t4 gpus, using a combination of loss functions, including edge preservation, and demonstrates a 2x speedup in model inference time compared to a sequential configuration. implementation is available at: https://github.com/suwesh/parallel-perception-network. learned parameters of the trained networks are provided at: https://huggingface.co/suwesh/parallelperceptionnetwork.",10.1109/ised63599.2024.10956572,2024-12-23,,['suwesh prasad sah']
2412.18834,adaptive rate control for deep video compression with rate-distortion   prediction,cs.mm cs.cv cs.it math.it,"deep video compression has made significant progress in recent years, achieving rate-distortion performance that surpasses that of traditional video compression methods. however, rate control schemes tailored for deep video compression have not been well studied. in this paper, we propose a neural network-based $\lambda$-domain rate control scheme for deep video compression, which determines the coding parameter $\lambda$ for each to-be-coded frame based on the rate-distortion-$\lambda$ (r-d-$\lambda$) relationships directly learned from uncompressed frames, achieving high rate control accuracy efficiently without the need for pre-encoding. moreover, this content-aware scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt changes in video content. specifically, we introduce two neural network-based predictors to estimate the relationship between bitrate and $\lambda$, as well as the relationship between distortion and $\lambda$ for each frame. then we determine the coding parameter $\lambda$ for each frame to achieve the target bitrate. experimental results demonstrate that our approach achieves high rate control accuracy at the mini-gop level with low time overhead and mitigates inter-frame quality fluctuations across video content of varying resolutions.",,2024-12-25,2025-05-08,"['bowen gu', 'hao chen', 'ming lu', 'jie yao', 'zhan ma']"
2412.19404,spectral-temporal fusion representation for person-in-bed detection,eess.sp cs.cv cs.lg,"this study is based on the icassp 2025 signal processing grand challenge's accelerometer-based person-in-bed detection challenge, which aims to determine bed occupancy using accelerometer signals. the task is divided into two tracks: ""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing challenges such as individual differences, posture variations, and external disturbances. we propose a spectral-temporal fusion-based feature representation method with mixup data augmentation, and adopt intersection over union (iou) loss to optimize detection accuracy. in the two tracks, our method achieved outstanding results of 100.00% and 95.55% in detection scores, securing first place and third place, respectively.",,2024-12-26,,"['xuefeng yang', 'shiheng zhang', 'jian guan', 'feiyang xiao', 'wei lu', 'qiaoxi zhu']"
2412.19723,os-genesis: automating gui agent trajectory construction via reverse   task synthesis,cs.ai cs.cl cs.cv cs.hc,"graphical user interface (gui) agents powered by vision-language models (vlms) have demonstrated human-like computer control capability. despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. to address these challenges, we propose os-genesis, a novel gui data synthesis pipeline that reverses the conventional trajectory collection process. instead of relying on pre-defined tasks, os-genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. a trajectory reward model is then employed to ensure the quality of the generated trajectories. we demonstrate that training gui agents with os-genesis significantly improves their performance on highly challenging online benchmarks. in-depth analysis further validates os-genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. our codes, data, and checkpoints are available at https://qiushisun.github.io/os-genesis-home/.",,2024-12-27,2025-04-30,"['qiushi sun', 'kanzhi cheng', 'zichen ding', 'chuanyang jin', 'yian wang', 'fangzhi xu', 'zhenyu wu', 'chengyou jia', 'liheng chen', 'zhoumianze liu', 'ben kao', 'guohao li', 'junxian he', 'yu qiao', 'zhiyong wu']"
2501.01728,multimodal classification of forest biodiversity potential from 2d   orthophotos and 3d airborne laser scanning point clouds,cs.cv,"assessment of forest biodiversity is crucial for ecosystem management and conservation. while traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. this study investigates whether deep learning-based fusion of close-range sensing data from 2d orthophotos and 3d airborne laser scanning (als) point clouds can reliable assess the biodiversity potential of forests. we introduce the biovista dataset, comprising 44 378 paired samples of orthophotos and als point clouds from temperate forests in denmark, designed to explore multimodal fusion approaches. using deep neural networks (resnet for orthophotos and pointvector for als point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving overall accuracies of 76.7% and 75.8%, respectively. we explore various 2d and 3d fusion approaches: confidence-based ensembling, feature-level concatenation, and end-to-end training, achieving overall accuracies of 80.5%, 81.4% and 80.4% respectively. our results demonstrate that spectral information from orthophotos and structural information from als point clouds effectively complement each other in forest biodiversity assessment.",,2025-01-03,2025-05-01,"['simon b. jensen', 'stefan oehmcke', 'andreas møgelmose', 'meysam madadi', 'christian igel', 'sergio escalera', 'thomas b. moeslund']"
2501.01991,a hybrid deep learning and model-checking framework for accurate brain   tumor detection and validation,cs.cv cs.ai,"model checking, a formal verification technique, ensures systems meet predefined requirements, playing a crucial role in minimizing errors and enhancing quality during development. this paper introduces a novel hybrid framework integrating model checking with deep learning for brain tumor detection and validation in medical imaging. by combining model-checking principles with cnn-based feature extraction and k-fcm clustering for segmentation, the proposed approach enhances the reliability of tumor detection and segmentation. experimental results highlight the framework's effectiveness, achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its potential as a robust tool for advanced medical image analysis.",,2024-12-31,2025-04-29,"['elhoucine elfatimi', 'lahcen el fatimi', 'hanifa bouchaneb']"
2501.02180,quaternionic reweighted amplitude flow for phase retrieval in image   reconstruction,cs.cv math.cv,"quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. in this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. specifically, we propose the quaternionic reweighted amplitude flow (qraf) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted qraf algorithms. in addition, we introduce the quaternionic perturbed amplitude flow (qpaf) algorithm, which has linear convergence. extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.",,2025-01-03,2025-05-07,"['ren hu', 'pan lian']"
2501.04206,graphite: graph-based interpretable tissue examination for enhanced   explainability in breast cancer histopathology,eess.iv cs.cv,"explainable ai (xai) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. however, the black-box nature of these models often limits their clinical adoption. we introduce graphite (graph-based interpretable tissue examination), a post-hoc explainable framework designed for breast cancer tissue microarray (tma) analysis. graphite employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (gat) with scalewise attention (san) to capture scale-dependent features. we trained the model on 140 tumour tma cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated tma samples. graphite outperformed traditional xai methods, achieving a mean average precision (map) of 0.56, an area under the receiver operating characteristic curve (auroc) of 0.94, and a threshold robustness (thr) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. in clinical utility, graphite achieved the highest area under the decision curve (audc) of 4.17e+5, indicating reliable decision support across thresholds. these results highlight graphite's potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists' diagnostic reasoning and support precision medicine.",,2025-01-07,2025-05-05,"['raktim kumar mondol', 'ewan k. a. millar', 'peter h. graham', 'lois browne', 'arcot sowmya', 'erik meijering']"
2501.04597,frontiernet: learning visual cues to explore,cs.ro cs.cv,"exploration of unknown environments is crucial for autonomous robots; it allows them to actively reason and decide on what new data to acquire for different tasks, such as mapping, object discovery, and environmental assessment. existing solutions, such as frontier-based exploration approaches, rely heavily on 3d map operations, which are limited by map quality and, more critically, often overlook valuable context from visual cues. this work aims at leveraging 2d visual cues for efficient autonomous exploration, addressing the limitations of extracting goal poses from a 3d map. we propose a visual-only frontier-based exploration system, with frontiernet as its core component. frontiernet is a learning-based model that (i) proposes frontiers, and (ii) predicts their information gain, from posed rgb images enhanced by monocular depth priors. our approach provides an alternative to existing 3d-dependent goal-extraction approaches, achieving a 15\% improvement in early-stage exploration efficiency, as validated through extensive simulations and real-world experiments. the project is available at https://github.com/cvg/frontiernet.",,2025-01-08,2025-05-07,"['boyang sun', 'hanzhi chen', 'stefan leutenegger', 'cesar cadena', 'marc pollefeys', 'hermann blum']"
2501.04666,enhancing virtual try-on with synthetic pairs and error-aware noise   scheduling,cs.cv,"given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. our work explores ways to tackle these issues through both synthetic data as well as model refinement. we introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. the synthetic pairs can then be used to augment the training of virtual try-on. we also propose an error-aware refinement-based schr\""odinger bridge (earsb) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. to identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the schr\""odinger bridge's noise schedule with its confidence heatmap. experiments on viton-hd and dresscode-upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while earsb improves the overall image quality. in user studies, our model is preferred by the users in an average of 59% of cases.",,2025-01-08,2025-05-07,"['nannan li', 'kevin j. shih', 'bryan a. plummer']"
2501.08545,t2veval: benchmark dataset and objective evaluation method for   t2v-generated videos,cs.cv,"recent advances in text-to-video (t2v) technology, as demonstrated by models such as runway gen-3, pika, sora, and kling, have significantly broadened the applicability and popularity of the technology. this progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of t2v-generated videos and optimize video generation models. however, assessing the quality of text-to-video outputs remain challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. to address these challenges, we constructed t2veval-bench, a multi-dimensional benchmark dataset for text-to-video quality evaluation, which contains 148 textual prompts and 1,783 videos generated by 13 t2v models. to ensure a comprehensive evaluation, we scored each video on four dimensions in the subjective experiment, which are overall impression, text-video consistency, realness, and technical quality. based on t2veval-bench, we developed t2veval, a multi-branch fusion scheme for t2v quality evaluation. t2veval assesses videos across three branches: text-video consistency, realness, and technical quality. using an attention-based fusion module, t2veval effectively integrates features from each branch and predicts scores with the aid of a large language model. additionally, we implemented a divide-and-conquer training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. experimental results demonstrate that t2veval achieves state-of-the-art performance across multiple metrics.",,2025-01-14,2025-04-30,"['zelu qi', 'ping shi', 'shuqi wang', 'chaoyang zhang', 'fei zhao', 'zefeng ying', 'da pan', 'xi yang', 'zheqi he', 'teng dai']"
2501.08659,bright-vo: brightness-guided hybrid transformer for visual odometry with   multi-modality refinement module,cs.cv,"visual odometry (vo) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. significant progress has been made in data-driven vo methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. however, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. to address this limitation, we introduce brightvo, a novel vo model based on transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates inertial measurement unit (imu) data. using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. furthermore, we create a synthetic low-light dataset, kic4r, which includes a variety of lighting conditions to facilitate the training and evaluation of vo frameworks in challenging environments. experimental results demonstrate that brightvo achieves state-of-the-art performance on both the kic4r dataset and the kitti benchmarks. specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. for widespread use and further development, the research work is fully open-source at https://github.com/anastasiawd/brightvo.",,2025-01-15,2025-04-29,"['dongzhihan wang', 'yang yang', 'liang xu']"
2501.09503,anystory: towards unified single and multiple subject personalization in   text-to-image generation,cs.cv,"recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. however, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. in this paper, we propose anystory, a unified approach for personalized subject generation. anystory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. specifically, anystory models the subject personalization problem in an ""encode-then-route"" manner. in the encoding step, anystory utilizes a universal and powerful image encoder, i.e., referencenet, in conjunction with clip vision encoder to achieve high-fidelity encoding of subject features. in the routing step, anystory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. the project page is at https://aigcdesigngroup.github.io/anystory/ .",,2025-01-16,2025-05-01,"['junjie he', 'yuxiang tuo', 'binghui chen', 'chongyang zhong', 'yifeng geng', 'liefeng bo']"
2501.10098,landmarker: a toolkit for anatomical landmark localization in 2d/3d   images,cs.cv cs.ai cs.lg,"anatomical landmark localization in 2d/3d images is a critical task in medical imaging. although many general-purpose tools exist for landmark localization in classical computer vision tasks, such as pose estimation, they lack the specialized features and modularity necessary for anatomical landmark localization applications in the medical domain. therefore, we introduce landmarker, a python package built on pytorch. the package provides a comprehensive, flexible toolkit for developing and evaluating landmark localization algorithms, supporting a range of methodologies, including static and adaptive heatmap regression. landmarker enhances the accuracy of landmark identification, streamlines research and development processes, and supports various image formats and preprocessing pipelines. its modular design allows users to customize and extend the toolkit for specific datasets and applications, accelerating innovation in medical imaging. landmarker addresses a critical need for precision and customization in landmark localization tasks not adequately met by existing general-purpose pose estimation tools.",10.1016/j.softx.2025.102165,2025-01-17,2025-05-05,"['jef jonkers', 'luc duchateau', 'glenn van wallendael', 'sofie van hoecke']"
2501.10977,smarte-vr: student monitoring and adaptive response technology for   e-learning in virtual reality,cs.hc cs.cv,"this work introduces smarte-vr, a platform for student monitoring in an immersive virtual reality environment designed for online education. smarte-vr aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. the platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an autoqa system to evaluate understanding, interaction tools (e.g., textbook highlighting and lecture tagging), and real-time feedback. furthermore, we released a dataset that contains 5 research challenges with data from 10 users in vr-based toeic sessions. this data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. alongside the database, we present preliminary experiments using item response theory models, adapted for understanding detection using facial features. two architectures were explored: a temporal convolutional network for local features and a multilayer perceptron for global features.",,2025-01-19,2025-05-03,"['roberto daza', 'lin shengkai', 'aythami morales', 'julian fierrez', 'katashi nagao']"
2501.11124,rethinking pseudo-label guided learning for weakly supervised temporal   action localization from the perspective of noise correction,cs.cv,"pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. we argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. to target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. first, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the thumos14 and activitynet v1.2 benchmarks.",,2025-01-19,2025-04-29,"['quan zhang', 'yuxin qi', 'xi tang', 'rui yuan', 'xi lin', 'ke zhang', 'chun yuan']"
2501.11653,dynamic scene understanding from vision-language representations,cs.cv cs.lg,"images depicting complex, dynamic scenes are challenging to parse automatically, requiring both high-level comprehension of the overall situation and fine-grained identification of participating entities and their interactions. current approaches use distinct methods tailored to sub-tasks such as situation recognition and detection of human-human and human-object interactions. however, recent advances in image understanding have often leveraged web-scale vision-language (v&l) representations to obviate task-specific engineering. in this work, we propose a framework for dynamic scene understanding tasks by leveraging knowledge from modern, frozen v&l representations. by framing these tasks in a generic manner - as predicting and parsing structured text, or by directly concatenating representations to the input of existing models - we achieve state-of-the-art results while using a minimal number of trainable parameters relative to existing approaches. moreover, our analysis of dynamic knowledge of these representations shows that recent, more powerful representations effectively encode dynamic scene semantics, making this approach newly possible.",,2025-01-20,2025-05-03,"['shahaf pruss', 'morris alper', 'hadar averbuch-elor']"
2501.13620,a cognitive paradigm approach to probe the perception-reasoning   interface in vlms,cs.cv cs.ai,"a fundamental challenge in artificial intelligence involves understanding the cognitive mechanisms underlying visual reasoning in sophisticated models like vision-language models (vlms). how do these models integrate visual perception with abstract thought, especially when reasoning across multiple images or requiring fine-grained compositional understanding? drawing inspiration from cognitive science, this paper introduces a structured evaluation framework using diverse visual reasoning tasks-bongard problems (bps) and winoground-to dissect the perception-reasoning interface in vlms. we propose three distinct evaluation paradigms, mirroring human problem-solving strategies: direct visual rule learning (dvrl; holistic processing), deductive rule learning (drl; rule extraction and application), and componential analysis (ca; analytical decomposition via task-agnostic textual descriptions). these paradigms systematically vary cognitive load and probe processing stages. notably, ca enables multi-image reasoning evaluation even for single-image architectures and isolates reasoning from perception by operating on textual descriptions. applying this framework, we demonstrate that ca, leveraging powerful language models for reasoning over rich, independently generated descriptions, achieves new state-of-the-art (sota) performance on challenging benchmarks including bongard-openworld, bongard-hoi, and winoground. ablation studies confirm reasoning improves significantly when perceptual challenges are mitigated, revealing a critical perception bottleneck. our framework provides a valuable diagnostic tool and suggests that decoupling perception (via rich, task-agnostic description) from reasoning is a promising direction for robust and general visual intelligence.",,2025-01-23,2025-05-06,"['mohit vaishnav', 'tanel tammet']"
2501.14066,segment-and-classify: roi-guided generalizable contrast phase   classification in ct using xgboost,eess.iv cs.cv,"purpose: to automate contrast phase classification in ct using organ-specific features extracted from a widely used segmentation tool with a lightweight decision tree classifier.   materials and methods: this retrospective study utilized three public ct datasets from separate institutions. the phase prediction model was trained on the waw-tace (median age: 66 [60,73]; 185 males) dataset, and externally validated on the vindr-multiphase (146 males; 63 females; 56 unk) and c4kc-kits (median age: 61 [50.68; 123 males) datasets. contrast phase classification was performed using organ-specific features extracted by totalsegmentator, followed by prediction using a gradient-boosted decision tree classifier.   results: on the vindr-multiphase dataset, the phase prediction model achieved the highest or comparable aucs across all phases (>0.937), with superior f1-scores in the non-contrast (0.994), arterial (0.937), and delayed (0.718) phases. statistical testing indicated significant performance differences only in the arterial and delayed phases (p<0.05). on the c4kc-kits dataset, the phase prediction model achieved the highest aucs across all phases (>0.991), with superior f1-scores in arterial/venous (0.968) and delayed (0.935) phases. statistical testing confirmed significant improvements over all baseline models in these two phases (p<0.05). performance in the non-contrast class, however, was comparable across all models, with no statistically significant differences observed (p>0.05).   conclusion: the lightweight model demonstrated strong performance relative to all baseline models, and exhibited robust generalizability across datasets from different institutions.",,2025-01-23,2025-04-30,"['benjamin hou', 'tejas sudharshan mathai', 'pritam mukherjee', 'xinya wang', 'ronald m. summers', 'zhiyong lu']"
2501.15326,recognize any surgical object: unleashing the power of weakly-supervised   data,cs.cv,"we present raso, a foundation model designed to recognize any surgical object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. raso leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. our experiments show that raso achieves improvements of 2.9 map, 4.5 map, 10.6 map, and 7.2 map on four standard surgical benchmarks, respectively, in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. code, model, and demo are available at https://ntlm1686.github.io/raso.",,2025-01-25,2025-05-05,"['jiajie li', 'brian r quaranto', 'chenhui xu', 'ishan mishra', 'ruiyang qin', 'dancheng liu', 'peter c w kim', 'jinjun xiong']"
2501.15955,rethinking the bias of foundation model under long-tailed distribution,cs.lg cs.cv stat.ml,"long-tailed learning has garnered increasing attention due to its practical significance. among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. however, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. in this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. during fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. to tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. to resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. notably, we achieve an average performance increase of about $1.67\%$ on each dataset.",,2025-01-27,2025-05-04,"['jiahao chen', 'bin qin', 'jiangmeng li', 'hao chen', 'bing su']"
2501.16289,multi-view structural convolution network for domain-invariant point   cloud recognition of autonomous vehicles,cs.cv,"point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. however, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. this variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. in this paper, we present the multi-view structural convolution network (mscn) designed for domain-invariant point cloud recognition. mscn comprises structural convolution layers (scl) that extract local context geometric features from point clouds and structural aggregation layers (sal) that extract and aggregate both local and overall context features from point clouds. additionally, our mscn enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. this method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. this highlights mscn's potential to significantly improve the reliability and domain invariant features in different environments. our code is available at https://github.com/mlmlab/mscn.",,2025-01-27,2025-04-30,"['younggun kim', 'beomsik cho', 'seonghoon ryoo', 'soomok lee']"
2501.17690,segmentation-aware generative reinforcement network (grn) for tissue   layer segmentation in 3-d ultrasound images for chronic low-back pain (clbp)   assessment,cs.cv cs.ai cs.lg,"we introduce a novel segmentation-aware joint training framework called generative reinforcement network (grn) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. an image enhancement technique called segmentation-guided enhancement (sge) is also developed, where the generator produces images tailored specifically for the segmentation model. two variants of grn were also developed, including grn for sample-efficient learning (grn-sel) and grn for semi-supervised learning (grn-ssl). grn's performance was evaluated using a dataset of 69 fully annotated 3d ultrasound scans from 29 subjects. the annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (sfm), deep fat, deep fascial membrane (dfm), and muscle. our results show that grn-sel with sge reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the dice similarity coefficient (dsc) compared to models trained on fully labeled datasets. grn-sel alone reduces labeling efforts by 60%, grn-ssl with sge decreases labeling requirements by 70%, and grn-ssl alone by 60%, all while maintaining performance comparable to fully supervised models. these findings suggest the effectiveness of the grn framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.",,2025-01-29,2025-04-30,"['zixue zeng', 'xiaoyan zhao', 'matthew cartier', 'tong yu', 'jing wang', 'xin meng', 'zhiyu sheng', 'maryam satarpour', 'john m cormack', 'allison bean', 'ryan nussbaum', 'maya maurer', 'emily landis-walkenhorst', 'dinesh kumbhare', 'kang kim', 'ajay wasan', 'jiantao pu']"
2501.18504,clear: cue learning using evolution for accurate recognition applied to   sustainability data extraction,cs.cv cs.ai cs.ne,"large language model (llm) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. we introduce cue learning using evolution for accurate recognition (clear), which uses a combination of llms and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. it achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. we apply clear to the real-world task of identifying sustainability data from interior and exterior images of buildings. we investigate the effects of using a variable-length representation compared to fixed-length and show how llm consistency can be improved by refactoring from categorical to real-valued estimates. we show that clear enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.",10.1145/3712256.3726317,2025-01-30,2025-05-07,"['peter j. bentley', 'soo ling lim', 'fuyuki ishikawa']"
2501.18630,deformable beta splatting,cs.cv cs.gr,"3d gaussian splatting (3dgs) has advanced radiance field reconstruction by enabling real-time rendering. however, its reliance on gaussian kernels for geometry and low-order spherical harmonics (sh) for color encoding limits its ability to capture complex geometries and diverse colors. we introduce deformable beta splatting (dbs), a deformable and compact approach that enhances both geometry and color representation. dbs replaces gaussian kernels with deformable beta kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. in addition, we extended the beta kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to sh-based methods. furthermore, unlike prior densification techniques that depend on gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved markov chain monte carlo (mcmc), independent of the splatting kernel type. experimental results demonstrate that dbs achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3dgs-mcmc, highlighting the superior performance of dbs for real-time radiance field rendering. interactive demonstrations and source code are available on our project website: https://rongliu-leo.github.io/beta-splatting/.",,2025-01-27,2025-05-06,"['rong liu', 'dylan sun', 'meida chen', 'yue wang', 'andrew feng']"
2501.18635,towards understanding depth perception in foveated rendering,cs.cv cs.gr,"the true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. to this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. however, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. it is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. in this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. we design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. furthermore, we analyze the model in the context of common foveation practices reported in literature. the findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected with up to 2x stronger foveation than commonly used. finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.",10.1145/3721238.3730609,2025-01-28,2025-04-30,"['sophie kergaßner', 'taimoor tariq', 'piotr didyk']"
2501.18851,project-and-fuse: improving rgb-d semantic segmentation via graph   convolution networks,cs.cv,"most existing rgb-d semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. however, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ graph neural networks (gnns) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. at the 3d feature extraction stage, we argue that traditional cnns are not efficient enough for depth maps. so, we encode depth map into normal map, after which cnns can easily extract object surface tendencies.at projection matrix generation stage, we find the existence of biased-assignment and ambiguous-locality issues in the original pipeline. therefore, we propose to 1) adopt the kullback-leibler loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. extensive experiments on two public datasets, nyu-depthv2 and sun rgb-d, have shown that our approach can consistently boost the performance of rgb-d semantic segmentation task.",,2025-01-30,2025-05-02,"['xiaoyan jiang', 'bohan wang', 'xinlong wan', 'shanshan chen', 'hamido fujita', 'hanan abd. al juaid']"
2501.19243,accelerating diffusion transformer via error-optimized cache,cs.cv,"diffusion transformer (dit) is a crucial method for content generation. however, it needs a lot of time to sample. many studies have attempted to use caching to reduce the time consumption of sampling. existing caching methods accelerate generation by reusing dit features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. to solve this problem, we propose the error-optimized cache (eoc). this method introduces three key improvements: (1) prior knowledge extraction: extract and process the caching differences; (2) a judgment method for cache optimization: determine whether certain caching steps need to be optimized; (3) cache optimization: reduce caching errors. experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. on the imagenet dataset, without substantially increasing the computational load, this method improves the fid of the generated images when the rule-based model fora has a caching level of 75%, 50%, and 25%, and the training-based model learning-to-cache has a caching level of 22%. specifically, the fid values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.",,2025-01-31,2025-04-30,"['junxiang qiu', 'shuo wang', 'jinda lu', 'lin liu', 'houcheng jiang', 'xingyu zhu', 'yanbin hao']"
2502.00205,ecoweednet: a lightweight and automated weed detection method for   sustainable next-generation agricultural consumer electronics,cs.cv cs.ai,"sustainable agriculture plays a crucial role in ensuring world food security for consumers. a critical challenge faced by sustainable precision agriculture is weed growth, as weeds compete for essential resources with crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. the adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. however, prior works suffer from issues such as low accuracy and precision, as well as high computational expense. this work proposes ecoweednet, a novel model that enhances weed detection performance without introducing significant computational complexity, aligning with the goals of low-carbon agricultural practices. the effectiveness of the proposed model is demonstrated through comprehensive experiments on the cottonweeddet12 benchmark dataset, which reflects real-world scenarios. ecoweednet achieves performance comparable to that of large models (map@0.5 = 95.2%), yet with significantly fewer parameters (approximately 4.21% of the parameters of yolov4), lower computational complexity and better computational efficiency 6.59% of the gflops of yolov4). these key findings indicate ecoweednet's deployability on low-power consumer hardware, lower energy consumption, and hence reduced carbon footprint, thereby emphasizing the application prospects of ecoweednet in next-generation sustainable agriculture. these findings provide the way forward for increased application of environmentally-friendly agricultural consumer technologies.",,2025-01-31,2025-05-07,"['omar h. khater', 'abdul jabbar siddiqui', 'm. shamim hossain', 'aiman el-maleh']"
2502.00456,explorations of the softmax space: knowing when the neural network   doesn't know,cs.lg cs.cv,"ensuring the reliability of automated decision-making based on neural networks will be crucial as artificial intelligence systems are deployed more widely in critical situations. this paper proposes a new approach for measuring confidence in the predictions of any neural network that relies on the predictions of a softmax layer. we identify that a high-accuracy trained network may have certain outputs for which there should be low confidence. in such cases, decisions should be deferred and it is more appropriate for the network to provide a \textit{not known} answer to a corresponding classification task. our approach clusters the vectors in the softmax layer to measure distances between cluster centroids and network outputs. we show that a cluster with centroid calculated simply as the mean softmax output for all correct predictions can serve as a suitable proxy in the evaluation of confidence. defining a distance threshold for a class as the smallest distance from an incorrect prediction to the given class centroid offers a simple approach to adding \textit{not known} answers to any network classification falling outside of the threshold. we evaluate the approach on the mnist and cifar-10 datasets using a convolutional neural network and a vision transformer, respectively. the results show that our approach is consistent across datasets and network models, and indicate that the proposed distance metric can offer an efficient way of determining when automated predictions are acceptable and when they should be deferred to human operators.",,2025-02-01,2025-04-30,"['daniel sikar', ""artur d'avila garcez"", 'tillman weyde']"
2502.00968,code: blockwise control for denoising diffusion models,cs.cv cs.lg,"aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. in this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (code), that circumvents the need for differentiable guidance functions and model finetuning. code is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. our experiments demonstrate that, despite its simplicity, code offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. our code is available at: https://github.com/anujinho/code.",,2025-02-02,2025-05-03,"['anuj singh', 'sayak mukherjee', 'ahmad beirami', 'hadi jamali-rad']"
2502.01547,mwhisper-flamingo for multilingual audio-visual noise-robust speech   recognition,eess.as cs.cv cs.sd,"audio-visual speech recognition (avsr) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on english data. one limitation is the lack of large-scale multilingual video data, which makes it hard to train models from scratch. in this work, we propose mwhisper-flamingo for multilingual avsr which combines the strengths of a pre-trained audio model (whisper) and video model (av-hubert). to enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs. mwhisper-flamingo achieves state-of-the-art wer on muavic, an avsr dataset of 9 languages. audio-visual mwhisper-flamingo consistently outperforms audio-only whisper on all languages in noisy conditions.",,2025-02-03,2025-05-07,"['andrew rouditchenko', 'samuel thomas', 'hilde kuehne', 'rogerio feris', 'james glass']"
2502.01710,dagnet: a dual-view attention-guided network for efficient x-ray   security inspection,cs.cv,"with the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent x-ray-based security inspection systems play a crucial role in public safety. although single-view x-ray baggage scanner is widely deployed, they struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. to address this, we propose a dual-view attention-guided network for efficient x-ray security inspection (dagnet). this study builds on a shared-weight backbone network as the foundation and constructs three key modules that work together: (1) frequency domain interaction module (fdim) dynamically enhances features by adjusting frequency components based on inter-view relationships; (2) dual-view hierarchical enhancement module (dvhem) employs cross-attention to align features between views and capture hierarchical associations; (3) convolutional guided fusion module (cgfm) fuses features to suppress redundancy while retaining critical discriminative information. collectively, these modules substantially improve the performance of dual-view x-ray security inspection. experimental results demonstrate that dagnet outperforms existing state-of-the-art approaches across multiple backbone architectures. the code is available at:https://github.com/shilonghong/dagnet.",,2025-02-03,2025-05-05,"['shilong hong', 'yanzhou zhou', 'weichao xu']"
2502.01842,texture image synthesis using spatial gan based on vision transformers,cs.cv cs.ai,"texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. while traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. in this paper, we propose vit-sgan, a new hybrid model that fuses vision transformers (vits) with a spatial generative adversarial network (sgan) to address the limitations of previous methods. by incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of vits, our model achieves superior texture synthesis. this approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. comparison experiments with metrics such as fid, is, ssim, and lpips demonstrate the substantial improvement of vit-sgan, which underlines its efficiency in generating diverse realistic textures.",,2025-02-03,2025-05-07,"['elahe salari', 'zohreh azimifar']"
2502.02454,imdprompter: adapting sam to image manipulation detection by cross-view   automated prompt learning,cs.cv,"using extensive training data from sa-1b, the segment anything model (sam) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. however, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. there are two main challenges in applying sam to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. to address these challenges, we develops a cross-view prompt learning paradigm called imdprompter based on sam. benefiting from the design of automated prompts, imdprompter no longer relies on manual guidance, enabling automated detection and localization. additionally, we propose components such as cross-view feature perception, optimal prompt selection, and cross-view prompt consistency, which facilitate cross-view perceptual learning and guide sam to generate accurate masks. extensive experimental results from five datasets (casia, columbia, coverage, imd2020, and nist16) validate the effectiveness of our proposed method.",,2025-02-04,2025-04-29,"['quan zhang', 'yuxin qi', 'xi tang', 'jinwei fang', 'xi lin', 'ke zhang', 'chun yuan']"
2502.02922,elucidating the preconditioning in consistency distillation,cs.lg cs.cv,"consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (pf) ordinary differential equation (ode) trajectory determined by the teacher model. preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. it imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. however, previous preconditionings are hand-crafted and may be suboptimal choices. in this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ode trajectory. based on these analyses, we further propose a principled way dubbed \textit{analytic-precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ode. we demonstrate that analytic-precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.",,2025-02-05,2025-04-30,"['kaiwen zheng', 'guande he', 'jianfei chen', 'fan bao', 'jun zhu']"
2502.03270,when pre-trained visual representations fall short: limitations in   visuo-motor robot learning,cs.ro cs.ai cs.cv cs.lg,"the integration of pre-trained visual representations (pvrs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. however, pvrs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. these limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. this work identifies these shortcomings and proposes solutions to address them. first, we augment pvr features with temporal perception and a sense of task completion, effectively disentangling them in time. second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. our experiments demonstrate significant performance improvements, particularly in pvrs trained with masking objectives, and validate the effectiveness of our enhancements in addressing pvr-specific limitations.",,2025-02-05,2025-05-05,"['nikolaos tsagkas', 'andreas sochopoulos', 'duolikun danier', 'sethu vijayakumar', 'chris xiaoxuan lu', 'oisin mac aodha']"
2502.03649,all-in-one image compression and restoration,cs.cv,"visual images corrupted by various types and levels of degradations are commonly encountered in practical image compression. however, most existing image compression methods are tailored for clean images, therefore struggling to achieve satisfying results on these images. joint compression and restoration methods typically focus on a single type of degradation and fail to address a variety of degradations in practice. to this end, we propose a unified framework for all-in-one image compression and restoration, which incorporates the image restoration capability against various degradations into the process of image compression. the key challenges involve distinguishing authentic image content from degradations, and flexibly eliminating various degradations without prior knowledge. specifically, the proposed framework approaches these challenges from two perspectives: i.e., content information aggregation, and degradation representation aggregation. extensive experiments demonstrate the following merits of our model: 1) superior rate-distortion (rd) performance on various degraded inputs while preserving the performance on clean data; 2) strong generalization ability to real-world and unseen scenarios; 3) higher computing efficiency over compared methods. our code is available at https://github.com/zeldam1/all-in-one.",10.1109/wacv61041.2025.00069,2025-02-05,,"['huimin zeng', 'jiacheng li', 'ziqiang zheng', 'zhiwei xiong']"
2502.08821,dejaivu: identifying and explaining ai art on the web in real-time with   saliency maps,cs.cv cs.ai cs.lg,"the recent surge in advanced generative models, such as diffusion models and generative adversarial networks (gans), has led to an alarming rise in ai-generated images across various domains on the web. while such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. additionally, the uncredited use of ai-generated images in media and marketing has sparked significant backlash from online communities. in response to this, we introduce dejaivu, a chrome web extension that combines real-time ai-generated image detection with saliency-based explainability while users browse the web. using an onnx-optimized deep learning model, dejaivu automatically analyzes images on websites such as google images, identifies ai-generated content using model inference, and overlays a saliency heatmap to highlight ai-related artifacts. our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that ai detection is both transparent and interpretable. we also evaluate dejaivu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing ai image accountability. the code for this system can be found at https://github.com/noodulz/dejaivu.",,2025-02-12,2025-05-08,['jocelyn dzuong']
2502.09608,instance segmentation of scene sketches using natural image priors,cs.cv cs.gr,"sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. it serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. while image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. we introduce inklayer, a method for instance segmentation of raster scene sketches. our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. as existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, inkscenes, featuring sketches with diverse brush strokes and varying levels of detail. we use this dataset to demonstrate the robustness of our approach.",10.1145/3721238.3730606,2025-02-13,2025-05-06,"['mia tang', 'yael vinker', 'chuan yan', 'lvmin zhang', 'maneesh agrawala']"
2502.10156,monoforce: learnable image-conditioned physics engine,cs.ro cs.cv,"we propose a novel model for the prediction of robot trajectories on rough offroad terrain from the onboard camera images. this model enforces the laws of classical mechanics through a physics-aware neural symbolic layer while preserving the ability to learn from large-scale data as it is end-to-end differentiable. the proposed hybrid model integrates a black-box component that predicts robot-terrain interaction forces with a neural-symbolic layer. this layer includes a differentiable physics engine that computes the robot's trajectory by querying these forces at the points of contact with the terrain. as the proposed architecture comprises substantial geometrical and physics priors, the resulting model can also be seen as a learnable physics engine conditioned on real images that delivers $10^4$ trajectories per second. we argue and empirically demonstrate that this architecture reduces the sim-to-real gap and mitigates out-of-distribution sensitivity. the differentiability, in conjunction with the rapid simulation speed, makes the model well-suited for various applications including model predictive control, trajectory shooting, supervised and reinforcement learning or slam. the codes and data are publicly available.",,2025-02-14,2025-05-07,"['ruslan agishev', 'karel zimmermann']"
2502.11178,da-mamba: domain adaptive hybrid mamba-transformer based one-stage   object detection,cs.cv,"recent 2d cnn-based domain adaptation approaches struggle with long-range dependencies due to limited receptive fields, making it difficult to adapt to target domains with significant spatial distribution changes. while transformer-based domain adaptation methods better capture distant relationships through self-attention mechanisms that facilitate more effective cross-domain feature alignment, their quadratic computational complexity makes practical deployment challenging for object detection tasks across diverse domains. inspired by the global modeling and linear computation complexity of the mamba architecture, we present the first domain-adaptive mamba-based one-stage object detection model, termed da-mamba. specifically, we combine mamba's efficient state-space modeling with attention mechanisms to address domain-specific spatial and channel-wise variations. our design leverages domain-adaptive spatial and channel-wise scanning within the mamba block to extract highly transferable representations for efficient sequential processing, while cross-attention modules generate long-range, mixed-domain spatial features to enable robust soft alignment across domains. besides, motivated by the observation that hybrid architectures introduce feature noise in domain adaptation tasks, we propose an entropy-based knowledge distillation framework with margin relu, which adaptively refines multi-level representations by suppressing irrelevant activations and aligning uncertainty across source and target domains. finally, to prevent overfitting caused by the mixed-up features generated through cross-attention mechanisms, we propose entropy-driven gating attention with random perturbations that simultaneously refine target features and enhance model generalization.",,2025-02-16,2025-05-07,"['a. enes doruk', 'hasan f. ates']"
2502.15251,simhand: mining similar hands for large-scale 3d hand pose pre-training,cs.cv,"we present a framework for pre-training of 3d hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed simhand. pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3d hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. to facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. specifically, we collect over 2.0m hand images from recent human-centric videos, such as 100doh and ego4d. to extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. we then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. we achieve significant improvements over the state-of-the-art method (peclr) in various datasets, with gains of 15% on freihand, 10% on dexycb, and 4% on assemblyhands.   our code is available at https://github.com/ut-vision/simhand.",,2025-02-21,2025-05-05,"['nie lin', 'takehiko ohkawa', 'yifei huang', 'mingfang zhang', 'minjie cai', 'ming li', 'ryosuke furuta', 'yoichi sato']"
2502.17289,a novel approach to navigate the taxonomic hierarchy to address the   open-world scenarios in medicinal plant classification,cs.ai cs.cv,"in this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. it is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. thus we address the problem of unknown species classification by assigning it best hierarchical labels. we propose a novel method, which integrates densenet121, multi-scale self-attention (mssa) and cascaded classifiers for hierarchical classification. the approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. it uses attention scores to focus on important features across multiple scales. the proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. the model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. we used unknown species for testing our model. for unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application.",,2025-02-24,2025-05-04,"['soumen sinha', 'tanisha rana', 'rahul roy']"
2502.17648,calibrefine: deep learning-based online automatic targetless   lidar-camera calibration with iterative and attention-driven post-refinement,cs.cv cs.sy eess.sy,"accurate multi-sensor calibration is essential for deploying robust perception systems in applications such as autonomous driving and intelligent transportation. existing lidar-camera calibration methods often rely on manually placed targets, preliminary parameter estimates, or intensive data preprocessing, limiting their scalability and adaptability in real-world settings. in this work, we propose a fully automatic, targetless, and online calibration framework, calibrefine, which directly processes raw lidar point clouds and camera images. our approach is divided into four stages: (1) a common feature discriminator that leverages relative spatial positions, visual appearance embeddings, and semantic class cues to identify and generate reliable lidar-camera correspondences, (2) a coarse homography-based calibration that uses the matched feature correspondences to estimate an initial transformation between the lidar and camera frames, serving as the foundation for further refinement, (3) an iterative refinement to incrementally improve alignment as additional data frames become available, and (4) an attention-based refinement that addresses non-planar distortions by leveraging a vision transformer and cross-attention mechanisms. extensive experiments on two urban traffic datasets demonstrate that calibrefine achieves high-precision calibration with minimal human input, outperforming state-of-the-art targetless methods and matching or surpassing manually tuned baselines. our results show that robust object-level feature matching, combined with iterative refinement and self-supervised attention-based refinement, enables reliable sensor alignment in complex real-world conditions without ground-truth matrices or elaborate preprocessing. code is available at https://github.com/radar-lab/lidar_camera_automatic_calibration",,2025-02-24,2025-05-05,"['lei cheng', 'lihao guo', 'tianya zhang', 'tam bang', 'austin harris', 'mustafa hajij', 'mina sartipi', 'siyang cao']"
2502.18137,spargeattn: accurate sparse attention accelerating any model inference,cs.lg cs.ai cs.cv cs.pf,"an efficient attention implementation is essential for large models due to its quadratic time complexity. fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. many studies have utilized the sparse pattern to accelerate attention. however, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. a universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. in this paper, we propose spargeattn, a universal sparse and quantized attention for any model. our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. in the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. the codes are available at https://github.com/thu-ml/spargeattn.",,2025-02-25,2025-05-01,"['jintao zhang', 'chendong xiang', 'haofeng huang', 'jia wei', 'haocheng xi', 'jun zhu', 'jianfei chen']"
2502.20292,visual adaptive prompting for compositional zero-shot learning,cs.cv cs.lg,"vision-language models (vlms) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as compositional zero-shot learning (czsl). czsl requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. recent works in prompting for czsl have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. however, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. to address this, we propose visual adaptive prompting system (vaps) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of vlms to bridge the gap between semantic and visual features. our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. experiments on three czsl benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.",,2025-02-27,2025-05-02,"['kyle stein', 'arash mahyari', 'guillermo francia', 'eman el-sheikh']"
2502.20490,egonormia: benchmarking physical social norm understanding,cs.cv cs.ai cs.cl,"human activity is moderated by norms. however, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. to improve and evaluate the normative reasoning capability of vision-language models (vlms), we present \dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage mcq questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. the normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. to compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human bench of 92\%). our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. we additionally show that through a retrieval-based generation (rag) method, it is possible to use \dataset{} to enhance normative reasoning in vlms.",,2025-02-27,2025-05-04,"['mohammadhossein rezaei', 'yicheng fu', 'phil cuvin', 'caleb ziems', 'yanzhe zhang', 'hao zhu', 'diyi yang']"
2502.20824,mfsr-gan: multi-frame super-resolution with handheld motion modeling,cs.cv eess.iv,"smartphone cameras have become ubiquitous imaging tools, yet their small sensors and compact optics often limit spatial resolution and introduce distortions. combining information from multiple low-resolution (lr) frames to produce a high-resolution (hr) image has been explored to overcome the inherent limitations of smartphone cameras. despite the promise of multi-frame super-resolution (mfsr), current approaches are hindered by datasets that fail to capture the characteristic noise and motion patterns found in real-world handheld burst images. in this work, we address this gap by introducing a novel synthetic data engine that uses multi-exposure static images to synthesize lr-hr training pairs while preserving sensor-specific noise characteristics and image motion found during handheld burst photography. we also propose mfsr-gan: a multi-scale raw-to-rgb network for mfsr. compared to prior approaches, mfsr-gan emphasizes a ""base frame"" throughout its architecture to mitigate artifacts. experimental results on both synthetic and real data demonstrates that mfsr-gan trained with our synthetic engine yields sharper, more realistic reconstructions than existing methods for real-world mfsr.",,2025-02-28,2025-05-01,"['fadeel sher khan', 'joshua ebenezer', 'hamid sheikh', 'seok-jun lee']"
2503.01284,soybean disease detection via interpretable hybrid cnn-gnn: integrating   mobilenetv2 and graphsage with cross-modal attention,cs.cv cs.lg,"soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. while convolutional neural networks (cnns) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. this paper proposes an interpretable hybrid sequential cnn-graph neural network (gnn) framework that synergizes mobilenetv2 for localized feature extraction and graphsage for relational modeling. the framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. this design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. cross-modal interpretability is achieved via grad-cam and eigen-cam visualizations, generating heatmaps to highlight disease-influential regions. evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\%$ accuracy, surpassing standalone cnns ($\le95.04\%$) and traditional machine learning models ($\le77.05\%$). ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. with only 2.3 million parameters, the lightweight mobilenetv2-graphsage combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. the proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing cnn-gnn integration in plant pathology research.",,2025-03-03,2025-05-02,"['md abrar jahin', 'soudeep shahriar', 'm. f. mridha', 'md. jakir hossen', 'nilanjan dey']"
2503.01453,ac-lite : a lightweight image captioning model for low-resource assamese   language,cs.cv cs.ai cs.cl,"most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in english language. this often restricts this important assistive tool for widespread use across language and accessibility barriers. this work presents ac-lite, a computationally efficient model for image captioning in low-resource assamese language. ac-lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. the ac-lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. a combination of shufflenetv2x1.5 with gru based language decoder along with bilinear attention is found to provide the best performance with minimum compute. ac-lite was observed to achieve an 82.3 cider score on the coco-ac dataset with 2.45 gflops and 22.87m parameters.",,2025-03-03,2025-04-30,"['pankaj choudhury', 'yogesh aggarwal', 'prabhanjan jadhav', 'prithwijit guha', 'sukumar nandi']"
2503.01894,livs: a pluralistic alignment dataset for inclusive public spaces,cs.cv cs.ai cs.hc,"we introduce the local intersectional visual spaces (livs) dataset, a benchmark for multi-criteria alignment, developed through a two-year participatory process with 30 community organizations to support the pluralistic alignment of text-to-image (t2i) models in inclusive urban planning. the dataset encodes 37,710 pairwise comparisons across 13,462 images, structured along six criteria - accessibility, safety, comfort, invitingness, inclusivity, and diversity - derived from 634 community-defined concepts. using direct preference optimization (dpo), we fine-tune stable diffusion xl to reflect multi-criteria spatial preferences and evaluate the livs dataset and the fine-tuned model through four case studies: (1) dpo increases alignment with annotated preferences, particularly when annotation volume is high; (2) preference patterns vary across participant identities, underscoring the need for intersectional data; (3) human-authored prompts generate more distinctive visual outputs than llm-generated ones, influencing annotation decisiveness; and (4) intersectional groups assign systematically different ratings across criteria, revealing the limitations of single-objective alignment. while dpo improves alignment under specific conditions, the prevalence of neutral ratings indicates that community values are heterogeneous and often ambiguous. livs provides a benchmark for developing t2i models that incorporate local, stakeholder-driven preferences, offering a foundation for context-aware alignment in spatial design.",,2025-02-27,2025-05-07,"['rashid mushkani', 'shravan nayak', 'hugo berard', 'allison cohen', 'shin koseki', 'hadrien bertrand']"
2503.02910,langgas: introducing language in selective zero-shot background   subtraction for semi-transparent gas leak detection with a new dataset,cs.cv cs.ai,"gas leakage poses a significant hazard that requires prevention. traditionally, human inspection has been used for detection, a slow and labour-intensive process. recent research has applied machine learning techniques to this problem, yet there remains a shortage of high-quality, publicly available datasets. this paper introduces a synthetic dataset, simgas, featuring diverse backgrounds, interfering foreground objects, diverse leak locations, and precise segmentation ground truth. we propose a zero-shot method that combines background subtraction, zero-shot object detection, filtering, and segmentation to leverage this dataset. experimental results indicate that our approach significantly outperforms baseline methods based solely on background subtraction and zero-shot object detection with segmentation, reaching an iou of 69%. we also present an analysis of various prompt configurations and threshold settings to provide deeper insights into the performance of our method. finally, we qualitatively (because of the lack of ground truth) tested our performance on gasvid and reached decent results on the real-world dataset. the dataset, code, and full qualitative results are available at https://github.com/weathon/lang-gas.",,2025-03-04,2025-04-14,"['wenqi guo', 'yiyang du', 'shan du']"
2503.03307,full-dof egomotion estimation for event cameras using geometric solvers,cs.cv,"for event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an imu. thus, they can only recover the translational motion parameters. recovering full-dof motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. in this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. our method leverages event manifolds induced by line segments. the problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. we demonstrate the possibility of recovering full-dof egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. to achieve efficient optimization, we exploit the adam framework with a first-order approximation of rotations for quick initialization. experiments on both synthetic and real-world data demonstrate the effectiveness of our method. the code is available at https://github.com/jizhaox/relpose-event.",,2025-03-05,2025-05-06,"['ji zhao', 'banglei guan', 'zibin liu', 'laurent kneip']"
2503.03327,scalefusionnet: transformer-guided multi-scale feature fusion for skin   lesion segmentation,eess.iv cs.cv,"melanoma is a malignant tumor originating from skin cell lesions. accurate and efficient segmentation of skin lesions is essential for quantitative medical analysis but remains challenging. to address this, we propose scalefusionnet, a segmentation model that integrates cross-attention transformer module (catm) and adaptivefusionblock to enhance feature extraction and fusion. the model employs a hybrid architecture encoder that effectively captures both local and global features. we introduce catm, which utilizes swin transformer blocks and cross attention fusion (caf) to adaptively refine encoder-decoder feature fusion, reducing semantic gaps and improving segmentation accuracy. additionally, the adaptivefusionblock is improved by integrating adaptive multi-scale fusion, where swin transformer-based attention complements deformable convolution-based multi-scale feature extraction. this enhancement refines lesion boundaries and preserves fine-grained details. scalefusionnet achieves dice scores of 92.94% and 91.65% on isic-2016 and isic-2018 datasets, respectively, demonstrating its effectiveness in skin lesion analysis. our code implementation is publicly available at github.",,2025-03-05,2025-04-30,"['saqib qamar', 'syed furqan qadri', 'roobaea alroobaea', 'goram mufarah m alshmrani', 'richard jiang']"
2503.03355,rethinking video super-resolution: towards diffusion-based methods   without motion alignment,cs.cv cs.lg eess.iv,"in this work, we rethink the approach to video super-resolution by introducing a method based on the diffusion posterior sampling framework, combined with an unconditional video diffusion transformer operating in latent space. the video generation model, a diffusion transformer, functions as a space-time model. we argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.",,2025-03-05,2025-05-08,"['zhihao zhan', 'wang pang', 'xiang zhu', 'yechao bai']"
2503.05214,gaussian random fields as an abstract representation of patient metadata   for multimodal medical image segmentation,eess.iv cs.cv,"the growing rate of chronic wound occurrence, especially in patients with diabetes, has become a concerning trend in recent years. chronic wounds are difficult and costly to treat, and have become a serious burden on health care systems worldwide. chronic wounds can have devastating consequences for the patient, with infection often leading to reduced quality of life and increased mortality risk. innovative deep learning methods for the detection and monitoring of such wounds have the potential to reduce the impact to both patient and clinician. we present a novel multimodal segmentation method which allows for the introduction of patient metadata into the training workflow whereby the patient data are expressed as gaussian random fields. our results indicate that the proposed method improved performance when utilising multiple models, each trained on different metadata categories. using the diabetic foot ulcer challenge 2022 test set, when compared to the baseline results (intersection over union = 0.4670, dice similarity coefficient = 0.5908) we demonstrate improvements of +0.0220 and +0.0229 for intersection over union and dice similarity coefficient respectively. this paper presents the first study to focus on integrating patient data into a chronic wound segmentation workflow. our results show significant performance gains when training individual models using specific metadata categories, followed by average merging of prediction masks using distance transforms. all source code for this study is available at: https://github.com/mmu-dermatology-research/multimodal-grf",,2025-03-07,2025-03-19,"['bill cassidy', 'christian mcbride', 'connah kendrick', 'neil d. reeves', 'joseph m. pappachan', 'shaghayegh raad', 'moi hoon yap']"
2503.06222,vision-based 3d semantic scene completion via capture dynamic   representations,cs.cv,"the vision-based semantic scene completion task aims to predict dense geometric and semantic 3d scene representations from 2d images. however, the presence of dynamic objects in the scene seriously affects the accuracy of the model inferring 3d structures from 2d images. existing methods simply stack multiple frames of image input to increase dense scene semantic information, but ignore the fact that dynamic objects and non-texture areas violate multi-view consistency and matching reliability. to address these issues, we propose a novel method, cdscene: vision-based robust semantic scene completion via capturing dynamic representations. first, we leverage a multimodal large-scale model to extract 2d explicit semantics and align them into 3d space. second, we exploit the characteristics of monocular and stereo depth to decouple scene information into dynamic and static features. the dynamic features contain structural relationships around dynamic objects, and the static features contain dense contextual spatial information. finally, we design a dynamic-static adaptive fusion module to effectively extract and aggregate complementary features, achieving robust and accurate semantic scene completion in autonomous driving scenarios. extensive experimental results on the semantickitti, sscbench-kitti360, and semantickitti-c datasets demonstrate the superiority and robustness of cdscene over existing state-of-the-art methods.",,2025-03-08,2025-05-04,"['meng wang', 'fan wu', 'yunchuan qin', 'ruihui li', 'zhuo tang', 'kenli li']"
2503.06451,"a quantitative evaluation of the expressivity of bmi, pose and gender in   body embeddings for recognition and identification",cs.cv,"person re-identification (reid) systems that match individuals across images or video frames are essential in many real-world applications. however, existing methods are often influenced by attributes such as gender, pose, and body mass index (bmi), which vary in unconstrained settings and raise concerns related to fairness and generalization. to address this, we extend the notion of expressivity, defined as the mutual information between learned features and specific attributes, using a secondary neural network to quantify how strongly attributes are encoded. applying this framework to three reid models, we find that bmi consistently shows the highest expressivity in the final layers, indicating its dominant role in recognition. in the last attention layer, attributes are ranked as bmi > pitch > gender > yaw, revealing their relative influences in representation learning. expressivity values also evolve across layers and training epochs, reflecting a dynamic encoding of attributes. these findings demonstrate the central role of body attributes in reid and establish a principled approach for uncovering attribute driven correlations.",,2025-03-09,2025-05-07,"['basudha pal', 'siyuan huang', 'rama chellappa']"
2503.06457,geometric knowledge-guided localized global distribution alignment for   federated learning,cs.cv cs.ai,"data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. to address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. we first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. subsequently, we propose ggeur, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. in single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. code published at: https://github.com/weidai-david/2025cvpr_ggeur",,2025-03-09,2025-05-05,"['yanbiao ma', 'wei dai', 'wenke huang', 'jiayi chen']"
2503.06669,agibot world colosseo: a large-scale manipulation platform for scalable   and intelligent embodied systems,cs.ro cs.cv cs.lg,"we explore how scalable robot data can address real-world challenges for generalized robotic manipulation. introducing agibot world, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. accelerated by a standardized collection pipeline with human-in-the-loop verification, agibot world guarantees high-quality and diverse data distribution. it is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. building on top of data, we introduce genie operator-1 (go-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. policies pre-trained on our dataset achieve an average performance improvement of 30% over those trained on open x-embodiment, both in in-domain and out-of-distribution scenarios. go-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60% success rate on complex tasks and outperforming prior rdt approach by 32%. by open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.",,2025-03-09,2025-04-30,"['n/a agibot-world-contributors', 'qingwen bu', 'jisong cai', 'li chen', 'xiuqi cui', 'yan ding', 'siyuan feng', 'shenyuan gao', 'xindong he', 'xuan hu', 'xu huang', 'shu jiang', 'yuxin jiang', 'cheng jing', 'hongyang li', 'jialu li', 'chiming liu', 'yi liu', 'yuxiang lu', 'jianlan luo', 'ping luo', 'yao mu', 'yuehan niu', 'yixuan pan', 'jiangmiao pang', 'yu qiao', 'guanghui ren', 'cheng ruan', 'jiaqi shan', 'yongjian shen', 'chengshi shi', 'mingkang shi', 'modi shi', 'chonghao sima', 'jianheng song', 'huijie wang', 'wenhao wang', 'dafeng wei', 'chengen xie', 'guo xu', 'junchi yan', 'cunbiao yang', 'lei yang', 'shukai yang', 'maoqing yao', 'jia zeng', 'chi zhang', 'qinglin zhang', 'bin zhao', 'chengyue zhao', 'jiaqi zhao', 'jianchao zhu']"
2503.06685,asymmetric decision-making in online knowledge distillation:unifying   consensus and divergence,cs.cv,"online knowledge distillation (okd) methods streamline the distillation training process into a single stage, eliminating the need for knowledge transfer from a pretrained teacher network to a more compact student network. this paper presents an innovative approach to leverage intermediate spatial representations. our analysis of the intermediate features from both teacher and student models reveals two pivotal insights: (1) the similar features between students and teachers are predominantly focused on foreground objects. (2) teacher models emphasize foreground objects more than students. building on these findings, we propose asymmetric decision-making (adm) to enhance feature consensus learning for student models while continuously promoting feature diversity in teacher models. specifically, consensus learning for student models prioritizes spatial features with high consensus relative to teacher models. conversely, divergence learning for teacher models highlights spatial features with lower similarity compared to student models, indicating superior performance by teacher models in these regions. consequently, adm facilitates the student models to catch up with the feature learning process of the teacher models. extensive experiments demonstrate that adm consistently surpasses existing okd methods across various online knowledge distillation settings and also achieves superior results when applied to offline knowledge distillation, semantic segmentation and diffusion distillation tasks.",,2025-03-09,,"['zhaowei chen', 'borui zhao', 'yuchen ge', 'yuhao chen', 'renjie song', 'jiajun liang']"
2503.10686,maskattn-unet: a mask attention-driven framework for universal   low-resolution image segmentation,cs.cv cs.lg eess.iv,"low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. to address this challenge, we propose maskattn-unet, a novel segmentation framework that enhances the traditional u-net architecture via a mask attention mechanism. our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. unlike conventional u-net variants, maskattn-unet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. we evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. our results show that maskattn-unet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios.",,2025-03-11,2025-05-07,"['anzhe cheng', 'chenzhong yin', 'yu chang', 'heng ping', 'shixuan li', 'shahin nazarian', 'paul bogdan']"
2503.12026,leveraging motion information for better self-supervised video   correspondence learning,cs.cv,"self-supervised video correspondence learning depends on the ability to accurately associate pixels between video frames that correspond to the same visual object. however, achieving reliable pixel matching without supervision remains a major challenge. to address this issue, recent research has focused on feature learning techniques that aim to encode unique pixel representations for matching. despite these advances, existing methods still struggle to achieve exact pixel correspondences and often suffer from false matches, limiting their effectiveness in self-supervised settings.   to this end, we explore an efficient self-supervised video correspondence learning framework (mer) that aims to accurately extract object details from unlabeled videos. first, we design a dedicated motion enhancement engine that emphasizes capturing the dynamic motion of objects in videos. in addition, we introduce a flexible sampling strategy for inter-pixel correspondence information (multi-cluster sampler) that enables the model to pay more attention to the pixel changes of important objects in motion. through experiments, our algorithm outperforms the state-of-the-art competitors on video correspondence learning tasks such as video object segmentation and video object keypoint tracking.",,2025-03-15,2025-04-30,"['zihan zhou', 'changrui dai', 'aibo song', 'xiaolin fang']"
2503.12623,maven: multi-modal attention for valence-arousal emotion network,cs.lg cs.ai cs.cv cs.mm,"dynamic emotion recognition in the wild remains challenging due to the transient nature of emotional expressions and temporal misalignment of multi-modal cues. traditional approaches predict valence and arousal and often overlook the inherent correlation between these two dimensions. the proposed multi-modal attention for valence-arousal emotion network (maven) integrates visual, audio, and textual modalities through a bi-directional cross-modal attention mechanism. maven uses modality-specific encoders to extract features from synchronized video frames, audio segments, and transcripts, predicting emotions in polar coordinates following russell's circumplex model. the evaluation of the aff-wild2 dataset using maven achieved a concordance correlation coefficient (ccc) of 0.3061, surpassing the resnet-50 baseline model with a ccc of 0.22. the multistage architecture captures the subtle and transient nature of emotional expressions in conversational videos and improves emotion recognition in real-world situations. the code is available at: https://github.com/vrushank-ahire/maven_8th_abaw",,2025-03-16,2025-05-02,"['vrushank ahire', 'kunal shah', 'mudasir nazir khan', 'nikhil pakhale', 'lownish rai sookha', 'm. a. ganaie', 'abhinav dhall']"
2503.13309,integrating ai for human-centric breast cancer diagnostics: a   multi-scale and multi-view swin transformer framework,eess.iv cs.ai cs.cv,"despite advancements in computer-aided diagnosis (cad) systems, breast cancer remains one of the leading causes of cancer-related deaths among women worldwide. recent breakthroughs in artificial intelligence (ai) have shown significant promise in development of advanced deep learning (dl) architectures for breast cancer diagnosis through mammography. in this context, the paper focuses on the integration of ai within a human-centric workflow to enhance breast cancer diagnostics. key challenges are, however, largely overlooked such as reliance on detailed tumor annotations and susceptibility to missing views, particularly during test time. to address these issues, we propose a hybrid, multi-scale and multi-view swin transformer-based framework (msmv-swin) that enhances diagnostic robustness and accuracy. the proposed msmv-swin framework is designed to work as a decision-support tool, helping radiologists analyze multi-view mammograms more effectively. more specifically, the msmv-swin framework leverages the segment anything model (sam) to isolate the breast lobe, reducing background noise and enabling comprehensive feature extraction. the multi-scale nature of the proposed msmv-swin framework accounts for tumor-specific regions as well as the spatial characteristics of tissues surrounding the tumor, capturing both localized and contextual information. the integration of contextual and localized data ensures that msmv-swin's outputs align with the way radiologists interpret mammograms, fostering better human-ai interaction and trust. a hybrid fusion structure is then designed to ensure robustness against missing views, a common occurrence in clinical practice when only a single mammogram view is available.",,2025-03-17,2025-05-07,"['farnoush bayatmakou', 'reza taleei', 'milad amir toutounchian', 'arash mohammadi']"
2503.13435,widerange4d: enabling high-quality 4d reconstruction with wide-range   movements and scenes,cs.cv,"with the rapid development of 3d reconstruction technology, research in 4d reconstruction is also advancing, existing 4d reconstruction methods can generate high-quality 4d scenes. however, due to the challenges in acquiring multi-view video data, the current 4d reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. in practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4d reconstruction datasets. additionally, existing 4d reconstruction methods rely on deformation fields to estimate the dynamics of 3d objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4d scene reconstruction with wide-range spatial movements. in this paper, we focus on 4d scene reconstruction with significant object spatial movements and propose a novel 4d reconstruction benchmark, widerange4d. this benchmark includes rich 4d scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4d generation methods. furthermore, we introduce a new 4d reconstruction method, progress4d, which generates stable and high-quality 4d results across various complex 4d scene reconstruction tasks. we conduct both quantitative and qualitative comparison experiments on widerange4d, showing that our progress4d outperforms existing state-of-the-art 4d reconstruction methods. project: https://github.com/gen-verse/widerange4d",,2025-03-17,2025-04-29,"['ling yang', 'kaixin zhu', 'juanxi tian', 'bohan zeng', 'mingbao lin', 'hongjuan pei', 'wentao zhang', 'shuicheng yan']"
2503.14331,adapt: an autonomous forklift for construction site operation,cs.ro cs.cv cs.sy eess.sy,"efficient material logistics play a critical role in controlling costs and schedules in the construction industry. however, manual material handling remains prone to inefficiencies, delays, and safety risks. autonomous forklifts offer a promising solution to streamline on-site logistics, reducing reliance on human operators and mitigating labor shortages. this paper presents the development and evaluation of adapt (autonomous dynamic all-terrain pallet transporter), a fully autonomous off-road forklift designed for construction environments. unlike structured warehouse settings, construction sites pose significant challenges, including dynamic obstacles, unstructured terrain, and varying weather conditions. to address these challenges, our system integrates ai-driven perception techniques with traditional approaches for decision making, planning, and control, enabling reliable operation in complex environments. we validate the system through extensive real-world testing, comparing its continuous performance against an experienced human operator across various weather conditions. our findings demonstrate that autonomous outdoor forklifts can operate near human-level performance, offering a viable path toward safer and more efficient construction logistics.",,2025-03-18,2025-05-02,"['johannes huemer', 'markus murschitz', 'matthias schörghuber', 'lukas reisinger', 'thomas kadiofsky', 'christoph weidinger', 'mario niedermeyer', 'benedikt widy', 'marcel zeilinger', 'csaba beleznai', 'tobias glück', 'andreas kugi', 'patrik zips']"
2503.15661,ui-vision: a desktop-centric gui benchmark for visual perception and   interaction,cs.cv cs.ai cs.cl,"autonomous agents that navigate graphical user interfaces (guis) to automate tasks like document editing and file management can greatly enhance computer workflows. while existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges and licensing issues. we introduce ui-vision, the first comprehensive, license-permissive benchmark for offline, fine-grained evaluation of computer use agents in real-world desktop environments. unlike online benchmarks, ui-vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, ui labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-element grounding, layout grounding, and action prediction-with well-defined metrics to rigorously evaluate agents' performance in desktop environments. our evaluation reveals critical limitations in state-of-the-art models like ui-tars-72b, including issues with understanding professional software, spatial reasoning, and complex actions like drag-and-drop. these findings highlight the challenges in developing fully autonomous computer use agents. by releasing ui-vision as open-source, we aim to advance the development of more capable agents for real-world desktop tasks.",,2025-03-19,2025-05-06,"['shravan nayak', 'xiangru jian', 'kevin qinghong lin', 'juan a. rodriguez', 'montek kalsi', 'rabiul awal', 'nicolas chapados', 'm. tamer özsu', 'aishwarya agrawal', 'david vazquez', 'christopher pal', 'perouz taslakian', 'spandana gella', 'sai rajeswar']"
2503.19769,biprompt-sam: enhancing image segmentation via explicit selection   between point and text prompts,cs.cv cs.lg,"segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. the segment anything model (sam) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like beit-3, provide rich semantic understanding. however, effectively combining these complementary modalities remains a challenge. this paper introduces biprompt-sam, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. we leverage sam's ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via evf-sam with beit-3) to select the point-generated mask that best aligns spatially, measured by intersection over union (iou). this approach, interpretable as a simplified mixture of experts (moe), effectively fuses spatial precision and semantic context without complex model modifications. notably, our method achieves strong zero-shot performance on the endovis17 medical dataset (89.55% mdice, 81.46% miou) using only a single point prompt per instance. this significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the method's effectiveness without domain-specific training. on the refcoco series, biprompt-sam attained 87.1%, 86.5%, and 85.8% iou, significantly outperforming existing approaches. experiments show biprompt-sam excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion.",,2025-03-25,2025-04-30,"['suzhe xu', 'jialin peng', 'chengyuan zhang']"
2503.21510,uncertainty-aware bayesian machine learning modelling of land cover   classification,cs.lg cs.cv stat.ml,"land cover classification involves the production of land cover maps, which determine the type of land through remote sensing imagery. over recent years, such classification is being performed by machine learning classification models, which can give highly accurate predictions on land cover per pixel using large quantities of input training data. however, such models do not currently take account of input measurement uncertainty, which is vital for traceability in metrology. in this work we propose a bayesian classification framework using generative modelling to take account of input measurement uncertainty. we take the specific case of bayesian quadratic discriminant analysis, and apply it to land cover datasets from copernicus sentinel-2 in 2020 and 2021. we benchmark the performance of the model against more popular classification models used in land cover maps such as random forests and neural networks. we find that such bayesian models are more trustworthy, in the sense that they are more interpretable, explicitly model the input measurement uncertainty, and maintain predictive performance of class probability outputs across datasets of different years and sizes, whilst also being computationally efficient.",,2025-03-27,2025-05-01,"['samuel bilson', 'anna pustogvar']"
2503.22676,transplat: lighting-consistent cross-scene object transfer with 3d   gaussian splatting,cs.cv,"we present transplat, a 3d scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the gaussian splatting framework. our approach addresses two critical challenges: (1) precise 3d object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. transplat fits a splatting model to the source scene, using 2d object masks to drive fine-grained 3d segmentation. following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, transplat derives per-gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. this relighting strategy does not require explicitly estimating physical scene properties such as brdfs. evaluated on several synthetic and real-world scenes and objects, transplat yields excellent 3d object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. we conclude by discussing the limitations of the approach.",,2025-03-28,2025-05-07,"['tony yu', 'yanlin jin', 'ashok veeraraghavan', 'akshat dave', 'guha balakrishnan']"
2504.00159,sonarsplat: novel view synthesis of imaging sonar via gaussian splatting,cs.cv,"in this paper, we present sonarsplat, a novel gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. our method represents the scene as a set of 3d gaussians with acoustic reflectance and saturation properties. we develop a novel method to efficiently rasterize gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. in particular, we develop a novel approach to model azimuth streaking in a gaussian splatting framework. we evaluate sonarsplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. compared to the state-of-the-art, sonarsplat offers improved image synthesis capabilities (+3.2 db psnr) and more accurate 3d reconstruction (52% lower chamfer distance). we also demonstrate that sonarsplat can be leveraged for azimuth streak removal.",,2025-03-31,2025-05-04,"['advaith v. sethuraman', 'max rucker', 'onur bagoren', 'pou-chun kung', 'nibarkavi n. b. amutha', 'katherine a. skinner']"
2504.00879,gise-ttt:a framework for global informationsegmentation and enhancement,cs.cv,"this paper addresses the challenge of capturing global temporaldependencies in long video sequences for video object segmentation (vos). existing architectures often fail to effectively model these dependencies acrossextended temporal horizons. to overcome this limitation, we introduce gise-ttt, anovel architecture that integrates temporal transformer (ttt) layers intotransformer-based frameworks through a co-designed hierarchical approach.the tttlayer systematically condenses historical temporal information into hidden states thatencode globally coherent contextual representations. by leveraging multi-stagecontextual aggregation through hierarchical concatenation, our frameworkprogressively refines spatiotemporal dependencies across network layers. this designrepresents the first systematic empirical evidence that distributing global informationacross multiple network layers is critical for optimal dependency utilization in videosegmentation tasks.ablation studies demonstrate that incorporating ttt modules athigh-level feature stages significantly enhances global modeling capabilities, therebyimproving the network's ability to capture long-range temporal relationships. extensive experiments on davis 2017 show that gise-ttt achieves a 3.2%improvement in segmentation accuracy over the baseline model, providingcomprehensive evidence that global information should be strategically leveragedthroughout the network architecture.the code will be made available at:https://github.com/uuool/gise-ttt.",,2025-04-01,2025-04-29,"['fenglei hao', 'yuliang yang', 'ruiyuan su', 'zhengran zhao', 'yukun qiao', 'mengyu zhu']"
2504.02287,multisensor-home: a wide-area multi-modal multi-view dataset for action   recognition and transformer-based sensor fusion,cs.cv,"multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. however, current datasets often fail to address real-world challenges such as wide-area distributed settings, asynchronous data streams, and the lack of frame-level annotations. furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. in this paper, we introduce the multisensor-home dataset, a novel benchmark designed for comprehensive action recognition in home environments, and also propose the multi-modal multi-view transformer-based sensor fusion (multitsf) method. the proposed multisensor-home dataset features untrimmed videos captured by distributed sensors, providing high-resolution rgb and audio data along with detailed multi-view frame-level action labels. the proposed multitsf method leverages a transformer-based fusion mechanism to dynamically model inter-view relationships. furthermore, the proposed method integrates a human detection module to enhance spatial feature learning, guiding the model to prioritize frames with human activity to enhance action the recognition accuracy. experiments on the proposed multisensor-home and the existing mm-office datasets demonstrate the superiority of multitsf over the state-of-the-art methods. quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition. the source code is available at https://github.com/thanhhff/multitsf.",,2025-04-03,2025-05-07,"['trung thanh nguyen', 'yasutomo kawanishi', 'vijay john', 'takahiro komamizu', 'ichiro ide']"
2504.02782,gpt-imgeval: a comprehensive benchmark for diagnosing gpt4o in image   generation,cs.cv,"the recent breakthroughs in openai's gpt4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. this technical report presents the first-look evaluation benchmark (named gpt-imgeval), quantitatively and qualitatively diagnosing gpt-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. across all three tasks, gpt-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. furthermore, based on the gpt-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of gpt-4o, where our empirical results suggest the model consists of an auto-regressive (ar) combined with a diffusion-based head for image decoding, rather than the var-like architectures. we also provide a complete speculation on gpt-4o's overall architecture. in addition, we conduct a series of analyses to identify and visualize gpt-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. we also present a comparative study of multi-round image editing between gpt-4o and gemini 2.0 flash, and discuss the safety implications of gpt-4o's outputs, particularly their detectability by existing image forensic models. we hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. the codes and datasets used for evaluating gpt-4o can be found at https://github.com/picotrex/gpt-imgeval.",,2025-04-03,2025-05-02,"['zhiyuan yan', 'junyan ye', 'weijia li', 'zilong huang', 'shenghai yuan', 'xiangyang he', 'kaiqing lin', 'jun he', 'conghui he', 'li yuan']"
2504.03471,dynamic importance in diffusion u-net for enhanced image synthesis,cs.cv,"traditional diffusion models typically employ a u-net architecture. previous studies have unveiled the roles of attention blocks in the u-net. however, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. in this study, we first theoretically proved that, re-weighting the outputs of the transformer blocks within the u-net is a ""free lunch"" for improving the signal-to-noise ratio during the sampling process. next, we proposed importance probe to uncover and quantify the dynamic shifts in importance of the transformer blocks throughout the denoising process. finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. our method can be seamlessly integrated into any u-net-based architecture. code: https://github.com/hytidel/unetreweighting",,2025-04-04,2025-05-05,"['xi wang', 'ziqi he', 'yang zhou']"
2504.04318,variational self-supervised learning,cs.lg cs.cv,"we present variational self-supervised learning (vssl), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. unlike traditional vaes that rely on input reconstruction via a decoder, vssl symmetrically couples two encoders with gaussian outputs. a momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. the reconstruction term in the elbo is replaced with a cross-view denoising objective, preserving the analytical tractability of gaussian kl divergence. we further introduce cosine-based formulations of kl and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. experiments on cifar-10, cifar-100, and imagenet-100 show that vssl achieves competitive or superior performance to leading self-supervised methods, including byol and moco v3. vssl offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.",,2025-04-05,2025-05-01,"['mehmet can yavuz', 'berrin yanikoglu']"
2504.04519,sam2mot: a novel paradigm of multi-object tracking by segmentation,cs.cv,"segment anything 2 (sam2) enables robust single-object tracking using segmentation. to extend this to multi-object tracking (mot), we propose sam2mot, introducing a novel tracking by segmentation paradigm. unlike tracking by detection or tracking by query, sam2mot directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. sam2mot has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from sam2. to further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. experiments on dancetrack, uavdt, and bdd100k show state-of-the-art results. notably, sam2mot outperforms existing methods on dancetrack by +2.1 hota and +4.5 idf1, highlighting its effectiveness in mot. code is available at https://github.com/triplejoy/sam2mot.",,2025-04-06,2025-05-05,"['junjie jiang', 'zelin wang', 'manqi zhao', 'yin li', 'dongsheng jiang']"
2504.05184,msa-unet3+: multi-scale attention unet3+ with new supervised   prototypical contrastive loss for coronary dsa image segmentation,cs.cv,"accurate segmentation of coronary digital subtraction angiography images is essential to diagnose and treat coronary artery diseases. despite advances in deep learning, challenges such as high intra-class variance and class imbalance limit precise vessel delineation. most existing approaches for coronary dsa segmentation cannot address these issues. also, existing segmentation network's encoders do not directly generate semantic embeddings, which could enable the decoder to reconstruct segmentation masks effectively from these well-defined features. we propose a supervised prototypical contrastive loss that fuses supervised and prototypical contrastive learning to enhance coronary dsa image segmentation. the supervised contrastive loss enforces semantic embeddings in the encoder, improving feature differentiation. the prototypical contrastive loss allows the model to focus on the foreground class while alleviating the high intra-class variance and class imbalance problems by concentrating only on the hard-to-classify background samples. we implement the proposed spcl loss within an msa-unet3+: a multi-scale attention-enhanced unet3+ architecture. the architecture integrates key components: a multi-scale attention encoder and a multi-scale dilated bottleneck designed to enhance multi-scale feature extraction and a contextual attention fusion module built to keep fine-grained details while improving contextual understanding. experiments on a private coronary dsa dataset show that msa-unet3+ outperforms state-of-the-art methods, achieving the highest dice coefficient and f1-score and significantly reducing asd and acd. the developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. the code will be released at https://github.com/rayanmerghani/msa-unet3plus.",,2025-04-07,2025-05-06,"['rayan merghani ahmed', 'adnan iltaf', 'mohamed elmanna', 'gang zhao', 'hongliang li', 'yue du', 'bin li', 'shoujun zhou']"
2504.05304,gaussian mixture flow matching models,cs.lg cs.cv,"diffusion models approximate the denoising distribution as a gaussian and predict its mean, whereas flow matching models reparameterize the gaussian mean as flow velocity. however, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (cfg). to address these limitations, we propose a novel gaussian mixture flow matching (gmflow) model: instead of predicting the mean, gmflow predicts dynamic gaussian mixture (gm) parameters to capture a multi-modal flow velocity distribution, which can be learned with a kl divergence loss. we demonstrate that gmflow generalizes previous diffusion and flow matching models where a single gaussian is learned with an $l_2$ denoising loss. for inference, we derive gm-sde/ode solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of cfg and improves image generation quality. extensive experiments demonstrate that gmflow consistently outperforms flow matching baselines in generation quality, achieving a precision of 0.942 with only 6 sampling steps on imagenet 256$\times$256.",,2025-04-07,2025-05-01,"['hansheng chen', 'kai zhang', 'hao tan', 'zexiang xu', 'fujun luan', 'leonidas guibas', 'gordon wetzstein', 'sai bi']"
2504.06675,probability density geodesics in image diffusion latent space,cs.cv,"diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. in this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. in this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. we present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.",,2025-04-09,2025-05-06,"['qingtao yu', 'jaskirat singh', 'zhaoyuan yang', 'peter henry tu', 'jing zhang', 'hongdong li', 'richard hartley', 'dylan campbell']"
2504.07392,id-booth: identity-consistent face generation with diffusion models,cs.cv,"recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. in contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. to address these issues, we present in this paper a novel generative diffusion-based framework, called id-booth. id-booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. the framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. in turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. the source code for the id-booth framework is publicly available at https://github.com/dariant/id-booth.",,2025-04-09,2025-05-04,"['darian tomašević', 'fadi boutros', 'chenhao lin', 'naser damer', 'vitomir štruc', 'peter peer']"
2504.07606,heart failure prediction using modal decomposition and masked   autoencoders for scarce echocardiography databases,eess.iv cs.cv,"heart diseases constitute the main cause of international human defunction. according to the world health organization (who), approximately 18 million deaths happen each year due to precisely heart diseases. in particular, heart failures (hf) press the healthcare industry to develop systems for their early, rapid, and effective prediction. this work presents an automatic system based on a novel deep learning framework which analyses in real-time echocardiography video sequences for the challenging and more specific task of heart failure time prediction. this system works in two stages. the first one transforms the data from a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any machine learning-based framework, including a deep learning-based one. this stage includes the use of the higher order dynamic mode decomposition (hodmd) algorithm for both data augmentation and feature extraction. the second stage builds and trains a vision transformer (vit). self-supervised learning (ssl) methods, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the vit from scratch, even with scarce databases. the designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. the results obtained show the efficacy of the hodmd algorithm and the superiority of the proposed system with respect to several established vit and convolutional neural network (cnn) architectures. the source code will be incorporated into the next version release of the modelflows-app software (https://github.com/modelflows/modelflows-app).",,2025-04-10,2025-05-06,"['andrés bell-navas', 'maría villalba-orero', 'enrique lara-pezzi', 'jesús garicano-mena', 'soledad le clainche']"
2504.08280,pne-sgan: probabilistic ndt-enhanced semantic graph attention network   for lidar loop closure detection,cs.cv cs.ro,"lidar loop closure detection (lcd) is crucial for consistent simultaneous localization and mapping (slam) but faces challenges in robustness and accuracy. existing methods, including semantic graph approaches, often suffer from coarse geometric representations and lack temporal robustness against noise, dynamics, and viewpoint changes. we introduce pne-sgan, a probabilistic ndt-enhanced semantic graph attention network, to overcome these limitations. pne-sgan enhances semantic graphs by using normal distributions transform (ndt) covariance matrices as rich, discriminative geometric node features, processed via a graph attention network (gat). crucially, it integrates graph similarity scores into a probabilistic temporal filtering framework (modeled as an hmm/bayes filter), incorporating uncertain odometry for motion modeling and utilizing forward-backward smoothing to effectively handle ambiguities. evaluations on challenging kitti sequences (00 and 08) demonstrate state-of-the-art performance, achieving average precision of 96.2\% and 95.1\%, respectively. pne-sgan significantly outperforms existing methods, particularly in difficult bidirectional loop scenarios where others falter. by synergizing detailed ndt geometry with principled probabilistic temporal reasoning, pne-sgan offers a highly accurate and robust solution for lidar lcd, enhancing slam reliability in complex, large-scale environments.",,2025-04-11,2025-05-06,"['xiong li', 'shulei liu', 'xingning chen', 'yisong wu', 'dong zhu']"
2504.08603,findanything: open-vocabulary and object-centric mapping for robot   exploration in any environment,cs.ro cs.ai cs.cv,"geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. in this paper we present findanything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. thanks to the use of vision-language features, findanything bridges the gap between pure geometric and open-vocabulary semantic information for a higher level of understanding while allowing to explore any environment without the help of any external source of ground-truth pose information. we represent the environment as a series of volumetric occupancy submaps, resulting in a robust and accurate map representation that deforms upon pose updates when the underlying slam system corrects its drift, allowing for a locally consistent representation between submaps. pixel-wise vision-language features are aggregated from efficient sam (esam)-generated segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3d geometry that is scalable also in terms of memory usage. the open-vocabulary map representation of findanything achieves state-of-the-art semantic accuracy in closed-set evaluations on the replica dataset. this level of scene understanding allows a robot to explore environments based on objects or areas of interest selected via natural language queries. our system is the first of its kind to be deployed on resource-constrained devices, such as mavs, leveraging vision-language information for real-world robotic tasks.",,2025-04-11,2025-05-08,"['sebastián barbas laina', 'simon boche', 'sotiris papatheodorou', 'simon schaefer', 'jaehyung jung', 'stefan leutenegger']"
2504.08685,seaweed-7b: cost-effective training of video generation foundation model,cs.cv cs.ai,"this technical report presents a cost-efficient strategy for training a video generation foundation model. we present a mid-sized research model with approximately 7 billion parameters (7b) called seaweed-7b trained from scratch using 665,000 h100 gpu hours. despite being trained with moderate computational resources, seaweed-7b demonstrates highly competitive performance compared to contemporary video generation models of much larger size. design choices are especially crucial in a resource-constrained setting. this technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. empirically, we make two observations: (1) seaweed-7b achieves performance comparable to, or even surpasses, larger models trained on substantially greater gpu resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. see the project page at https://seaweed.video/",,2025-04-11,2025-05-04,"['team seawead', 'ceyuan yang', 'zhijie lin', 'yang zhao', 'shanchuan lin', 'zhibei ma', 'haoyuan guo', 'hao chen', 'lu qi', 'sen wang', 'feng cheng', 'feilong zuo', 'xuejiao zeng', 'ziyan yang', 'fangyuan kong', 'meng wei', 'zhiwu qing', 'fei xiao', 'tuyen hoang', 'siyu zhang', 'peihao zhu', 'qi zhao', 'jiangqiao yan', 'liangke gui', 'sheng bi', 'jiashi li', 'yuxi ren', 'rui wang', 'huixia li', 'xuefeng xiao', 'shu liu', 'feng ling', 'heng zhang', 'houmin wei', 'huafeng kuang', 'jerry duncan', 'junda zhang', 'junru zheng', 'li sun', 'manlin zhang', 'renfei sun', 'xiaobin zhuang', 'xiaojie li', 'xin xia', 'xuyan chi', 'yanghua peng', 'yuping wang', 'yuxuan wang', 'zhongkai zhao', 'zhuo chen', 'zuquan song', 'zhenheng yang', 'jiashi feng', 'jianchao yang', 'lu jiang']"
2504.08982,adaptive additive parameter updates of vision transformers for few-shot   continual learning,cs.cv,"integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. few-shot class incremental learning (fscil) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. however, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. in this work, we propose a simple yet effective novel fscil framework that leverages a frozen vision transformer (vit) backbone augmented with parameter-efficient additive updates. our approach freezes the pre-trained vit parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. this design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. by fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen vit while reducing the risk of overfitting. furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline fscil methods.",,2025-04-11,2025-05-03,"['kyle stein', 'andrew arash mahyari', 'guillermo francia', 'eman el-sheikh']"
2504.09149,mash: masked anchored spherical distances for 3d shape representation   and generation,cs.cv cs.cg,"we introduce masked anchored spherical distances (mash), a novel multi-view and parametrized representation of 3d shapes. inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3d shapes, mash represents a 3d shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. we further leverage the compactness of spherical harmonics to encode the mash functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. we develop a differentiable optimization algorithm capable of converting any point cloud into a mash representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. extensive experiments demonstrate that mash is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.",,2025-04-12,2025-05-02,"['changhao li', 'yu xin', 'xiaowei zhou', 'ariel shamir', 'hao zhang', 'ligang liu', 'ruizhen hu']"
2504.09948,omni-dish: photorealistic and faithful image generation and editing for   arbitrary chinese dishes,cs.cv cs.ai cs.mm,"dish images play a crucial role in the digital era, with the demand for culturally distinctive dish images continuously increasing due to the digitization of the food industry and e-commerce. in general cases, existing text-to-image generation models excel in producing high-quality images; however, they struggle to capture diverse characteristics and faithful details of specific domains, particularly chinese dishes. to address this limitation, we propose omni-dish, the first text-to-image generation model specifically tailored for chinese dishes. we develop a comprehensive dish curation pipeline, building the largest dish dataset to date. additionally, we introduce a recaption strategy and employ a coarse-to-fine training scheme to help the model better learn fine-grained culinary nuances. during inference, we enhance the user's textual input using a pre-constructed high-quality caption library and a large language model, enabling more photorealistic and faithful image generation. furthermore, to extend our model's capability for dish editing tasks, we propose concept-enhanced p2p. based on this approach, we build a dish editing dataset and train a specialized editing model. extensive experiments demonstrate the superiority of our methods.",,2025-04-14,2025-04-30,"['huijie liu', 'bingcan wang', 'jie hu', 'xiaoming wei', 'guoliang kang']"
2504.10880,safe-construct: redefining construction safety violation recognition as   3d multi-view engagement task,cs.cv cs.ro,"recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. existing models predominantly rely on 2d object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. to address these challenges, we introduce safe-construct, the first framework that reformulates violation recognition as a 3d multi-view engagement task, leveraging scene-level worker-object context and 3d spatial understanding. we also propose the synthetic indoor construction site generator (sicsg) to create diverse, scalable training data, overcoming data limitations. safe-construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. we rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). by integrating 3d multi-view spatial understanding and synthetic data generation, safe-construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. project website: https://safe-construct.github.io/safe-construct",,2025-04-15,,"['aviral chharia', 'tianyu ren', 'tomotake furuhata', 'kenji shimada']"
2504.11014,gate3d: generalized attention-based task-synergized estimation in 3d*,cs.cv cs.ai,"the emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. such universality typically requires joint training across multi-domain datasets to ensure effective generalization. however, monocular 3d object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3d ground-truth labels, especially beyond typical road-based autonomous driving contexts. to address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. unlike generalized image-based 2d object detection models, achieving similar generalization in monocular 3d detection remains largely unexplored. in this paper, we propose gate3d, a novel framework designed specifically for generalized monocular 3d object detection via weak supervision. gate3d effectively bridges domain gaps by employing consistency losses between 2d and 3d predictions. remarkably, our model achieves competitive performance on the kitti benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. our results demonstrate that gate3d significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. project page: https://ies0411.github.io/gate3d/",,2025-04-15,2025-04-29,"['eunsoo im', 'changhyun jee', 'jung kwon lee']"
2504.11022,meta-learning for few-shot time series crop type classification: a   benchmark on the eurocropsml dataset,cs.lg cs.cv,"spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. however, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. this study benchmarks transfer learning and several meta-learning algorithms, including (first-order) model-agnostic meta-learning ((fo)-maml), almost no inner loop (anil), and task-informed meta-learning (timl), on the real-world eurocropsml time series dataset, which combines farmer-reported crop data with sentinel-2 satellite observations from estonia, latvia, and portugal. our findings indicate that maml-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in estonia after pre-training on data from latvia. however, this improvement comes at the cost of increased computational demands and training time. moreover, we find that the transfer of knowledge between geographically disparate regions, such as estonia and portugal, poses significant challenges to all investigated algorithms. these insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the earth. to facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. the corresponding code is publicly available at https://github.com/dida-do/eurocrops-meta-learning.",,2025-04-15,,"['joana reuss', 'jan macdonald', 'simon becker', 'konrad schultka', 'lorenz richter', 'marco körner']"
2504.11485,deciphering scrolls with tomography: a training experiment,eess.iv cs.cv,"the recovery of severely damaged ancient written documents has proven to be a major challenge for many scientists, mainly due to the impracticality of physical unwrapping them. non-destructive techniques, such as x-ray computed tomography (ct), combined with computer vision algorithms, have emerged as a means of facilitating the virtual reading of the hidden contents of the damaged documents. this paper proposes an educational laboratory aimed at simulating the entire process of acquisition and virtual recovery of the ancient works. we have developed an experimental setup that uses visible light to replace the detrimental x-rays, and a didactic software pipeline that allows students to virtually reconstruct a transparent rolled sheet with printed text on it, the wrapped scroll.",,2025-04-14,2025-05-02,"['sonia foschiatti', 'axel kittenberger', 'otmar scherzer']"
2504.11515,graph-driven multimodal feature learning framework for apparent   personality assessment,cs.cv cs.cl cs.mm,"predicting personality traits automatically has become a challenging problem in computer vision. this paper introduces an innovative multimodal feature learning framework for personality analysis in short video clips. for visual processing, we construct a facial graph and design a geo-based two-stream network incorporating an attention mechanism, leveraging both graph convolutional networks (gcn) and convolutional neural networks (cnn) to capture static facial expressions. additionally, resnet18 and vggface networks are employed to extract global scene and facial appearance features at the frame level. to capture dynamic temporal information, we integrate a bigru with a temporal attention module for extracting salient frame representations. to enhance the model's robustness, we incorporate the vggish cnn for audio-based features and xlm-roberta for text-based features. finally, a multimodal channel attention mechanism is introduced to integrate different modalities, and a multi-layer perceptron (mlp) regression model is used to predict personality traits. experimental results confirm that our proposed framework surpasses existing state-of-the-art approaches in performance.",10.62762/tetai.2025.279350,2025-04-15,,"['kangsheng wang', 'chengwei ye', 'huanzhen zhang', 'linuo xu', 'shuyan liu']"
2504.11739,the devil is in the prompts: retrieval-augmented prompt optimization for   text-to-video generation,cs.cv cs.cl,"the evolution of text-to-video (t2v) generative models, trained on large-scale datasets, has been marked by significant progress. however, the sensitivity of t2v generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. prior research has predominantly relied on large language models (llms) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. to this end, we introduce rapo, a novel retrieval-augmented prompt optimization framework. in order to address potential inaccuracies and ambiguous details generated by llm-generated prompts. rapo refines the naive prompts through dual optimization branches, selecting the superior prompt for t2v generation. the first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned llm. conversely, the second branch rewrites the naive prompt using a pre-trained llm following a well-defined instruction set. extensive experiments demonstrate that rapo can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts.",,2025-04-15,2025-05-05,"['bingjie gao', 'xinyu gao', 'xiaoxue wu', 'yujie zhou', 'yu qiao', 'li niu', 'xinyuan chen', 'yaohui wang']"
2504.11895,search is all you need for few-shot anomaly detection,cs.cv,"few-shot anomaly detection (fsad) has emerged as a crucial yet challenging task in industrial inspection, where normal distribution modeling must be accomplished with only a few normal images. while existing approaches typically employ multi-modal foundation models combining language and vision modalities for prompt-guided anomaly detection, these methods often demand sophisticated prompt engineering and extensive manual tuning. in this paper, we demonstrate that a straightforward nearest-neighbor search framework can surpass state-of-the-art performance in both single-class and multi-class fsad scenarios. our proposed method, visionad, consists of four simple yet essential components: (1) scalable vision foundation models that extract universal and discriminative features; (2) dual augmentation strategies - support augmentation to enhance feature matching adaptability and query augmentation to address the oversights of single-view prediction; (3) multi-layer feature integration that captures both low-frequency global context and high-frequency local details with minimal computational overhead; and (4) a class-aware visual memory bank enabling efficient one-for-all multi-class detection. extensive evaluations across mvtec-ad, visa, and real-iad benchmarks demonstrate visionad's exceptional performance. using only 1 normal images as support, our method achieves remarkable image-level auroc scores of 97.4%, 94.8%, and 70.8% respectively, outperforming current state-of-the-art approaches by significant margins (+1.6%, +3.2%, and +1.4%). the training-free nature and superior few-shot capabilities of visionad make it particularly appealing for real-world applications where samples are scarce or expensive to obtain. code is available at https://github.com/qiqigeww/visionad.",,2025-04-16,2025-05-08,"['qishan wang', 'jia guo', 'shuyong gao', 'haofen wang', 'li xiong', 'junjie hu', 'hanqi guo', 'wenqiang zhang']"
2504.12240,cobra: efficient line art colorization with broader references,cs.cv,"the comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. a comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. we investigate the necessity of extensive contextual image guidance on the quality of line art colorization. to address these challenges, we introduce cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. central to cobra is a causal sparse dit architecture, which leverages specially designed positional encodings, causal sparse attention, and key-value cache to effectively manage long-context references and ensure color identity consistency. results demonstrate that cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. we release our codes and models on our project page: https://zhuang2002.github.io/cobra/.",,2025-04-16,2025-05-06,"['junhao zhuang', 'lingen li', 'xuan ju', 'zhaoyang zhang', 'chun yuan', 'ying shan']"
2504.13460,chain-of-thought textual reasoning for few-shot temporal action   localization,cs.cv cs.ai,"traditional temporal action localization (tal) methods rely on large amounts of detailed annotated data, whereas few-shot tal reduces this dependence by using only a few training samples to identify unseen action categories. however, existing few-shot tal methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. therefore, we propose a new few-shot temporal action localization method by chain-of-thought textual reasoning to improve localization performance. specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a chain of thought (cot)-like reasoning method that progressively guides the vision language model (vlm) and large language model (llm) to generate cot-like text descriptions for videos. the generated texts can capture more variance of action than visual features. we conduct extensive experiments on the publicly available activitynet1.3 and thumos14 datasets. we introduce the first dataset named human-related anomaly localization and explore the application of the tal task in human anomaly detection. the experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. we will release our code, data and benchmark.",,2025-04-18,2025-05-06,"['hongwei ji', 'wulian yun', 'mengshi qi', 'huadong ma']"
2504.13754,towards accurate and interpretable neuroblastoma diagnosis via   contrastive multi-scale pathological image analysis,cs.cv cs.ai,"neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole-slide images is critical for patient prognosis. however, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. existing automated whole-slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. to overcome these limitations, we propose cmswinkan, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the swin transformer architecture by integrating a kernel activation network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. by fusing multi-scale features and leveraging contrastive learning strategies, cmswinkan mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to bridge patch-level predictions to whole-slide image-level classifications seamlessly. we verified the cmswinkan on the publicly available breakhis dataset and the ppnts dataset, which was established by our hospital. results demonstrate that cmswinkan performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. our source code is available at https://github.com/jsliam94/cmswinkan.",,2025-04-18,2025-05-06,"['zhu zhu', 'shuo jiang', 'jingyuan zheng', 'yawen li', 'yifei chen', 'manli zhao', 'weizhong gu', 'feiwei qin', 'jinhu wang', 'gang yu']"
2504.14467,lgd: leveraging generative descriptions for zero-shot referring image   segmentation,cs.cv,"zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. previous works address this challenge by utilizing vision-language models and mask proposal networks for region-text matching. however, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. to alleviate this issue, we present lgd (leveraging generative descriptions), a framework that utilizes the advanced language generation capabilities of multi-modal large language models to enhance region-text matching performance in vision-language models. specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the multi-modal large language models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. the proposed method achieves new state-of-the-art performance on three public datasets refcoco, refcoco+ and refcocog, with maximum improvements of 9.97% in oiou and 11.29% in miou compared to previous methods.",,2025-04-19,2025-05-01,"['jiachen li', 'qing xie', 'renshu gu', 'jinyu xu', 'yongjian liu', 'xiaohan yu']"
2504.14693,video-mmlu: a massive multi-discipline lecture understanding benchmark,cs.cv cs.ai,"recent advancements in language multimodal models (lmms) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. we introduce video-mmlu, a massive benchmark designed to evaluate the capabilities of lmms in understanding multi-discipline lectures. we evaluate over 90 open-source and proprietary models, ranging from 0.5b to 40b parameters. our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension.",,2025-04-20,2025-05-02,"['enxin song', 'wenhao chai', 'weili xu', 'jianwen xie', 'yuxuan liu', 'gaoang wang']"
2504.15032,dyst-xl: dynamic layout planning and content control for compositional   text-to-video generation,cs.cv,"compositional text-to-video generation, which requires synthesizing dynamic scenes with multiple interacting entities and precise spatial-temporal relationships, remains a critical challenge for diffusion-based models. existing methods struggle with layout discontinuity, entity identity drift, and implausible interaction dynamics due to unconstrained cross-attention mechanisms and inadequate physics-aware reasoning. to address these limitations, we propose dyst-xl, a \textbf{training-free} framework that enhances off-the-shelf text-to-video models (e.g., cogvideox-5b) through frame-aware control. dyst-xl integrates three key innovations: (1) a dynamic layout planner that leverages large language models (llms) to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts, with intermediate frames interpolated via trajectory optimization; (2) a dual-prompt controlled attention mechanism that enforces localized text-video alignment through frame-aware attention masking, achieving precise control over individual entities; and (3) an entity-consistency constraint strategy that propagates first-frame feature embeddings to subsequent frames during denoising, preserving object identity without manual annotation. experiments demonstrate that dyst-xl excels in compositional text-to-video generation, significantly improving performance on complex prompts and bridging a crucial gap in training-free video synthesis. the code is released in https://github.com/xiaobul/dyst-xl.",,2025-04-21,2025-04-29,"['weijie he', 'mushui liu', 'yunlong yu', 'zhao wang', 'chao wu']"
2504.15122,mobgs: motion deblurring dynamic 3d gaussian splatting for blurry   monocular video,cs.cv,"we present mobgs, a novel deblurring dynamic 3d gaussian splatting (3dgs) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. existing dynamic novel view synthesis (nvs) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. while recent approaches address motion-blurred inputs for nvs, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. to overcome these limitations, our mobgs introduces a novel blur-adaptive latent camera estimation (blce) method for effective latent camera trajectory estimation, improving global camera motion deblurring. in addition, we propose a physically-inspired latent camera-induced exposure estimation (lcee) method to ensure consistent deblurring of both global camera and local object motion. our mobgs framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. extensive experiments on the stereo blur dataset and real-world blurry videos show that our mobgs significantly outperforms the very recent advanced methods (dyblurf and deblur4dgs), achieving state-of-the-art performance for dynamic nvs under motion blur.",,2025-04-21,2025-05-02,"['minh-quan viet bui', 'jongmin park', 'juan luis gonzalez bello', 'jaeho moon', 'jihyong oh', 'munchurl kim']"
2504.15252,suoiai: building a dataset for aquatic invertebrates in vietnam,cs.ai cs.cv cs.lg,"understanding and monitoring aquatic biodiversity is critical for ecological health and conservation efforts. this paper proposes suoiai, an end-to-end pipeline for building a dataset of aquatic invertebrates in vietnam and employing machine learning (ml) techniques for species classification. we outline the methods for data collection, annotation, and model training, focusing on reducing annotation effort through semi-supervised learning and leveraging state-of-the-art object detection and classification models. our approach aims to overcome challenges such as data scarcity, fine-grained classification, and deployment in diverse environmental conditions.",,2025-04-21,,"['tue vo', 'lakshay sharma', 'tuan dinh', 'khuong dinh', 'trang nguyen', 'trung phan', 'minh do', 'duong vu']"
2504.16062,foresightnav: learning scene imagination for efficient exploration,cs.ro cs.cv,"understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. in this work, we propose foresightnav, a novel exploration strategy inspired by human imagination and reasoning. our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. these predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. we validate our imagination-based approach using the structured3d dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for pointnav and an spl of 67% for objectnav on the structured3d validation split. these contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.",,2025-04-22,2025-05-05,"['hardik shah', 'jiaxu xing', 'nico messikommer', 'boyang sun', 'marc pollefeys', 'davide scaramuzza']"
2504.16276,an automated pipeline for few-shot bird call classification: a case   study with the tooth-billed pigeon,cs.lg cs.ai cs.cv cs.sd,"this paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like birdnet and perch. while these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. to address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. we evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with xeno-canto recordings and a real-world test on the critically endangered tooth-billed pigeon (didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. the final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. this open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.",,2025-04-22,2025-05-02,"['abhishek jana', 'moeumu uili', 'james atherton', ""mark o'brien"", 'joe wood', 'leandra brickson']"
2504.16290,naturally computed scale invariance in the residual stream of resnet18,cs.cv cs.lg,"an important capacity in visual object recognition is invariance to image-altering variables which leave the identity of objects unchanged, such as lighting, rotation, and scale. how do neural networks achieve this? prior mechanistic interpretability research has illuminated some invariance-building circuitry in inceptionv1, but the results are limited and networks with different architectures have remained largely unexplored. this work investigates resnet18 with a particular focus on its residual stream, an architectural component which inceptionv1 lacks. we observe that many convolutional channels in intermediate blocks exhibit scale invariant properties, computed by the element-wise residual summation of scale equivariant representations: the block input's smaller-scale copy with the block pre-sum output's larger-scale copy. through subsequent ablation experiments, we attempt to causally link these neural properties with scale-robust object recognition behavior. our tentative findings suggest how the residual stream computes scale invariance and its possible role in behavior. code is available at: https://github.com/cest-andre/residual-stream-interp",,2025-04-22,2025-04-29,['andré longon']
2504.16570,countingdino: a training-free pipeline for class-agnostic counting using   unsupervised backbones,cs.cv,"class-agnostic counting (cac) aims to estimate the number of objects in images without being restricted to predefined categories. however, while current exemplar-based cac methods offer flexibility at inference time, they still rely heavily on labeled data for training, which limits scalability and generalization to many downstream use cases. in this paper, we introduce countingdino, the first training-free exemplar-based cac framework that exploits a fully unsupervised feature extractor. specifically, our approach employs self-supervised vision-only backbones to extract object-aware features, and it eliminates the need for annotated data throughout the entire proposed pipeline. at inference time, we extract latent object prototypes via roi-align from dino features and use them as convolutional kernels to generate similarity maps. these are then transformed into density maps through a simple yet effective normalization scheme. we evaluate our approach on the fsc-147 benchmark, where we consistently outperform a baseline based on an sota unsupervised object detector under the same label- and training-free setting. additionally, we achieve competitive results -- and in some cases surpass -- training-free methods that rely on supervised backbones, non-training-free unsupervised methods, as well as several fully supervised sota approaches. this demonstrates that label- and training-free cac can be both scalable and effective. code: https://lorebianchi98.github.io/countingdino/.",,2025-04-23,2025-04-30,"['giacomo pacini', 'lorenzo bianchi', 'luca ciampi', 'nicola messina', 'giuseppe amato', 'fabrizio falchi']"
2504.16612,federated endovit: pretraining vision transformers via federated   learning on endoscopic image collections,cs.cv cs.lg,"purpose: in this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. methods: inspired by the endovit study, we adapt the masked autoencoder for federated learning, enhancing it with adaptive sharpness-aware minimization (fedsam) and stochastic weight averaging (swa). our model is pretrained on the endo700k dataset collection and later fine-tuned and evaluated for tasks such as semantic segmentation, action triplet recognition, and surgical phase recognition. results: our findings demonstrate that integrating adaptive fedsam into the federated mae approach improves pretraining, leading to a reduction in reconstruction loss per patch. the application of fl-endovit in surgical downstream tasks results in performance comparable to cen-endovit. furthermore, fl-endovit exhibits advantages over cen-endovit in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. conclusion: these findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. effective collaboration requires adapting federated learning methods, such as the integration of fedsam, which can accommodate the inherent data heterogeneity across institutions. in future, exploring fl in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.",,2025-04-23,2025-05-08,"['max kirchner', 'alexander c. jenke', 'sebastian bodenstedt', 'fiona r. kolbinger', 'oliver l. saldanha', 'jakob n. kather', 'martin wagner', 'stefanie speidel']"
2504.17761,step1x-edit: a practical framework for general image editing,cs.cv,"in recent years, image editing models have witnessed remarkable and rapid development. the recent unveiling of cutting-edge multimodal models such as gpt-4o and gemini2 flash has introduced highly promising image editing capabilities. these models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. however, there is still a large gap between the open-source algorithm with these closed-source models. thus, in this paper, we aim to release a state-of-the-art image editing model, called step1x-edit, which can provide comparable performance against the closed-source models like gpt-4o and gemini2 flash. more specifically, we adopt the multimodal llm to process the reference image and the user's editing instruction. a latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. to train the model, we build a data generation pipeline to produce a high-quality dataset. for evaluation, we develop the gedit-bench, a novel benchmark rooted in real-world user instructions. experimental results on gedit-bench demonstrate that step1x-edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.",,2025-04-24,2025-05-06,"['shiyu liu', 'yucheng han', 'peng xing', 'fukun yin', 'rui wang', 'wei cheng', 'jiaqi liao', 'yingming wang', 'honghao fu', 'chunrui han', 'guopeng li', 'yuang peng', 'quan sun', 'jingwei wu', 'yan cai', 'zheng ge', 'ranchen ming', 'lei xia', 'xianfang zeng', 'yibo zhu', 'binxing jiao', 'xiangyu zhang', 'gang yu', 'daxin jiang']"
2504.18468,rgs-dr: reflective gaussian surfels with deferred rendering for shiny   objects,cs.cv,"we introduce rgs-dr, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. unlike existing methods (e.g., nerf and 3d gaussian splatting), which struggle with view-dependent effects, rgs-dr utilizes a 2d gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. by employing a multi-level cube mipmap, rgs-dr accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. a residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. experiments demonstrate that rgs-dr achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.",,2025-04-25,2025-05-05,"['georgios kouros', 'minye wu', 'tinne tuytelaars']"
2504.18768,transparentgs: fast inverse rendering of transparent objects with   gaussians,cs.gr cs.cv,"the emergence of neural and gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3d object reconstruction. nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. currently, even 3d gaussian splatting (3d-gs), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. to address this issue, we propose transparentgs, a fast inverse rendering pipeline for transparent objects based on 3d-gs. the main contributions are three-fold. firstly, an efficient representation of transparent objects, transparent gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. secondly, we leverage gaussian light field probes (gaussprobe) to encode both ambient light and nearby contents in a unified framework. thirdly, a depth-based iterative probes query (iterquery) algorithm is proposed to reduce the parallax errors in our probe-based framework. experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",10.1145/3730892,2025-04-25,2025-05-01,"['letian huang', 'dongwei ye', 'jialin dan', 'chengzhi tao', 'huiwen liu', 'kun zhou', 'bo ren', 'yuanqi li', 'yanwen guo', 'jie guo']"
2504.19174,clr-wire: towards continuous latent representations for 3d curve   wireframe generation,cs.gr cs.cv,"we introduce clr-wire, a novel framework for 3d curve-based wireframe generation that integrates geometry and topology into a unified continuous latent representation. unlike conventional methods that decouple vertices, edges, and faces, clr-wire encodes curves as neural parametric curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (vae). this unified approach facilitates joint learning and generation of both geometry and topology. to generate wireframes, we employ a flow matching model to progressively map gaussian noise to these latents, which are subsequently decoded into complete 3d wireframes. our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for cad design, geometric reconstruction, and 3d content creation.",,2025-04-27,2025-05-03,"['xueqi ma', 'yilin liu', 'tianlong gao', 'qirui huang', 'hui huang']"
2504.19186,lrfusionpr: a polar bev-based lidar-radar fusion network for place   recognition,cs.cv cs.ro,"in autonomous driving, place recognition is critical for global localization in gps-denied environments. lidar and radar-based place recognition methods have garnered increasing attention, as lidar provides precise ranging, whereas radar excels in adverse weather resilience. however, effectively leveraging lidar-radar fusion for place recognition remains challenging. the noisy and sparse nature of radar data limits its potential to further improve recognition accuracy. in addition, heterogeneous radar configurations complicate the development of unified cross-modality fusion frameworks. in this paper, we propose lrfusionpr, which improves recognition accuracy and robustness by fusing lidar with either single-chip or scanning radar. technically, a dual-branch network is proposed to fuse different modalities within the unified polar coordinate bird's eye view (bev) representation. in the fusion branch, cross-attention is utilized to perform cross-modality feature interactions. the knowledge from the fusion branch is simultaneously transferred to the distillation branch, which takes radar as its only input to further improve the robustness. ultimately, the descriptors from both branches are concatenated, producing the multimodal global descriptor for place retrieval. extensive evaluations on multiple datasets demonstrate that our lrfusionpr achieves accurate place recognition, while maintaining robustness under varying weather conditions. our open-source code will be released at https://github.com/qizs-bit/lrfusionpr.",,2025-04-27,2025-05-07,"['zhangshuo qi', 'luqi cheng', 'zijie zhou', 'guangming xiong']"
2504.19244,semantic-aligned learning with collaborative refinement for unsupervised   vi-reid,cs.cv,"unsupervised visible-infrared person re-identification (usl-vi-reid) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. however, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. this insight results in insufficient modality-shared learning when only global features are optimized. to address this issue, we propose a semantic-aligned learning with collaborative refinement (salcr) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. specifically, we first introduce a dual association with global learning (dagi) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. afterward, a fine-grained semantic-aligned learning (fgsal) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. to alleviate the side-effects arising from noisy pseudo-labels, we propose a global-part collaborative refinement (gpcr) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. our code is available at \href{https://github.com/franklinlingfeng/code-for-salcr}.",,2025-04-27,2025-05-05,"['de cheng', 'lingfeng he', 'nannan wang', 'dingwen zhang', 'xinbo gao']"
2504.19258,opal: visibility-aware lidar-to-openstreetmap place recognition via   adaptive radial fusion,cs.cv cs.ro,"lidar place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. existing approaches predominantly depend on pre-built 3d dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. in this paper, we propose opal, a novel network for lidar place recognition that leverages openstreetmap (osm) as a lightweight and up-to-date prior. our key innovation lies in bridging the domain disparity between sparse lidar scans and structured osm data through two carefully designed components. first, a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning. second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. extensive experiments on the kitti and kitti-360 datasets demonstrate opal's superiority, achieving 15.98% higher recall at @1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. code and datasets will be publicly available.",,2025-04-27,2025-04-30,"['shuhao kang', 'martin y. liao', 'yan xia', 'olaf wysocki', 'boris jutzi', 'daniel cremers']"
2504.19267,vist-gpt: ushering in the era of visual storytelling with llms?,cs.cl cs.ai cs.cv cs.lg,"visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. this paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. leveraging the large-scale visual storytelling (vist) dataset, our vist-gpt model produces visually grounded, contextually appropriate narratives. we address the limitations of traditional evaluation metrics, such as bleu, meteor, rouge, and cider, which are not suitable for this task. instead, we utilize rovist and groovist, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. these metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.",,2025-04-27,2025-05-03,"['mohamed gado', 'towhid taliee', 'muhammad memon', 'dmitry ignatov', 'radu timofte']"
2504.19589,magnifier: a multi-grained neural network-based architecture for burned   area delineation,cs.cv eess.iv,"in crisis management and remote sensing, image segmentation plays a crucial role, enabling tasks like disaster response and emergency planning by analyzing visual data. neural networks are able to analyze satellite acquisitions and determine which areas were affected by a catastrophic event. the problem in their development in this context is the data scarcity and the lack of extensive benchmark datasets, limiting the capabilities of training large neural network models. in this paper, we propose a novel methodology, namely magnifier, to improve segmentation performance with limited data availability. the magnifier methodology is applicable to any existing encoder-decoder architecture, as it extends a model by merging information at different contextual levels through a dual-encoder approach: a local and global encoder. magnifier analyzes the input data twice using the dual-encoder approach. in particular, the local and global encoders extract information from the same input at different granularities. this allows magnifier to extract more information than the other approaches given the same set of input images. magnifier improves the quality of the results of +2.65% on average iou while leading to a restrained increase in terms of the number of trainable parameters compared to the original model. we evaluated our proposed approach with state-of-the-art burned area segmentation models, demonstrating, on average, comparable or better performances in less than half of the gflops.",10.1109/jstars.2025.3565819,2025-04-28,,"['daniele rege cambrin', 'luca colomba', 'paolo garza']"
2504.19684,clearvision: leveraging cyclegan and siglip-2 for robust all-weather   classification in traffic camera imagery,cs.cv cs.lg,"adverse weather conditions challenge safe transportation, necessitating robust real-time weather detection from traffic camera imagery. we propose a novel framework combining cyclegan-based domain adaptation with efficient contrastive learning to enhance weather classification, particularly in low-light nighttime conditions. our approach leverages the lightweight siglip-2 model, which employs pairwise sigmoid loss to reduce computational demands, integrated with cyclegan to transform nighttime images into day-like representations while preserving weather cues. evaluated on an iowa department of transportation dataset, the baseline eva-02 model with clip achieves a per-class overall accuracy of 96.55\% across three weather conditions (no precipitation, rain, snow) and a day/night overall accuracy of 96.55\%, but shows a significant day-night gap (97.21\% day vs.\ 63.40\% night). with cyclegan, eva-02 improves to 97.01\% per-class accuracy and 96.85\% day/night accuracy, boosting nighttime performance to 82.45\%. our vision-siglip-2 + text-siglip-2 + cyclegan + contrastive configuration excels in nighttime scenarios, achieving the highest nighttime accuracy of 85.90\%, with 94.00\% per-class accuracy and 93.35\% day/night accuracy. this model reduces training time by 89\% (from 6 hours to 40 minutes) and inference time by 80\% (from 15 seconds to 3 seconds) compared to eva-02. by narrowing the day-night performance gap from 33.81 to 8.90 percentage points, our framework provides a scalable, efficient solution for all-weather classification using existing camera infrastructure.",,2025-04-28,2025-05-01,"['anush lakshman sivaraman', 'kojo adu-gyamfi', 'ibne farabi shihab', 'anuj sharma']"
2504.19735,measuring train driver performance as key to approval of driverless   trains,cs.cv,"points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in annex i of implementing regulation (eu) no. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. the human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. however, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. this article summarizes the data published so far. this article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. the measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. the measured values are reaction time and distance to the obstacle. the goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. the dataset with supplementing information and literature is published on https://data.fid-move.de/de/dataset/atosensedata",,2025-04-28,2025-04-29,"['rustam tagiew', 'prasannavenkatesh balaji']"
2504.20091,videomultiagents: a multi-agent framework for video question answering,cs.cv cs.ma,"video question answering (vqa) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. however, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. to address this limitation, we introduce videomultiagents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. it enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. experimental results demonstrate that our method achieves state-of-the-art performance on intent-qa (79.0%, +6.2% over previous sota), egoschema subset (75.4%, +3.4%), and next-qa (79.6%, +0.4%). the source code is available at https://github.com/panasonicconnect/videomultiagents.",,2025-04-25,2025-04-29,"['noriyuki kugo', 'xiang li', 'zixin li', 'ashish gupta', 'arpandeep khatua', 'nidhish jain', 'chaitanya patel', 'yuta kyuragi', 'yasunori ishii', 'masamoto tanabiki', 'kazuki kozuka', 'ehsan adeli']"
2504.20379,gsfeatloc: visual localization using feature correspondence on 3d   gaussian splatting,cs.cv cs.ro,"in this paper, we present a method for localizing a query image with respect to a precomputed 3d gaussian splatting (3dgs) scene representation. first, the method uses 3dgs to render a synthetic rgbd image at some initial pose estimate. second, it establishes 2d-2d correspondences between the query image and this synthetic image. third, it uses the depth map to lift the 2d-2d correspondences to 2d-3d correspondences and solves a perspective-n-point (pnp) problem to produce a final pose estimate. results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. results also show that our method tolerates large errors in the initial pose estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\deg} in rotation and 0.05 units in translation on 90% of images from the synthetic nerf and mip-nerf360 datasets and on 42% of images from the more challenging tanks and temples dataset.",,2025-04-28,2025-04-30,"['jongwon lee', 'timothy bretl']"
2504.20438,pixelhacker: image inpainting with structural and semantic consistency,cs.cv,"image inpainting is a fundamental research area between image editing and image generation. recent state-of-the-art (sota) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. however, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. to address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named pixelhacker. specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain pixelhacker. extensive experiments show that pixelhacker comprehensively outperforms the sota on a wide range of datasets (places2, celeba-hq, and ffhq) and exhibits remarkable consistency in both structure and semantics. project page at https://hustvl.github.io/pixelhacker.",,2025-04-29,2025-04-30,"['ziyang xu', 'kangsheng duan', 'xiaolei shen', 'zhifeng ding', 'wenyu liu', 'xiaohu ruan', 'xiaoxin chen', 'xinggang wang']"
2504.20466,lmme3dhf: benchmarking and evaluating multimodal 3d human face   generation with lmms,cs.cv,"the rapid advancement in generative artificial intelligence have enabled the creation of 3d human faces (hfs) for applications including media production, virtual reality, security, healthcare, and game development, etc. however, assessing the quality and realism of these ai-generated 3d human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. to this end, we conduct a comprehensive study on the quality assessment of ai-generated 3d human faces. we first introduce gen3dhf, a large-scale benchmark comprising 2,000 videos of ai-generated 3d human faces along with 4,000 mean opinion scores (mos) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. based on gen3dhf, we propose lmme3dhf, a large multimodal model (lmm)-based metric for evaluating 3dhf capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. experimental results show that lmme3dhf achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for ai-generated 3d human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. both the gen3dhf database and the lmme3dhf will be released upon the publication.",,2025-04-29,2025-05-05,"['woo yi yang', 'jiarui wang', 'sijing wu', 'huiyu duan', 'yuxin zhu', 'liu yang', 'kang fu', 'guangtao zhai', 'xiongkuo min']"
2504.20468,antidote: a unified framework for mitigating lvlm hallucinations in   counterfactual presupposition and object perception,cs.cv,"large vision-language models (lvlms) have achieved impressive results across various cross-modal tasks. however, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlooking the task question itself. this paper discusses the vulnerability of lvlms in solving counterfactual presupposition questions (cpqs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. to this end, we introduce ""antidote"", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. it leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouple the mitigation process into a preference optimization problem. furthermore, we construct ""cp-bench"", a novel benchmark to evaluate lvlms' ability to correctly handle cpqs and produce factual responses. applied to the llava series, antidote can simultaneously enhance performance on cp-bench by over 50%, pope by 1.8-3.3%, and chair & shr by 30-50%, all without relying on external supervision from stronger lvlms or human feedback and introducing noticeable catastrophic forgetting issues.",,2025-04-29,2025-05-07,"['yuanchen wu', 'lu zhang', 'hang yao', 'junlong du', 'ke yan', 'shouhong ding', 'yunsheng wu', 'xiaoqiang li']"
2504.20682,og-hfyolo :orientation gradient guidance and heterogeneous feature   fusion for deformation table cell instance segmentation,cs.cv,"table structure recognition is a key task in document analysis. however, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. to obtain fine-grained spatial coordinates of cells, we propose the og-hfyolo model, which enhances the edge response by gradient orientation-aware extractor, combines a heterogeneous kernel cross fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named deformation wired table (dwtal). experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. the dataset and the source code are open source: https://github.com/justliulong/oghfyolo.",,2025-04-29,2025-05-03,"['long liu', 'cihui yang']"
2504.20898,cbm-rag: demonstrating enhanced interpretability in radiology report   generation with multi-agent rag and concept bottleneck models,cs.ai cs.cv cs.ir,"advancements in generative artificial intelligence (ai) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. this paper presents an automated radiology report generation framework that combines concept bottleneck models (cbms) with a multi-agent retrieval-augmented generation (rag) system to bridge ai performance with clinical explainability. cbms map chest x-ray features to human-understandable clinical concepts, enabling transparent disease classification. meanwhile, the rag system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. this framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.",10.1145/3731406.3731970,2025-04-29,2025-05-04,"['hasan md tusfiqur alam', 'devansh srivastav', 'abdulrahman mohamed selim', 'md abdul kadir', 'md moktadirul hoque shuvo', 'daniel sonntag']"
2504.20923,end-to-end audio deepfake detection from raw waveforms: a rawnet-based   approach with cross-dataset evaluation,cs.sd cs.cv eess.as,"audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. in this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. our model, rawnetlite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. to enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts focal loss to emphasize difficult or ambiguous samples. we further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. the proposed model achieves over 99.7% f1 and 0.25% eer on in-domain data (fakeorreal), and up to 83.4% f1 with 16.4% eer on a challenging out-of-distribution test set (avspoof2021 + codecfake). these findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. code and pretrained models are available at https://iplab.dmi.unict.it/mfs/deepfakes/paperrawnet2025/.",,2025-04-29,2025-04-30,"['andrea di pierno', 'luca guarnera', 'dario allegra', 'sebastiano battiato']"
2504.20948,ds_fusionnet: dynamic dual-stream fusion with bidirectional knowledge   distillation for plant disease recognition,cs.cv,"given the severe challenges confronting the global growth security of economic crops, precise identification and prevention of plant diseases has emerged as a critical issue in artificial intelligence-enabled agricultural technology. to address the technical challenges in plant disease recognition, including small-sample learning, leaf occlusion, illumination variations, and high inter-class similarity, this study innovatively proposes a dynamic dual-stream fusion network (ds_fusionnet). the network integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation strategy, significantly enhancing recognition accuracy. experimental results demonstrate that ds_fusionnet achieves classification accuracies exceeding 90% using only 10% of the plantdisease and cifar-10 datasets, while maintaining 85% accuracy on the complex plantwild dataset, exhibiting exceptional generalization capabilities. this research not only provides novel technical insights for fine-grained image classification but also establishes a robust foundation for precise identification and management of agricultural diseases.",,2025-04-29,2025-04-30,"['yanghui song', 'chengfu yang']"
2504.21033,transcending dimensions using generative ai: real-time 3d model   generation in augmented reality,cs.gr cs.ai cs.cv,"traditional 3d modeling requires technical expertise, specialized software, and time-intensive processes, making it inaccessible for many users. our research aims to lower these barriers by combining generative ai and augmented reality (ar) into a cohesive system that allows users to easily generate, manipulate, and interact with 3d models in real time, directly within ar environments. utilizing cutting-edge ai models like shap-e, we address the complex challenges of transforming 2d images into 3d representations in ar environments. key challenges such as object isolation, handling intricate backgrounds, and achieving seamless user interaction are tackled through advanced object detection methods, such as mask r-cnn. evaluation results from 35 participants reveal an overall system usability scale (sus) score of 69.64, with participants who engaged with ar/vr technologies more frequently rating the system significantly higher, at 80.71. this research is particularly relevant for applications in gaming, education, and ar-based e-commerce, offering intuitive, model creation for users without specialized skills.",,2025-04-27,,"['majid behravan', 'maryam haghani', 'denis gracanin']"
2504.21040,can a large language model assess urban design quality? evaluating   walkability metrics across expertise levels,cs.cv,"urban street environments are vital to supporting human activity in public spaces. the emergence of big data, such as street view images (svis) combined with multimodal large language models (mllms), is transforming how researchers and practitioners investigate, measure, and evaluate semantic and visual elements of urban environments. considering the low threshold for creating automated evaluative workflows using mllms, it is crucial to explore both the risks and opportunities associated with these probabilistic models. in particular, the extent to which the integration of expert knowledge can influence the performance of mllms in evaluating the quality of urban design has not been fully explored. this study sets out an initial exploration of how integrating more formal and structured representations of expert urban design knowledge into the input prompts of an mllm (chatgpt-4) can enhance the model's capability and reliability in evaluating the walkability of built environments using svis. we collect walkability metrics from the existing literature and categorize them using relevant ontologies. we then select a subset of these metrics, focusing on the subthemes of pedestrian safety and attractiveness, and develop prompts for the mllm accordingly. we analyze the mllm's ability to evaluate svi walkability subthemes through prompts with varying levels of clarity and specificity regarding evaluation criteria. our experiments demonstrate that mllms are capable of providing assessments and interpretations based on general knowledge and can support the automation of multimodal image-text evaluations. however, they generally provide more optimistic scores and can make mistakes when interpreting the provided metrics, resulting in incorrect evaluations. by integrating expert knowledge, the mllm's evaluative performance exhibits higher consistency and concentration.",,2025-04-28,,"['chenyi cai', 'kosuke kuriyama', 'youlong gu', 'filip biljecki', 'pieter herthogs']"
2504.21067,gauss-mi: gaussian splatting shannon mutual information for active 3d   reconstruction,cs.gr cs.cv cs.ro,"this research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3d reconstruction. visual quality is a critical aspect of 3d reconstruction. recent advancements such as neural radiance fields (nerf) and 3d gaussian splatting (3dgs) have notably enhanced the image rendering quality of reconstruction models. nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. to address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each gaussian. leveraging shannon mutual information, we formulate a criterion, gaussian splatting shannon mutual information (gauss-mi), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. gauss-mi is implemented within an active reconstruction system integrated with a view and motion planner. extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",,2025-04-29,,"['yuhan xie', 'yixi cai', 'yinqiang zhang', 'lei yang', 'jia pan']"
2504.21136,legilimens: performant video analytics on the system-on-chip edge,cs.cv cs.lg,"continually retraining models has emerged as a primary technique to enable high-accuracy video analytics on edge devices. yet, existing systems employ such adaptation by relying on the spare compute resources that traditional (memory-constrained) edge servers afford. in contrast, mobile edge devices such as drones and dashcams offer a fundamentally different resource profile: weak(er) compute with abundant unified memory pools. we present legilimens, a continuous learning system for the mobile edge's system-on-chip gpus. our driving insight is that visually distinct scenes that require retraining exhibit substantial overlap in model embeddings; if captured into a base model on device memory, specializing to each new scene can become lightweight, requiring very few samples. to practically realize this approach, legilimens presents new, compute-efficient techniques to (1) select high-utility data samples for retraining specialized models, (2) update the base model without complete retraining, and (3) time-share compute resources between retraining and live inference for maximal accuracy. across diverse workloads, legilimens lowers retraining costs by 2.8-10x compared to existing systems, resulting in 18-45% higher accuracies.",,2025-04-29,,"['murali ramanujam', 'yinwei dai', 'kyle jamieson', 'ravi netravali']"
2504.21154,emotion recognition in contemporary dance performances using laban   movement analysis,cs.cv cs.ai,"this paper presents a novel framework for emotion recognition in contemporary dance by improving existing laban movement analysis (lma) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. our approach extracts expressive characteristics from 3d keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including random forests and support vector machines. additionally, we provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human--computer interaction, with a highest accuracy of 96.85\%.",,2025-04-29,,"['muhammad turab', 'philippe colantoni', 'damien muselet', 'alain tremeau']"
2504.21166,dance style recognition using laban movement analysis,cs.cv cs.ai,"the growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. this study focuses on dance style recognition using features extracted using laban movement analysis. previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. this gap highlights the need for a method that can add temporal context to lma features. for this, we introduce a novel pipeline which combines 3d pose estimation, 3d human mesh reconstruction, and floor aware body modeling to effectively extract lma features. to address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. these features are then used to train various machine learning methods for classification, and their explainability explainable ai methods to evaluate the contribution of each feature to classification performance. our proposed method achieves a highest classification accuracy of 99.18\% which shows that the addition of temporal context significantly improves dance style recognition performance.",,2025-04-29,,"['muhammad turab', 'philippe colantoni', 'damien muselet', 'alain tremeau']"
2504.21188,light weight cnn for classification of brain tumors from mri images,eess.iv cs.ai cs.cv cs.lg,"this study presents a convolutional neural network (cnn)-based approach for the multi-class classification of brain tumors using magnetic resonance imaging (mri) scans. we utilize a publicly available dataset containing mri images categorized into four classes: glioma, meningioma, pituitary tumor, and no tumor. our primary objective is to build a light weight deep learning model that can automatically classify brain tumor types with high accuracy. to achieve this goal, we incorporate image preprocessing steps, including normalization, data augmentation, and a cropping technique designed to reduce background noise and emphasize relevant regions. the cnn architecture is optimized through hyperparameter tuning using keras tuner, enabling systematic exploration of network parameters. to ensure reliable evaluation, we apply 5-fold cross-validation, where each hyperparameter configuration is evaluated across multiple data splits to mitigate overfitting. experimental results demonstrate that the proposed model achieves a classification accuracy of 98.78%, indicating its potential as a diagnostic aid in clinical settings. the proposed method offers a low-complexity yet effective solution for assisting in early brain tumor diagnosis.",,2025-04-29,2025-05-05,['natnael alemayehu']
2504.21194,geolocating earth imagery from iss: integrating machine learning with   astronaut photography for enhanced geographic mapping,cs.cv cs.ai,"this paper presents a novel approach to geolocating images captured from the international space station (iss) using advanced machine learning algorithms. despite having precise iss coordinates, the specific earth locations depicted in astronaut-taken photographs often remain unidentified. our research addresses this gap by employing three distinct image processing pipelines: a neural network based approach, a sift based method, and gpt-4 model. each pipeline is tailored to process high-resolution iss imagery, identifying both natural and man-made geographical features. through extensive evaluation on a diverse dataset of over 140 iss images, our methods demonstrate significant promise in automated geolocation with varied levels of success. the nn approach showed a high success rate in accurately matching geographical features, while the sift pipeline excelled in processing zoomed-in images. gpt-4 model provided enriched geographical descriptions alongside location predictions. this research contributes to the fields of remote sensing and earth observation by enhancing the accuracy and efficiency of geolocating space-based imagery, thereby aiding environmental monitoring and global mapping efforts.",,2025-04-29,,"['vedika srivastava', 'hemant kumar singh', 'jaisal singh']"
2504.21226,memeblip2: a novel lightweight multimodal system to detect harmful memes,cs.cv cs.ai,"memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. in this paper, we introduces memeblip2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. we build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. using blip-2 as the core vision-language model, our system is evaluated on the pridemm datasets. the results show that memeblip2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.",,2025-04-29,2025-05-06,"['jiaqi liu', 'ran tong', 'aowei shen', 'shuzheng li', 'changlin yang', 'lisha xu']"
2504.21231,t2id-cas: diffusion model and class aware sampling to mitigate class   imbalance in neck ultrasound anatomical landmark detection,cs.cv cs.ai cs.lg,"neck ultrasound (us) plays a vital role in airway management by providing non-invasive, real-time imaging that enables rapid and precise interventions. deep learning-based anatomical landmark detection in neck us can further facilitate procedural efficiency. however, class imbalance within datasets, where key structures like tracheal rings and vocal folds are underrepresented, presents significant challenges for object detection models. to address this, we propose t2id-cas, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to generate high-quality synthetic samples for underrepresented classes. this approach, rarely explored in the ultrasound domain, improves the representation of minority classes. experimental results using yolov9 for anatomical landmark detection in neck us demonstrated that t2id-cas achieved a mean average precision of 88.2, significantly surpassing the baseline of 66. this highlights its potential as a computationally efficient and scalable solution for mitigating class imbalance in ai-assisted ultrasound-guided interventions.",,2025-04-29,,"['manikanta varaganti', 'amulya vankayalapati', 'nour awad', 'gregory r. dion', 'laura j. brattain']"
2504.21247,subject information extraction for novelty detection with domain shifts,cs.cv,"unsupervised novelty detection (und), aimed at identifying novel samples, is essential in fields like medical diagnosis, cybersecurity, and industrial quality control. most existing und methods assume that the training data and testing normal data originate from the same domain and only consider the distribution variation between training data and testing data. however, in real scenarios, it is common for normal testing and training data to originate from different domains, a challenge known as domain shift. the discrepancies between training and testing data often lead to incorrect classification of normal data as novel by existing methods. a typical situation is that testing normal data and training data describe the same subject, yet they differ in the background conditions. to address this problem, we introduce a novel method that separates subject information from background variation encapsulating the domain information to enhance detection performance under domain shifts. the proposed method minimizes the mutual information between the representations of the subject and background while modelling the background variation using a deep gaussian mixture model, where the novelty detection is conducted on the subject representations solely and hence is not affected by the variation of domains. extensive experiments demonstrate that our model generalizes effectively to unseen domains and significantly outperforms baseline methods, especially under substantial domain shifts between training and testing data.",,2025-04-29,,"['yangyang qu', 'dazhi fu', 'jicong fan']"
2504.21248,multi-modal transfer learning for dynamic facial emotion recognition in   the wild,cs.cv,"facial expression recognition (fer) is a subset of computer vision with important applications for human-computer-interaction, healthcare, and customer service. fer represents a challenging problem-space because accurate classification requires a model to differentiate between subtle changes in facial features. in this paper, we examine the use of multi-modal transfer learning to improve performance on a challenging video-based fer dataset, dynamic facial expression in-the-wild (dfew). using a combination of pretrained resnets, openpose, and omnivec networks, we explore the impact of cross-temporal, multi-modal features on classification accuracy. ultimately, we find that these finely-tuned multi-modal feature generators modestly improve accuracy of our transformer-based classification model.",,2025-04-29,,"['ezra engel', 'lishan li', 'chris hudy', 'robert schleusner']"
2504.21263,embracing collaboration over competition: condensing multiple prompts   for visual in-context learning,cs.cv cs.lg cs.mm,"visual in-context learning (vicl) enables adaptively solving vision tasks by leveraging pixel demonstrations, mimicking human-like task completion through analogy. prompt selection is critical in vicl, but current methods assume the existence of a single ""ideal"" prompt in a pool of candidates, which in practice may not hold true. multiple suitable prompts may exist, but individually they often fall short, leading to difficulties in selection and the exclusion of useful context. to address this, we propose a new perspective: prompt condensation. rather than relying on a single prompt, candidate prompts collaborate to efficiently integrate informative contexts without sacrificing resolution. we devise condenser, a lightweight external plugin that compresses relevant fine-grained context across multiple prompts. optimized end-to-end with the backbone, condenser ensures accurate integration of contextual cues. experiments demonstrate condenser outperforms state-of-the-arts across benchmark tasks, showing superior context compression, scalability with more prompts, and enhanced computational efficiency compared to ensemble methods, positioning it as a highly competitive solution for vicl. code is open-sourced at https://github.com/gimpong/cvpr25-condenser.",,2025-04-29,,"['jinpeng wang', 'tianci luo', 'yaohua zha', 'yan feng', 'ruisheng luo', 'bin chen', 'tao dai', 'long chen', 'yaowei wang', 'shu-tao xia']"
2504.21266,cocodiff: diversifying skeleton action features via coarse-fine   text-co-guided latent diffusion,cs.cv,"in action recognition tasks, feature diversity is essential for enhancing model generalization and performance. existing methods typically promote feature diversity by expanding the training data in the sample space, which often leads to inefficiencies and semantic inconsistencies. to overcome these problems, we propose a novel coarse-fine text co-guidance diffusion model (cocodiff). cocodiff generates diverse yet semantically consistent features in the latent space by leveraging diffusion and multi-granularity textual guidance. specifically, our approach feeds spatio-temporal features extracted from skeleton sequences into a latent diffusion model to generate diverse action representations. meanwhile, we introduce a coarse-fine text co-guided strategy that leverages textual information from large language models (llms) to ensure semantic consistency between the generated features and the original inputs. it is noted that cocodiff operates as a plug-and-play auxiliary module during training, incurring no additional inference cost. extensive experiments demonstrate that cocodiff achieves sota performance on skeleton-based action recognition benchmarks, including ntu rgb+d, ntu rgb+d 120 and kinetics-skeleton.",,2025-04-29,,"['zhifu zhao', 'hanyang hua', 'jianan li', 'shaoxin wu', 'fu li', 'yangtao zhou', 'yang li']"
2504.21281,mamba based feature extraction and adaptive multilevel feature fusion   for 3d tumor segmentation from multi-modal medical image,cs.cv,"multi-modal 3d medical image segmentation aims to accurately identify tumor regions across different modalities, facing challenges from variations in image intensity and tumor morphology. traditional convolutional neural network (cnn)-based methods struggle with capturing global features, while transformers-based methods, despite effectively capturing global context, encounter high computational costs in 3d medical image segmentation. the mamba model combines linear scalability with long-distance modeling, making it a promising approach for visual representation learning. however, mamba-based 3d multi-modal segmentation still struggles to leverage modality-specific features and fuse complementary information effectively. in this paper, we propose a mamba based feature extraction and adaptive multilevel feature fusion for 3d tumor segmentation using multi-modal medical image. we first develop the specific modality mamba encoder to efficiently extract long-range relevant features that represent anatomical and pathological structures present in each modality. moreover, we design an bi-level synergistic integration block that dynamically merges multi-modal and multi-level complementary features by the modality attention and channel attention learning. lastly, the decoder combines deep semantic information with fine-grained details to generate the tumor segmentation map. experimental results on medical image datasets (pet/ct and mri multi-sequence) show that our approach achieve competitive performance compared to the state-of-the-art cnn, transformer, and mamba-based approaches.",,2025-04-29,,"['zexin ji', 'beiji zou', 'xiaoyan kui', 'hua li', 'pierre vera', 'su ruan']"
2504.21292,can we achieve efficient diffusion without self-attention? distilling   self-attention into convolutions,cs.cv,"contemporary diffusion models built upon u-net or diffusion transformer (dit) architectures have revolutionized image generation through transformer-based attention mechanisms. the prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. this suggests that global interactions in self-attention may be less critical than commonly assumed.driven by this, we propose \(\delta\)convfusion to replace conventional self-attention modules with pyramid convolution blocks (\(\delta\)convblocks).by distilling attention patterns into localized convolutional operations while keeping other components frozen, \(\delta\)convfusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\times$ and surpassing linfusion by 5.42$\times$ in efficiency--all without compromising generative fidelity.",,2025-04-29,,"['ziyi dong', 'chengxing zhou', 'weijian deng', 'pengxu wei', 'xiangyang ji', 'liang lin']"
2504.21294,learning multi-view multi-class anomaly detection,cs.cv,"the latest trend in anomaly detection is to train a unified model instead of training a separate model for each category. however, existing multi-class anomaly detection (mcad) models perform poorly in multi-view scenarios because they often fail to effectively model the relationships and complementary information among different views. in this paper, we introduce a multi-view multi-class anomaly detection model (mvmcad), which integrates information from multiple views to accurately identify anomalies. specifically, we propose a semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added before the frozen encoder, enabling stable cross-view feature modeling and efficient adaptation for improved anomaly detection. furthermore, we propose an anomaly amplification module (aam) that models global token interactions and suppresses normal regions to enhance anomaly signals, leading to improved detection performance in multi-view settings. finally, we propose a cross-feature loss that aligns shallow encoder features with deep decoder features and vice versa, enhancing the model's sensitivity to anomalies at different semantic levels under multi-view scenarios. extensive experiments on the real-iad dataset for multi-view multi-class anomaly detection validate the effectiveness of our approach, achieving state-of-the-art performance of 91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level, respectively.",,2025-04-29,,"['qianzi yu', 'yang cao', 'yu kang']"
2504.21302,cmd: constraining multimodal distribution for domain adaptation in   stereo matching,cs.cv cs.ro,"recently, learning-based stereo matching methods have achieved great improvement in public benchmarks, where soft argmin and smooth l1 loss play a core contribution to their success. however, in unsupervised domain adaptation scenarios, we observe that these two operations often yield multimodal disparity probability distributions in target domains, resulting in degraded generalization. in this paper, we propose a novel approach, constrain multi-modal distribution (cmd), to address this issue. specifically, we introduce \textit{uncertainty-regularized minimization} and \textit{anisotropic soft argmin} to encourage the network to produce predominantly unimodal disparity distributions in the target domain, thereby improving prediction accuracy. experimentally, we apply the proposed method to multiple representative stereo-matching networks and conduct domain adaptation from synthetic data to unlabeled real-world scenes. results consistently demonstrate improved generalization in both top-performing and domain-adaptable stereo-matching models. the code for cmd will be available at: \href{https://github.com/gallenszl/cmd}{https://github.com/gallenszl/cmd}.",,2025-04-30,,"['zhelun shen', 'zhuo li', 'chenming wu', 'zhibo rao', 'lina liu', 'yuchao dai', 'liangjun zhang']"
2504.21307,the dual power of interpretable token embeddings: jailbreaking attacks   and defenses for diffusion model unlearning,cs.cv,"despite the remarkable generalization capabilities of diffusion models, recent studies have shown that these models can memorize and generate harmful content when prompted with specific text instructions. although fine-tuning approaches have been developed to mitigate this issue by unlearning harmful concepts, these methods can be easily circumvented through jailbreaking attacks. this indicates that the harmful concept has not been fully erased from the model. however, existing attack methods, while effective, lack interpretability regarding why unlearned models still retain the concept, thereby hindering the development of defense strategies. in this work, we address these limitations by proposing an attack method that learns an orthogonal set of interpretable attack token embeddings. the attack token embeddings can be decomposed into human-interpretable textual elements, revealing that unlearned models still retain the target concept through implicit textual components. furthermore, these attack token embeddings are robust and transferable across text prompts, initial noises, and unlearned models. finally, leveraging this diverse set of embeddings, we design a defense method applicable to both our proposed attack and existing attack methods. experimental results demonstrate the effectiveness of both our attack and defense strategies.",,2025-04-30,,"['siyi chen', 'yimeng zhang', 'sijia liu', 'qing qu']"
2504.21308,aghi-qa: a subjective-aligned dataset and metric for ai-generated human   images,cs.cv,"the rapid development of text-to-image (t2i) generation approaches has attracted extensive interest in evaluating the quality of generated images, leading to the development of various quality assessment methods for general-purpose t2i outputs. however, existing image quality assessment (iqa) methods are limited to providing global quality scores, failing to deliver fine-grained perceptual evaluations for structurally complex subjects like humans, which is a critical challenge considering the frequent anatomical and textural distortions in ai-generated human images (aghis). to address this gap, we introduce aghi-qa, the first large-scale benchmark specifically designed for quality assessment of aghis. the dataset comprises 4,000 images generated from 400 carefully crafted text prompts using 10 state of-the-art t2i models. we conduct a systematic subjective study to collect multidimensional annotations, including perceptual quality scores, text-image correspondence scores, visible and distorted body part labels. based on aghi-qa, we evaluate the strengths and weaknesses of current t2i methods in generating human images from multiple dimensions. furthermore, we propose aghi-assessor, a novel quality metric that integrates the large multimodal model (lmm) with domain-specific human features for precise quality prediction and identification of visible and distorted body parts in aghis. extensive experimental results demonstrate that aghi-assessor showcases state-of-the-art performance, significantly outperforming existing iqa methods in multidimensional quality assessment and surpassing leading lmms in detecting structural distortions in aghis.",,2025-04-30,,"['yunhao li', 'sijing wu', 'wei sun', 'zhichao zhang', 'yucheng zhu', 'zicheng zhang', 'huiyu duan', 'xiongkuo min', 'guangtao zhai']"
2504.21309,an evaluation of a visual question answering strategy for zero-shot   facial expression recognition in still images,cs.cv,"facial expression recognition (fer) is a key research area in computer vision and human-computer interaction. despite recent advances in deep learning, challenges persist, especially in generalizing to new scenarios. in fact, zero-shot fer significantly reduces the performance of state-of-the-art fer models. to address this problem, the community has recently started to explore the integration of knowledge from large language models for visual tasks. in this work, we evaluate a broad collection of locally executed visual language models (vlms), avoiding the lack of task-specific knowledge by adopting a visual question answering strategy. we compare the proposed pipeline with state-of-the-art fer models, both integrating and excluding vlms, evaluating well-known fer benchmarks: affectnet, ferplus, and raf-db. the results show excellent performance for some vlms in zero-shot fer scenarios, indicating the need for further exploration to improve fer generalization.",,2025-04-30,,"['modesto castrillón-santana', 'oliverio j santana', 'david freire-obregón', 'daniel hernández-sosa', 'javier lorenzo-navarro']"
2504.21325,text-conditioned diffusion model for high-fidelity korean font   generation,cs.cv,"automatic font generation (afg) is the process of creating a new font using only a few examples of the style images. generating fonts for complex languages like korean and chinese, particularly in handwritten styles, presents significant challenges. traditional afgs, like generative adversarial networks (gans) and variational auto-encoders (vaes), are usually unstable during training and often face mode collapse problems. they also struggle to capture fine details within font images. to address these problems, we present a diffusion-based afg method which generates high-quality, diverse korean font images using only a single reference image, focusing on handwritten and printed styles. our approach refines noisy images incrementally, ensuring stable training and visually appealing results. a key innovation is our text encoder, which processes phonetic representations to generate accurate and contextually correct characters, even for unseen characters. we used a pre-trained style encoder from dg font to effectively and accurately encode the style images. to further enhance the generation quality, we used perceptual loss that guides the model to focus on the global style of generated images. experimental results on over 2000 korean characters demonstrate that our model consistently generates accurate and detailed font images and outperforms benchmark methods, making it a reliable tool for generating authentic korean fonts across different styles.",,2025-04-30,,"['abdul sami', 'avinash kumar', 'irfanullah memon', 'youngwon jo', 'muhammad rizwan', 'jaeyoung choi']"
2504.21331,towards space group determination from ebsd patterns: the role of deep   learning and high-throughput dynamical simulations,cond-mat.mtrl-sci cs.cv,"the design of novel materials hinges on the understanding of structure-property relationships. however, in recent times, our capability to synthesize a large number of materials has outpaced our speed at characterizing them. while the overall chemical constituents can be readily known during synthesis, the structural evolution and characterization of newly synthesized samples remains a bottleneck for the ultimate goal of high throughput nanomaterials discovery. thus, scalable methods for crystal symmetry determination that can analyze a large volume of material samples within a short time-frame are especially needed. kikuchi diffraction in the sem is a promising technique for this due to its sensitivity to dynamical scattering, which may provide information beyond just the seven crystal systems and fourteen bravais lattices. after diffraction patterns are collected from material samples, deep learning methods may be able to classify the space group symmetries using the patterns as input, which paired with the elemental composition, would help enable the determination of the crystal structure. to investigate the feasibility of this solution, neural networks were trained to predict the space group type of background corrected ebsd patterns. our networks were first trained and tested on an artificial dataset of ebsd patterns of 5,148 different cubic phases, created through physics-based dynamical simulations. next, maximum classifier discrepancy, an unsupervised deep learning-based domain adaptation method, was utilized to train neural networks to make predictions for experimental ebsd patterns. we introduce a relabeling scheme, which enables our models to achieve accuracy scores higher than 90% on simulated and experimental data, suggesting that neural networks are capable of making predictions of crystal symmetry from an ebsd pattern.",,2025-04-30,2025-05-02,"['alfred yan', 'muhammad nur talha kilic', 'gert nolze', 'ankit agrawal', 'alok choudhary', 'roberto dos reis', 'vinayak dravid']"
2504.21334,simple visual artifact detection in sora-generated videos,cs.cv,"the december 2024 release of openai's sora, a powerful video generation model driven by natural language prompts, highlights a growing convergence between large language models (llms) and video synthesis. as these multimodal systems evolve into video-enabled llms (vidllms), capable of interpreting, generating, and interacting with visual content, understanding their limitations and ensuring their safe deployment becomes essential. this study investigates visual artifacts frequently found and reported in sora-generated videos, which can compromise quality, mislead viewers, or propagate disinformation. we propose a multi-label classification framework targeting four common artifact label types: label 1: boundary / edge defects, label 2: texture / noise issues, label 3: movement / joint anomalies, and label 4: object mismatches / disappearances. using a dataset of 300 manually annotated frames extracted from 15 sora-generated videos, we trained multiple 2d cnn architectures (resnet-50, efficientnet-b3 / b4, vit-base). the best-performing model trained by resnet-50 achieved an average multi-label classification accuracy of 94.14%. this work supports the broader development of vidllms by contributing to (1) the creation of datasets for video quality evaluation, (2) interpretable artifact-based analysis beyond language metrics, and (3) the identification of visual risks relevant to factuality and safety.",,2025-04-30,,"['misora sugiyama', 'hirokatsu kataoka']"
2504.21336,unibiomed: a universal foundation model for grounded biomedical image   interpretation,cs.cv,"multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. conventional ai approaches typically rely on disjointed training, i.e., large language models (llms) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. to this end, we introduce unibiomed, the first universal foundation model for grounded biomedical image interpretation. unibiomed is based on a novel integration of multi-modal large language model (mllm) and segment anything model (sam), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. in this way, unibiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. to develop unibiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. extensive validation on 84 internal and external datasets demonstrated that unibiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, unibiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. this represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. in summary, unibiomed represents a novel breakthrough in biomedical ai, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.",,2025-04-30,,"['linshan wu', 'yuxiang nie', 'sunan he', 'jiaxin zhuang', 'hao chen']"
2504.21340,towards improved cervical cancer screening: vision transformer-based   classification and interpretability,cs.cv cs.lg,"we propose a novel approach to cervical cell image classification for cervical cancer screening using the eva-02 transformer model. we developed a four-step pipeline: fine-tuning eva-02, feature extraction, selecting important features through multiple machine learning models, and training a new artificial neural network with optional loss weighting for improved generalization. with this design, our best model achieved an f1-score of 0.85227, outperforming the baseline eva-02 model (0.84878). we also utilized kernel shap analysis and identified key features correlating with cell morphology and staining characteristics, providing interpretable insights into the decision-making process of the fine-tuned model. our code is available at https://github.com/khoa-nt/isbi2025_ps3c.",,2025-04-30,,"['khoa tuan nguyen', 'ho-min park', 'gaeun oh', 'joris vankerschaver', 'wesley de neve']"
2504.21344,vision-language model-based semantic-guided imaging biomarker for early   lung cancer detection,cs.cv cs.ai q-bio.qm,"objective: a number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. however, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. methods: we obtained 938 low-dose ct scans from the national lung screening trial with 1,246 nodules and semantic features. the lung image database consortium dataset contains 1,018 ct scans, with 2,625 lesions annotated for nodule characteristics. three external datasets were obtained from ucla health, the lungx challenge, and the duke lung cancer screening. we finetuned a pretrained contrastive language-image pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. results: we evaluated the performance of the one-year diagnosis of lung cancer with auroc and auprc and compared it to three state-of-the-art models. our model demonstrated an auroc of 0.90 and auprc of 0.78, outperforming baseline state-of-the-art models on external datasets. using clip, we also obtained predictions on semantic features, such as nodule margin (auroc: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. conclusion: our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. this approach also prevents the model from learning shortcuts and generalizes across clinical settings.",,2025-04-30,,"['luoting zhuang', 'seyed mohammad hossein tabatabaei', 'ramin salehi-rad', 'linh m. tran', 'denise r. aberle', 'ashley e. prosper', 'william hsu']"
2504.21356,"nexus-gen: a unified model for image understanding, generation, and   editing",cs.cv cs.ai,"unified multimodal large language models (mllms) aim to integrate multimodal understanding and generation abilities through a single framework. despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. to bridge this gap, we present nexus-gen, a unified model that synergizes the language reasoning capabilities of llms with the image synthesis power of diffusion models. to align the embedding space of the llm and diffusion model, we conduct a dual-phase alignment training process. (1) the autoregressive llm learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. during training the llm, we identified a critical discrepancy between the autoregressive paradigm's training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. to avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. through dual-phase training, nexus-gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. all models, datasets, and codes are published at https://github.com/modelscope/nexus-gen.git to facilitate further advancements across the field.",,2025-04-30,2025-05-08,"['hong zhang', 'zhongjie duan', 'xingjun wang', 'yuze zhao', 'weiyi lu', 'zhipeng di', 'yixuan xu', 'yingda chen', 'yu zhang']"
2504.21368,revisiting diffusion autoencoder training for image reconstruction   quality,cs.cv cs.ai,"diffusion autoencoders (daes) are typically formulated as a noise prediction model and trained with a linear-$\beta$ noise schedule that spends much of its sampling steps at high noise levels. because high noise levels are associated with recovering large-scale image structures and low noise levels with recovering details, this configuration can result in low-quality and blurry images. however, it should be possible to improve details while spending fewer steps recovering structures because the latent code should already contain structural information. based on this insight, we propose a new dae training method that improves the quality of reconstructed images. we divide training into two phases. in the first phase, the dae is trained as a vanilla autoencoder by always setting the noise level to the highest, forcing the encoder and decoder to populate the latent code with structural information. in the second phase, we incorporate a noise schedule that spends more time in the low-noise region, allowing the dae to learn how to perfect the details. our method results in images that have accurate high-level structures and low-level details while still preserving useful properties of the latent codes.",,2025-04-30,,"['pramook khungurn', 'sukit seripanitkarn', 'phonphrm thawatdamrongkit', 'supasorn suwajanakorn']"
2504.21380,sparse-to-sparse training of diffusion models,cs.lg cs.cv,"diffusion models (dms) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. despite their stable training dynamics and ability to produce diverse high-quality samples, dms are notorious for requiring significant computational resources, both in the training and inference stages. previous work has focused mostly on increasing the efficiency of model inference. this paper introduces, for the first time, the paradigm of sparse-to-sparse training to dms, with the aim of improving both training and inference efficiency. we focus on unconditional generation and train sparse dms from scratch (latent diffusion and chirodiff) on six datasets using three different methods (static-dm, rigl-dm, and magran-dm) to study the effect of sparsity in model performance. our experiments show that sparse dms are able to match and often outperform their dense counterparts, while substantially reducing the number of trainable parameters and flops. we also identify safe and effective values to perform sparse-to-sparse training of dms.",,2025-04-30,,"['inês cardoso oliveira', 'decebal constantin mocanu', 'luis a. leiva']"
2504.21385,iddm: bridging synthetic-to-real domain gap from physics-guided   diffusion for real-world image dehazing,cs.cv,"due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. to address this challenge, we propose \textbf{i}mage \textbf{d}ehazing \textbf{d}iffusion \textbf{m}odels (iddm), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. iddm aims to use the gradual haze formation process to help the denoising unet robustly learn the distribution of clear images from the conditional input hazy images. we design a specialized training strategy centered around iddm. diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. during the forward process, iddm simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. by training with physics-guided information, iddm shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches.",,2025-04-30,,"['shijun zhou', 'yajing liu', 'chunhui hao', 'zhiyuan liu', 'jiandong tian']"
2504.21387,comparison of different deep neural network models in the cultural   heritage domain,cs.cv,"the integration of computer vision and deep learning is an essential part of documenting and preserving cultural heritage, as well as improving visitor experiences. in recent years, two deep learning paradigms have been established in the field of computer vision: convolutional neural networks and transformer architectures. the present study aims to make a comparative analysis of some representatives of these two techniques of their ability to transfer knowledge from generic dataset, such as imagenet, to cultural heritage specific tasks. the results of testing examples of the architectures vgg, resnet, densenet, visual transformer, swin transformer, and poolformer, showed that densenet is the best in terms of efficiency-computability ratio.",,2025-04-30,,"['teodor boyadzhiev', 'gabriele lagani', 'luca ciampi', 'giuseppe amato', 'krassimira ivanova']"
2504.21403,static or dynamic: towards query-adaptive token selection for video   question answering,cs.cv,"video question answering benefits from the rich information available in videos, enabling a wide range of applications. however, the large volume of tokens generated from longer videos presents significant challenges to memory efficiency and model performance. to alleviate this issue, existing works propose to compress video inputs, but usually overlooking the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. to tackle this, we propose a novel token selection strategy, explore-then-select, that adaptively adjust static and dynamic information needed based on question requirements. our framework first explores different token allocations between static frames, which preserve spatial details, and dynamic frames, which capture temporal changes. next, it employs a query-aware attention-based metric to select the optimal token combination without model updates. our proposed framework is plug-and-play that can be seamlessly integrated within diverse video-language models. extensive experiments show that our method achieves significant performance improvements (up to 5.8%) among various video question answering benchmarks.",,2025-04-30,,"['yumeng shi', 'quanyu long', 'wenya wang']"
2504.21423,diff-prompt: diffusion-driven prompt generator with mask supervision,cs.cv,"prompt learning has demonstrated promising results in fine-tuning pre-trained multimodal models. however, the performance improvement is limited when applied to more complex and fine-grained tasks. the reason is that most existing methods directly optimize the parameters involved in the prompt generation process through loss backpropagation, which constrains the richness and specificity of the prompt representations. in this paper, we propose diffusion-driven prompt generator (diff-prompt), aiming to use the diffusion model to generate rich and fine-grained prompt information for complex downstream tasks. specifically, our approach consists of three stages. in the first stage, we train a mask-vae to compress the masks into latent space. in the second stage, we leverage an improved diffusion transformer (dit) to train a prompt generator in the latent space, using the masks for supervision. in the third stage, we align the denoising process of the prompt generator with the pre-trained model in the semantic space, and use the generated prompts to fine-tune the model. we conduct experiments on a complex pixel-level downstream task, referring expression comprehension, and compare our method with various parameter-efficient fine-tuning approaches. diff-prompt achieves a maximum improvement of 8.87 in r@1 and 14.05 in r@5 compared to the foundation model and also outperforms other state-of-the-art methods across multiple metrics. the experimental results validate the effectiveness of our approach and highlight the potential of using generative models for prompt generation. code is available at https://github.com/kelvin-ywc/diff-prompt.",,2025-04-30,,"['weicai yan', 'wang lin', 'zirun guo', 'ye wang', 'fangming feng', 'xiaoda yang', 'zehan wang', 'tao jin']"
2504.21432,uav-vln: end-to-end vision language guided navigation for uavs,cs.ro cs.cv,"a core challenge in ai-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. we propose uav-vln, a novel end-to-end vision-language navigation (vln) framework for unmanned aerial vehicles (uavs) that seamlessly integrates large language models (llms) with visual perception to facilitate human-interactive navigation. our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.   uav-vln leverages the common-sense reasoning capabilities of llms to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. by fusing these modalities, the uav can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. to ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.   we evaluate uav-vln across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of llm-driven vision-language interfaces for safe, intuitive, and generalizable uav autonomy.",,2025-04-30,,"['pranav saxena', 'nishant raghuvanshi', 'neena goveas']"
2504.21447,rethinking visual layer selection in multimodal llms,cs.cv cs.ai,"multimodal large language models (mllms) have achieved impressive performance across a wide range of tasks, typically using clip-vit as their visual encoder due to its strong text-image alignment capabilities. while prior studies suggest that different clip-vit layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most mllms still select visual features based on empirical heuristics rather than systematic analysis. in this work, we propose a layer-wise representation similarity approach to group clip-vit layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on mllm performance. building on this foundation, we revisit the visual layer selection problem in mllms at scale, training llava-style models ranging from 1.4b to 7b parameters. through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for ocr tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. our work offers the first principled study of visual layer selection in mllms, laying the groundwork for deeper investigations into visual representation learning for mllms.",,2025-04-30,,"['haoran chen', 'junyan lin', 'xinhao chen', 'yue fan', 'xin jin', 'hui su', 'jianfeng dong', 'jinlan fu', 'xiaoyu shen']"
2504.21467,multiview point cloud registration via optimization in an autoencoder   latent space,cs.cv,"point cloud rigid registration is a fundamental problem in 3d computer vision. in the multiview case, we aim to find a set of 6d poses to align a set of objects. methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. generative approaches overcome this limitation, but are based on gaussian mixture models and use an expectation-maximization algorithm. hence, they are not well suited to handle large transformations. moreover, most existing methods cannot handle high levels of degradations. in this paper, we introduce polar (point cloud latent registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. to achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. polar is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.",10.1109/tip.2025.3565998,2025-04-30,,"['luc vedrenne', 'sylvain faisan', 'denis fortun']"
2504.21468,quaternion nuclear norms over frobenius norms minimization for robust   matrix completion,cs.cv,"recovering hidden structures from incomplete or noisy data remains a pervasive challenge across many fields, particularly where multi-dimensional data representation is essential. quaternion matrices, with their ability to naturally model multi-dimensional data, offer a promising framework for this problem. this paper introduces the quaternion nuclear norm over the frobenius norm (qnof) as a novel nonconvex approximation for the rank of quaternion matrices. qnof is parameter-free and scale-invariant. utilizing quaternion singular value decomposition, we prove that solving the qnof can be simplified to solving the singular value $l_1/l_2$ problem. additionally, we extend the qnof to robust quaternion matrix completion, employing the alternating direction multiplier method to derive solutions that guarantee weak convergence under mild conditions. extensive numerical experiments validate the proposed model's superiority, consistently outperforming state-of-the-art quaternion methods.",,2025-04-30,,"['yu guo', 'guoqing chen', 'tieyong zeng', 'qiyu jin', 'michael kwok-po ng']"
2504.21472,robust orthogonal nmf with label propagation for image clustering,cs.cv,"non-negative matrix factorization (nmf) is a popular unsupervised learning approach widely used in image clustering. however, in real-world clustering scenarios, most existing nmf methods are highly sensitive to noise corruption and are unable to effectively leverage limited supervised information. to overcome these drawbacks, we propose a unified non-convex framework with label propagation called robust orthogonal nonnegative matrix factorization (ronmf). this method not only considers the graph laplacian and label propagation as regularization terms but also introduces a more effective non-convex structure to measure the reconstruction error and imposes orthogonal constraints on the basis matrix to reduce the noise corruption, thereby achieving higher robustness. to solve ronmf, we develop an alternating direction method of multipliers (admm)-based optimization algorithm. in particular, all subproblems have closed-form solutions, which ensures its efficiency. experimental evaluations on eight public image datasets demonstrate that the proposed ronmf outperforms state-of-the-art nmf methods across various standard metrics and shows excellent robustness. the code will be available at https://github.com/slinda-liu.",,2025-04-30,,"['jingjing liu', 'nian wu', 'xianchao xiu', 'jianhua zhang']"
2504.21478,cae-dfkd: bridging the transferability gap in data-free knowledge   distillation,cs.cv cs.ne,"data-free knowledge distillation (dfkd) enables the knowledge transfer from the given pre-trained teacher network to the target student model without access to the real training data. existing dfkd methods focus primarily on improving image recognition performance on associated datasets, often neglecting the crucial aspect of the transferability of learned representations. in this paper, we propose category-aware embedding data-free knowledge distillation (cae-dfkd), which addresses at the embedding level the limitations of previous rely on image-level methods to improve model generalization but fail when directly applied to dfkd. the superiority and flexibility of cae-dfkd are extensively evaluated, including: \textit{\textbf{i.)}} significant efficiency advantages resulting from altering the generator training paradigm; \textit{\textbf{ii.)}} competitive performance with existing dfkd state-of-the-art methods on image recognition tasks; \textit{\textbf{iii.)}} remarkable transferability of data-free learned representations demonstrated in downstream tasks.",,2025-04-30,,"['zherui zhang', 'changwei wang', 'rongtao xu', 'wenhao xu', 'shibiao xu', 'yu zhang', 'li guo']"
2504.21487,dgsolver: diffusion generalist solver with universal posterior sampling   for image restoration,cs.cv,"diffusion models have achieved remarkable progress in universal image restoration. while existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. moreover, they struggle to balance the commonality of degradation representations and restoration quality. to address these challenges, we introduce \textbf{dgsolver}, a diffusion generalist solver with universal posterior sampling. we first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. we then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. extensive experiments show that dgsolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. code and models will be available at https://github.com/mililab/dgsolver.",,2025-04-30,2025-05-08,"['hebaixu wang', 'jing zhang', 'haonan guo', 'di wang', 'jiayi ma', 'bo du']"
2504.21491,classwise-crf: category-specific fusion for enhanced semantic   segmentation of remote sensing imagery,cs.cv cs.ai cs.lg cs.mm,"we propose a result-level category-specific fusion architecture called classwise-crf. this architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. inspired by conditional random field (crf), the classwise-crf architecture treats the segmentation predictions from multiple networks as confidence vector fields. it leverages segmentation metrics (such as intersection over union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. this fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. building on this, the architecture further optimizes the fused results using unary and pairwise potentials in crf to ensure spatial consistency and boundary accuracy. to validate the effectiveness of classwise-crf, we conducted experiments on two remote sensing datasets, loveda and vaihingen, using eight classic and advanced semantic segmentation networks. the results show that the classwise-crf architecture significantly improves segmentation performance: on the loveda dataset, the mean intersection over union (miou) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the vaihingen dataset, the miou improved by 0.87% on the validation set and by 0.91% on the test set. these results fully demonstrate the effectiveness and generality of the classwise-crf architecture in semantic segmentation of remote sensing images. the full code is available at https://github.com/zhuqinfeng1999/classwise-crf.",,2025-04-30,,"['qinfeng zhu', 'yunxi jiang', 'lei fan']"
2504.21495,consistency-aware fake videos detection on short video platforms,cs.cv cs.mm,"this paper focuses to detect the fake news on the short video platforms. while significant research efforts have been devoted to this task with notable progress in recent years, current detection accuracy remains suboptimal due to the rapid evolution of content manipulation and generation technologies. existing approaches typically employ a cross-modal fusion strategy that directly combines raw video data with metadata inputs before applying a classification layer. however, our empirical observations reveal a critical oversight: manipulated content frequently exhibits inter-modal inconsistencies that could serve as valuable discriminative features, yet remain underutilized in contemporary detection frameworks. motivated by this insight, we propose a novel detection paradigm that explicitly identifies and leverages cross-modal contradictions as discriminative cues. our approach consists of two core modules: cross-modal consistency learning (cmcl) and multi-modal collaborative diagnosis (mmcd). cmcl includes pseudo-label generation (plg) and cross-modal consistency diagnosis (cmcd). in plg, a multimodal large language model is used to generate pseudo-labels for evaluating cross-modal semantic consistency. then, cmcd extracts [cls] tokens and computes cosine loss to quantify cross-modal inconsistencies. mmcd further integrates multimodal features through multimodal feature fusion (mff) and probability scores fusion (psf). mff employs a co-attention mechanism to enhance semantic interactions across different modalities, while a transformer is utilized for comprehensive feature fusion. meanwhile, psf further integrates the fake news probability scores obtained in the previous step. extensive experiments on established benchmarks (fakesv and fakett) demonstrate our model exhibits outstanding performance in fake videos detection.",,2025-04-30,,"['junxi wang', 'jize liu', 'na zhang', 'yaxiong wang']"
2504.21530,roboground: robotic manipulation with grounded vision-language priors,cs.ro cs.cv,"recent advancements in robotic manipulation have highlighted the potential of intermediate representations for improving policy generalization. in this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by large-scale vision-language models pretrained on diverse grounding datasets. we introduce roboground, a grounding-aware robotic manipulation system that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks. to further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions. extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies.",,2025-04-30,,"['haifeng huang', 'xinyi chen', 'yilun chen', 'hao li', 'xiaoshen han', 'zehan wang', 'tai wang', 'jiangmiao pang', 'zhou zhao']"
2504.21544,sam4em: efficient memory-based two stage prompt-free segment anything   model adapter for complex 3d neuroscience electron microscopy stacks,cs.cv,"we present sam4em, a novel approach for 3d segmentation of complex neural structures in electron microscopy (em) data by leveraging the segment anything model (sam) alongside advanced fine-tuning strategies. our contributions include the development of a prompt-free adapter for sam using two stage mask decoding to automatically generate prompt embeddings, a dual-stage fine-tuning method based on low-rank adaptation (lora) for enhancing segmentation with limited annotated data, and a 3d memory attention mechanism to ensure segmentation consistency across 3d stacks. we further release a unique benchmark dataset for the segmentation of astrocytic processes and synapses. we evaluated our method on challenging neuroscience segmentation benchmarks, specifically targeting mitochondria, glia, and synapses, with significant accuracy improvements over state-of-the-art (sota) methods, including recent sam-based adapters developed for the medical domain and other vision transformer-based approaches. experimental results indicate that our approach outperforms existing solutions in the segmentation of complex processes like glia and post-synaptic densities. our code and models are available at https://github.com/uzshah/sam4em.",,2025-04-30,,"['uzair shah', 'marco agus', 'daniya boges', 'vanessa chiappini', 'mahmood alzubaidi', 'jens schneider', 'markus hadwiger', 'pierre j. magistretti', 'mowafa househ', 'corrado calı']"
2504.21559,black-box visual prompt engineering for mitigating object hallucination   in large vision language models,cs.cv cs.ai cs.cl,"large vision language models (lvlms) often suffer from object hallucination, which undermines their reliability. surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (vps) vary in effectiveness. to address this, we propose black-box visual prompt engineering (bbvpe), a framework to identify optimal vps that enhance lvlm responses without needing access to model internals. our approach employs a pool of candidate vps and trains a router model to dynamically select the most effective vp for a given input image. this black-box approach is model-agnostic, making it applicable to both open-source and proprietary lvlms. evaluations on benchmarks such as pope and chair demonstrate that bbvpe effectively reduces object hallucination.",,2025-04-30,,"['sangmin woo', 'kang zhou', 'yun zhou', 'shuai wang', 'sheng guan', 'haibo ding', 'lin lee cheong']"
2504.21562,encapsulate: nca for precision diagnosis on capsule endoscopes,cs.cv cs.ai,"wireless capsule endoscopy is a non-invasive imaging method for the entire gastrointestinal tract, and is a pain-free alternative to traditional endoscopy. it generates extensive video data that requires significant review time, and localizing the capsule after ingestion is a challenge. techniques like bleeding detection and depth estimation can help with localization of pathologies, but deep learning models are typically too large to run directly on the capsule. neural cellular automata (nca) for bleeding segmentation and depth estimation are trained on capsule endoscopic images. for monocular depth estimation, we distill a large foundation model into the lean nca architecture, by treating the outputs of the foundation model as pseudo ground truth. we then port the trained nca to the esp32 microcontroller, enabling efficient image processing on hardware as small as a camera capsule. nca are more accurate (dice) than other portable segmentation models, while requiring more than 100x fewer parameters stored in memory than other small-scale models. the visual results of nca depth estimation look convincing, and in some cases beat the realism and detail of the pseudo ground truth. runtime optimizations on the esp32-s3 accelerate the average inference speed significantly, by more than factor 3. with several algorithmic adjustments and distillation, it is possible to encapsulate nca models into microcontrollers that fit into wireless capsule endoscopes. this is the first work that enables reliable bleeding segmentation and depth estimation on a miniaturized device, paving the way for precise diagnosis combined with visual odometry as a means of precise localization of the capsule -- on the capsule.",,2025-04-30,,"['henry john krumb', 'anirban mukhopadhyay']"
2504.21598,cascade detector analysis and application to biomedical microscopy,cs.cv,"as both computer vision models and biomedical datasets grow in size, there is an increasing need for efficient inference algorithms. we utilize cascade detectors to efficiently identify sparse objects in multiresolution images. given an object's prevalence and a set of detectors at different resolutions with known accuracies, we derive the accuracy, and expected number of classifier calls by a cascade detector. these results generalize across number of dimensions and number of cascade levels. finally, we compare one- and two-level detectors in fluorescent cell detection, organelle segmentation, and tissue segmentation across various microscopy modalities. we show that the multi-level detector achieves comparable performance in 30-75% less time. our work is compatible with a variety of computer vision models and data domains.",,2025-04-30,,"['thomas l. athey', 'shashata sawmya', 'nir shavit']"
2504.21614,mcity data engine: iterative model improvement through open-vocabulary   data selection,cs.cv,"with an ever-increasing availability of data, it has become more and more challenging to select and label appropriate samples for the training of machine learning models. it is especially difficult to detect long-tail classes of interest in large amounts of unlabeled data. this holds especially true for intelligent transportation systems (its), where vehicle fleets and roadside perception systems generate an abundance of raw data. while industrial, proprietary data engines for such iterative data selection and model training processes exist, researchers and the open-source community suffer from a lack of an openly available system. we present the mcity data engine, which provides modules for the complete data-based development cycle, beginning at the data acquisition phase and ending at the model deployment stage. the mcity data engine focuses on rare and novel classes through an open-vocabulary data selection process. all code is publicly available on github under an mit license: https://github.com/mcity/mcity_data_engine",,2025-04-30,,"['daniel bogdoll', 'rajanikant patnaik ananta', 'abeyankar giridharan', 'isabel moore', 'gregory stevens', 'henry x. liu']"
2504.21646,diffusion-based adversarial identity manipulation for facial privacy   protection,cs.cv,"the success of face recognition (fr) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. in this paper, we propose diffusion-based adversarial identity manipulation (diffaim) to generate natural and highly transferable adversarial faces against malicious fr systems. to be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. this involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. the guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. we further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, diffaim achieves stronger black-box attack transferability while maintaining superior visual quality. we also demonstrate the effectiveness of the proposed approach for commercial fr apis, including face++ and aliyun.",,2025-04-30,,"['liqin wang', 'qianyue hu', 'wei lu', 'xiangyang luo']"
2504.21682,visual text processing: a comprehensive review and unified evaluation,cs.cv,"visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. in this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) what textual features are most suitable for different visual text processing tasks? (2) how can these distinctive text features be effectively incorporated into processing frameworks? furthermore, we introduce vtpbench, a new benchmark that encompasses a broad range of visual text processing datasets. leveraging the advanced visual quality assessment capabilities of multimodal large language models (mllms), we propose vtpscore, a novel evaluation metric designed to ensure fair and reliable evaluation. our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. the relevant repository is available at https://github.com/shuyansy/visual-text-processing-survey.",,2025-04-30,,"['yan shu', 'weichao zeng', 'fangmin zhao', 'zeyu chen', 'zhenhang li', 'xiaomeng yang', 'yu zhou', 'paolo rota', 'xiang bai', 'lianwen jin', 'xu-cheng yin', 'nicu sebe']"
2504.21692,enhancing self-supervised fine-grained video object tracking with   dynamic memory prediction,cs.cv cs.ai,"successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. in this paper, we introduce a dynamic memory prediction (dmp) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. its core component is a reference frame memory engine that dynamically selects frames based on object pixel features to improve tracking accuracy. in addition, a bidirectional target prediction network is built to utilize multiple reference frames to improve the robustness of the model. through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.",10.1145/3731715.3733327,2025-04-30,,"['zihan zhou', 'changrui dai', 'aibo song', 'xiaolin fang']"
2504.21699,rehearse-3d: a multi-modal emulated rain dataset for 3d point cloud   de-raining,cs.cv cs.ro,"sensor degradation poses a significant challenge in autonomous driving. during heavy rainfall, the interference from raindrops can adversely affect the quality of lidar point clouds, resulting in, for instance, inaccurate point measurements. this, in turn, can potentially lead to safety concerns if autonomous driving systems are not weather-aware, i.e., if they are unable to discern such changes. in this study, we release a new, large-scale, multi-modal emulated rain dataset, rehearse-3d, to promote research advancements in 3d point cloud de-raining. distinct from the most relevant competitors, our dataset is unique in several respects. first, it is the largest point-wise annotated dataset, and second, it is the only one with high-resolution lidar data (lidar-256) enriched with 4d radar point clouds logged in both daytime and nighttime conditions in a controlled weather environment. furthermore, rehearse-3d involves rain-characteristic information, which is of significant value not only for sensor noise modeling but also for analyzing the impact of weather at a point level. leveraging rehearse-3d, we benchmark raindrop detection and removal in fused lidar and 4d radar point clouds. our comprehensive study further evaluates the performance of various statistical and deep-learning models. upon publication, the dataset and benchmark models will be made publicly available at: https://sporsho.github.io/rehearse3d.",,2025-04-30,2025-05-08,"['abu mohammed raisuddin', 'jesper holmblad', 'hamed haghighi', 'yuri poledna', 'maikol funk drechsler', 'valentina donzella', 'eren erdal aksoy']"
2504.21707,recursive kl divergence optimization: a dynamic framework for   representation learning,cs.lg cs.ai cs.cv cs.it cs.ne math.it,we propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions while recent frameworks like information contrastive learning i-con unify multiple learning paradigms through kl divergence between fixed neighborhood conditionals we argue this view underplays a crucial recursive structure inherent in the learning process. we introduce recursive kl divergence optimization rkdo a dynamic formalism where representation learning is framed as the evolution of kl divergences across data neighborhoods. this formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. our experiments demonstrate that rkdo offers dual efficiency advantages approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. this suggests that rkdos recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.,,2025-04-30,,['anthony d martin']
2504.21718,vividlistener: expressive and controllable listener dynamics modeling   for multi-modal responsive interaction,cs.cv,"generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. previous studies mainly focus on the direct short-term production of listener behavior. they overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.therefore, we first newly collect a large-scale multi-turn dataset of 3d dyadic conversation containing more than 1.4m valid frames for multi-modal responsive interaction, dubbed listenerx. additionally, we propose vividlistener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. this framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.specifically, we design the responsive interaction module (rim) to adaptively represent the multi-modal interactive embeddings. rim ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. meanwhile, we design the emotional intensity tags (eit) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.extensive experiments conducted on our newly collected listenerx dataset demonstrate that vividlistener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.",,2025-04-30,2025-05-06,"['shiying li', 'xingqun qi', 'bingkun yang', 'chen weile', 'zezhao tian', 'muyi sun', 'qifeng liu', 'man zhang', 'zhenan sun']"
2504.21730,cert-ssb: toward certified sample-specific backdoor defense,cs.cr cs.ai cs.cv cs.lg,"deep neural networks (dnns) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. the compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world dnn applications. currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. in contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. in this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. however, it may not hold in practice, leading to suboptimal certification performance. to address this issue, we propose a sample-specific certified backdoor defense method, termed cert-ssb. cert-ssb first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. after that, cert-ssb aggregates the predictions of multiple smoothed models to generate the final robust prediction. in particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. to conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. we conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. our code is available at https://github.com/ncepuqiaoting/cert-ssb.",,2025-04-30,,"['ting qiao', 'yingjia wang', 'xing liu', 'sixing wu', 'jianbing li', 'yiming li']"
2504.21731,adaptive 3d ui placement in mixed reality using deep reinforcement   learning,cs.hc cs.ai cs.cv,"mixed reality (mr) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. however, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of mr experiences. in contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (rl) could assist with continuous 3d content placement that is aware of users' poses and their surrounding environments. through an initial exploration and preliminary evaluation, our results demonstrate the potential of rl to position content that maximizes the reward for users on the go. we further identify future directions for research that could harness the power of rl for personalized and optimized ui and content placement in mr.",10.1145/3613905.3651059,2025-04-30,,"['feiyu lu', 'mengyu chen', 'hsiang hsu', 'pranav deshpande', 'cheng yao wang', 'blair macintyre']"
2504.21749,common3d: self-supervised learning of 3d morphable models for common   objects in neural feature space,cs.cv,"3d morphable models (3dmms) are a powerful tool to represent the possible shapes and appearances of an object category. given a single test image, 3dmms can be used to solve various tasks, such as predicting the 3d shape, pose, semantic correspondence, and instance segmentation of an object. unfortunately, 3dmms are only available for very few object categories that are of particular interest, like faces or human bodies, as they require a demanding 3d data acquisition and category-specific training process. in contrast, we introduce a new method, common3d, that learns 3dmms of common objects in a fully self-supervised manner from a collection of object-centric videos. for this purpose, our model represents objects as a learned 3d template mesh and a deformation field that is parameterized as an image-conditioned neural network. different from prior works, common3d represents the object appearance with neural features instead of rgb colors, which enables the learning of more generalizable representations through an abstraction from pixel intensities. importantly, we train the appearance features using a contrastive objective by exploiting the correspondences defined through the deformable template mesh. this leads to higher quality correspondence features compared to related works and a significantly improved model performance at estimating 3d object pose and semantic correspondence. common3d is the first completely self-supervised method that can solve various vision tasks in a zero-shot manner.",,2025-04-30,,"['leonhard sommer', 'olaf dünkel', 'christian theobalt', 'adam kortylewski']"
2504.21771,anatomical similarity as a new metric to evaluate brain generative   models,cs.cv,"generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. despite advances in realistic synthetic mris, evaluations focus on texture and perception, lacking sensitivity to crucial anatomical fidelity. this study proposes a new metric, called wasabi (wasserstein-based anatomical brain index), to assess the anatomical realism of synthetic brain mris. wasabi leverages \textit{synthseg}, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each mri and uses the multivariate wasserstein distance to compare distributions between real and synthetic anatomies. based on controlled experiments on two real datasets and synthetic mris from five generative models, wasabi demonstrates higher sensitivity in quantifying anatomical discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing anatomical fidelity as a crucial benchmark for clinically meaningful brain mri synthesis. our code is available at https://github.com/bahramjafrasteh/wasabi-mri.",,2025-04-30,,"['bahram jafrasteh', 'wei peng', 'cheng wan', 'yimin luo', 'ehsan adeli', 'qingyu zhao']"
2504.21778,loc-lic: low complexity learned image coding using hierarchical feature   transforms,eess.iv cs.cv,"current learned image compression models typically exhibit high complexity, which demands significant computational resources. to overcome these challenges, we propose an innovative approach that employs hierarchical feature extraction transforms to significantly reduce complexity while preserving bit rate reduction efficiency. our novel architecture achieves this by using fewer channels for high spatial resolution inputs/feature maps. on the other hand, feature maps with a large number of channels have reduced spatial dimensions, thereby cutting down on computational load without sacrificing performance. this strategy effectively reduces the forward pass complexity from \(1256 \, \text{kmac/pixel}\) to just \(270 \, \text{kmac/pixel}\). as a result, the reduced complexity model can open the way for learned image compression models to operate efficiently across various devices and pave the way for the development of new architectures in image compression technology.",,2025-04-30,,"['ayman a. ameen', 'thomas richter', 'andré kaup']"
2504.21789,anomaly-driven approach for enhanced prostate cancer segmentation,cs.cv cs.lg,"magnetic resonance imaging (mri) plays an important role in identifying clinically significant prostate cancer (cspca), yet automated methods face challenges such as data imbalance, variable tumor sizes, and a lack of annotated data. this study introduces anomaly-driven u-net (adu-net), which incorporates anomaly maps derived from biparametric mri sequences into a deep learning-based segmentation framework to improve cspca identification. we conduct a comparative analysis of anomaly detection methods and evaluate the integration of anomaly maps into the segmentation pipeline. anomaly maps, generated using fixed-point gan reconstruction, highlight deviations from normal prostate tissue, guiding the segmentation model to potential cancerous regions. we compare the performance by using the average score, computed as the mean of the auroc and average precision (ap). on the external test set, adu-net achieves the best average score of 0.618, outperforming the baseline nnu-net model (0.605). the results demonstrate that incorporating anomaly detection into segmentation improves generalization and performance, particularly with adc-based anomaly maps, offering a promising direction for automated cspca identification.",,2025-04-30,,"['alessia hu', 'regina beets-tan', 'lishan cai', 'eduardo pooch']"
2504.21810,a simple and effective approach for body part recognition on ct scans   based on projection estimation,cs.cv,"it is well known that machine learning models require a high amount of annotated data to obtain optimal performance. labelling computed tomography (ct) data can be a particularly challenging task due to its volumetric nature and often missing and$/$or incomplete associated meta-data. even inspecting one ct scan requires additional computer software, or in the case of programming languages $-$ additional programming libraries. this study proposes a simple, yet effective approach based on 2d x-ray-like estimation of 3d ct scans for body region identification. although body region is commonly associated with the ct scan, it often describes only the focused major body region neglecting other anatomical regions present in the observed ct. in the proposed approach, estimated 2d images were utilized to identify 14 distinct body regions, providing valuable information for constructing a high-quality medical dataset. to evaluate the effectiveness of the proposed method, it was compared against 2.5d, 3d and foundation model (mi2) based approaches. our approach outperformed the others, where it came on top with statistical significance and f1-score for the best-performing model effnet-b0 of 0.980 $\pm$ 0.016 in comparison to the 0.840 $\pm$ 0.114 (2.5d densenet-161), 0.854 $\pm$ 0.096 (3d voxcnn), and 0.852 $\pm$ 0.104 (mi2 foundation model). the utilized dataset comprised three different clinical centers and counted 15,622 ct scans (44,135 labels).",,2025-04-30,,"['franko hrzic', 'mohammadreza movahhedi', 'ophelie lavoie-gagne', 'ata kiapour']"
2504.21814,why compress what you can generate? when gpt-4o generation ushers in   image compression fields,cs.cv,"the rapid development of aigc foundation models has revolutionized the paradigm of image compression, which paves the way for the abandonment of most pixel-level transform and coding, compelling us to ask: why compress what you can generate if the aigc foundation model is powerful enough to faithfully generate intricate structure and fine-grained details from nothing more than some compact descriptors, i.e., texts, or cues. fortunately, recent gpt-4o image generation of openai has achieved impressive cross-modality generation, editing, and design capabilities, which motivates us to answer the above question by exploring its potential in image compression fields. in this work, we investigate two typical compression paradigms: textual coding and multimodal coding (i.e., text + extremely low-resolution image), where all/most pixel-level information is generated instead of compressing via the advanced gpt-4o image generation function. the essential challenge lies in how to maintain semantic and structure consistency during the decoding process. to overcome this, we propose a structure raster-scan prompt engineering mechanism to transform the image into textual space, which is compressed as the condition of gpt-4o image generation. extensive experiments have shown that the combination of our designed structural raster-scan prompts and gpt-4o's image generation function achieved the impressive performance compared with recent multimodal/generative image compression at ultra-low bitrate, further indicating the potential of aigc generation in image compression fields.",,2025-04-30,,"['yixin gao', 'xiaohan pan', 'xin li', 'zhibo chen']"
2504.21831,early exit and multi stage knowledge distillation in vlms for video   summarization,cs.cv cs.ai,"we introduce deevisum (distilled early exit vision language model for summarization), a lightweight, efficient, and scalable vision language model designed for segment wise video summarization. leveraging multi modal prompts that combine textual and audio derived signals, deevisum incorporates multi stage knowledge distillation (mskd) and early exit (ee) to strike a balance between performance and efficiency. mskd offers a 1.33% absolute f1 improvement over baseline distillation (0.5%), while ee reduces inference time by approximately 21% with a 1.3 point drop in f1. evaluated on the tvsum dataset, our best model pali gemma2 3b + mskd achieves an f1 score of 61.1, competing the performance of significantly larger models, all while maintaining a lower computational footprint. we publicly release our code and processed dataset to support further research.",,2025-04-30,,"['anas anwarul haq khan', 'utkarsh verma', 'prateek chanda', 'ganesh ramakrishnan']"
2504.21836,3d stylization via large reconstruction model,cs.cv,"with the growing success of text or image guided 3d generators, users demand more control over the generation process, appearance stylization being one of them. given a reference image, this requires adapting the appearance of a generated 3d asset to reflect the visual style of the reference while maintaining visual consistency from multiple viewpoints. to tackle this problem, we draw inspiration from the success of 2d stylization methods that leverage the attention mechanisms in large image generation models to capture and transfer visual style. in particular, we probe if large reconstruction models, commonly used in the context of 3d generation, has a similar capability. we discover that the certain attention blocks in these models capture the appearance specific features. by injecting features from a visual style image to such blocks, we develop a simple yet effective 3d appearance stylization method. our method does not require training or test time optimization. through both quantitative and qualitative evaluations, we demonstrate that our approach achieves superior results in terms of 3d appearance stylization, significantly improving efficiency while maintaining high-quality visual outcomes.",,2025-04-30,,"['ipek oztas', 'duygu ceylan', 'aysegul dundar']"
2504.21846,active light modulation to counter manipulation of speech visual content,cs.cv cs.ai cs.cr,"high-profile speech videos are prime targets for falsification, owing to their accessibility and influence. this work proposes spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. unlike predominant falsification detection methods operating in the digital domain, spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. these physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. the signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. key elements of spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. prototype experiments on extensive video datasets show spotlight achieves aucs $\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. further, spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.",,2025-04-30,,"['hadleigh schwartz', 'xiaofeng yan', 'charles j. carver', 'xia zhou']"
2504.21847,differentiable room acoustic rendering with multi-view vision priors,cs.cv cs.sd,"an immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. however, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. in this work, we introduce audio-visual differentiable room acoustic rendering (av-dar), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. notably, on the real acoustic field dataset, av-dar achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.",,2025-04-30,,"['derong jin', 'ruohan gao']"
2504.21850,compact: compositional atomic-to-complex visual capability tuning,cs.cv,"multimodal large language models (mllms) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. this might be partially the result of the fact that visual instruction tuning (vit), a critical training step for mllms, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. we propose compact (compositional atomic-to-complex visual capability tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. the data from compact allows mllms to train on combinations of atomic capabilities to learn complex capabilities more efficiently. across all benchmarks, compact achieves comparable performance to the llava-665k vit while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. for example, compact achieves substantial 83.3% improvement on mmstar and 94.0% improvement on mm-vet compared to the full-scale vit on particularly complex questions that require four or more atomic capabilities. compact offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.",,2025-04-30,,"['xindi wu', 'hee seung hwang', 'polina kirichenko', 'olga russakovsky']"
2504.21853,a survey of interactive generative video,cs.cv,"interactive generative video (igv) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. in this paper, we define igv as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. we survey the current landscape of igv applications, focusing on three major domains: 1) gaming, where igv enables infinite exploration in virtual worlds; 2) embodied ai, where igv serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where igv provides closed-loop simulation capabilities for safety-critical testing and validation. to guide future development, we propose a comprehensive framework that decomposes an ideal igv system into five essential modules: generation, control, memory, dynamics, and intelligence. furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal igv system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. we believe that this systematic analysis will facilitate future research and development in the field of igv, ultimately advancing the technology toward more sophisticated and practical applications.",,2025-04-30,,"['jiwen yu', 'yiran qin', 'haoxuan che', 'quande liu', 'xintao wang', 'pengfei wan', 'di zhang', 'kun gai', 'hao chen', 'xihui liu']"
2504.21855,"revision: high-quality, low-cost video generation with explicit 3d   physics modeling for complex motion and interaction",cs.cv,"in recent years, video generation has seen significant advancements. however, challenges still persist in generating complex motions and interactions. to address these challenges, we introduce revision, a plug-and-play framework that explicitly integrates parameterized 3d physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. specifically, revision consists of three stages. first, a video diffusion model is used to generate a coarse video. next, we extract a set of 2d and 3d features from the coarse video to construct a 3d object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3d motion sequence. finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. we validate the effectiveness of our approach on stable video diffusion, where revision significantly improves motion fidelity and coherence. remarkably, with only 1.5b parameters, it even outperforms a state-of-the-art video generation model with over 13b parameters on complex video generation by a substantial margin. our results suggest that, by incorporating 3d physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.",,2025-04-30,,"['qihao liu', 'ju he', 'qihang yu', 'liang-chieh chen', 'alan yuille']"
2505.00044,learning to borrow features for improved detection of small objects in   single-shot detectors,cs.cv math.oc,"detecting small objects remains a significant challenge in single-shot object detectors due to the inherent trade-off between spatial resolution and semantic richness in convolutional feature maps. to address this issue, we propose a novel framework that enables small object representations to ""borrow"" discriminative features from larger, semantically richer instances within the same class. our architecture introduces three key components: the feature matching block (fmb) to identify semantically similar descriptors across layers, the feature representing block (frb) to generate enhanced shallow features through weighted aggregation, and the feature fusion block (ffb) to refine feature maps by integrating original, borrowed, and context information. built upon the ssd framework, our method improves the descriptive capacity of shallow layers while maintaining real-time detection performance. experimental results demonstrate that our approach significantly boosts small object detection accuracy over baseline methods, offering a promising direction for robust object detection in complex visual environments.",,2025-04-29,,['richard schmit']
2505.00046,sr-nerv: improving embedding efficiency of neural video representation   via super-resolution,eess.iv cs.cv,"implicit neural representations (inrs) have garnered significant attention for their ability to model complex signals across a variety of domains. recently, inr-based approaches have emerged as promising frameworks for neural video compression. while conventional methods primarily focus on embedding video content into compact neural networks for efficient representation, they often struggle to reconstruct high-frequency details under stringent model size constraints, which are critical in practical compression scenarios. to address this limitation, we propose an inr-based video representation method that integrates a general-purpose super-resolution (sr) network. motivated by the observation that high-frequency components exhibit low temporal redundancy across frames, our method entrusts the reconstruction of fine details to the sr network. experimental results demonstrate that the proposed method outperforms conventional inr-based baselines in terms of reconstruction quality, while maintaining comparable model sizes.",,2025-04-29,,"['taiga hayami', 'kakeru koizumi', 'hiroshi watanabe']"
2505.00115,rootlets-based registration to the spinal cord pam50 template,eess.iv cs.cv,"spinal cord functional mri studies require precise localization of spinal levels for reliable voxelwise group analyses. traditional template-based registration of the spinal cord uses intervertebral discs for alignment. however, substantial anatomical variability across individuals exists between vertebral and spinal levels. this study proposes a novel registration approach that leverages spinal nerve rootlets to improve alignment accuracy and reproducibility across individuals. we developed a registration method leveraging dorsal cervical rootlets segmentation and aligning them non-linearly with the pam50 spinal cord template. validation was performed on a multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset with various neck positions (n=10, 3 sessions). we further validated the method on task-based functional mri (n=23) to compare group-level activation maps using rootlet-based registration to traditional disc-based methods. rootlet-based registration showed superior alignment across individuals compared to the traditional disc-based method. notably, rootlet positions were more stable across neck positions. group-level analysis of task-based functional mri using rootlet-based increased z scores and activation cluster size compared to disc-based registration (number of active voxels from 3292 to 7978). rootlet-based registration enhances both inter- and intra-subject anatomical alignment and yields better spatial normalization for group-level fmri analyses. our findings highlight the potential of rootlet-based registration to improve the precision and reliability of spinal cord neuroimaging group analysis.",,2025-04-30,,"['sandrine bédard', 'jan valošek', 'valeria oliva', 'kenneth a. weber', 'julien cohen-adad']"
2505.00133,efficient and robust 3d blind harmonization for large domain gaps,eess.iv cs.cv,"blind harmonization has emerged as a promising technique for mr image harmonization to achieve scale-invariant representations, requiring only target domain data (i.e., no source domain data necessary). however, existing methods face limitations such as inter-slice heterogeneity in 3d, moderate image quality, and limited performance for a large domain gap. to address these challenges, we introduce blindharmonydiff, a novel blind 3d harmonization framework that leverages an edge-to-image model tailored specifically to harmonization. our framework employs a 3d rectified flow trained on target domain images to reconstruct the original image from an edge map, then yielding a harmonized image from the edge of a source domain image. we propose multi-stride patch training for efficient 3d training and a refinement module for robust inference by suppressing hallucination. extensive experiments demonstrate that blindharmonydiff outperforms prior arts by harmonizing diverse source domain images to the target domain, achieving higher correspondence to the target domain characteristics. downstream task-based quality assessments such as tissue segmentation and age prediction on diverse mr scanners further confirm the effectiveness of our approach and demonstrate the capability of our robust and generalizable blind harmonization.",,2025-04-30,,"['hwihun jeong', 'hayeon lee', 'se young chun', 'jongho lee']"
2505.00134,investigating zero-shot diagnostic pathology in vision-language models   with efficient prompt design,cs.cv,"vision-language models (vlms) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (wsi). however, their sensitivity to large-scale clinical data, task formulations, and prompt design remains an open question, particularly in terms of diagnostic accuracy. in this paper, we present a systematic investigation and analysis of three state of the art vlms for histopathology, namely quilt-net, quilt-llava, and conch, on an in-house digestive pathology dataset comprising 3,507 wsis, each in giga-pixel form, across distinct tissue types. through a structured ablative study on cancer invasiveness and dysplasia status, we develop a comprehensive prompt engineering framework that systematically varies domain specificity, anatomical precision, instructional framing, and output constraints. our findings demonstrate that prompt engineering significantly impacts model performance, with the conch model achieving the highest accuracy when provided with precise anatomical references. additionally, we identify the critical importance of anatomical context in histopathological image analysis, as performance consistently degraded when reducing anatomical precision. we also show that model complexity alone does not guarantee superior performance, as effective domain alignment and domain-specific training are critical. these results establish foundational guidelines for prompt engineering in computational pathology and highlight the potential of vlms to enhance diagnostic accuracy when properly instructed with domain-appropriate prompts.",,2025-04-30,,"['vasudev sharma', 'ahmed alagha', 'abdelhakim khellaf', 'vincent quoc-huy trinh', 'mahdi s. hosseini']"
2505.00135,eye2eye: a simple approach for monocular-to-stereo video synthesis,cs.cv,"the rising popularity of immersive visual experiences has increased interest in stereoscopic 3d video generation. despite significant advances in video synthesis, creating 3d videos remains challenging due to the relative scarcity of 3d video data. we propose a simple approach for transforming a text-to-video generator into a video-to-stereo generator. given an input video, our framework automatically produces the video frames from a shifted viewpoint, enabling a compelling 3d effect. prior and concurrent approaches for this task typically operate in multiple phases, first estimating video disparity or depth, then warping the video accordingly to produce a second view, and finally inpainting the disoccluded regions. this approach inherently fails when the scene involves specular surfaces or transparent objects. in such cases, single-layer disparity estimation is insufficient, resulting in artifacts and incorrect pixel shifts during warping. our work bypasses these restrictions by directly synthesizing the new viewpoint, avoiding any intermediate steps. this is achieved by leveraging a pre-trained video model's priors on geometry, object materials, optics, and semantics, without relying on external geometry models or manually disentangling geometry from the synthesis process. we demonstrate the advantages of our approach in complex, real-world scenarios featuring diverse object materials and compositions. see videos on https://video-eye2eye.github.io",,2025-04-30,,"['michal geyer', 'omer tov', 'linyi jin', 'richard tucker', 'inbar mosseri', 'tali dekel', 'noah snavely']"
2505.00150,detecting and mitigating hateful content in multimodal memes with   vision-language models,cs.cv cs.ai cs.cl,"the rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. while the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. leveraging the powerful generation and reasoning capabilities of vision-language models (vlms), we address the tasks of detecting and mitigating hateful content. this paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named unhatememe, which works by replacing hateful textual and/or visual components. with our definition-guided prompts, vlms achieve impressive performance on hateful memes detection task. furthermore, our unhatememe framework, integrated with vlms, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. through empirical experiments, we show the effectiveness of state-of-the-art pretrained vlms such as llava, gemini and gpt-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. this paper aims to shed light on important applications of vlms for ensuring safe and respectful online environments.",,2025-04-30,,"['minh-hao van', 'xintao wu']"
2505.00156,v3lma: visual 3d-enhanced language model for autonomous driving,cs.cv,"large vision language models (lvlms) have shown strong capabilities in understanding and analyzing visual scenes across various domains. however, in the context of autonomous driving, their limited comprehension of 3d environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. to address this, we introduce v3lma, a novel approach that enhances 3d scene understanding by integrating large language models (llms) with lvlms. v3lma leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. through a dedicated preprocessing pipeline that extracts 3d object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the lingoqa benchmark. we further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.",,2025-04-30,,"['jannik lübberstedt', 'esteban rivera', 'nico uhlemann', 'markus lienkamp']"
2505.00186,neuroevolution of self-attention over proto-objects,cs.ne cs.ai cs.cv,"proto-objects - image regions that share common visual properties - offer a promising alternative to traditional attention mechanisms based on rectangular-shaped image patches in neural networks. although previous work demonstrated that evolving a patch-based hard-attention module alongside a controller network could achieve state-of-the-art performance in visual reinforcement learning tasks, our approach leverages image segmentation to work with higher-level features. by operating on proto-objects rather than fixed patches, we significantly reduce the representational complexity: each image decomposes into fewer proto-objects than regular patches, and each proto-object can be efficiently encoded as a compact feature vector. this enables a substantially smaller self-attention module that processes richer semantic information. our experiments demonstrate that this proto-object-based approach matches or exceeds the state-of-the-art performance of patch-based implementations with 62% less parameters and 2.6 times less training time.",10.1145/3712256.3726451,2025-04-30,,"['rafael c. pinto', 'anderson r. tavares']"
2505.00209,direct motion models for assessing generated videos,cs.cv cs.lg,"a current limitation of video generative video models is that they generate plausible looking frames, but poor motion -- an issue that is not well captured by fvd and other popular methods for evaluating generated videos. here we go beyond fvd by developing a metric which better measures plausible object interactions and motion. our novel approach is based on auto-encoding point tracks and yields motion features that can be used to not only compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), but also for evaluating motion of single videos. we show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives. we also show that by using a point track representation, we can spatiotemporally localize generative video inconsistencies, providing extra interpretability of generated video errors relative to prior work. an overview of the results and link to the code can be found on the project page: http://trajan-paper.github.io.",,2025-04-30,,"['kelsey allen', 'carl doersch', 'guangyao zhou', 'mohammed suhail', 'danny driess', 'ignacio rocco', 'yulia rubanova', 'thomas kipf', 'mehdi s. m. sajjadi', 'kevin murphy', 'joao carreira', 'sjoerd van steenkiste']"
2505.00220,towards robust and generalizable gerchberg saxton based physics inspired   neural networks for computer generated holography: a sensitivity analysis   framework,cs.cv physics.optics,"computer-generated holography (cgh) enables applications in holographic augmented reality (ar), 3d displays, systems neuroscience, and optical trapping. the fundamental challenge in cgh is solving the inverse problem of phase retrieval from intensity measurements. physics-inspired neural networks (pinns), especially gerchberg-saxton-based pinns (gs-pinns), have advanced phase retrieval capabilities. however, their performance strongly depends on forward models (fms) and their hyperparameters (fmhs), limiting generalization, complicating benchmarking, and hindering hardware optimization. we present a systematic sensitivity analysis framework based on saltelli's extension of sobol's method to quantify fmh impacts on gs-pinn performance. our analysis demonstrates that slm pixel-resolution is the primary factor affecting neural network sensitivity, followed by pixel-pitch, propagation distance, and wavelength. free space propagation forward models demonstrate superior neural network performance compared to fourier holography, providing enhanced parameterization and generalization. we introduce a composite evaluation metric combining performance consistency, generalization capability, and hyperparameter perturbation resilience, establishing a unified benchmarking standard across cgh configurations. our research connects physics-inspired deep learning theory with practical cgh implementations through concrete guidelines for forward model selection, neural network architecture, and performance evaluation. our contributions advance the development of robust, interpretable, and generalizable neural networks for diverse holographic applications, supporting evidence-based decisions in cgh research and implementation.",,2025-04-30,,"['ankit amrutkar', 'björn kampa', 'volkmar schulz', 'johannes stegmaier', 'markus rothermel', 'dorit merhof']"
2505.00259,pack-ptq: advancing post-training quantization of neural networks by   pack-wise reconstruction,cs.cv cs.ai,"post-training quantization (ptq) has evolved as a prominent solution for compressing complex models, which advocates a small calibration dataset and avoids end-to-end retraining. however, most existing ptq methods employ block-wise reconstruction, which neglects cross-block dependency and exhibits a notable accuracy drop in low-bit cases. to address these limitations, this paper presents a novel ptq method, dubbed pack-ptq. first, we design a hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, which serve as the base unit for reconstruction, thereby preserving the cross-block dependency and enabling accurate quantization parameters estimation. second, based on the pack configuration, we propose a mixed-precision quantization approach to assign varied bit-widths to packs according to their distinct sensitivities, thereby further enhancing performance. extensive experiments on 2d image and 3d point cloud classification tasks, using various network architectures, demonstrate the superiority of our method over the state-of-the-art ptq methods.",,2025-04-30,,"['changjun li', 'runqing jiang', 'zhuo song', 'pengpeng yu', 'ye zhang', 'yulan guo']"
2505.00275,adcare-vlm: leveraging large vision language model (lvlm) to monitor   long-term medication adherence and care,cs.cv,"chronic diseases, including diabetes, hypertension, asthma, hiv-aids, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. we propose adcare-vlm, a specialized video-llava-based multimodal large vision language model (lvlm) aimed at visual question answering (vqa) concerning medication adherence through patient videos. we employ a private dataset comprising 806 custom-annotated tuberculosis (tb) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. we present llm-tb-vqa, a detailed medical adherence vqa dataset that encompasses positive, negative, and ambiguous adherence cases. our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. this facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. experimental results indicate that our method surpasses parameter-efficient fine-tuning (peft) enabled vlm models, such as llava-v1.5 and chat-univi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (lora) configurations. comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.",,2025-04-30,,"['md asaduzzaman jabin', 'hanqi jiang', 'yiwei li', 'patrick kaggwa', 'eugene douglass', 'juliet n. sekandi', 'tianming liu']"
2505.00295,fine-grained spatial-temporal perception for gas leak segmentation,cs.cv cs.ai,"gas leaks pose significant risks to human health and the environment. despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. in this paper, we propose a fine-grained spatial-temporal perception (fgstp) algorithm for gas leak segmentation. fgstp captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. specifically, we first construct a correlation volume to capture motion information between consecutive frames. then, the fine-grained perception progressively refines the object-level features using previous outputs. finally, a decoder is employed to optimize boundary segmentation. because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, gasvid. experimental results on gasvid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (sota) models.",,2025-05-01,,"['xinlong zhao', 'shan du']"
2505.00312,aware-net: adaptive weighted averaging for robust ensemble network in   deepfake detection,cs.cv,"deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. while multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. in response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: xception, res2net101, and efficientnet-b7. our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. our experiments achieved state-of-the-art intra-dataset performance with auc scores of 99.22% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.06% (ff++) and 99.94% (celebdf-v2) without augmentation. with augmentation, we achieve auc scores of 99.47% (ff++) and 100.00% (celebdf-v2), and f1 scores of 98.43% (ff++) and 99.95% (celebdf-v2). the framework demonstrates robust cross-dataset generalization, achieving auc scores of 88.20% and 72.52%, and f1 scores of 93.16% and 80.62% in cross-dataset evaluations.",10.1049/icp.2025.1162,2025-05-01,,"['muhammad salman', 'iqra tariq', 'mishal zulfiqar', 'muqadas jalal', 'sami aujla', 'sumbal fatima']"
2505.00334,quaternion wavelet-conditioned diffusion models for image   super-resolution,cs.cv cs.lg,"image super-resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. the ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. while deep learning has significantly advanced sr, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. in this work, we introduce resqu a novel sr framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. furthermore, we also leverage the generative priors of foundation models such as stable diffusion. extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding sr results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. the code will be available after the revision process.",,2025-05-01,2025-05-05,"['luigi sigillo', 'christian bianchi', 'aurelio uncini', 'danilo comminiello']"
2505.00335,efficient neural video representation with temporally coherent   modulation,cs.cv cs.ai,"implicit neural representations (inr) has found successful applications across diverse domains. to employ inr in real-life, it is important to speed up training. in the field of inr for video applications, the state-of-the-art approach employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors. however, the grid usage, which does not consider the video's dynamic nature, leads to redundant use of trainable parameters. as a result, it has significantly lower parameter efficiency and higher bitrate compared to nerv-style methods that do not use a parametric encoding. to address the problem, we propose neural video representation with temporally coherent modulation (nvtm), a novel framework that can capture dynamic characteristics of video. by decomposing the spatio-temporal 3d video data into a set of 2d grids with flow information, nvtm enables learning video representation rapidly and uses parameter efficiently. our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the nerv-style method, with a speed increase of over 3 times. also, it remarks an average of 1.54db/0.019 improvements in psnr/lpips on uvg (dynamic) (even with 10% fewer parameters) and an average of 1.84db/0.013 improvements in psnr/lpips on mcl-jcv (dynamic), compared to previous grid-type works. by expanding this to compression tasks, we demonstrate comparable performance to video compression standards (h.264, hevc) and recent inr approaches for video compression. additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. project page is https://sujiikim.github.io/nvtm/.",,2025-05-01,,"['seungjun shin', 'suji kim', 'dokwan oh']"
2505.00337,t2vphysbench: a first-principles benchmark for physical consistency in   text-to-video generation,cs.lg cs.ai cs.cl cs.cv,"text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. to fill this gap, we introduce \textbf{t2vphysbench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including newtonian mechanics, conservation principles, and phenomenological effects. our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. the results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.",,2025-05-01,,"['xuyang guo', 'jiayan huo', 'zhenmei shi', 'zhao song', 'jiahao zhang', 'jiale zhao']"
2505.00369,automated segmenta-on of pediatric neuroblastoma on multi-modal mri:   results of the sppin challenge at miccai 2023,cs.cv,"surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. this requires careful planning, often via magnetic resonance imaging (mri)-based anatomical 3d models. however, creating these models is often time-consuming and user dependent. we organized the surgical planning in pediatric neuroblastoma (sppin) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model mri. the challenge started with a training phase, where teams received 78 sets of mri scans from 34 patients, consisting of both diagnostic and post-chemotherapy mri scans. the final test phase, consisting of 18 mri sets from 9 patients, determined the ranking of the teams. ranking was based on the dice similarity coefficient (dice score), the 95th percentile of the hausdorff distance (hd95) and the volumetric similarity (vs). the sppin challenge was hosted at miccai 2023. the final leaderboard consisted of 9 teams. the highest-ranking team achieved a median dice score 0.82, a median hd95 of 7.69 mm and a vs of 0.91, utilizing a large, pretrained network called stu-net. a significant difference for the segmentation results between diagnostic and post-chemotherapy mri scans was observed (dice = 0.89 vs dice = 0.59, p = 0.01) for the highest-ranking team. sppin is the first medical segmentation challenge in extracranial pediatric oncology. the highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.",,2025-05-01,,"['m. a. d. buser', 'd. c. simons', 'm. fitski', 'm. h. w. a. wijnen', 'a. s. littooij', 'a. h. ter brugge', 'i. n. vos', 'm. h. a. janse', 'm. de boer', 'r. ter maat', 'j. sato', 's. kido', 's. kondo', 's. kasai', 'm. wodzinski', 'h. muller', 'j. ye', 'j. he', 'y. kirchhoff', 'm. r. rokkus', 'g. haokai', 's. zitong', 'm. fernández-patón', 'd. veiga-canuto', 'd. g. ellis', 'm. r. aizenberg', 'b. h. m. van der velden', 'h. kuijf', 'a. de luca', 'a. f. w. van der steeg']"
2505.00374,towards lightweight hyperspectral image super-resolution with depthwise   separable dilated convolutional network,eess.iv cs.cv,"deep neural networks have demonstrated highly competitive performance in super-resolution (sr) for natural images by learning mappings from low-resolution (lr) to high-resolution (hr) images. however, hyperspectral super-resolution remains an ill-posed problem due to the high spectral dimensionality of the data and the scarcity of available training samples. moreover, existing methods often rely on large models with a high number of parameters or require the fusion with panchromatic or rgb images, both of which are often impractical in real-world scenarios. inspired by the mobilenet architecture, we introduce a lightweight depthwise separable dilated convolutional network (dsdcn) to address the aforementioned challenges. specifically, our model leverages multiple depthwise separable convolutions, similar to the mobilenet architecture, and further incorporates a dilated convolution fusion block to make the model more flexible for the extraction of both spatial and spectral features. in addition, we propose a custom loss function that combines mean squared error (mse), an l2 norm regularization-based constraint, and a spectral angle-based loss, ensuring the preservation of both spectral and spatial details. the proposed model achieves very competitive performance on two publicly available hyperspectral datasets, making it well-suited for hyperspectral image super-resolution tasks. the source codes are publicly available at: \href{https://github.com/usman1021/lightweight}{https://github.com/usman1021/lightweight}.",,2025-05-01,,"['usman muhammad', 'jorma laaksonen', 'lyudmila mihaylova']"
2505.00378,cues3d: unleashing the power of sole nerf for consistent and unique   instances in open-vocabulary 3d panoptic segmentation,cs.cv,"open-vocabulary 3d panoptic segmentation has recently emerged as a significant trend. top-performing methods currently integrate 2d segmentation with geometry-aware 3d primitives. however, the advantage would be lost without high-fidelity 3d point clouds, such as methods based on neural radiance field (nerf). these methods are limited by the insufficient capacity to maintain consistency across partial observations. to address this, recent works have utilized contrastive loss or cross-view association pre-processing for view consensus. in contrast to them, we present cues3d, a compact approach that relies solely on nerf instead of pre-associations. the core idea is that nerf's implicit 3d field inherently establishes a globally consistent geometry, enabling effective object distinction without explicit cross-view supervision. we propose a three-phase training framework for nerf, initialization-disambiguation-refinement, whereby the instance ids are corrected using the initially-learned knowledge. additionally, an instance disambiguation method is proposed to match nerf-rendered 3d masks and ensure globally unique 3d instance identities. with the aid of cues3d, we obtain highly consistent and unique 3d instance id for each object across views with a balanced version of nerf. our experiments are conducted on scannet v2, scannet200, scannet++, and replica datasets for 3d instance, panoptic, and semantic segmentation tasks. cues3d outperforms other 2d image-based methods and competes with the latest 2d-3d merging based methods, while even surpassing them when using additional 3d point clouds. the code link could be found in the appendix and will be released on \href{https://github.com/mrobotit/cues3d}{github}",,2025-05-01,,"['feng xue', 'wenzhuang xu', 'guofeng zhong', 'anlong minga', 'nicu sebe']"
2505.00380,the invisible threat: evaluating the vulnerability of cross-spectral   face recognition to presentation attacks,cs.cv,"cross-spectral face recognition systems are designed to enhance the performance of facial recognition systems by enabling cross-modal matching under challenging operational conditions. a particularly relevant application is the matching of near-infrared (nir) images to visible-spectrum (vis) images, enabling the verification of individuals by comparing nir facial captures acquired with vis reference images. the use of nir imaging offers several advantages, including greater robustness to illumination variations, better visibility through glasses and glare, and greater resistance to presentation attacks. despite these claimed benefits, the robustness of nir-based systems against presentation attacks has not been systematically studied in the literature. in this work, we conduct a comprehensive evaluation into the vulnerability of nir-vis cross-spectral face recognition systems to presentation attacks. our empirical findings indicate that, although these systems exhibit a certain degree of reliability, they remain vulnerable to specific attacks, emphasizing the need for further research in this area.",,2025-05-01,,"['anjith george', 'sebastien marcel']"
2505.00394,sota: spike-navigated optimal transport saliency region detection in   composite-bias videos,cs.cv,"existing saliency detection methods struggle in real-world scenarios due to motion blur and occlusions. in contrast, spike cameras, with their high temporal resolution, significantly enhance visual saliency maps. however, the composite noise inherent to spike camera imaging introduces discontinuities in saliency detection. low-quality samples further distort model predictions, leading to saliency bias. to address these challenges, we propose spike-navigated optimal transport saliency region detection (sota), a framework that leverages the strengths of spike cameras while mitigating biases in both spatial and temporal dimensions. our method introduces spike-based micro-debias (sm) to capture subtle frame-to-frame variations and preserve critical details, even under minimal scene or lighting changes. additionally, spike-based global-debias (sg) refines predictions by reducing inconsistencies across diverse conditions. extensive experiments on real and synthetic datasets demonstrate that sota outperforms existing methods by eliminating composite noise bias. our code and dataset will be released at https://github.com/lwxfight/sota.",,2025-05-01,,"['wenxuan liu', 'yao deng', 'kang chen', 'xian zhong', 'zhaofei yu', 'tiejun huang']"
2505.00421,real-time animatable 2dgs-avatars with detail enhancement from monocular   videos,cs.cv,"high-quality, animatable 3d human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. however, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. to address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2d gaussian splatting (2dgs). by leveraging 2dgs and global smpl pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. furthermore, we introduce a rotation compensation network (rcn) that learns rotation residuals by integrating local geometric features with global pose parameters. this network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.",,2025-05-01,,"['xia yuan', 'hai yuan', 'wenyi ge', 'ying fu', 'xi wu', 'guanyu xing']"
2505.00426,leveraging pretrained diffusion models for zero-shot part assembly,cs.cv,"3d part assembly aims to understand part relationships and predict their 6-dof poses to construct realistic 3d shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. however, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. in this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an iterative closest point (icp) process. then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. to verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. the code has been released on https://github.com/ruiyuan-zhang/zero-shot-assembly.",,2025-05-01,,"['ruiyuan zhang', 'qi wang', 'jiaxiang liu', 'yu zhang', 'yuchi huo', 'chao wu']"
2505.00452,clearlines - camera calibration from straight lines,cs.cv,"the problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. however, its practical applicability remains limited, particularly in real-world outdoor scenarios. these environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3d lines, and varying lighting conditions, making the task notoriously difficult. furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. in this study, we present a small dataset named ""clearlines"", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3d line detection algorithms.",,2025-05-01,,"['gregory schroeder', 'mohamed sabry', 'cristina olaverri-monreal']"
2505.00462,"corstitch - a free, open source software for stitching and   georeferencing underwater coral reef videos",eess.iv cs.cv,"corstitch is an open-source software developed to automate the creation of accurate georeferenced reef mosaics from video transects obtained through automated rapid reef assessment system surveys. we utilized a fourier-based image correlation algorithm to stitch sequential video frames, aligning them with synchronized gnss timestamps. the resulting compressed keyhole markup language files, compatible with geographic information systems such as google earth, enable detailed spatial analysis. validation through comparative analysis of mosaics from two temporally distinct surveys of the same reef demonstrated the software's consistent and reliable performance.",,2025-05-01,,"['julian christopher l. maypa', 'johnenn r. manalang', 'maricor n. soriano']"
2505.00482,jointdit: enhancing rgb-depth joint modeling with diffusion transformers,cs.cv cs.ai,"we present jointdit, a diffusion transformer that models the joint distribution of rgb and depth. by leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, jointdit not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. this solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. with these techniques, we train our model across all noise levels for each modality, enabling jointdit to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. jointdit demonstrates outstanding joint generation performance. furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. the project page is available at https://byungki-k.github.io/jointdit/.",,2025-05-01,,"['kwon byung-ki', 'qi dai', 'lee hyoseok', 'chong luo', 'tae-hyun oh']"
2505.00497,keysync: a robust approach for leakage-free lip synchronization in high   resolution,cs.cv,"lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. however, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. to address these shortcomings, we present keysync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. we show that keysync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to lipleak, our novel leakage metric. furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. code and model weights can be found at https://antonibigata.github.io/keysync.",,2025-05-01,,"['antoni bigata', 'rodrigo mira', 'stella bounareli', 'michał stypułkowski', 'konstantinos vougioukas', 'stavros petridis', 'maja pantic']"
2505.00502,towards scalable human-aligned benchmark for text-guided image editing,cs.cv,"a variety of text-guided image editing models have been proposed recently. however, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. to address this, we introduce a novel human-aligned benchmark for text-guided image editing (hatie). providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. also, hatie provides a fully-automated and omnidirectional evaluation pipeline. particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. we empirically verify that the evaluation of hatie is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.",,2025-05-01,,"['suho ryu', 'kihyun kim', 'eugene baek', 'dongsoo shin', 'joonseok lee']"
2505.00507,heal3d: heuristical-enhanced active learning for 3d object detection,cs.cv,"active learning has proved to be a relevant approach to perform sample selection for training models for autonomous driving. particularly, previous works on active learning for 3d object detection have shown that selection of samples in uncontrolled scenarios is challenging. furthermore, current approaches focus exclusively on the theoretical aspects of the sample selection problem but neglect the practical insights that can be obtained from the extensive literature and application of 3d detection models. in this paper, we introduce heal (heuristical-enhanced active learning for 3d object detection) which integrates those heuristical features together with localization and classification to deliver the most contributing samples to the model's training. in contrast to previous works, our approach integrates heuristical features such as object distance and point-quantity to estimate the uncertainty, which enhance the usefulness of selected samples to train detection models. our quantitative evaluation on kitti shows that heal presents competitive map with respect to the state-of-the-art, and achieves the same map as the full-supervised baseline with only 24% of the samples.",,2025-05-01,2025-05-05,"['esteban rivera', 'surya prabhakaran', 'markus lienkamp']"
2505.00511,inconsistency-based active learning for lidar object detection,cs.cv,"deep learning models for object detection in autonomous driving have recently achieved impressive performance gains and are already being deployed in vehicles worldwide. however, current models require increasingly large datasets for training. acquiring and labeling such data is costly, necessitating the development of new strategies to optimize this process. active learning is a promising approach that has been extensively researched in the image domain. in our work, we extend this concept to the lidar domain by developing several inconsistency-based sample selection strategies and evaluate their effectiveness in various settings. our results show that using a naive inconsistency approach based on the number of detected boxes, we achieve the same map as the random sampling strategy with 50% of the labeled data.",,2025-05-01,,"['esteban rivera', 'loic stratil', 'markus lienkamp']"
2505.00512,interloc: lidar-based intersection localization using road segmentation   with automated evaluation method,cs.cv cs.ro,"online localization of road intersections is beneficial for autonomous vehicle localization, mapping and motion planning. intersections offer strong landmarks to correct vehicle pose estimation in gnss dropouts and anchor new sensor data in up-to-date maps. they are also decisive routing nodes in road network graphs. despite that importance, intersection localization has not been widely studied, with existing methods either ignore the rich semantic information already computed onboard or depend on scarce, hand-labeled intersection datasets. to close that gap, this paper presents a lidar-based method for online vehicle-centric intersection localization. we fuse semantic road segmentation with vehicle local pose to detect intersection candidates in a bird's eye view (bev) representation. we then refine those candidates by analyzing branch topology and correcting the intersection point in a least squares formulation. to evaluate our method, we introduce an automated benchmarking pipeline that pairs localized intersection points with openstreetmap (osm) intersection nodes using precise gnss/ins ground-truth poses. experiments on semantickitti show that the method outperforms the latest learning-based baseline in accuracy and reliability. moreover, sensitivity tests demonstrate that our method is robust to challenging segmentation error levels, highlighting its applicability in the real world.",,2025-05-01,2025-05-02,"['nguyen hoang khoi tran', 'julie stephany berrio', 'mao shan', 'zhenxing ming', 'stewart worrall']"
2505.00525,a methodological and structural review of parkinsons disease detection   across diverse data modalities,eess.iv cs.cv cs.lg,"parkinsons disease (pd) is a progressive neurological disorder that primarily affects motor functions and can lead to mild cognitive impairment (mci) and dementia in its advanced stages. with approximately 10 million people diagnosed globally 1 to 1.8 per 1,000 individuals, according to reports by the japan times and the parkinson foundation early and accurate diagnosis of pd is crucial for improving patient outcomes. while numerous studies have utilized machine learning (ml) and deep learning (dl) techniques for pd recognition, existing surveys are limited in scope, often focusing on single data modalities and failing to capture the potential of multimodal approaches. to address these gaps, this study presents a comprehensive review of pd recognition systems across diverse data modalities, including magnetic resonance imaging (mri), gait-based pose analysis, gait sensory data, handwriting analysis, speech test data, electroencephalography (eeg), and multimodal fusion techniques. based on over 347 articles from leading scientific databases, this review examines key aspects such as data collection methods, settings, feature representations, and system performance, with a focus on recognition accuracy and robustness. this survey aims to serve as a comprehensive resource for researchers, providing actionable guidance for the development of next generation pd recognition systems. by leveraging diverse data modalities and cutting-edge machine learning paradigms, this work contributes to advancing the state of pd diagnostics and improving patient care through innovative, multimodal approaches.",,2025-05-01,,"['abu saleh musa miah', 'taro suzuki', 'jungpil shin']"
2505.00534,a robust deep networks based multi-object multicamera tracking system   for city scale traffic,cs.cv,"vision sensors are becoming more important in intelligent transportation systems (its) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. however, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. these challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. to address these issues, we propose an efficient and cost-effective deep learning-based framework for multi-object multi-camera tracking (mo-mct). the proposed framework utilizes mask r-cnn for object detection and employs non-maximum suppression (nms) to select target objects from overlapping detections. transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. the final solution identification module performs feature extraction using resnet-152 coupled with deep sort based vehicle tracking. the proposed framework is evaluated on the 5th ai city challenge dataset (track 3), comprising 46 camera feeds. among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. the proposed framework achieves competitive performance with an idf1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.",10.1007/s11042-023-16243-7,2025-05-01,,"['muhammad imran zaman', 'usama ijaz bajwa', 'gulshan saleem', 'rana hammad raza']"
2505.00564,x-ray illicit object detection using hybrid cnn-transformer neural   network architectures,cs.cv,"in the field of x-ray security applications, even the smallest details can significantly impact outcomes. objects that are heavily occluded or intentionally concealed pose a great challenge for detection, whether by human observation or through advanced technological applications. while certain deep learning (dl) architectures demonstrate strong performance in processing local information, such as convolutional neural networks (cnns), others excel in handling distant information, e.g., transformers. in x-ray security imaging the literature has been dominated by the use of cnn-based methods, while the integration of the two aforementioned leading architectures has not been sufficiently explored. in this paper, various hybrid cnn-transformer architectures are evaluated against a common cnn object detection baseline, namely yolov8. in particular, a cnn (hgnetv2) and a hybrid cnn-transformer (next-vit-s) backbone are combined with different cnn/transformer detection heads (yolov8 and rt-detr). the resulting architectures are comparatively evaluated on three challenging public x-ray inspection datasets, namely eds, hixray, and pidray. interestingly, while the yolov8 detector with its default backbone (csp-darknet53) is generally shown to be advantageous on the hixray and pidray datasets, when a domain distribution shift is incorporated in the x-ray images (as happens in the eds datasets), hybrid cnn-transformer architectures exhibit increased robustness. detailed comparative evaluation results, including object-level detection performance and object-size error analysis, demonstrate the strengths and weaknesses of each architectural combination and suggest guidelines for future research. the source code and network weights of the models employed in this study are available at https://github.com/jgenc/xray-comparative-evaluation.",,2025-05-01,,"['jorgen cani', 'christos diou', 'spyridon evangelatos', 'panagiotis radoglou-grammatikis', 'vasileios argyriou', 'panagiotis sarigiannidis', 'iraklis varlamis', 'georgios th. papadopoulos']"
2505.00568,multimodal masked autoencoder pre-training for 3d mri-based brain tumor   analysis with missing modalities,cs.cv cs.ai,"multimodal magnetic resonance imaging (mri) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. this behavior is especially valuable in medical imaging, where annotations are often scarce. however, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. in practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. therefore, we introduce bm-mae, a masked image modeling pre-training strategy tailored for multimodal mri data. the same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. this allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. code and trained models are available at: https://github.com/lucas-rbnt/bm-mae",,2025-05-01,2025-05-02,"['lucas robinet', 'ahmad berjaoui', 'elizabeth cohen-jonathan moyal']"
2505.00569,animalmotionclip: embedding motion in clip for animal behavior analysis,cs.cv,"recently, there has been a surge of interest in applying deep learning techniques to animal behavior recognition, particularly leveraging pre-trained visual language models, such as clip, due to their remarkable generalization capacity across various downstream tasks. however, adapting these models to the specific domain of animal behavior recognition presents two significant challenges: integrating motion information and devising an effective temporal modeling scheme. in this paper, we propose animalmotionclip to address these challenges by interleaving video frames and optical flow information in the clip framework. additionally, several temporal modeling schemes using an aggregation of classifiers are proposed and compared: dense, semi dense, and sparse. as a result, fine temporal actions can be correctly recognized, which is of vital importance in animal behavior analysis. experiments on the animal kingdom dataset demonstrate that animalmotionclip achieves superior performance compared to state-of-the-art approaches.",,2025-04-30,,"['enmin zhong', 'carlos r. del-blanco', 'daniel berjón', 'fernando jaureguizar', 'narciso garcía']"
2505.00584,synthesizing and identifying noise levels in autonomous vehicle camera   radar datasets,cs.cv cs.ai eess.iv eess.sp,"detecting and tracking objects is a crucial component of any autonomous navigation method. for the past decades, object detection has yielded promising results using neural networks on various datasets. while many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. in this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar autonomous vehicle (av) datasets. our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. we also present our results of a baseline lightweight noise recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.",,2025-05-01,,"['mathis morales', 'golnaz habibi']"
2505.00592,uncertainty-aware multi-expert knowledge distillation for imbalanced   disease grading,cs.cv cs.lg,"automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. however, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. to address the problem, we propose a novel \textbf{u}ncertainty-aware \textbf{m}ulti-experts \textbf{k}nowledge \textbf{d}istillation (umkd) framework to transfer knowledge from multiple expert models to a single student model. specifically, to extract discriminative features, umkd decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. at the output space, an uncertainty-aware decoupled distillation (udd) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. additionally, umkd also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous kd approaches. extensive experiments on histology prostate grading (\textit{sicapv2}) and fundus image grading (\textit{aptos}) demonstrate that umkd achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.",,2025-05-01,,"['shuo tong', 'shangde gao', 'ke liu', 'zihang huang', 'hongxia xu', 'haochao ying', 'jian wu']"
2505.00599,visual trajectory prediction of vessels for inland navigation,cs.cv,"the future of inland navigation increasingly relies on autonomous systems and remote operations, emphasizing the need for accurate vessel trajectory prediction. this study addresses the challenges of video-based vessel tracking and prediction by integrating advanced object detection methods, kalman filters, and spline-based interpolation. however, existing detection systems often misclassify objects in inland waterways due to complex surroundings. a comparative evaluation of tracking algorithms, including bot-sort, deep oc-sort, and byetrack, highlights the robustness of the kalman filter in providing smoothed trajectories. experimental results from diverse scenarios demonstrate improved accuracy in predicting vessel movements, which is essential for collision avoidance and situational awareness. the findings underline the necessity of customized datasets and models for inland navigation. future work will expand the datasets and incorporate vessel classification to refine predictions, supporting both autonomous systems and human operators in complex environments.",,2025-05-01,,"['alexander puzicha', 'konstantin wüstefeld', 'kathrin wilms', 'frank weichert']"
2505.00606,dietary intake estimation via continuous 3d reconstruction of food,cs.cv cs.lg,"monitoring dietary habits is crucial for preventing health risks associated with overeating and undereating, including obesity, diabetes, and cardiovascular diseases. traditional methods for tracking food intake rely on self-reported data before or after the eating, which are prone to inaccuracies. this study proposes an approach to accurately monitor ingest behaviours by leveraging 3d food models constructed from monocular 2d video. using colmap and pose estimation algorithms, we generate detailed 3d representations of food, allowing us to observe changes in food volume as it is consumed. experiments with toy models and real food items demonstrate the approach's potential. meanwhile, we have proposed a new methodology for automated state recognition challenges to accurately detect state changes and maintain model fidelity. the 3d reconstruction approach shows promise in capturing comprehensive dietary behaviour insights, ultimately contributing to the development of automated and accurate dietary monitoring tools.",,2025-05-01,,"['wallace lee', 'yuhao chen']"
2505.00615,pixel3dmm: versatile screen-space priors for single-image 3d face   reconstruction,cs.cv cs.ai,"we address the 3d reconstruction of human faces from a single rgb image. to this end, we propose pixel3dmm, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3d morphable face model (3dmm). we exploit the latent features of the dino foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. we train our model by registering three high-quality 3d face datasets against the flame mesh topology, which results in a total of over 1,000 identities and 976k images. for 3d face reconstruction, we propose a flame fitting opitmization that solves for the 3dmm parameters from the uv-coordinate and normal estimates. to evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.",,2025-05-01,,"['simon giebenhain', 'tobias kirschstein', 'martin rünz', 'lourdes agapito', 'matthias nießner']"
2505.00619,diverse semantics-guided feature alignment and decoupling for   visible-infrared person re-identification,cs.cv,"visible-infrared person re-identification (vi-reid) is a challenging task due to the large modality discrepancy between visible and infrared images, which complicates the alignment of their features into a suitable common space. moreover, style noise, such as illumination and color contrast, reduces the identity discriminability and modality invariance of features. to address these challenges, we propose a novel diverse semantics-guided feature alignment and decoupling (dsfad) network to align identity-relevant features from different modalities into a textual embedding space and disentangle identity-irrelevant features within each modality. specifically, we develop a diverse semantics-guided feature alignment (dsfa) module, which generates pedestrian descriptions with diverse sentence structures to guide the cross-modality alignment of visual features. furthermore, to filter out style information, we propose a semantic margin-guided feature decoupling (smfd) module, which decomposes visual features into pedestrian-related and style-related components, and then constrains the similarity between the former and the textual embeddings to be at least a margin higher than that between the latter and the textual embeddings. additionally, to prevent the loss of pedestrian semantics during feature decoupling, we design a semantic consistency-guided feature restitution (scfr) module, which further excavates useful information for identification from the style-related features and restores it back into the pedestrian-related features, and then constrains the similarity between the features after restitution and the textual embeddings to be consistent with that between the features before decoupling and the textual embeddings. extensive experiments on three vi-reid datasets demonstrate the superiority of our dsfad.",,2025-05-01,,"['neng dong', 'shuanglin yan', 'liyan zhang', 'jinhui tang']"
2505.00627,brain foundation models with hypergraph dynamic adapter for brain   disease analysis,cs.cv,"brain diseases, such as alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. however, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. in this work, we propose sam-brain3d, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 mri sub-modalities, and hypergraph dynamic adapter (hyda), a lightweight adapter for efficient and effective downstream adaptation. sam-brain3d captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. hyda leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.",,2025-05-01,,"['zhongying deng', 'haoyu wang', 'ziyan huang', 'lipei zhang', 'angelica i. aviles-rivero', 'chaoyu liu', 'junjun he', 'zoe kourtzi', 'carola-bibiane schönlieb']"
2505.00630,"vision mamba in remote sensing: a comprehensive survey of techniques,   applications and outlook",cs.cv,"deep learning has profoundly transformed remote sensing, yet prevailing architectures like convolutional neural networks (cnns) and vision transformers (vits) remain constrained by critical trade-offs: cnns suffer from limited receptive fields, while vits grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. state space models (ssms), particularly the recently proposed mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. this survey presents a comprehensive review of mamba-based methodologies in remote sensing, systematically analyzing about 120 mamba-based remote sensing studies to construct a holistic taxonomy of innovations and applications. our contributions are structured across five dimensions: (i) foundational principles of vision mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid ssm formulations, (iii) macro-architectural integrations, including cnn-transformer-mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. by bridging the gap between ssm theory and remote sensing practice, this survey establishes mamba as a transformative framework for remote sensing analysis. to our knowledge, this paper is the first systematic review of mamba architectures in remote sensing. our work provides a structured foundation for advancing research in remote sensing systems through ssm-based methods. we curate an open-source repository (https://github.com/baobao0926/awesome-mamba-in-remote-sensing) to foster community-driven advancements.",,2025-05-01,2025-05-03,"['muyi bao', 'shuchang lyu', 'zhaoyang xu', 'huiyu zhou', 'jinchang ren', 'shiming xiang', 'xiangtai li', 'guangliang cheng']"
2505.00643,deep learning assisted outer volume removal for highly-accelerated   real-time dynamic mri,eess.iv cs.ai cs.cv physics.med-ph,"real-time (rt) dynamic mri plays a vital role in capturing rapid physiological processes, offering unique insights into organ motion and function. among these applications, rt cine mri is particularly important for functional assessment of the heart with high temporal resolution. rt imaging enables free-breathing, ungated imaging of cardiac motion, making it a crucial alternative for patients who cannot tolerate conventional breath-hold, ecg-gated acquisitions. however, achieving high acceleration rates in rt cine mri is challenging due to aliasing artifacts from extra-cardiac tissues, particularly at high undersampling factors. in this study, we propose a novel outer volume removal (ovr) method to address this challenge by eliminating aliasing contributions from non-cardiac regions in a post-processing framework. our approach estimates the outer volume signal for each timeframe using composite temporal images from time-interleaved undersampling patterns, which inherently contain pseudo-periodic ghosting artifacts. a deep learning (dl) model is trained to identify and remove these artifacts, producing a clean outer volume estimate that is subsequently subtracted from the corresponding k-space data. the final reconstruction is performed with a physics-driven dl (pd-dl) method trained using an ovr-specific loss function to restore high spatio-temporal resolution images. experimental results show that the proposed method at high accelerations achieves image quality that is visually comparable to clinical baseline images, while outperforming conventional reconstruction techniques, both qualitatively and quantitatively. the proposed approach provides a practical and effective solution for artifact reduction in rt cine mri without requiring acquisition modifications, offering a pathway to higher acceleration rates while preserving diagnostic quality.",,2025-05-01,,"['merve gülle', 'sebastian weingärtner', 'mehmet akçakaya']"
2505.00668,deep reinforcement learning for urban air quality management:   multi-objective optimization of pollution mitigation booth placement in   metropolitan environments,cs.cv cs.ai cs.lg,"urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like delhi, where exposure to harmful pollutants severely impacts public health. delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. this study presents a novel deep reinforcement learning (drl) framework to optimize the placement of air purification booths to improve the air quality index (aqi) in the city of delhi. we employ proximal policy optimization (ppo), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. our approach is benchmarked against conventional placement strategies, including random and greedy aqi-based methods, using multi-dimensional performance evaluation metrics such as aqi improvement, spatial coverage, population and traffic impact, and spatial entropy. experimental results demonstrate that the rl-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. notably, the drl framework achieves an optimal trade-off between aqi reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. the findings underscore the potential of ai-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.",,2025-05-01,,"['kirtan rajesh', 'suvidha rupesh kumar']"
2505.00681,minerva: evaluating complex video reasoning,cs.lg cs.cv,"multimodal llms are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. this makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. to remedy this, we provide a new video reasoning dataset called minerva for modern multimodal models. each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. we perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. we use this to explore both human and llm-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. the dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.",,2025-05-01,,"['arsha nagrani', 'sachit menon', 'ahmet iscen', 'shyamal buch', 'ramin mehran', 'nilpa jha', 'anja hauth', 'yukun zhu', 'carl vondrick', 'mikhail sirotenko', 'cordelia schmid', 'tobias weyand']"
2505.00684,visual test-time scaling for gui agent grounding,cs.cv cs.ai cs.lg,"we introduce regionfocus, a visual test-time scaling approach for vision language model agents. understanding webpages is challenging due to the visual complexity of gui images and the large number of interface elements, making accurate action selection difficult. our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. to support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. even with a simple region selection strategy, we observe significant performance gains of 28+\% on screenspot-pro and 24+\% on webvoyager benchmarks on top of two state-of-the-art open vision language model agents, ui-tars and qwen2.5-vl, highlighting the effectiveness of visual test-time scaling in interactive settings. we achieve a new state-of-the-art grounding performance of 61.6\% on the screenspot-pro benchmark by applying regionfocus to a qwen2.5-vl-72b model. our code will be released publicly at https://github.com/tiangeluo/regionfocus.",,2025-05-01,,"['tiange luo', 'lajanugen logeswaran', 'justin johnson', 'honglak lee']"
2505.00687,guidesr: rethinking guidance for one-step high-fidelity diffusion-based   super-resolution,eess.iv cs.cv,"in this paper, we propose guidesr, a novel single-step diffusion-based image super-resolution (sr) model specifically designed to enhance image fidelity. existing diffusion-based sr approaches typically adapt pre-trained generative models to image restoration tasks by adding extra conditioning on a vae-downsampled representation of the degraded input, which often compromises structural fidelity. guidesr addresses this limitation by introducing a dual-branch architecture comprising: (1) a guidance branch that preserves high-fidelity structures from the original-resolution degraded input, and (2) a diffusion branch, which a pre-trained latent diffusion model to enhance perceptual quality. unlike conventional conditioning mechanisms, our guidance branch features a tailored structure for image restoration tasks, combining full resolution blocks (frbs) with channel attention and an image guidance network (ign) with guided attention. by embedding detailed structural information directly into the restoration pipeline, guidesr produces sharper and more visually consistent results. extensive experiments on benchmark datasets demonstrate that guidesr achieves state-of-the-art performance while maintaining the low computational cost of single-step approaches, with up to 1.39db psnr gain on challenging real-world datasets. our approach consistently outperforms existing methods across various reference-based metrics including psnr, ssim, lpips, dists and fid, further representing a practical advancement for real-world image restoration.",,2025-05-01,,"['aditya arora', 'zhengzhong tu', 'yufei wang', 'ruizheng bai', 'jian wang', 'sizhuo ma']"
2505.00690,towards autonomous micromobility through scalable urban simulation,cs.cv cs.ai cs.ro,"micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. assisting humans with ai agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. in this work, we present a scalable urban simulation solution to advance autonomous micromobility. first, we build urban-sim - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. urban-sim contains three critical modules: hierarchical urban generation pipeline, interactive dynamics generation strategy, and asynchronous scene sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. then, we propose urban-bench - a suite of essential tasks and benchmarks to gauge various capabilities of the ai agents in achieving autonomous micromobility. urban-bench includes eight tasks based on three core skills of the agents: urban locomotion, urban navigation, and urban traverse. we evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.",,2025-05-01,,"['wayne wu', 'honglin he', 'chaoyuan zhang', 'jack he', 'seth z. zhao', 'ran gong', 'quanyi li', 'bolei zhou']"
2505.00693,robotic visual instruction,cs.ro cs.ai cs.cv,"recently, natural language has been the primary medium for human-robot interaction. however, its inherent lack of spatial precision introduces challenges for robotic task definition such as ambiguity and verbosity. moreover, in some public settings where quiet is required, such as libraries or hospitals, verbal communication with robots is inappropriate. to address these limitations, we introduce the robotic visual instruction (rovi), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. rovi effectively encodes spatial-temporal information into human-interpretable visual instructions through 2d sketches, utilizing arrows, circles, colors, and numbers to direct 3d robotic manipulation. to enable robots to understand rovi better and generate precise actions based on rovi, we present visual instruction embodied workflow (view), a pipeline formulated for rovi-conditioned policies. this approach leverages vision-language models (vlms) to interpret rovi inputs, decode spatial and temporal constraints from 2d pixel space via keypoint extraction, and then transform them into executable 3d action sequences. we additionally curate a specialized dataset of 15k instances to fine-tune small vlms for edge deployment,enabling them to effectively learn rovi capabilities. our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. notably, view achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. project website: https://robotic-visual-instruction.github.io/",,2025-05-01,2025-05-06,"['yanbang li', 'ziyang gong', 'haoyang li', 'xiaoqi huang', 'haolan kang', 'guangping bai', 'xianzheng ma']"
2505.00702,rayzer: a self-supervised large view synthesis model,cs.cv,"we present rayzer, a self-supervised multi-view 3d vision model trained without any 3d supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3d awareness. concretely, rayzer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. during training, rayzer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing rayzer to be trained with 2d image supervision. the emerging 3d awareness of rayzer is attributed to two key factors. first, we design a self-supervised framework, which achieves 3d-aware auto-encoding of input images by disentangling camera and scene representations. second, we design a transformer-based model in which the only 3d prior is the ray structure, connecting camera, pixel, and scene simultaneously. rayzer demonstrates comparable or even superior novel view synthesis performance than ``oracle'' methods that rely on pose annotations in both training and testing. project: https://hwjiang1510.github.io/rayzer/",,2025-05-01,,"['hanwen jiang', 'hao tan', 'peng wang', 'haian jin', 'yue zhao', 'sai bi', 'kai zhang', 'fujun luan', 'kalyan sunkavalli', 'qixing huang', 'georgios pavlakos']"
2505.00703,t2i-r1: reinforcing image generation with collaborative semantic-level   and token-level cot,cs.cv cs.ai cs.cl cs.lg,"recent advancements in large language models have demonstrated how chain-of-thought (cot) and reinforcement learning (rl) can improve performance. however, applying such reasoning strategies to the visual generation domain remains largely unexplored. in this paper, we present t2i-r1, a novel reasoning-enhanced text-to-image generation model, powered by rl with a bi-level cot reasoning process. specifically, we identify two levels of cot that can be utilized to enhance different stages of generation: (1) the semantic-level cot for high-level planning of the prompt and (2) the token-level cot for low-level pixel processing during patch-by-patch generation. to better coordinate these two levels of cot, we introduce bicot-grpo with an ensemble of generation rewards, which seamlessly optimizes both generation cots within the same training step. by applying our reasoning strategies to the baseline model, janus-pro, we achieve superior performance with 13% improvement on t2i-compbench and 19% improvement on the wise benchmark, even surpassing the state-of-the-art model flux.1. code is available at: https://github.com/caraj7/t2i-r1",,2025-05-01,,"['dongzhi jiang', 'ziyu guo', 'renrui zhang', 'zhuofan zong', 'hao li', 'le zhuo', 'shilin yan', 'pheng-ann heng', 'hongsheng li']"
2505.00704,controllable weather synthesis and removal with video diffusion models,cs.gr cs.cv,"generating realistic and controllable weather effects in videos is valuable for many applications. physics-based weather simulation requires precise reconstructions that are hard to scale to in-the-wild videos, while current video editing often lacks realism and control. in this work, we introduce weatherweaver, a video diffusion model that synthesizes diverse weather effects -- including rain, snow, fog, and clouds -- directly into any input video without the need for 3d modeling. our model provides precise control over weather effect intensity and supports blending various weather types, ensuring both realism and adaptability. to overcome the scarcity of paired training data, we propose a novel data strategy combining synthetic videos, generative image editing, and auto-labeled real-world videos. extensive evaluations show that our method outperforms state-of-the-art methods in weather simulation and removal, providing high-quality, physically plausible, and scene-identity-preserving results over various real-world videos.",,2025-05-01,,"['chih-hao lin', 'zian wang', 'ruofan liang', 'yuxuan zhang', 'sanja fidler', 'shenlong wang', 'zan gojcic']"
2505.00734,unconstrained large-scale 3d reconstruction and rendering across   altitudes,cs.cv eess.iv,"production of photorealistic, navigable 3d site models requires a large volume of carefully collected images that are often unavailable to first responders for disaster relief or law enforcement. real-world challenges include limited numbers of images, heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences for images collected from varying altitudes. to promote research aimed at addressing these challenges, we have developed the first public benchmark dataset for 3d reconstruction and novel view synthesis based on multiple calibrated ground-level, security-level, and airborne cameras. we present datasets that pose real-world challenges, independently evaluate calibration of unposed cameras and quality of novel rendered views, demonstrate baseline performance using recent state-of-practice methods, and identify challenges for further research.",,2025-04-29,,"['neil joshi', 'joshua carney', 'nathanael kuo', 'homer li', 'cheng peng', 'myron brown']"
2505.00735,leveraging depth maps and attention mechanisms for enhanced image   inpainting,eess.iv cs.cv,"existing deep learning-based image inpainting methods typically rely on convolutional networks with rgb images to reconstruct images. however, relying exclusively on rgb images may neglect important depth information, which plays a critical role in understanding the spatial and structural context of a scene. just as human vision leverages stereo cues to perceive depth, incorporating depth maps into the inpainting process can enhance the model's ability to reconstruct images with greater accuracy and contextual awareness. in this paper, we propose a novel approach that incorporates both rgb and depth images for enhanced image inpainting. our models employ a dual encoder architecture, where one encoder processes the rgb image and the other handles the depth image. the encoded features from both encoders are then fused in the decoder using an attention mechanism, effectively integrating the rgb and depth representations. we use two different masking strategies, line and square, to test the robustness of the model under different types of occlusions. to further analyze the effectiveness of our approach, we use gradient-weighted class activation mapping (grad-cam) visualizations to examine the regions of interest the model focuses on during inpainting. we show that incorporating depth information alongside the rgb image significantly improves the reconstruction quality. through both qualitative and quantitative comparisons, we demonstrate that the depth-integrated model outperforms the baseline, with attention mechanisms further enhancing inpainting performance, as evidenced by multiple evaluation metrics and visualization.",,2025-04-29,2025-05-08,"['jin hyun park', 'harine choi', 'praewa pitiphat']"
2505.00737,"a survey on 3d reconstruction techniques in plant phenotyping: from   classical methods to neural radiance fields (nerf), 3d gaussian splatting   (3dgs), and beyond",eess.iv cs.ai cs.cv,"plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3d reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. this paper provides a comprehensive review of the 3d reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging neural radiance fields (nerf), and the novel 3d gaussian splatting (3dgs) approach. classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. however, they face challenges such as data density, noise, and scalability. nerf, a recent advancement, enables high-quality, photorealistic 3d reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. the emerging 3dgs technique introduces a new paradigm in reconstructing plant structures by representing geometry through gaussian primitives, offering potential benefits in both efficiency and scalability. we review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/jiajiali04/3d-reconstruction-plants). through this review, we aim to provide insights into how these diverse 3d reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.",,2025-04-29,,"['jiajia li', 'xinda qi', 'seyed hamidreza nabaei', 'meiqi liu', 'dong chen', 'xin zhang', 'xunyuan yin', 'zhaojian li']"
2505.00739,mosam: motion-guided segment anything model with spatial-temporal memory   selection,cs.cv eess.iv,"the recent segment anything model 2 (sam2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. however, as a foundational model on interactive segmentation, sam2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. firstly, during inference in videos, objects may disappear since sam2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. to address these problems, we present mosam, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. firstly, we propose motion-guided prompting (mgp), which represents the object motion in both sparse and dense manners, then injects them into sam2 through a set of motion-guided prompts. mgp enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. furthermore, acknowledging that past segmentation results may be inaccurate, we devise a spatial-temporal memory selection (st-ms) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. by eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our mosam achieves state-of-the-art results compared to other competitors.",,2025-04-29,,"['qiushi yang', 'yuan yao', 'miaomiao cui', 'liefeng bo']"
2505.00740,fast2comm:collaborative perception combined with prior knowledge,cs.cv cs.ma,"collaborative perception has the potential to significantly enhance perceptual accuracy through the sharing of complementary information among agents. however, real-world collaborative perception faces persistent challenges, particularly in balancing perception performance and bandwidth limitations, as well as coping with localization errors. to address these challenges, we propose fast2comm, a prior knowledge-based collaborative perception framework. specifically, (1)we propose a prior-supervised confidence feature generation method, that effectively distinguishes foreground from background by producing highly discriminative confidence features; (2)we propose gt bounding box-based spatial prior feature selection strategy to ensure that only the most informative prior-knowledge features are selected and shared, thereby minimizing background noise and optimizing bandwidth efficiency while enhancing adaptability to localization inaccuracies; (3)we decouple the feature fusion strategies between model training and testing phases, enabling dynamic bandwidth adaptation. to comprehensively validate our framework, we conduct extensive experiments on both real-world and simulated datasets. the results demonstrate the superior performance of our model and highlight the necessity of the proposed methods. our code is available at https://github.com/zhangzhengbin-tj/fast2comm.",,2025-04-29,,"['zhengbin zhang', 'yan wu', 'hongkun zhang']"
2505.00741,detection and classification of diseases in multi-crop leaves using lstm   and cnn models,cs.cv cs.lg,"plant diseases pose a serious challenge to agriculture by reducing crop yield and affecting food quality. early detection and classification of these diseases are essential for minimising losses and improving crop management practices. this study applies convolutional neural networks (cnn) and long short-term memory (lstm) models to classify plant leaf diseases using a dataset containing 70,295 training images and 17,572 validation images across 38 disease classes. the cnn model was trained using the adam optimiser with a learning rate of 0.0001 and categorical cross-entropy as the loss function. after 10 training epochs, the model achieved a training accuracy of 99.1% and a validation accuracy of 96.4%. the lstm model reached a validation accuracy of 93.43%. performance was evaluated using precision, recall, f1-score, and confusion matrix, confirming the reliability of the cnn-based approach. the results suggest that deep learning models, particularly cnn, enable an effective solution for accurate and scalable plant disease classification, supporting practical applications in agricultural monitoring.",,2025-04-29,,"['srinivas kanakala', 'sneha ningappa']"
2505.00742,zoomer: adaptive image focus optimization for black-box mllm,cs.cv cs.ai eess.iv,"recent advancements in multimodal large language models (mllms) have broadened the scope of vision-language tasks, excelling in applications like image captioning and interactive question-answering. however, these models struggle with accurately processing visual data, particularly in tasks requiring precise object recognition and fine visual details. stringent token limits often result in the omission of critical information, hampering performance. to address these limitations, we introduce \sysname, a novel visual prompting mechanism designed to enhance mllm performance while preserving essential visual details within token limits. \sysname features three key innovations: a prompt-aware strategy that dynamically highlights relevant image regions, a spatial-preserving orchestration schema that maintains object integrity, and a budget-aware prompting method that balances global context with crucial visual details. comprehensive evaluations across multiple datasets demonstrate that \sysname consistently outperforms baseline methods, achieving up to a $26.9\%$ improvement in accuracy while significantly reducing token consumption.",,2025-04-29,,"['jiaxu qian', 'chendong wang', 'yifan yang', 'chaoyun zhang', 'huiqiang jiang', 'xufang luo', 'yu kang', 'qingwei lin', 'anlan zhang', 'shiqi jiang', 'ting cao', 'tianjun mao', 'suman banerjee', 'guyue liu', 'saravan rajmohan', 'dongmei zhang', 'yuqing yang', 'qi zhang', 'lili qiu']"
2505.00743,dope: dual object perception-enhancement network for vision-and-language   navigation,cs.cv cs.ro,"vision-and-language navigation (vln) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. the agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. despite significant advancements in this field, two major limitations persist: (1) many existing methods input complete language instructions directly into multi-layer transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. we propose a dual object perception-enhancement network (dope) to address these issues to improve navigation performance. first, we design a text semantic extraction (tse) to extract relatively essential phrases from the text and input them into the text object perception-augmentation (topa) to fully leverage details such as objects and actions within the instructions. second, we introduce an image object perception-augmentation (iopa), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. extensive experiments on the r2r and reverie datasets validate the efficacy of the proposed approach.",10.1145/3731715.3733315,2025-04-30,,"['yinfeng yu', 'dongsheng yang']"
2505.00745,responsive dnn adaptation for video analytics against environment shift   via hierarchical mobile-cloud collaborations,cs.cv cs.lg,"mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed ""expert dnn models"". existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. instead, this paper proposes mocha, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. specifically, mocha (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. extensive evaluations with real-world videos on three dnn tasks show mocha improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.",,2025-04-30,,"['maozhe zhao', 'shengzhong liu', 'fan wu', 'guihai chen']"
2505.00746,entropy heat-mapping: localizing gpt-based ocr errors with   sliding-window shannon analysis,cs.cv,"vision-language models such as openai gpt-4o can transcribe mathematical documents directly from images, yet their token-level confidence signals are seldom used to pinpoint local recognition mistakes. we present an entropy-heat-mapping proof-of-concept that turns per-token shannon entropy into a visual ''uncertainty landscape''. by scanning the entropy sequence with a fixed-length sliding window, we obtain hotspots that are likely to contain ocr errors such as missing symbols, mismatched braces, or garbled prose. using a small, curated set of scanned research pages rendered at several resolutions, we compare the highlighted hotspots with the actual transcription errors produced by gpt-4o. our analysis shows that the vast majority of true errors are indeed concentrated inside the high-entropy regions. this study demonstrates--in a minimally engineered setting--that sliding-window entropy can serve as a practical, lightweight aid for post-editing gpt-based ocr. all code and annotation guidelines are released to encourage replication and further research.",,2025-04-30,2025-05-05,['alexei kaltchenko']
2505.00747,wireless communication as an information sensor for multi-agent   cooperative perception: a survey,cs.oh cs.cv cs.ma cs.ro,"cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via vehicle-to-everything (v2x) communication. unlike traditional onboard sensors, v2x acts as a dynamic ""information sensor"" characterized by limited communication, heterogeneity, mobility, and scalability. this survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. we categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. in information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. finally, we summarize system-level approaches to support scalability in dense traffic scenarios. compared with existing surveys, this paper introduces a new perspective by treating v2x communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.",,2025-04-30,,"['zhiying song', 'tenghui xie', 'fuxi wen', 'jun li']"
2505.00751,instructattribute: fine-grained object attributes editing with   instruction,cs.cv,"text-to-image (t2i) diffusion models, renowned for their advanced generative abilities, are extensively utilized in image editing applications, demonstrating remarkable effectiveness. however, achieving precise control over fine-grained attributes still presents considerable challenges. existing image editing techniques either fail to modify the attributes of an object or struggle to preserve its structure and maintain consistency in other areas of the image. to address these challenges, we propose the structure-preserving and attribute amplification (spaa), a training-free method which enables precise control over the color and material transformations of objects by editing the self-attention maps and cross-attention values. furthermore, we constructed the attribute dataset, which encompasses nearly all colors and materials associated with various objects, by integrating multimodal large language models (mllm) to develop an automated pipeline for data filtering and instruction labeling. training on this dataset, we present our instructattribute, an instruction-based model designed to facilitate fine-grained editing of color and material attributes. extensive experiments demonstrate that our method achieves superior performance in object-level color and material editing, outperforming existing instruction-based image editing approaches.",,2025-04-30,,"['xingxi yin', 'jingfeng zhang', 'zhi li', 'yicheng li', 'yin zhang']"
2505.00755,p2p-insole: human pose estimation using foot pressure distribution and   motion sensors,cs.cv cs.ai,"this work presents p2p-insole, a low-cost approach for estimating and visualizing 3d human skeletal data using insole-type sensors integrated with imus. each insole, fabricated with e-textile garment techniques, costs under usd 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. the system employs a transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. these facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. this work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.",,2025-05-01,,"['atsuya watanabe', 'ratna aisuwarya', 'lei jing']"
2505.00757,efficient on-chip implementation of 4d radar-based 3d object detection   on hailo-8l,cs.cv cs.ai,"4d radar has attracted attention in autonomous driving due to its ability to enable robust 3d object detection even under adverse weather conditions. to practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. addressing this, we present the first on-chip implementation of a 4d radar-based 3d object detection model on the hailo-8l ai accelerator. although conventional 3d convolutional neural network (cnn) architectures require 5d inputs, the hailo-8l only supports 4d tensors, posing a significant challenge. to overcome this limitation, we introduce a tensor transformation method that reshapes 5d inputs into 4d formats during the compilation process, enabling direct deployment without altering the model structure. the proposed system achieves 46.47% ap_3d and 52.75% ap_bev, maintaining comparable accuracy to gpu-based models while achieving an inference speed of 13.76 hz. these results demonstrate the applicability of 4d radar-based perception technologies to autonomous driving systems.",,2025-05-01,,"['woong-chan byun', 'dong-hee paek', 'seung-hyun song', 'seung-hyun kong']"
2505.00772,person detection and re-identification in open-world settings of retail   stores and public spaces,cs.cv,"practical applications of computer vision in smart cities usually assume system integration and operation in challenging open-world environments. in the case of person re-identification task the main goal is to retrieve information whether the specific person has appeared in another place at a different time instance of the same video, or over multiple camera feeds. this typically assumes collecting raw data from video surveillance cameras in different places and under varying illumination conditions. in the considered open-world setting it also requires detection and localization of the person inside the analyzed video frame before the main re-identification step. with multi-person and multi-camera setups the system complexity becomes higher, requiring sophisticated tracking solutions and re-identification models. in this work we will discuss existing challenges in system design architectures, consider possible solutions based on different computer vision techniques, and describe applications of such systems in retail stores and public spaces for improved marketing analytics. in order to analyse sensitivity of person re-identification task under different open-world environments, a performance of one close to real-time solution will be demonstrated over several video captures and live camera feeds. finally, based on conducted experiments we will indicate further research directions and possible system improvements.",,2025-05-01,,"['branko brkljač', 'milan brkljač']"
2505.00786,ai-ready snow radar echogram dataset (sred) for climate change   monitoring,cs.cv,"tracking internal layers in radar echograms with high accuracy is essential for understanding ice sheet dynamics and quantifying the impact of accelerated ice discharge in greenland and other polar regions due to contemporary global climate warming. deep learning algorithms have become the leading approach for automating this task, but the absence of a standardized and well-annotated echogram dataset has hindered the ability to test and compare algorithms reliably, limiting the advancement of state-of-the-art methods for the radar echogram layer tracking problem. this study introduces the first comprehensive ``deep learning ready'' radar echogram dataset derived from snow radar airborne data collected during the national aeronautics and space administration operation ice bridge (oib) mission in 2012. the dataset contains 13,717 labeled and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation, wet) with varying along-track resolutions. to demonstrate its utility, we evaluated the performance of five deep learning models on the dataset. our results show that while current computer vision segmentation algorithms can identify and track snow layer pixels in echogram images, advanced end-to-end models are needed to directly extract snow depth and annual accumulation from echograms, reducing or eliminating post-processing. the dataset and accompanying benchmarking framework provide a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation, advancing our understanding of polar ice sheets response to climate warming.",,2025-05-01,,"['oluwanisola ibikunle', 'hara talasila', 'debvrat varshney', 'jilu li', 'john paden', 'maryam rahnemoonfar']"
2505.00788,spatialllm: a compound 3d-informed design towards spatially-intelligent   large multimodal models,cs.cv,"humans naturally understand 3d spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. current large multimodal models (lmms), however, lack of this capability of 3d spatial reasoning. this limitation stems from the scarcity of 3d training data and the bias in current model designs toward 2d data. in this paper, we systematically study the impact of 3d-informed data, architecture, and training setups, introducing spatialllm, a large multi-modal model with advanced 3d spatial reasoning abilities. to address data limitations, we develop two types of 3d-informed training datasets: (1) 3d-informed probing data focused on object's 3d location and orientation, and (2) 3d-informed conversation data for complex spatial relationships. notably, we are the first to curate vqa data that incorporate 3d orientation relationships on real images. furthermore, we systematically integrate these two types of training data with the architectural and training designs of lmms, providing a roadmap for optimal design aimed at achieving superior 3d reasoning capabilities. our spatialllm advances machines toward highly capable 3d-informed reasoning, surpassing gpt-4o performance by 8.7%. our systematic empirical design and the resulting findings offer valuable insights for future research in this direction.",,2025-05-01,,"['wufei ma', 'luoxin ye', 'nessa mcweeney', 'celso m de melo', 'alan yuille', 'jieneng chen']"
2505.00805,advancing wheat crop analysis: a survey of deep learning approaches   using hyperspectral imaging,cs.cv eess.iv,"as one of the most widely cultivated and consumed crops, wheat is essential to global food security. however, wheat production is increasingly challenged by pests, diseases, climate change, and water scarcity, threatening yields. traditional crop monitoring methods are labor-intensive and often ineffective for early issue detection. hyperspectral imaging (hsi) has emerged as a non-destructive and efficient technology for remote crop health assessment. however, the high dimensionality of hsi data and limited availability of labeled samples present notable challenges. in recent years, deep learning has shown great promise in addressing these challenges due to its ability to extract and analysis complex structures. despite advancements in applying deep learning methods to hsi data for wheat crop analysis, no comprehensive survey currently exists in this field. this review addresses this gap by summarizing benchmark datasets, tracking advancements in deep learning methods, and analyzing key applications such as variety classification, disease detection, and yield estimation. it also highlights the strengths, limitations, and future opportunities in leveraging deep learning methods for hsi-based wheat crop analysis. we have listed the current state-of-the-art papers and will continue tracking updating them in the following https://github.com/fadi-07/awesome-wheat-hsi-deeplearning.",,2025-05-01,,"['fadi abdeladhim zidi', 'abdelkrim ouafi', 'fares bougourzi', 'cosimo distante', 'abdelmalik taleb-ahmed']"
2505.00836,the comparability of model fusion to measured data in confuser rejection,cs.cv,"data collection has always been a major issue in the modeling and training of large deep learning networks, as no dataset can account for every slight deviation we might see in live usage. collecting samples can be especially costly for synthetic aperture radar (sar), limiting the amount of unique targets and operating conditions we are able to observe from. to counter this lack of data, simulators have been developed utilizing the shooting and bouncing ray method to allow for the generation of synthetic sar data on 3d models. while effective, the synthetically generated data does not perfectly correlate to the measured data leading to issues when training models solely on synthetic data. we aim to use computational power as a substitution for this lack of quality measured data, by ensembling many models trained on synthetic data. synthetic data is also not complete, as we do not know what targets might be present in a live environment. therefore we need to have our ensembling techniques account for these unknown targets by applying confuser rejection in which our models will reject unknown targets it is presented with, and only classify those it has been trained on.",,2025-05-01,,"['conor flynn', 'christopher ebersole', 'edmund zelnio']"
2505.00866,are minimal radial distortion solvers really necessary for relative pose   estimation?,cs.cv,"estimating the relative pose between two cameras is a fundamental step in many applications such as structure-from-motion. the common approach to relative pose estimation is to apply a minimal solver inside a ransac loop. highly efficient solvers exist for pinhole cameras. yet, (nearly) all cameras exhibit radial distortion. not modeling radial distortion leads to (significantly) worse results. however, minimal radial distortion solvers are significantly more complex than pinhole solvers, both in terms of run-time and implementation efforts. this paper compares radial distortion solvers with two simple-to-implement approaches that do not use minimal radial distortion solvers: the first approach combines an efficient pinhole solver with sampled radial undistortion parameters, where the sampled parameters are used for undistortion prior to applying the pinhole solver. the second approach uses a state-of-the-art neural network to estimate the distortion parameters rather than sampling them from a set of potential values. extensive experiments on multiple datasets, and different camera setups, show that complex minimal radial distortion solvers are not necessary in practice. we discuss under which conditions a simple sampling of radial undistortion parameters is preferable over calibrating cameras using a learning-based prior approach. code and newly created benchmark for relative pose estimation under radial distortion are available at https://github.com/kocurvik/rdnet.",,2025-05-01,,"['viktor kocur', 'charalambos tzamos', 'yaqing ding', 'zuzana berger haladova', 'torsten sattler', 'zuzana kukelova']"
2505.00935,autonomous embodied agents: when robotics meets deep learning reasoning,cs.ro cs.ai cs.cv,"the increase in available computing power and the deep learning revolution have allowed the exploration of new topics and frontiers in artificial intelligence research. a new field called embodied artificial intelligence, which places at the intersection of computer vision, robotics, and decision making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. the recent availability of large collections of 3d models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. these intelligent agents are intended to perform a certain task in a possibly unknown environment. to this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. this dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. we aim to contribute to research in embodied ai and autonomous agents, in order to foster future work in this field. we present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.",,2025-05-01,,['roberto bigazzi']
2505.00938,cdformer: cross-domain few-shot object detection transformer against   feature confusion,cs.cv cs.ai,"cross-domain few-shot object detection (cd-fsod) aims to detect novel objects across different domains with limited class instances. feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. in this work, we introduce cdformer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. the method specifically tackles feature confusion through two key modules: object-background distinguishing (obd) and object-object distinguishing (ood). the obd module leverages a learnable background token to differentiate between objects and background, while the ood module enhances the distinction between objects of different classes. experimental results demonstrate that cdformer outperforms previous state-of-the-art approaches, achieving 12.9% map, 11.0% map, and 10.4% map improvements under the 1/5/10 shot settings, respectively, when fine-tuned.",,2025-05-01,,"['boyuan meng', 'xiaohan zhang', 'peilin li', 'zhe wu', 'yiming li', 'wenkai zhao', 'beinan yu', 'hui-liang shen']"
2505.00975,generating animated layouts as structured text representations,cs.cv,"despite the remarkable progress in text-to-video models, achieving precise control over text elements and animated graphics remains a significant challenge, especially in applications such as video advertisements. to address this limitation, we introduce animated layout generation, a novel approach to extend static graphic layouts with temporal dynamics. we propose a structured text representation for fine-grained video control through hierarchical visual elements. to demonstrate the effectiveness of our approach, we present vaker (video ad maker), a text-to-video advertisement generation pipeline that combines a three-stage generation process with unstructured text reasoning for seamless integration with llms. vaker fully automates video advertisement generation by incorporating dynamic layout trajectories for objects and graphics across specific video frames. through extensive evaluations, we demonstrate that vaker significantly outperforms existing methods in generating video advertisements. project page: https://yeonsangshin.github.io/projects/vaker",,2025-05-01,,"['yeonsang shin', 'jihwan kim', 'yumin song', 'kyungseung lee', 'hyunhee chung', 'taeyoung na']"
2505.00980,lmdepth: lightweight mamba-based monocular depth estimation for   real-world deployment,cs.cv,"monocular depth estimation provides an additional depth dimension to rgb images, making it widely applicable in various fields such as virtual reality, autonomous driving and robotic navigation. however, existing depth estimation algorithms often struggle to effectively balance performance and computational efficiency, which poses challenges for deployment on resource-constrained devices. to address this, we propose lmdepth, a lightweight mamba-based monocular depth estimation network, designed to reconstruct high-precision depth information while maintaining low computational overhead. specifically, we propose a modified pyramid spatial pooling module that serves as a multi-scale feature aggregator and context extractor, ensuring global spatial information for accurate depth estimation. moreover, we integrate multiple depth mamba blocks into the decoder. designed with linear computations, the mamba blocks enable lmdepth to efficiently decode depth information from global features, providing a lightweight alternative to transformer-based architectures that depend on complex attention mechanisms. extensive experiments on the nyudv2 and kitti datasets demonstrate the effectiveness of our proposed lmdepth. compared to previous lightweight depth estimation methods, lmdepth achieves higher performance with fewer parameters and lower computational complexity (measured by gflops). we further deploy lmdepth on an embedded platform with int8 quantization, validating its practicality for real-world edge applications.",,2025-05-02,,"['jiahuan long', 'xin zhou']"
2505.00986,on-demand test-time adaptation for edge devices,cs.lg cs.cv,"continual test-time adaptation (ctta) continuously adapts the deployed model on every incoming batch of data. while achieving optimal accuracy, existing ctta approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. in this work, we first introduce a novel paradigm -- on-demand tta -- which triggers adaptation only when a significant domain shift is detected. then, we present od-tta, an on-demand tta framework for accurate and efficient adaptation on edge devices. od-tta comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate tta only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled batch normalization (bn) update scheme to enable memory-efficient adaptation with small batch sizes. extensive experiments show that od-tta achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making tta a practical reality.",,2025-05-02,,"['xiao ma', 'young d. kwon', 'dong ma']"
2505.00995,optimizing indoor farm monitoring efficiency using uav: yield estimation   in a gnss-denied cherry tomato greenhouse,cs.ro cs.cv,"as the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. while unmanned ground vehicles (ugvs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. to address these issues, we develop a lightweight unmanned aerial vehicle (uav) equipped with an rgb-d camera, a 3d lidar, and an imu sensor. the uav employs a lidar-inertial odometry algorithm for precise navigation in gnss-denied environments and utilizes a 3d multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. we evaluate the system using two dataset: one from a harvesting row and another from a growing row. in the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. for the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. our findings demonstrate the potential of uavs for efficient robotic yield estimation in commercial greenhouses.",,2025-05-02,,"['taewook park', 'jinwoo lee', 'hyondong oh', 'won-jae yun', 'kyu-wha lee']"
2505.00998,deterministic-to-stochastic diverse latent feature mapping for human   motion synthesis,cs.cv,"human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. recent score-based generative models (sgms) have demonstrated impressive results on this task. however, their training process involves complex curvature trajectories, leading to unstable training process. in this paper, we propose a deterministic-to-stochastic diverse latent feature mapping (dsdfm) method for human motion synthesis. dsdfm consists of two stages. the first human motion reconstruction stage aims to learn the latent space distribution of human motions. the second diverse motion generation stage aims to build connections between the gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. this stage is achieved by the designed deterministic feature mapping procedure with derode and stochastic diverse output generation procedure with divsde.dsdfm is easy to train compared to previous sgms-based methods and can enhance diversity without introducing additional training parameters.through qualitative and quantitative experiments, dsdfm achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.",,2025-05-02,,"['yu hua', 'weiming liu', 'gui xu', 'yaqing hou', 'yew-soon ong', 'qiang zhang']"
2505.01003,3d human pose estimation via spatial graph order attention and temporal   body aware transformer,cs.cv,"nowadays, transformers and graph convolutional networks (gcns) are the prevailing techniques for 3d human pose estimation. however, transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while gcn-based methods often neglect the need for pose-specific representations. to address these problems, we propose a new method that exploits the graph modeling capability of gcn to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced graph order attention module that dynamically emphasizes the most representative orders for each joint. the resulting spatial features of the sequence are further processed using a proposed temporal body aware transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. given that our 3d pose output aligns with the central 2d pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. extensive experiments on human3.6m, mpiinf-3dhp, and humaneva-i datasets demonstrate the effectiveness of the proposed method. code and models are made available on github.",,2025-05-02,,"['kamel aouaidjia', 'aofan li', 'wenhao zhang', 'chongsheng zhang']"
2505.01007,towards the resistance of neural network watermarking to fine-tuning,cs.lg cs.ai cs.cl cs.cv,"this paper proves a new watermarking method to embed the ownership information into a deep neural network (dnn), which is robust to fine-tuning. specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised fourier transform to extract frequency components from the convolutional filter. additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. in this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. preliminary experiments demonstrate the effectiveness of our method.",,2025-05-02,,"['ling tang', 'yuefeng chen', 'hui xue', 'quanshi zhang']"
2505.01016,fine-tuning without forgetting: adaptation of yolov8 preserves coco   performance,cs.cv cs.ai,"the success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. while fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. the critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. we adapt a standard yolov8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original coco validation set. our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\% absolute map50) on the fine-grained fruit task compared to only training the head. strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\% absolute map difference) on the coco benchmark across all tested freeze levels. we conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.",,2025-05-02,,"['vishal gandhi', 'sagar gandhi']"
2505.01032,edge-preserving image denoising via multi-scale adaptive statistical   independence testing,cs.cv,"edge detection is crucial in image processing, but existing methods often produce overly detailed edge maps, affecting clarity. fixed-window statistical testing faces issues like scale mismatch and computational redundancy. to address these, we propose a novel multi-scale adaptive independence testing-based edge detection and denoising (edd-mait), a multi-scale adaptive statistical testing-based edge detection and denoising method that integrates a channel attention mechanism with independence testing. a gradient-driven adaptive window strategy adjusts window sizes dynamically, improving detail preservation and noise suppression. edd-mait achieves better robustness, accuracy, and efficiency, outperforming traditional and learning-based methods on bsds500 and biped datasets, with improvements in f-score, mse, psnr, and reduced runtime. it also shows robustness against gaussian noise, generating accurate and clean edge maps in noisy environments.",,2025-05-02,,"['ruyu yan', 'da-qing zhang']"
2505.01040,edge detection based on channel attention and inter-region independence   test,cs.cv,"existing edge detection methods often suffer from noise amplification and excessive retention of non-salient details, limiting their applicability in high-precision industrial scenarios. to address these challenges, we propose cam-edit, a novel framework that integrates channel attention mechanism (cam) and edge detection via independence testing (edit). the cam module adaptively enhances discriminative edge features through multi-channel fusion, while the edit module employs region-wise statistical independence analysis (using fisher's exact test and chi-square test) to suppress uncorrelated noise.extensive experiments on bsds500 and nyudv2 datasets demonstrate state-of-the-art performance. among the nine comparison algorithms, the f-measure scores of cam-edit are 0.635 and 0.460, representing improvements of 19.2\% to 26.5\% over traditional methods (canny, cannysr), and better than the latest learning based methods (tip2020, mscngp). noise robustness evaluations further reveal a 2.2\% psnr improvement under gaussian noise compared to baseline methods. qualitative results exhibit cleaner edge maps with reduced artifacts, demonstrating its potential for high-precision industrial applications.",,2025-05-02,,"['ru-yu yan', 'da-qing zhang']"
2505.01050,transferable adversarial attacks on black-box vision-language models,cs.cv cs.lg,"vision large language models (vllms) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. while prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for vllms. we present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary vllms such as gpt-4o, claude, and gemini. we show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary vllms. our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of vllms.",,2025-05-02,,"['kai hu', 'weichen yu', 'li zhang', 'alexander robey', 'andy zou', 'chengming xu', 'haoqi hu', 'matt fredrikson']"
2505.01057,gelovec: higher dimensional geometric smoothing for coherent visual   feature extraction in image segmentation,cs.cv,"this paper introduces gelovec, a new cnn-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. while existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. gelovec combines modified chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. the core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. the multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean intersection over union (miou) gains of 2.1%, 2.7%, and 2.4% on caltech birds-200, lsdsc, and fssd datasets respectively compared to state-of-the-art methods. gelovec's mathematical foundation in riemannian geometry provides theoretical guarantees on segmentation stability. importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.",,2025-05-02,,"['boris kriuk', 'matey yordanov']"
2505.01064,efficient vocabulary-free fine-grained visual recognition in the age of   multimodal llms,cs.cv cs.lg,"fine-grained visual recognition (fgvr) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. in domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. in such scenarios lacking labeled data, an fgvr model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. we refer to this task as vocabulary-free fgvr (vf-fgvr), where a model must predict labels from an unconstrained output space without prior label information. while recent multimodal large language models (mllms) show potential for vf-fgvr, querying these models for each test input is impractical because of high costs and prohibitive inference times. to address these limitations, we introduce \textbf{nea}rest-neighbor label \textbf{r}efinement (near), a novel approach that fine-tunes a downstream clip model using labels generated by an mllm. our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging mllms for label generation. near is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by mllms, and establishes a new benchmark for efficient vf-fgvr.",,2025-05-02,,"['hari chandana kuchibhotla', 'sai srinivas kancheti', 'abbavaram gowtham reddy', 'vineeth n balasubramanian']"
2505.01079,improving editability in image generation with layer-wise memory,cs.cv eess.iv,"most real-world image editing tasks require multiple sequential edits to achieve desired results. current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. these limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. we address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. we propose background consistency guidance that leverages memorized latents to maintain scene coherence and multi-query disentanglement in cross-attention that ensures natural adaptation to existing content. to evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.",,2025-05-02,,"['daneul kim', 'jaeah lee', 'jaesik park']"
2505.01091,any-to-any vision-language model for multimodal x-ray imaging and   radiological report generation,cs.cv cs.ai,"generative models have revolutionized artificial intelligence (ai), particularly in multimodal applications. however, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. in this work, we introduce a framework specifically designed for multimodal medical data generation. by enabling the generation of multi-view chest x-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. leveraging the mimic-cxr dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. our quantitative evaluation reveals significant results in terms of fid and bleu scores, showcasing the quality of the generated data. notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. this study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.",,2025-05-02,,"['daniele molino', 'francesco di feola', 'linlin shen', 'paolo soda', 'valerio guarrasi']"
2505.01096,evaluating vision language model adaptations for radiology report   generation in low-resource languages,cs.cv cs.cl,"the integration of artificial intelligence in healthcare has opened new horizons for improving medical diagnostics and patient care. however, challenges persist in developing systems capable of generating accurate and contextually relevant radiology reports, particularly in low-resource languages. in this study, we present a comprehensive benchmark to evaluate the performance of instruction-tuned vision-language models (vlms) in the specialized task of radiology report generation across three low-resource languages: italian, german, and spanish. employing the llava architectural framework, we conducted a systematic evaluation of pre-trained models utilizing general datasets, domain-specific datasets, and low-resource language-specific datasets. in light of the unavailability of models that possess prior knowledge of both the medical domain and low-resource languages, we analyzed various adaptations to determine the most effective approach for these contexts. the results revealed that language-specific models substantially outperformed both general and domain-specific models in generating radiology reports, emphasizing the critical role of linguistic adaptation. additionally, models fine-tuned with medical terminology exhibited enhanced performance across all languages compared to models with generic knowledge, highlighting the importance of domain-specific training. we also explored the influence of the temperature parameter on the coherence of report generation, providing insights for optimal model settings. our findings highlight the importance of tailored language and domain-specific training for improving the quality and accuracy of radiological reports in multilingual settings. this research not only advances our understanding of vlms adaptability in healthcare but also points to significant avenues for future investigations into model tuning and language-specific adaptations.",,2025-05-02,,"['marco salmè', 'rosa sicilia', 'paolo soda', 'valerio guarrasi']"
2505.01104,vsc: visual search compositional text-to-image diffusion model,cs.cv,"text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. this challenge primarily arises from the limitations of commonly used text encoders, such as clip, which can fail to encode complex linguistic relationships and modifiers effectively. existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. in this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. by applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. our approaches outperform existing compositional text-to-image diffusion models on the benchmark t2i compbench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.",,2025-05-02,,"['do huu dat', 'nam hyeonu', 'po-yuan mao', 'tae-hyun oh']"
2505.01109,self-supervision enhances instance-based multiple instance learning   methods in digital pathology: a benchmark study,cs.cv cs.ai,"multiple instance learning (mil) has emerged as the best solution for whole slide image (wsi) classification. it consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. mil includes two main approaches: instance-based and embedding-based. in the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. in the latter, bag classification is performed after aggregating patch embeddings. even if instance-based methods are naturally more interpretable, embedding-based mils have usually been preferred in the past due to their robustness to poor feature extractors. however, recently, the quality of feature embeddings has drastically increased using self-supervised learning (ssl). nevertheless, many authors continue to endorse the superiority of embedding-based mil. to investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 mil strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. furthermore, we introduce 4 instance-based mil methods never used before in the pathology domain. through these extensive experiments, we show that with a good ssl feature extractor, simple instance-based mils, with very few parameters, obtain similar or better performance than complex, state-of-the-art (sota) embedding-based mil methods, setting new sota results on the bracs and camelyon16 datasets. since simple instance-based mil methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted ssl methods for wsi rather than into complex embedding-based mil methods.",,2025-05-02,,"['ali mammadov', 'loic le folgoc', 'julien adam', 'anne buronfosse', 'gilles hayem', 'guillaume hocquet', 'pietro gori']"
2505.01113,neuroloc: encoding navigation cells for 6-dof camera localization,cs.ro cs.cv cs.ne,"recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. however, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. to address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method, namely neuroloc. firstly, we designed a hebbian learning module driven by place cells to save and replay historical information, aiming to restore the details of historical representations and solve the issue of scene fuzziness. secondly, we utilized the head direction cell-inspired internal direction learning as multi-head attention embedding to help restore the true orientation in similar scenes. finally, we added a 3d grid center prediction in the pose regression module to reduce the final wrong prediction. we evaluate the proposed neuroloc on commonly used benchmark indoor and outdoor datasets. the experimental results show that our neuroloc can enhance the robustness in complex environments and improve the performance of pose regression by using only a single image.",,2025-05-02,,"['xun li', 'jian yang', 'fenli jia', 'muyu wang', 'qi wu', 'jun wu', 'jinpeng mi', 'jilin hu', 'peidong liang', 'xuan tang', 'ke li', 'xiong you', 'xian wei']"
2505.01172,freepca: integrating consistency information across long-short frames in   training-free long video generation via principal component analysis,cs.cv,"long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. it necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. in this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying principal component analysis (pca), allowing for refined complementary integration of global consistency and local quality. with this insight, we propose freepca, a training-free long video generation paradigm based on pca that simultaneously achieves high consistency and quality. concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. experiments demonstrate that freepca can be applied to various video diffusion models without requiring training, leading to substantial improvements. code is available at https://github.com/josephtitan/freepca.",,2025-05-02,,"['jiangtong tan', 'hu yu', 'jie huang', 'jie xiao', 'feng zhao']"
2505.01182,tstmotion: training-free scene-aware text-to-motion generation,cs.cv cs.ai,"text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. however, human motions commonly occur within diverse 3d scenes, which has prompted exploration into scene-aware text-to-motion generation methods. yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3d scenes, which poses practical challenges due to the expensive cost. to mitigate this challenge, we are the first to propose a \textbf{t}raining-free \textbf{s}cene-aware \textbf{t}ext-to-\textbf{motion} framework, dubbed as \textbf{tstmotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. specifically, conditioned on the given 3d scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. extensive experiments demonstrate the efficacy and generalizability of our proposed framework. we release our code in \href{https://tstmotion.github.io/}{project page}.",,2025-05-02,2025-05-05,"['ziyan guo', 'haoxuan qu', 'hossein rahmani', 'dewen soh', 'ping hu', 'qiuhong ke', 'jun liu']"
2505.01203,efficient vision-based vehicle speed estimation,cs.cv,"this paper presents a computationally efficient method for vehicle speed estimation from traffic camera footage. building upon previous work that utilizes 3d bounding boxes derived from 2d detections and vanishing point geometry, we introduce several improvements to enhance real-time performance. we evaluate our method in several variants on the brnocompspeed dataset in terms of vehicle detection and speed estimation accuracy. our extensive evaluation across various hardware platforms, including edge devices, demonstrates significant gains in frames per second (fps) compared to the prior state-of-the-art, while maintaining comparable or improved speed estimation accuracy. we analyze the trade-off between accuracy and computational cost, showing that smaller models utilizing post-training quantization offer the best balance for real-world deployment. our best performing model beats previous state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs. 83.32%) while also being 5.5 times faster.",,2025-05-02,,"['andrej macko', 'lukáš gajdošech', 'viktor kocur']"
2505.01207,t-graph: enhancing sparse-view camera pose estimation by pairwise   translation graph,cs.cv,"sparse-view camera pose estimation, which aims to estimate the 6-degree-of-freedom (6-dof) poses from a limited number of images captured from different viewpoints, is a fundamental yet challenging problem in remote sensing applications. existing methods often overlook the translation information between each pair of viewpoints, leading to suboptimal performance in sparse-view scenarios. to address this limitation, we introduce t-graph, a lightweight, plug-and-play module to enhance camera pose estimation in sparse-view settings. t-graph takes paired image features as input and maps them through a multilayer perceptron (mlp). it then constructs a fully connected translation graph, where nodes represent cameras and edges encode their translation relationships. it can be seamlessly integrated into existing models as an additional branch in parallel with the original prediction, maintaining efficiency and ease of use. furthermore, we introduce two pairwise translation representations, relative-t and pair-t, formulated under different local coordinate systems. while relative-t captures intuitive spatial relationships, pair-t offers a rotation-disentangled alternative. the two representations contribute to enhanced adaptability across diverse application scenarios, further improving our module's robustness. extensive experiments on two state-of-the-art methods (relpose++ and forge) using public datasets (c03d and imc phototourism) validate both the effectiveness and generalizability of t-graph. the results demonstrate consistent improvements across various metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8 viewpoints.",,2025-05-02,,"['qingyu xian', 'weiqin jiao', 'hao cheng', 'berend jan van der zwaag', 'yanqiu huang']"
2505.01224,rd-uie: relation-driven state space modeling for underwater image   enhancement,cs.cv eess.iv,"underwater image enhancement (uie) is a critical preprocessing step for marine vision applications, where wavelength-dependent attenuation causes severe content degradation and color distortion. while recent state space models like mamba show potential for long-range dependency modeling, their unfolding operations and fixed scan paths on 1d sequences fail to adapt to local object semantics and global relation modeling, limiting their efficacy in complex underwater environments. to address this, we enhance conventional mamba with the sorting-based scanning mechanism that dynamically reorders scanning sequences based on statistical distribution of spatial correlation of all pixels. in this way, it encourages the network to prioritize the most informative components--structural and semantic features. upon building this mechanism, we devise a visually self-adaptive state block (vssb) that harmonizes dynamic sorting of mamba with input-dependent dynamic convolution, enabling coherent integration of global context and local relational cues. this exquisite design helps eliminate global focus bias, especially for widely distributed contents, which greatly weakens the statistical frequency. for robust feature extraction and refinement, we design a cross-feature bridge (cfb) to adaptively fuse multi-scale representations. these efforts compose the novel relation-driven mamba framework for effective uie (rd-uie). extensive experiments on underwater enhancement benchmarks demonstrate rd-uie outperforms the state-of-the-art approach wmamba in both quantitative metrics and visual fidelity, averagely achieving 0.55 db performance gain on the three benchmarks. our code is available at https://github.com/kkoucy/rd-uie/tree/main",,2025-05-02,,"['kui jiang', 'yan luo', 'junjun jiang', 'xin xu', 'fei ma', 'fei yu']"
2505.01225,core-set selection for data-efficient land cover segmentation,cs.cv,"the increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many earth observation tasks. traditionally, such models must be trained on large datasets. however, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. therefore, effective solutions should consider both the quantity and quality of data. in this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. we benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: dfc2022, vaihingen, and potsdam. in each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. this result shows the importance and potential of data-centric learning for the remote sensing domain. the code is available at https://github.com/keillernogueira/data-centric-rs-classification/.",,2025-05-02,,"['keiller nogueira', 'akram zaytar', 'wanli ma', 'ribana roscher', 'ronny hänsch', 'caleb robinson', 'anthony ortiz', 'simone nsutezo', 'rahul dodhia', 'juan m. lavista ferres', 'oktay karakuş', 'paul l. rosin']"
2505.01235,compensating spatiotemporally inconsistent observations for online   dynamic 3d gaussian splatting,cs.cv,"online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. however, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. this paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. we propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. we show that our method restores the ideal observation by subtracting the learned error. we demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. code, video results, and checkpoints are available at https://bbangsik13.github.io/or2.",10.1145/3721238.3730678,2025-05-02,,"['youngsik yun', 'jeongmin bae', 'hyunseung son', 'seoha kim', 'hahyun lee', 'gun bang', 'youngjung uh']"
2505.01239,can foundation models really segment tumors? a benchmarking odyssey in   lung ct imaging,eess.iv cs.cv,"accurate lung tumor segmentation is crucial for improving diagnosis, treatment planning, and patient outcomes in oncology. however, the complexity of tumor morphology, size, and location poses significant challenges for automated segmentation. this study presents a comprehensive benchmarking analysis of deep learning-based segmentation models, comparing traditional architectures such as u-net and deeplabv3, self-configuring models like nnunet, and foundation models like medsam, and medsam~2. evaluating performance across two lung tumor segmentation datasets, we assess segmentation accuracy and computational efficiency under various learning paradigms, including few-shot learning and fine-tuning. the results reveal that while traditional models struggle with tumor delineation, foundation models, particularly medsam~2, outperform them in both accuracy and computational efficiency. these findings underscore the potential of foundation models for lung tumor segmentation, highlighting their applicability in improving clinical workflows and patient outcomes.",,2025-05-02,,"['elena mulero ayllón', 'massimiliano mantegna', 'linlin shen', 'paolo soda', 'valerio guarrasi', 'matteo tortora']"
2505.01249,fusing foveal fixations using linear retinal transformations and   bayesian experimental design,cs.cv cs.lg,"humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. in this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. this linear transformation allows us to carry out exact inference for the latent variables in factor analysis (fa) and mixtures of fa models of the scene. further, this allows us to formulate and solve the choice of ""where to look next"" as a bayesian experimental design problem using the expected information gain criterion. experiments on the frey faces and mnist datasets demonstrate the effectiveness of our models.",,2025-05-02,,['christopher k. i. williams']
2505.01257,cameltrack: context-aware multi-cue exploitation for online multi-object   tracking,cs.cv cs.lg,"online multi-object tracking has been recently dominated by tracking-by-detection (tbd) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. the key strength of tbd lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. however, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. in this work, we introduce camel, a novel association module for context-aware multi-cue exploitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining tbd's valuable modularity. at its core, camel employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. our proposed online tracking pipeline, cameltrack, achieves state-of-the-art performance on multiple tracking benchmarks. our code is available at https://github.com/trackinglaboratory/cameltrack.",,2025-05-02,,"['vladimir somers', 'baptiste standaert', 'victor joos', 'alexandre alahi', 'christophe de vleeschouwer']"
2505.01263,flowdubber: movie dubbing with llm-based semantic-aware learning and   flow matching based voice enhancing,cs.mm cs.cv cs.sd eess.as,"movie dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. to address these issues, we propose a large language model (llm) based flow matching architecture for dubbing, named flowdubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. first, we introduce qwen2.5 as the backbone of llm to learn the in-context sequence from movie scripts and reference audio. then, the proposed semantic-aware learning focuses on capturing llm semantic knowledge at the phoneme level. next, dual contrastive aligning (dca) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. finally, the proposed flow-based voice enhancing (fve) improves acoustic quality in two aspects, which introduces an llm-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. the demos are available at {\href{https://galaxycong.github.io/llm-flow-dubber/}{\textcolor{red}{https://galaxycong.github.io/llm-flow-dubber/}}}.",,2025-05-02,,"['gaoxiang cong', 'liang li', 'jiadong pan', 'zhedong zhang', 'amin beheshti', 'anton van den hengel', 'yuankai qi', 'qingming huang']"
2505.01267,diffusion-based adversarial purification from the perspective of the   frequency domain,cs.cv,"the diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. we turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. we find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. this means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. for the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.",,2025-05-02,,"['gaozheng pei', 'ke ma', 'yingfei sun', 'qianqian xu', 'qingming huang']"
2505.01313,a neural architecture search method using auxiliary evaluation metric   based on resnet architecture,cs.ne cs.cv,"this paper proposes a neural architecture search space using resnet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. in addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. the experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the mnist, fashion-mnist and cifar100 datasets.",,2025-05-02,,"['shang wang', 'huanrong tang', 'jianquan ouyang']"
2505.01322,freeinsert: disentangled text-guided object insertion in 3d gaussian   scene without spatial priors,cs.cv,"text-driven object insertion in 3d scenes is an emerging task that enables intuitive scene editing through natural language. however, existing 2d editing-based methods often rely on spatial priors such as 2d masks or 3d bounding boxes, and they struggle to ensure consistency of the inserted object. these limitations hinder flexibility and scalability in real-world applications. in this paper, we propose freeinsert, a novel framework that leverages foundation models including mllms, lgms, and diffusion models to disentangle object generation from spatial placement. this enables unsupervised and flexible object insertion in 3d scenes without spatial priors. freeinsert starts with an mllm-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. these semantics guide both the reconstruction of the inserted object for 3d consistency and the learning of its degrees of freedom. we leverage the spatial reasoning capabilities of mllms to initialize object pose and scale. a hierarchical, spatially aware refinement stage further integrates spatial semantics and mllm-inferred priors to enhance placement. finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. experimental results demonstrate that freeinsert achieves semantically coherent, spatially precise, and visually realistic 3d insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.",,2025-05-02,,"['chenxi li', 'weijie wang', 'qiang li', 'bruno lepri', 'nicu sebe', 'weizhi nie']"
2505.01364,monitoring morphometric drift in lifelong learning segmentation of the   spinal cord,cs.cv,"morphometric measures derived from spinal cord segmentations can serve as diagnostic and prognostic biomarkers in neurological diseases and injuries affecting the spinal cord. while robust, automatic segmentation methods to a wide variety of contrasts and pathologies have been developed over the past few years, whether their predictions are stable as the model is updated using new datasets has not been assessed. this is particularly important for deriving normative values from healthy participants. in this study, we present a spinal cord segmentation model trained on a multisite $(n=75)$ dataset, including 9 different mri contrasts and several spinal cord pathologies. we also introduce a lifelong learning framework to automatically monitor the morphometric drift as the model is updated using additional datasets. the framework is triggered by an automatic github actions workflow every time a new model is created, recording the morphometric values derived from the model's predictions over time. as a real-world application of the proposed framework, we employed the spinal cord segmentation model to update a recently-introduced normative database of healthy participants containing commonly used measures of spinal cord morphometry. results showed that: (i) our model outperforms previous versions and pathology-specific models on challenging lumbar spinal cord cases, achieving an average dice score of $0.95 \pm 0.03$; (ii) the automatic workflow for monitoring morphometric drift provides a quick feedback loop for developing future segmentation models; and (iii) the scaling factor required to update the database of morphometric measures is nearly constant among slices across the given vertebral levels, showing minimum drift between the current and previous versions of the model monitored by the framework. the model is freely available in spinal cord toolbox v7.0.",,2025-05-02,,"['enamundram naga karthik', 'sandrine bédard', 'jan valošek', 'christoph s. aigner', 'elise bannier', 'josef bednařík', 'virginie callot', 'anna combes', 'armin curt', 'gergely david', 'falk eippert', 'lynn farner', 'michael g fehlings', 'patrick freund', 'tobias granberg', 'cristina granziera', 'rhscir network imaging group', 'ulrike horn', 'tomáš horák', 'suzanne humphreys', 'markus hupp', 'anne kerbrat', 'nawal kinany', 'shannon kolind', 'petr kudlička', 'anna lebret', 'lisa eunyoung lee', 'caterina mainero', 'allan r. martin', 'megan mcgrath', 'govind nair', ""kristin p. o'grady"", 'jiwon oh', 'russell ouellette', 'nikolai pfender', 'dario pfyffer', 'pierre-françois pradat', 'alexandre prat', 'emanuele pravatà', 'daniel s. reich', 'ilaria ricchi', 'naama rotem-kohavi', 'simon schading-sassenhausen', 'maryam seif', 'andrew smith', 'seth a smith', 'grace sweeney', 'roger tam', 'anthony traboulsee', 'constantina andrada treaba', 'charidimos tsagkas', 'zachary vavasour', 'dimitri van de ville', 'kenneth arnold weber', 'sarath chandar', 'julien cohen-adad']"
2505.01385,global collinearity-aware polygonizer for polygonal building mapping in   remote sensing,cs.cv cs.lg,"this paper addresses the challenge of mapping polygonal buildings from remote sensing images and introduces a novel algorithm, the global collinearity-aware polygonizer (gcp). gcp, built upon an instance segmentation framework, processes binary masks produced by any instance segmentation model. the algorithm begins by collecting polylines sampled along the contours of the binary masks. these polylines undergo a refinement process using a transformer-based regression module to ensure they accurately fit the contours of the targeted building instances. subsequently, a collinearity-aware polygon simplification module simplifies these refined polylines and generate the final polygon representation. this module employs dynamic programming technique to optimize an objective function that balances the simplicity and fidelity of the polygons, achieving globally optimal solutions. furthermore, the optimized collinearity-aware objective is seamlessly integrated into network training, enhancing the cohesiveness of the entire pipeline. the effectiveness of gcp has been validated on two public benchmarks for polygonal building mapping. further experiments reveal that applying the collinearity-aware polygon simplification module to arbitrary polylines, without prior knowledge, enhances accuracy over traditional methods such as the douglas-peucker algorithm. this finding underscores the broad applicability of gcp. the code for the proposed method will be made available at https://github.com/zhu-xlab.",,2025-05-02,,"['fahong zhang', 'yilei shi', 'xiao xiang zhu']"
2505.01390,multimodal doctor-in-the-loop: a clinically-guided explainable framework   for predicting pathological response in non-small cell lung cancer,cs.cv cs.ai cs.lg,"this study proposes a novel approach combining multimodal deep learning with intrinsic explainable artificial intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. the proposed multimodal doctor-in-the-loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.",,2025-05-02,,"['alice natalina caragliano', 'claudia tacconi', 'carlo greco', 'lorenzo nibid', 'edy ippolito', 'michele fiore', 'giuseppe perrone', 'sara ramella', 'paolo soda', 'valerio guarrasi']"
2505.01406,vidstamp: a temporally-aware watermark for ownership and integrity in   video diffusion models,cs.cv cs.cr cs.lg,"the rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. in this work, we introduce vidstamp, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. by fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, vidstamp learns to embed high-capacity, flexible watermarks with minimal perceptual impact. leveraging architectural components such as 3d convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. vidstamp embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log p-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. code: code: \url{https://github.com/spin-umass/vidstamp}",,2025-05-02,,"['mohammadreza teymoorianfard', 'shiqing ma', 'amir houmansadr']"
2505.01425,genmo: a generalist model for human motion,cs.gr cs.ai cs.cv cs.lg cs.ro,"human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. we present genmo, a unified generalist model for human motion that bridges motion estimation and generation in a single framework. our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. leveraging the synergy between regression and diffusion, genmo achieves accurate global motion estimation while enabling diverse motion generation. we also introduce an estimation-guided training objective that exploits in-the-wild videos with 2d annotations and text descriptions to enhance generative diversity. furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. this unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. extensive experiments demonstrate genmo's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.",,2025-05-02,,"['jiefeng li', 'jinkun cao', 'haotian zhang', 'davis rempe', 'jan kautz', 'umar iqbal', 'ye yuan']"
2505.01428,multi-party collaborative attention control for image customization,cs.cv,"the rapid advancement of diffusion models has increased the need for customized image generation. however, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. to address these issues, this paper introduces multi-party collaborative attention control (mca-ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. specifically, mca-ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. this approach allows mca-ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a subject localization module that extracts precise subject and editable image layers based on user instructions. extensive quantitative and human evaluation experiments show that mca-ctrl outperforms existing methods in zero-shot image customization, effectively resolving the mentioned issues.",,2025-04-02,,"['han yang', 'chuanguang yang', 'qiuli wang', 'zhulin an', 'weilun feng', 'libo huang', 'yongjun xu']"
2505.01429,explainable ai-driven detection of human monkeypox using deep learning   and vision transformers: a comprehensive analysis,cs.cv,"since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. it is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. medical imaging combined with deep learning (dl) techniques has shown promise in improving disease detection by analyzing affected skin areas. our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. we used transfer learning with the help of pre-trained models to get a better classifier. the mobilenet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average f1 score. vit b16 and resnet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. to further validate the performance of the models, we applied explainable ai techniques.",,2025-04-03,,"['md. zahid hossain', 'md. rakibul islam', 'most. sharmin sultana samu']"
2505.01430,deconstructing bias: a multifaceted framework for diagnosing cultural   and compositional inequities in text-to-image generative models,cs.cv,"the transformative potential of text-to-image (t2i) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. however, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. this paper benchmarks the component inclusion score (cis), a metric designed to evaluate the fidelity of image generation across cultural contexts. through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between western and non-western cultural prompts. our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. by benchmarking models like stable diffusion with cis, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in ai-generated imagery. this work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in t2i generation, advocating for more equitable ai systems.",,2025-04-05,,"['muna numan said', 'aarib zaidi', 'rabia usman', 'sonia okon', 'praneeth medepalli', 'kevin zhu', 'vasu sharma', ""sean o'brien""]"
2505.01431,zs-vcos: zero-shot outperforms supervised video camouflaged object   segmentation,cs.cv,"camouflaged object segmentation presents unique challenges compared to traditional segmentation tasks, primarily due to the high similarity in patterns and colors between camouflaged objects and their backgrounds. effective solutions to this problem have significant implications in critical areas such as pest control, defect detection, and lesion segmentation in medical imaging. prior research has predominantly emphasized supervised or unsupervised pre-training methods, leaving zero-shot approaches significantly underdeveloped. existing zero-shot techniques commonly utilize the segment anything model (sam) in automatic mode or rely on vision-language models to generate cues for segmentation; however, their performances remain unsatisfactory, likely due to the similarity of the camouflaged object and the background. optical flow, commonly utilized for detecting moving objects, has demonstrated effectiveness even with camouflaged entities. our method integrates optical flow, a vision-language model, and sam 2 into a sequential pipeline. evaluated on the moca-mask dataset, our approach achieves outstanding performance improvements, significantly outperforming existing zero-shot methods by raising the f-measure ($f_\beta^w$) from 0.296 to 0.628. remarkably, our approach also surpasses supervised methods, increasing the f-measure from 0.476 to 0.628. additionally, evaluation on the moca-filter dataset demonstrates an increase in the success rate from 0.628 to 0.697 when compared with flowsam, a supervised transfer method. a thorough ablation study further validates the individual contributions of each component. more details can be found on https://github.com/weathon/vcos.",,2025-04-10,,"['wenqi guo', 'shan du']"
2505.01456,unlearning sensitive information in multimodal llms: benchmark and   attack-defense evaluation,cs.cl cs.ai cs.cv,"llms trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. this risk is further heightened in multimodal llms as they integrate information from multiple modalities (image and text). adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. evaluating how effectively mllms can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. while prior work on unlearning has focused on text, multimodal unlearning remains underexplored. to address this gap, we first introduce a multimodal unlearning benchmark, unlok-vqa (unlearning outside knowledge vqa), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from mllms. we extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. we then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. unlok-vqa provides a rigorous benchmark for advancing unlearning in mllms.",,2025-04-30,,"['vaidehi patil', 'yi-lin sung', 'peter hase', 'jie peng', 'tianlong chen', 'mohit bansal']"
2505.01457,a multi-granularity retrieval framework for visually-rich documents,cs.ir cs.cv,"retrieval-augmented generation (rag) systems have predominantly focused on text-based retrieval, limiting their effectiveness in handling visually-rich documents that encompass text, images, tables, and charts. to bridge this gap, we propose a unified multi-granularity multimodal retrieval framework tailored for two benchmark tasks: mmdocir and m2kr. our approach integrates hierarchical encoding strategies, modality-aware retrieval mechanisms, and vision-language model (vlm)-based candidate filtering to effectively capture and utilize the complex interdependencies between textual and visual modalities. by leveraging off-the-shelf vision-language models and implementing a training-free hybrid retrieval strategy, our framework demonstrates robust performance without the need for task-specific fine-tuning. experimental evaluations reveal that incorporating layout-aware search and vlm-based candidate verification significantly enhances retrieval accuracy, achieving a top performance score of 65.56. this work underscores the potential of scalable and reproducible solutions in advancing multimodal document retrieval systems.",,2025-04-30,2025-05-06,"['mingjun xu', 'zehui wang', 'hengxing cai', 'renxin zhong']"
2505.01490,worldgenbench: a world-knowledge-integrated benchmark for   reasoning-driven text-to-image generation,cs.cv,"recent advances in text-to-image (t2i) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. to address this gap, we introduce \textbf{worldgenbench}, a benchmark designed to systematically evaluate t2i models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. we propose the \textbf{knowledge checklist score}, a structured metric that measures how well generated images satisfy key semantic expectations. experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like gpt-4o exhibit significantly stronger reasoning and knowledge integration. our findings highlight the need for deeper understanding and inference capabilities in next-generation t2i systems. project page: \href{https://dwanzhang-ai.github.io/worldgenbench/}{https://dwanzhang-ai.github.io/worldgenbench/}",,2025-05-02,,"['daoan zhang', 'che jiang', 'ruoshi xu', 'biaoxiang chen', 'zijian jin', 'yutian lu', 'jianguo zhang', 'liang yong', 'jiebo luo', 'shengda luo']"
2505.01530,automated parsing of engineering drawings for structured information   extraction using a fine-tuned document understanding transformer,cs.cv cs.ai,"accurate extraction of key information from 2d engineering drawings is crucial for high-precision manufacturing. manual extraction is time-consuming and error-prone, while traditional optical character recognition (ocr) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. to address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (obb) detection model with a transformer-based document parsing model (donut). an in-house annotated dataset is used to train yolov11 for detecting nine key categories: geometric dimensioning and tolerancing (gd&t), general tolerances, measures, materials, notes, radii, surface roughness, threads, and title blocks. detected obbs are cropped into images and labeled to fine-tune donut for structured json output. fine-tuning strategies include a single model trained across all categories and category-specific models. results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for gd&t), recall (100% for most), and f1 score (97.3%), while reducing hallucination (5.23%). the proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.",,2025-05-02,,"['muhammad tayyab khan', 'zane yong', 'lequn chen', 'jun ming tan', 'wenhe feng', 'seung ki moon']"
2505.01548,rethinking rgb-event semantic segmentation with a novel bidirectional   motion-enhanced event representation,cs.cv,"event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. however, rgb-event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of rgb modality. to tackle these challenges, we propose a novel event representation, motion-enhanced event tensor (met), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. in addition, we introduce a frequency-aware bidirectional flow aggregation module (bfam) and a temporal fusion module (tfm). bfam leverages the frequency domain and met to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art rgb-event semantic segmentation approaches. our code is available at: https://github.com/zyaocoder/brenet.",,2025-05-02,,"['zhen yao', 'xiaowen ying', 'mooi choo chuah']"
2505.01558,a sensor agnostic domain generalization framework for leveraging   geospatial foundation models: enhancing semantic segmentation viasynergistic   pseudo-labeling and generative learning,cs.cv,"remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. domain adaptation offers a promising solution to improve model generalization. this paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. we further provide new mathematical insights into mae-based generative learning for domain-invariant feature learning. experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.",,2025-05-02,,"['anan yaghmour', 'melba m. crawford', 'saurabh prasad']"
2505.01578,grounding task assistance with multimodal cues from a single   demonstration,cs.cv,"a person's demonstration often serves as a key reference for others learning the same task. however, rgb video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. this sensory gap fundamentally limits the ability of vision language models (vlms) to reason about why actions occur and how they should adapt to individual users. to address this, we introduce mica (multimodal interactive contextualized assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. mica segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. these results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world ai task assistance.",,2025-05-02,,"['gabriel sarch', 'balasaravanan thoravi kumaravel', 'sahithya ravi', 'vibhav vineet', 'andrew d. wilson']"
2505.01583,tempura: temporal event masked prediction and understanding for   reasoning in action,cs.cv cs.ai,"understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. we propose tempura (temporal event masked prediction and understanding for reasoning in action), a two-stage training framework that enhances video temporal understanding. tempura first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. tempura then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. we train tempura on ver, a large-scale dataset curated by us that comprises 1m training instances and 500k videos with temporally aligned event descriptions and structured reasoning steps. experiments on temporal grounding and highlight detection benchmarks demonstrate that tempura outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.",,2025-05-02,,"['jen-hao cheng', 'vivian wang', 'huayu wang', 'huapeng zhou', 'yi-hao peng', 'hou-i liu', 'hsiang-wei huang', 'kuang-ming chen', 'cheng-yen yang', 'wenhao chai', 'yi-ling chen', 'vibhav vineet', 'qin cai', 'jenq-neng hwang']"
2505.01615,multimodal and multiview deep fusion for autonomous marine navigation,cs.cv cs.ai,we propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. the model deeply fuses multiview rgb and long wave infrared images with sparse lidar point clouds. training also integrates x band radar and electronic chart data to inform predictions. the resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.,,2025-05-02,,"['dimitrios dagdilelis', 'panagiotis grigoriadis', 'roberto galeazzi']"
2505.01638,seeing heat with color -- rgb-only wildfire temperature inference from   sam-guided multimodal distillation using radiometric ground truth,eess.iv cs.ai cs.cv,"high-fidelity wildfire monitoring using unmanned aerial vehicles (uavs) typically requires multimodal sensing - especially rgb and thermal imagery - which increases hardware cost and power consumption. this paper introduces sam-tiff, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using rgb input only. a multimodal teacher network trained on paired rgb-thermal imagery and radiometric tiff ground truth distills knowledge to a unimodal rgb student network, enabling thermal-sensor-free inference. segmentation supervision is generated using a hybrid approach of segment anything (sam)-guided mask generation, and selection via topsis, along with canny edge detection and otsu's thresholding pipeline for automatic point prompt selection. our method is the first to perform per-pixel temperature regression from rgb uav data, demonstrating strong generalization on the recent flame 3 dataset. this work lays the foundation for lightweight, cost-effective uav-based wildfire monitoring systems without thermal sensors.",,2025-05-02,,"['michael marinaccio', 'fatemeh afghah']"
2505.01644,a dual-task synergy-driven generalization framework for pancreatic   cancer segmentation in ct scans,eess.iv cs.cv,"pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. the generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. this framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model's generalization ability from a dual-task perspective. besides, dual self-supervised learning in feature spaces and output spaces augments the model's representational capability and stability across different imaging views. experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (dice: 84.07%). more importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. the codes will be released at https://github.com/sjtubme-qianlab/dual-task-seg.",10.1109/tmi.2025.3566376,2025-05-02,,"['jun li', 'yijue zhang', 'haibo shi', 'minhong li', 'qiwei li', 'xiaohua qian']"
2505.01650,toward onboard ai-enabled solutions to space object detection for space   sustainability,cs.cv eess.iv,"the rapid expansion of advanced low-earth orbit (leo) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. a solution to the challenge is effective space object detection (sod) for collision assessment and avoidance. in sod, an leo satellite must detect other satellites and objects with high precision and minimal delay. this paper investigates the feasibility and effectiveness of employing vision sensors for sod tasks based on deep learning (dl) models. it introduces models based on the squeeze-and-excitation (se) layer, vision transformer (vit), and the generalized efficient layer aggregation network (gelan) and evaluates their performance under sod scenarios. experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (map50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (map50:95) scores of up to 0.280. compared to the baseline gelan-t model, the proposed gelan-vit-se model increases the average map50 from 0.721 to 0.751, improves the map50:95 from 0.266 to 0.274, reduces giga floating point operations (gflops) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mw to 2028.7 mw by 2.5\%.",,2025-05-02,,"['wenxuan zhang', 'peng hu']"
2505.01656,a novel waveinst-based network for tree trunk structure extraction and   pattern analysis in forest inventory,cs.cv,"the pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. the current trunk and branch extraction technologies are mainly lidar-based or uav-based. the former approaches obtain high-precision 3d data, but its equipment cost is high and the three-dimensional (3d) data processing is complex. the latter approaches efficiently capture canopy information, but they miss the 3-d structure of trees. in order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel waveinst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. experimental results of the proposed model show superior performance on synthtree43k, canetree100, urban street and our poplardataset. moreover, we present a new phenotypic dataset poplardataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. the proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2d images directly. this study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.",,2025-05-02,,"['chenyang fan', 'xujie zhu', 'taige luo', 'sheng xu', 'zhulin chen', 'hongxin yang']"
2505.01657,ragar: retrieval augment personalized image generation guided by   recommendation,cs.ir cs.cv,"personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. although effective, existing methods face two main issues. first, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. to address these issues, we propose retrieval augment personalized image generation guided by recommendation (ragar). our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. extensive experiments and human evaluations on three real-world datasets demonstrate that ragar achieves significant improvements in both personalization and semantic metrics compared to five baselines.",,2025-05-02,,"['run ling', 'wenji wang', 'yuting liu', 'guibing guo', 'linying jiang', 'xingwei wang']"
2505.01664,soft-masked semi-dual optimal transport for partial domain adaptation,cs.cv cs.ai,"visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. partial domain adaptation (pda) is a general and practical scenario in which the target label space is a subset of the source one. the challenges of pda exist due to not only domain shift but also the non-identical label spaces of domains. in this paper, a soft-masked semi-dual optimal transport (ssot) method is proposed to deal with the pda problem. specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. a soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. to deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized kantorovich problem is employed since it can be optimized by gradient-based algorithms. further, a neural network is exploited to approximate the kantorovich potential due to its strong fitting ability. this network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. the ssot model is built upon neural networks, which can be optimized alternately in an end-to-end manner. extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of ssot.",,2025-05-02,,"['yi-ming zhai', 'chuan-xian ren', 'hong yan']"
2505.01670,efficient multi subject visual reconstruction from fmri using aligned   representations,eess.iv cs.cv cs.lg,"this work introduces a novel approach to fmri-based visual image reconstruction using a subject-agnostic common representation space. we show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. this is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. our approach excels in low-data scenarios. we evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.",,2025-05-02,,"['christos zangos', 'danish ebadulla', 'thomas christopher sprague', 'ambuj singh']"
2505.01680,"automated arat scoring using multimodal video analysis, multi-view   fusion, and hierarchical bayesian models: a clinician study",cs.cv cs.ai cs.hc math.pr,"manual scoring of the action research arm test (arat) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. we propose an automated arat scoring system integrating multimodal video analysis with slowfast, i3d, and transformer-based models using openpose keypoints and object locations. our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. hierarchical bayesian models (hbms) infer movement quality components, enhancing interpretability. a clinician dashboard displays task scores, execution times, and quality assessments. we conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with hbms aligning closely with manual assessments. this work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.",,2025-05-03,,"['tamim ahmed', 'thanassis rikakis']"
2505.01694,topology-aware clip few-shot learning,cs.cv cs.ai,"efficiently adapting large vision-language models (vlms) like clip for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. existing methods often overlook valuable structural information within the vlm's latent space. we introduce a topology-aware tuning approach integrating representation topology divergence (rtd) into the task residual (tr) framework. by explicitly aligning the topological structures of visual and text representations using a combined rtd and cross-entropy loss, while freezing base vlm encoders, our method enhances few-shot performance. we optimize only lightweight task residual parameters, effectively leveraging topological information. across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\% over relevant baseline methods in few-shot settings. this work presents an effective strategy to boost vlm few-shot capabilities by incorporating topological alignment.",,2025-05-03,,['dazhi huang']
2505.01699,component-based fairness in face attribute classification with bayesian   network-informed meta learning,cs.cv cs.ai,"the widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. while previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. in this paper, we focus on face component fairness, a fairness notion defined by biological face features. to our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. in this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. to address these issues, we propose \textbf{b}ayesian \textbf{n}etwork-informed \textbf{m}eta \textbf{r}eweighting (bnmr), which incorporates a bayesian network calibrator to guide an adaptive meta-learning-based sample reweighting process. during the training process of our approach, the bayesian network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. to demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. our results show that bnmr is able to consistently outperform recent face bias mitigation baselines. moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \textit{gender}). our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. the code for our work is publicly available~\footnote{https://github.com/yliuaa/bnmr-faircompface.git}.",,2025-05-03,,"['yifan liu', 'ruichen yao', 'yaokun liu', 'ruohan zong', 'zelin li', 'yang zhang', 'dong wang']"
2505.01709,robridge: a hierarchical architecture bridging cognition and execution   for general robotic manipulation,cs.ro cs.ai cs.cv,"operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. while recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. existing methods often compromise cognitive and executive capabilities. to address these challenges, in this paper, we propose robridge, a hierarchical intelligent architecture for general robotic manipulation. it consists of a high-level cognitive planner (hcp) based on a large-scale pre-trained vision-language model (vlm), an invariant operable representation (ior) serving as a symbolic bridge, and a generalist embodied agent (gea). robridge maintains the declarative skill of vlm and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. robridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. this work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.",,2025-05-03,2025-05-07,"['kaidong zhang', 'rongtao xu', 'pengzhen ren', 'junfan lin', 'hefeng wu', 'liang lin', 'xiaodan liang']"
2505.01711,knowledge-augmented language models interpreting structured chest x-ray   findings,cs.cv,"automated interpretation of chest x-rays (cxr) is a critical task with the potential to significantly improve clinical workflow and patient care. while recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (llms) for this visual task remains an underexplored area. this paper introduces cxr-textinter, a novel framework that repurposes powerful text-centric llms for cxr interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. we augment this llm-centric approach with an integrated medical knowledge module to enhance clinical reasoning. to facilitate training and evaluation, we developed the mediinstruct-cxr dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the cxr-clineval benchmark for comprehensive assessment across various interpretation tasks. extensive experiments on cxr-clineval demonstrate that cxr-textinter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. ablation studies confirm the critical contribution of the knowledge integration module. furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by cxr-textinter. our work validates an alternative paradigm for medical image ai, showcasing the potential of harnessing advanced llm capabilities when visual information is effectively structured and domain knowledge is integrated.",,2025-05-03,,"['alexander davis', 'rafael souza', 'jia-hao lim']"
2505.01713,vision and intention boost large language model in long-term action   anticipation,cs.cv,"long-term action anticipation (lta) aims to predict future actions over an extended period. previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. recent researches leverage large language models (llms) by utilizing text-based inputs which suffer severe information loss. to tackle these limitations single-modality methods face, we propose a novel intention-conditioned vision-language (icvl) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of llms. considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (vlm) to infer behavioral intentions as comprehensive textual features directly from video inputs. the inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. these enhanced visual representations, along with textual prompts, are fed into llm for future action anticipation. furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. extensive experiments with state-of-the-art performance on ego4d, epic-kitchens-55, and egtea gaze+ datasets fully demonstrate the effectiveness and superiority of the proposed method.",,2025-05-03,,"['congqi cao', 'lanshu hu', 'yating yu', 'yanning zhang']"
2505.01729,posepilot: steering camera pose for generative world models with   self-supervised depth,cs.cv,"recent advancements in autonomous driving (ad) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. however, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. in this paper, we introduce posepilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. drawing inspiration from self-supervised depth estimation, posepilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. these outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. to further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. extensive experiments on autonomous driving and general-domain video datasets demonstrate that posepilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. by steering camera pose with self-supervised depth, posepilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.",,2025-05-03,,"['bu jin', 'weize li', 'baihan yang', 'zhenxin zhu', 'junpeng jiang', 'huan-ang gao', 'haiyang sun', 'kun zhan', 'hengtong hu', 'xueyang zhang', 'peng jia', 'hao zhao']"
2505.01737,learning multi-frame and monocular prior for estimating geometry in   dynamic scenes,cs.cv,"in monocular videos that capture dynamic scenes, estimating the 3d geometry of video contents has been a fundamental challenge in computer vision. specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. to address the challenge, we present a new model, coined mmp, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. specifically, based on the recent siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. in our experiments, we find mmp can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error.",,2025-05-03,,"['seong hyeon park', 'jinwoo shin']"
2505.01741,clog-cd: curriculum learning based on oscillating granularity of class   decomposed medical image classification,eess.iv cs.cv,"curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. it has the ability to improve the final model's performance and accelerate the training process. however, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. in this paper, we present a novel convolutional neural network (cnn) training method based on the curriculum learning strategy and the class decomposition approach, which we call clog-cd, to improve the performance of medical image classification. we evaluated our method on four different imbalanced medical image datasets, such as chest x-ray (cxr), brain tumour, digital knee x-ray, and histopathology colorectal cancer (crc). clog-cd utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e., anti-curriculum technique). we also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. we used two pre-trained networks, resnet-50 and densenet-121, as the backbone for clog-cd. the results with resnet-50 show that clog-cd has the ability to improve classification performance with an accuracy of 96.08% for the cxr dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee x-ray, and 99.17% for the crc dataset, compared to other training strategies. in addition, with densenet-121, clog-cd has achieved 94.86%, 94.63%, 76.19%, and 99.45% for cxr, brain tumour, digital knee x-ray, and crc datasets, respectively",10.1109/tetc.2025.3562620,2025-05-03,,"['asmaa abbas', 'mohamed gaber', 'mohammed m. abdelsamea']"
2505.01743,an llm-empowered low-resolution vision system for on-device human   behavior understanding,cs.cv cs.ai cs.lg,"the rapid advancements in large vision language models (lvlms) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (hbu) in low-resolution vision systems, such as depth, thermal, and infrared. however, existing large vision language model (lvlm) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as rgb images. a quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. in this paper, we propose a novel, labor-saving system, llambda, designed to support low-resolution hbu. the core idea is to leverage limited labeled data and a large amount of unlabeled data to guide llms in generating informative captions, which can be combined with raw data to effectively fine-tune lvlm models for understanding low-resolution videos in hbu. first, we propose a contrastive-oriented data labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. second, we propose a physical-knowledge guided captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. therefore, it can improve llms' understanding of sequential data and then generate high-quality video captions. finally, to ensure on-device deployability, we employ lora-based efficient fine-tuning to adapt lvlms for low-resolution data. we evaluate llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that llambda outperforms several state-of-the-art lvlm systems up to $40.03\%$ on average bert-score.",,2025-05-03,,"['siyang jiang', 'bufang yang', 'lilin xu', 'mu yuan', 'yeerzhati abudunuer', 'kaiwei liu', 'liekang zeng', 'hongkai chen', 'zhenyu yan', 'xiaofan jiang', 'guoliang xing']"
2505.01746,co$^{3}$gesture: towards coherent concurrent co-speech 3d gesture   generation with interactive diffusion,cs.cv,"generating gestures from human speech has gained tremendous progress in animating virtual avatars. while the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. to fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7m frames for diverse two-person interactive posture sequences, dubbed ges-inter. additionally, we propose co$^3$gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a temporal interaction module (tim). tim can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected ges-inter dataset. the dataset and source code are publicly available at \href{https://mattie-e.github.io/co3/}{\textit{https://mattie-e.github.io/co3/}}.",,2025-05-03,,"['xingqun qi', 'yatian wang', 'hengyuan zhang', 'jiahao pan', 'wei xue', 'shanghang zhang', 'wenhan luo', 'qifeng liu', 'yike guo']"
2505.01755,lensnet: an end-to-end learning framework for empirical point spread   function modeling and lensless imaging reconstruction,eess.iv cs.cv,"lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. however, such systems are fundamentally governed by the point spread function (psf), which dictates how a point source contributes to the final captured signal. traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate psf models. these rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. in this paper, we propose lensnet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. central to our approach is a learnable coded mask simulator (cms) that enables dynamic, data-driven estimation of the psf during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. by embedding a wiener filtering component, lensnet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. extensive experiments demonstrate lensnet's robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. the proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. the link of code is https://github.com/baijiesong/lensnet.",,2025-05-03,,"['jiesong bai', 'yuhao yin', 'yihang dong', 'xiaofeng zhang', 'chi-man pun', 'xuhang chen']"
2505.01766,multimodal graph representation learning for robust surgical workflow   recognition with adversarial feature disentanglement,cs.cv cs.ro,"surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. however, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. in this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. we propose a multimodal graph representation network with adversarial feature disentanglement (grad) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. specifically, we introduce a multimodal disentanglement graph network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. to align feature spaces across modalities, we propose a vision-kinematic adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. furthermore, we design a contextual calibrated decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.",,2025-05-03,,"['long bai', 'boyi ma', 'ruohan wang', 'guankun wang', 'beilei cui', 'zhongliang jiang', 'mobarakol islam', 'zhe min', 'jiewen lai', 'nassir navab', 'hongliang ren']"
2505.01768,continuous filtered backprojection by learnable interpolation network,eess.iv cs.cv,"accurate reconstruction of computed tomography (ct) images is crucial in medical imaging field. however, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. in this study, to address this issue, we propose a novel deep learning model, named leanable-interpolation-based fbp or linfbp shortly, to enhance the reconstructed ct image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (fbp) and alleviates the interpolation errors. specifically, in the proposed linfbp, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in fbp. extensive experiments, which encompass diverse ct scenarios, demonstrate the effectiveness of the proposed linfbp in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability.",,2025-05-03,,"['hui lin', 'dong zeng', 'qi xie', 'zerui mao', 'jianhua ma', 'deyu meng']"
2505.01790,enhancing the learning experience: using vision-language models to   generate questions for educational videos,cs.cv cs.cl cs.mm,"web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. however, improving user engagement and knowledge retention remains a challenge. automatically generated questions can activate learners and support their knowledge acquisition. further, they can help teachers and learners assess their understanding. while large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. in this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. we assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. we identify requirements for future multimodal datasets and outline promising research directions.",,2025-05-03,,"['markos stamatakis', 'joshua berger', 'christian wartena', 'ralph ewerth', 'anett hoppe']"
2505.01799,aquags: fast underwater scene reconstruction with sfm-free gaussian   splatting,cs.cv,"underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3d models from images captured by underwater platforms. however, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of structure-from-motion (sfm) pose estimation, leading to subsequent reconstruction failures. additionally, sfm methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. in this paper, we introduce aquags, an sfm-free underwater scene reconstruction model based on the seathru algorithm, which facilitates rapid and accurate separation of scene details and medium features. our approach initializes gaussians by integrating state-of-the-art multi-view stereo (mvs) technology, employs implicit neural radiance fields (nerf) for rendering translucent media and utilizes the latest explicit 3d gaussian splatting (3dgs) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.",,2025-05-03,,"['junhao shi', 'jisheng xu', 'jianping he', 'zhiliang lin']"
2505.01802,efficient 3d full-body motion generation from sparse tracking inputs   with temporal windows,cs.cv,"to have a seamless user experience on immersive ar/vr applications, the importance of efficient and effective neural network (nn) models is undeniable, since missing body parts that cannot be captured by limited sensors should be generated using these models for a complete 3d full-body reconstruction in virtual environment. however, the state-of-the-art nn-models are typically computational expensive and they leverage longer sequences of sparse tracking inputs to generate full-body movements by capturing temporal context. inevitably, longer sequences increase the computation overhead and introduce noise in longer temporal dependencies that adversely affect the generation performance. in this paper, we propose a novel multi-layer perceptron (mlp)-based method that enhances the overall performance while balancing the computational cost and memory overhead for efficient 3d full-body generation. precisely, we introduce a nn-mechanism that divides the longer sequence of inputs into smaller temporal windows. later, the current motion is merged with the information from these windows through latent representations to utilize the past context for the generation. our experiments demonstrate that generation accuracy of our method with this nn-mechanism is significantly improved compared to the state-of-the-art methods while greatly reducing computational costs and memory overhead, making our method suitable for resource-constrained devices.",,2025-05-03,,"['georgios fotios angelis', 'savas ozkan', 'sinan mutlu', 'paul wisbey', 'anastasios drosou', 'mete ozay']"
2505.01805,not every tree is a forest: benchmarking forest types from satellite   remote sensing,cs.cv,"developing accurate and reliable models for forest types mapping is critical to support efforts for halting deforestation and for biodiversity conservation (such as european union deforestation regulation (eudr)). this work introduces forty, a benchmark for global-scale forest types mapping using multi-temporal satellite data1. the benchmark comprises 200,000 time series of image patches, each consisting of sentinel-2, sentinel-1, climate, and elevation data. each time series captures variations at monthly or seasonal cadence. per-pixel annotations, including forest types and other land use classes, support image segmentation tasks. unlike most existing land use products that often categorize all forest areas into a single class, our benchmark differentiates between three forest types classes: natural forest, planted forest, and tree crops. by leveraging multiple public data sources, we achieve global coverage with this benchmark. we evaluate the forest types dataset using several baseline models, including convolution neural networks and transformer-based models. additionally, we propose a novel transformer-based model specifically designed to handle multi-modal, multi-temporal satellite data for forest types mapping. our experimental results demonstrate that the proposed model surpasses the baseline models in performance.",,2025-05-03,,"['yuchang jiang', 'maxim neumann']"
2505.01809,3dwg: 3d weakly supervised visual grounding via category and   instance-level alignment,cs.cv,"the 3d weakly-supervised visual grounding task aims to localize oriented 3d boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. this setting presents two primary challenges: category-level ambiguity and instance-level complexity. category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. to address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. in the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. in the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. these designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: nr3d, sr3d, and scanref.",,2025-05-03,,"['xiaoqi li', 'jiaming liu', 'nuowei han', 'liang heng', 'yandong guo', 'hao dong', 'yang liu']"
2505.01831,multi-scale target-aware representation learning for fundus image   enhancement,eess.iv cs.cv,"high-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. recent years have witnessed promising progress in fundus image enhancement. however, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. to address these challenges, we propose a multi-scale target-aware representation learning framework (mtrl-fie) for efficient fundus image enhancement. specifically, we propose a multi-scale feature encoder (mfe) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. next, we design a structure-preserving hierarchical decoder (shd) to fuse multi-scale feature embeddings for real fundus image restoration. shd integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. meanwhile, a target-aware feature aggregation (tfa) module is used to enhance pathological regions and reduce artifacts. experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of mtrl-fie for fundus image enhancement. compared to state-of-the-art methods, mtrl-fie achieves superior enhancement performance with a more lightweight architecture. furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications.",,2025-05-03,,"['haofan wu', 'yin huang', 'yuqing wu', 'qiuyu yang', 'bingfang wang', 'li zhang', 'muhammad fahadullah khan', 'ali zia', 'm. saleh memon', 'syed sohail bukhari', 'abdul fattah memon', 'daizong ji', 'ya zhang', 'ghulam mustafa', 'yin fang']"
2505.01837,cvvnet: a cross-vertical-view network for gait recognition,cs.cv,"gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. while existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. our experiments show up to 60\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. current cnn and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. to tackle this challenge, we propose cvvnet (cross-vertical-view network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. cvvnet employs a high-low frequency extraction module (hlfe) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. we also introduce the dynamic gated aggregation (dga) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. the integration of our core multi-scale attention gated aggregation (msaga) module, hlfe and dga enables cvvnet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. experimental results show that our cvvnet achieves state-of-the-art performance, with $8.6\%$ improvement on dronegait and $2\%$ on gait3d compared with the best existing methods.",,2025-05-03,,"['xiangru li', 'wei song', 'yingda huang', 'wei meng', 'le chang']"
2505.01838,mvhumannet++: a large-scale dataset of multi-view daily dressing human   captures with richer annotations for 3d human digitization,cs.cv,"in this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. however, in the realm of 3d vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like objaverse and mvimgnet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. to bridge this gap, we present mvhumannet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. the primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2d and 3d keypoints, smpl/smplx parameters, and corresponding textual descriptions. additionally, the proposed mvhumannet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. to explore the potential of our proposed mvhumannet++ dataset in various 2d and 3d visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by mvhumannet++. as the current largest-scale 3d human dataset, we hope that the release of mvhumannet++ dataset with annotations will foster further innovations in the domain of 3d human-centric tasks at scale. mvhumannet++ is publicly available at https://kevinlee09.github.io/research/mvhumannet++/.",,2025-05-03,,"['chenghong li', 'hongjie liao', 'yihao zhi', 'xihe yang', 'zhengwentai sun', 'jiahao chang', 'shuguang cui', 'xiaoguang han']"
2505.01851,mitigating group-level fairness disparities in federated visual language   models,cs.cv,"visual language models (vlms) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (fl) environments. this paper addresses the critical issue of group fairness in federated vlms by introducing fvl-fp, a novel framework that combines fl with fair prompt tuning techniques. we focus on mitigating demographic biases while preserving model performance through three innovative components: (1) cross-layer demographic fair prompting (cdfp), which adjusts potentially biased embeddings through counterfactual regularization; (2) demographic subspace orthogonal projection (dsop), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) fair-aware prompt fusion (fpf), which dynamically balances client contributions based on both performance and fairness metrics. extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\% compared to standard fl approaches, while maintaining task performance within 6\% of state-of-the-art results. fvl-fp effectively addresses the challenges of non-iid data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.",,2025-05-03,,"['chaomeng chen', 'zitong yu', 'junhao dong', 'sen su', 'linlin shen', 'shutao xia', 'xiaochun cao']"
2505.01854,accelerating volumetric medical image annotation via short-long memory   sam 2,eess.iv cs.ai cs.cv,"manual annotation of volumetric medical images, such as magnetic resonance imaging (mri) and computed tomography (ct), is a labor-intensive and time-consuming process. recent advancements in foundation models for video object segmentation, such as segment anything model 2 (sam 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. however, the performance of sam 2 in this context varies. our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. to address this problem, we propose short-long memory sam 2 (slm-sam 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. we evaluate slm-sam 2 on three public datasets covering organs, bones, and muscles across mri and ct modalities. we show that the proposed method markedly outperforms the default sam 2, achieving average dice similarity coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. slm-sam 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development.",,2025-05-03,,"['yuwen chen', 'zafer yildiz', 'qihang li', 'yaqian chen', 'haoyu dong', 'hanxue gu', 'nicholas konz', 'maciej a. mazurowski']"
2505.01857,dualdiff: dual-branch diffusion model for autonomous driving with   semantic fusion,cs.cv,"accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. however, existing approaches, which primarily use 3d bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. in this paper, we propose dualdiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. we introduce occupancy ray sampling (ors), a semantic-rich 3d representation, alongside numerical driving scene representation, for comprehensive foreground and background control. to improve cross-modal information integration, we propose a semantic fusion attention (sfa) mechanism that aligns and fuses features across modalities. furthermore, we design a foreground-aware masked (fgm) loss to enhance the generation of tiny objects. dualdiff achieves state-of-the-art performance in fid score, as well as consistently better results in downstream bev segmentation and 3d object detection tasks.",,2025-05-03,,"['haoteng li', 'zhao yang', 'zezhong qian', 'gongpeng zhao', 'yuqi huang', 'jun yu', 'huazheng zhou', 'longjun liu']"
2505.01869,visual enhancement and 3d representation for underwater scenes: a review,cs.cv,"underwater visual enhancement (uve) and underwater 3d reconstruction pose significant challenges in   computer vision and ai-based tasks due to complex imaging conditions in aquatic environments. despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   uve and underwater 3d reconstruction remains absent. to advance research in these areas, we present an   in-depth review from multiple perspectives. first, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. we survey advanced methods for visual enhancement and   3d reconstruction specifically designed for underwater scenarios. the paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including neural radiance fields and 3d gaussian   splatting, discussing their effectiveness in handling underwater distortions. finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art uve and underwater 3d reconstruction algorithms across multiple   benchmark datasets. finally, we highlight key research directions for future advancements in underwater vision.",,2025-05-03,,"['guoxi huang', 'haoran wang', 'brett seymour', 'evan kovacs', 'john ellerbrock', 'dave blackham', 'nantheera anantrasirichai']"
2505.01880,weakly-supervised audio temporal forgery localization via progressive   audio-language co-learning network,cs.sd cs.cv cs.mm eess.as,"audio temporal forgery localization (atfl) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. existing atfl methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. to meet this challenge, in this paper, we propose a progressive audio-language co-learning network (loco) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. in this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. in addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. extensive experiments show that the proposed loco achieves sota performance on three public benchmarks.",,2025-05-03,2025-05-07,"['junyan wu', 'wenbo xu', 'wei lu', 'xiangyang luo', 'rui yang', 'shize guo']"
2505.01881,physnav-dg: a novel adaptive framework for robust vlm-sensor fusion in   navigation applications,cs.cv cs.ai cs.lg cs.mm cs.ro,"robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. we present physnav-dg, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. a modified adaptive kalman filter dynamically adjusts its noise parameters based on environmental context. it leverages several streams of raw sensor data along with semantic insights from models such as llama 3.2 11b and blip-2. to evaluate our approach, we introduce the md-nex benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. extensive experiments and ablations show that physnav-dg improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. this work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,2025-05-03,,"['trisanth srinivasan', 'santosh patapati']"
2505.01882,cmawrnet: multiple adverse weather removal via a unified quaternion   neural architecture,cs.cv,"images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. when designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. recently, deep-learning methods offered a solution for a single type of degradation. current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. this work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called cmawrnet. it is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. we also introduce a quaternion similarity loss function to preserve color information better. the quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed cmawrnet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. extensive computer simulations validate that cmawrnet improves the performance of downstream applications such as object detection. this is the first time the decomposition approach has been applied to the universal weather removal task.",,2025-05-03,,"['vladimir frants', 'sos agaian', 'karen panetta', 'peter huang']"
2505.01884,adversarial robustness of deep learning models for inland water body   segmentation from sar images,eess.iv cs.ai cs.cv cs.lg,"inland water body segmentation from synthetic aperture radar (sar) images is an important task needed for several applications, such as flood mapping. while sar sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from sar images is not straightforward. inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. u-net is a widely used deep learning model for land-water segmentation of sar images. in practice, manual annotation is often used to generate the corresponding water masks as ground truth. manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. in this work, we simulate manual errors in the form of adversarial attacks on the u-net model and study the robustness of the model to human errors in annotation. our results indicate that u-net can tolerate a certain level of corruption before its performance drops significantly. this finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. the code and the new dataset, along with adversarial examples for robust training, are publicly available. (github link - https://github.com/gvcl/iwseg-sar-poison.git)",,2025-05-03,2025-05-06,"['siddharth kothari', 'srinivasan murali', 'sankalp kothari', 'ujjwal verma', 'jaya sreevalsan-nair']"
2505.01888,rethinking score distilling sampling for 3d editing and generation,cs.cv,"score distillation sampling (sds) has emerged as a prominent method for text-to-3d generation by leveraging the strengths of 2d diffusion models. however, sds is limited to generation tasks and lacks the capability to edit existing 3d assets. conversely, variants of sds that introduce editing capabilities often can not generate new 3d assets effectively. in this work, we observe that the processes of generation and editing within sds and its variants have unified underlying gradient terms. building on this insight, we propose unified distillation sampling (uds), a method that seamlessly integrates both the generation and editing of 3d assets. essentially, uds refines the gradient terms used in vanilla sds methods, unifying them to support both tasks. extensive experiments demonstrate that uds not only outperforms baseline methods in generating 3d assets with richer details but also excels in editing tasks, thereby bridging the gap between 3d generation and editing. the code is available on: https://github.com/xingy038/uds.",,2025-05-03,,"['xingyu miao', 'haoran duan', 'yang long', 'jungong han']"
2505.01928,gensync: a generalized talking head framework for audio-driven   multi-subject lip-sync using 3d gaussian splatting,cs.cv,"we introduce gensync, a novel framework for multi-identity lip-synced video synthesis using 3d gaussian splatting. unlike most existing 3d methods that require training a new model for each identity , gensync learns a unified network that synthesizes lip-synced videos for multiple speakers. by incorporating a disentanglement module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. this design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",,2025-05-03,,"['anushka agarwal', 'muhammad yusuf hassan', 'talha chafekar']"
2505.01934,gaus-slam: dense rgb-d slam with gaussian surfels,cs.cv,"we propose gaus-slam, a dense rgb-d slam system that leverages 2d gaussian surfels to achieve robust tracking and high-fidelity mapping. our investigations reveal that gaussian-based scene representations exhibit geometry distortion under novel viewpoints, which significantly degrades the accuracy of gaussian-based tracking methods. these geometry inconsistencies arise primarily from the depth modeling of gaussian primitives and the mutual interference between surfaces during the depth blending. to address these, we propose a 2d gaussian-based incremental reconstruction strategy coupled with a surface-aware depth rendering mechanism, which significantly enhances geometry accuracy and multi-view consistency. additionally, the proposed local map design dynamically isolates visible surfaces during tracking, mitigating misalignment caused by occluded regions in global maps while maintaining computational efficiency with increasing gaussian density. extensive experiments across multiple datasets demonstrate that gaus-slam outperforms comparable methods, delivering superior tracking precision and rendering fidelity. the project page will be made available at https://gaus-slam.github.io.",,2025-05-03,,"['yongxin su', 'lin chen', 'kaiting zhang', 'zhongliang zhao', 'chenfeng hou', 'ziping yu']"
2505.01938,hybridgs: high-efficiency gaussian splatting data compression using   dual-channel sparse representation and point cloud encoder,cs.cv eess.iv,"most existing 3d gaussian splatting (3dgs) compression schemes focus on producing compact 3dgs representation via implicit data embedding. they have long coding times and highly customized data format, making it difficult for widespread deployment. this paper presents a new 3dgs compression framework called hybridgs, which takes advantage of both compact generation and standardized point cloud data encoding. hybridgs first generates compact and explicit 3dgs data. a dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. it then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. a simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. at the current stage, hybridgs does not include any modules aimed at improving 3dgs quality during generation. but experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. the code is publicly available at https://github.com/qi-yangsjtu/hybridgs.",,2025-05-03,,"['qi yang', 'le yang', 'geert van der auwera', 'zhu li']"
2505.01950,segment any rgb-thermal model with language-aided distillation,cs.cv cs.ai,"the recent segment anything model (sam) demonstrates strong instance segmentation performance across various downstream tasks. however, sam is trained solely on rgb data, limiting its direct applicability to rgb-thermal (rgb-t) semantic segmentation. given that rgb-t provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, sartm, which customizes the powerful sam for rgb-t semantic segmentation. our key idea is to unleash the potential of sam while introduce semantic understanding modules for rgb-t data pairs. specifically, our framework first involves fine tuning the original sam by adding extra lora layers, aiming at preserving sam's strong generalization and segmentation capabilities for downstream tasks. secondly, we introduce language information as guidance for training our sartm. to address cross-modal inconsistencies, we introduce a cross-modal knowledge distillation(cmkd) module that effectively achieves modality adaptation while maintaining its generalization capabilities. this semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. furthermore, we enhance the segmentation performance by adjusting the segmentation head of sam and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. extensive experiments are conducted across three multi-modal rgbt semantic segmentation benchmarks: mfnet, pst900, and fmb. both quantitative and qualitative results consistently demonstrate that the proposed sartm significantly outperforms state-of-the-art approaches across a variety of conditions.",,2025-05-03,,"['dong xing', 'xianxun zhu', 'wei zhou', 'qika lin', 'hang yang', 'yuqing wang']"
2505.01958,a comprehensive analysis for visual object hallucination in large   vision-language models,cs.cv cs.cl,"large vision-language models (lvlms) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. it refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. in this paper, we analyze each component of llava-like lvlms -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. based on our observations, we propose methods to mitigate hallucination for each problematic component. additionally, we developed two hallucination benchmarks: qa-visualgenome, which emphasizes attribute and relation hallucinations, and qa-fb15k, which focuses on cognition-based hallucinations.",,2025-05-03,,"['liqiang jing', 'guiming hardy chen', 'ehsan aghazadeh', 'xin eric wang', 'xinya du']"
2505.01969,mc3d-ad: a unified geometry-aware reconstruction model for   multi-category 3d anomaly detection,cs.cv,"3d anomaly detection (ad) is a promising means of controlling the quality of manufactured products. however, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. therefore, this paper presents a novel unified model for multi-category 3d anomaly detection (mc3d-ad) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. first, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. this leads to local and global geometry-aware reconstructed feature tokens for the ad task. mc3d-ad is evaluated on two publicly available real3d-ad and anomaly-shapenet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement in object-level auroc over real3d-ad and anomaly-shapenet, respectively. the source code will be released upon acceptance.",,2025-05-03,,"['jiayi cheng', 'can gao', 'jie zhou', 'jiajun wen', 'tao dai', 'jinbao wang']"
2505.01973,visual dominance and emerging multimodal approaches in distracted   driving detection: a review of machine learning techniques,cs.cv,"distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. recent developments in machine learning (ml) and deep learning (dl) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. this systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ml/dl techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. the review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (cnns) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (rf) methods, offer privacy-aware alternatives. multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. these findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (adas) and road safety interventions.",,2025-05-03,,"['anthony dontoh', 'stephanie ivey', 'logan sirbaugh', 'andrews danyo', 'armstrong aboah']"
2505.01984,lifelong whole slide image analysis: online vision-language adaptation   and past-to-present gradient distillation,cs.cv,"whole slide images (wsis) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. however, the rapid growth of computational tasks involving wsis poses significant challenges. given that wsis are gigapixels in size, they present difficulties in terms of storage, processing, and model training. therefore, it is essential to develop lifelong learning approaches for wsi analysis. in scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. in this study, we introduce adafgrad, a method designed to enhance lifelong learning for whole-slide image (wsi) analysis. first, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. we construct a sequence of six tcga datasets for training and evaluation. experimental results show that adafgrad outperforms both state-of-the-art wsi-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). moreover, adafgrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules.",,2025-05-04,,"['doanh c. bui', 'hoai luan pham', 'vu trung duong le', 'tuan hai vu', 'van duy tran', 'khang nguyen', 'yasuhiko nakashima']"
2505.01986,drug classification based on x-ray spectroscopy combined with machine   learning,cs.cv,"the proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. traditional detection methods have high requirements for instruments and environments, making the operation complex. x-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. in this study, we constructed a classification model using convolutional neural networks (cnn), support vector machines (svm), and particle swarm optimization (pso) to classify and identify drugs based on their x-ray spectral profiles. in the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. we utilized cnn to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an svm model. we also utilized pso to optimize two critical initial parameters of the svm. the experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of pso and svm. therefore, the combined approach of x-ray absorption spectroscopy with cnn, pso, and svm provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application.",,2025-05-04,,"['yongming li', 'peng wang', 'bangdong han']"
2505.01996,always skip attention,cs.lg cs.cv,"we highlight a curious empirical result within modern vision transformers (vits). specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. this is in contrast to other elements of a vit that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\eg, cnns) exhibiting good performance in their absence. in this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. additionally, we propose token graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. we validate our approach in both supervised and self-supervised training methods.",,2025-05-04,,"['yiping ji', 'hemanth saratchandran', 'peyman moghaddam', 'simon lucey']"
2505.02001,hybrid image resolution quality metric (hirqm):a comprehensive   perceptual image quality assessment framework,eess.iv cs.cv,"traditional image quality assessment metrics like mean squared error and structural similarity index often fail to reflect perceptual quality under complex distortions. we propose the hybrid image resolution quality metric (hirqm), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. hirqm combines three components: probability density function for local pixel distribution analysis, multi-scale feature similarity for structural integrity across resolutions, and hierarchical deep image features using a pre-trained vgg16 network for semantic alignment with human perception. a dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. our contributions include a unified metric and dynamic weighting for better perceptual alignment. evaluated on tid2013 and live datasets, hirqm achieves pearson and spearman correlations of 0.92 and 0.90, outperforming traditional metrics. it excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration.",,2025-05-04,,['vineesh kumar reddy mondem']
2505.02005,learning heterogeneous mixture of scene experts for large-scale neural   radiance fields,cs.cv,"recent nerf methods on large-scale scenes have underlined the importance of scene decomposition for scalable nerfs. although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. in this paper, we introduce switch-nerf++, a heterogeneous mixture of hash experts (hmohe) network that addresses these challenges within a unified framework. it is a highly scalable nerf that learns heterogeneous decomposition and heterogeneous nerfs efficiently for large-scale scenes in an end-to-end manner. in our framework, a gating network learns to decomposes scenes and allocates 3d points to specialized nerf experts. this gating network is co-optimized with the experts, by our proposed sparsely gated mixture of experts (moe) nerf framework. we incorporate a hash-based gating network and distinct heterogeneous hash experts. the hash-based gating efficiently learns the decomposition of the large-scale scene. the distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. these design choices make our framework an end-to-end and highly scalable nerf solution for real-world large-scale scene modeling to achieve both quality and efficiency. we evaluate our accuracy and scalability on existing large-scale nerf datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from urbanbis. extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to switch-nerf. codes will be released in https://github.com/mizhenxing/switch-nerf.",,2025-05-04,,"['zhenxing mi', 'ping yin', 'xue xiao', 'dan xu']"
2505.02007,efficient noise calculation in deep learning-based mri reconstructions,cs.cv,"accelerated mri reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. noise analyses are central to mri reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. however, deep learning (dl)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. this work proposes a theoretically grounded, memory-efficient technique to calculate voxel-wise variance for quantifying uncertainty due to acquisition noise in accelerated mri reconstructions. our approach approximates noise covariance using the dl network's jacobian, which is intractable to calculate. to circumvent this, we derive an unbiased estimator for the diagonal of this covariance matrix (voxel-wise variance) and introduce a jacobian sketching technique to efficiently implement it. we evaluate our method on knee and brain mri datasets for both data- and physics-driven networks trained in supervised and unsupervised manners. compared to empirical references obtained via monte carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. our work reintroduces accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy dl-based mri. our code will be made publicly available upon acceptance.",,2025-05-04,,"['onat dalmaz', 'arjun d. desai', 'reinhard heckel', 'tolga çukur', 'akshay s. chaudhari', 'brian a. hargreaves']"
2505.02013,mllm-enhanced face forgery detection: a vision-language fusion solution,cs.cv,"reliable face forgery detection algorithms are crucial for countering the growing threat of deepfake-driven disinformation. previous research has demonstrated the potential of multimodal large language models (mllms) in identifying manipulated faces. however, existing methods typically depend on either the large language model (llm) alone or an external detector to generate classification results, which often leads to sub-optimal integration of visual and textual modalities. in this paper, we propose vlf-ffd, a novel vision-language fusion solution for mllm-enhanced face forgery detection. our key contributions are twofold. first, we present eff++, a frame-level, explainability-driven extension of the widely used faceforensics++ (ff++) dataset. in eff++, each manipulated video frame is paired with a textual annotation that describes both the forgery artifacts and the specific manipulation technique applied, enabling more effective and informative mllm training. second, we design a vision-language fusion network (vlf-net) that promotes bidirectional interaction between visual and textual features, supported by a three-stage training pipeline to fully leverage its potential. vlf-ffd achieves state-of-the-art (sota) performance in both cross-dataset and intra-dataset evaluations, underscoring its exceptional effectiveness in face forgery detection.",,2025-05-04,,"['siran peng', 'zipei wang', 'li gao', 'xiangyu zhu', 'tianshuo zhang', 'ajian liu', 'haoyuan zhang', 'zhen lei']"
2505.02018,r-bench: graduate-level multi-disciplinary benchmarks for llm & mllm   complex reasoning evaluation,cs.cv,"reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. in this paper, we introduce a graduate-level, multi-disciplinary, englishchinese benchmark, dubbed as reasoning bench (r-bench), for assessing the reasoning capability of both language and multimodal models. rbench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both english and chinese. these questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an olympiad-level multi-disciplinary benchmark. we evaluate widely used models, including openai o1, gpt-4o, deepseek-r1, etc. experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. even the top-performing model openai o1 achieves only 53.2% accuracy on our multimodal evaluation. data and code are made publicly available at here.",,2025-05-04,,"['meng-hao guo', 'jiajun xu', 'yi zhang', 'jiaxi song', 'haoyang peng', 'yi-xuan deng', 'xinzhi dong', 'kiyohiro nakayama', 'zhengyang geng', 'chen wang', 'bolin ni', 'guo-wei yang', 'yongming rao', 'houwen peng', 'han hu', 'gordon wetzstein', 'shi-min hu']"
2505.02025,a birotation solution for relative pose problems,cs.cv,"relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. in this article, we break the mold by tackling this traditional problem with a novel birotation solution. we first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be estimated and its corresponding basis transformation. three energy functions, designed based on these metrics, are then minimized on the riemannian manifold $\mathrm{so(3)}$ by iteratively updating the two rotation matrices. the two rotation matrices and the basis transformation corresponding to the minimum energy are ultimately utilized to recover the relative pose. extensive quantitative and qualitative evaluations across diverse relative pose estimation tasks demonstrate the superior performance of our proposed birotation solution. source code, demo video, and datasets will be available at \href{https://mias.group/birotation-solution}{mias.group/birotation-solution} upon publication.",,2025-05-04,,"['hongbo zhao', 'ziwei long', 'mengtan zhang', 'hanli wang', 'qijun chen', 'rui fan']"
2505.02046,a unet model for accelerated preprocessing of crism hyperspectral data   for mineral identification on mars,cs.cv,"accurate mineral identification on the martian surface is critical for understanding the planet's geological history. this paper presents a unet-based autoencoder model for efficient spectral preprocessing of crism mtrdr hyperspectral data, addressing the limitations of traditional methods that are computationally intensive and time-consuming. the proposed model automates key preprocessing steps, such as smoothing and continuum removal, while preserving essential mineral absorption features. trained on augmented spectra from the mica spectral library, the model introduces realistic variability to simulate mtrdr data conditions. by integrating this framework, preprocessing time for an 800x800 mtrdr scene is reduced from 1.5 hours to just 5 minutes on an nvidia t1600 gpu. the preprocessed spectra are subsequently classified using micanet, a deep learning model for martian mineral identification. evaluation on labeled crism trdr data demonstrates that the proposed approach achieves competitive accuracy while significantly enhancing preprocessing efficiency. this work highlights the potential of the unet-based preprocessing framework to improve the speed and reliability of mineral mapping on mars.",,2025-05-04,,"['priyanka kumari', 'sampriti soor', 'amba shetty', 'archana m. nair']"
2505.02048,regression is all you need for medical image translation,eess.iv cs.ai cs.cv,"the acquisition of information-rich images within a limited time budget is crucial in medical imaging. medical image translation (mit) can help enhance and supplement existing datasets by generating synthetic images from acquired data. while generative adversarial nets (gans) and diffusion models (dms) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. in fact, the imitation of acquisition noise or content hallucination hinder clinical utility. here, we introduce yoda (you only denoise once - or average), a novel 2.5d diffusion-based framework for volumetric mit. yoda unites diffusion and regression paradigms to produce realistic or noise-free outputs. furthermore, we propose expectation-approximation (expa) dm sampling, which draws inspiration from mri signal averaging. expa-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain mri and pelvic mri-ct - we show that diffusion and regression sampling yield similar results in practice. as such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. building on these insights, we demonstrate that yoda outperforms several state-of-the-art gan and dm methods. notably, yoda-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. our findings challenge the presumed advantages of dms in mit and pave the way for the practical application of mit in medical imaging.",,2025-05-04,2025-05-06,"['sebastian rassmann', 'david kügler', 'christian ewert', 'martin reuter']"
2505.02052,txp: reciprocal generation of ground pressure dynamics and activity   descriptions for improving human activity recognition,cs.ai cs.cv,"sensor-based human activity recognition (har) has predominantly focused on inertial measurement units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the har domain due to limited datasets. to bridge this gap, we propose to exploit generative foundation models with pressure-specific har techniques. specifically, we present a bidirectional text$\times$pressure model that uses generative foundation models to interpret pressure data as natural language. txp accomplishes two tasks: (1) text2pressure, converting activity text descriptions into pressure sequences, and (2) pressure2text, generating activity descriptions and classifications from dynamic pressure maps. leveraging pre-trained models like clip and llama 2 13b chat, txp is trained on our synthetic presslang dataset, containing over 81,100 text-pressure pairs. validated on real-world data for activities such as yoga and daily tasks, txp provides novel approaches to data augmentation and classification grounded in atomic actions. this consequently improved har performance by up to 12.4\% in macro f1 score compared to the state-of-the-art, advancing pressure-based har with broader applications and deeper insights into human movement.",,2025-05-04,,"['lala shakti swarup ray', 'lars krupp', 'vitor fortes rey', 'bo zhou', 'sungho suh', 'paul lukowicz']"
2505.02056,handling imbalanced pseudolabels for vision-language models with concept   alignment and confusion-aware calibrated margin,cs.cv cs.lg,"adapting vision-language models (vlms) to downstream tasks with pseudolabels has gained increasing attention. a major obstacle is that the pseudolabels generated by vlms tend to be imbalanced, leading to inferior performance. while existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. to fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. to mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. the core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the sota method. our code is avaliable at https://anonymous.4open.science/r/cap-c642/",,2025-05-04,,"['yuchen wang', 'xuefeng bai', 'xiucheng li', 'weili guan', 'liqiang nie', 'xinyang chen']"
2505.02060,transforming faces into video stories -- videoface2.0,cs.cv,"face detection and face recognition have been in the focus of vision community since the very beginnings. inspired by the success of the original videoface digitizer, a pioneering device that allowed users to capture video signals from any source, we have designed an advanced video analytics tool to efficiently create structured video stories, i.e. identity-based information catalogs. videoface2.0 is the name of the developed system for spatial and temporal localization of each unique face in the input video, i.e. face re-identification (reid), which also allows their cataloging, characterization and creation of structured video outputs for later downstream tasks. developed near real-time solution is primarily designed to be utilized in application scenarios involving tv production, media analysis, and as an efficient tool for creating large video datasets necessary for training machine learning (ml) models in challenging vision tasks such as lip reading and multimodal speech recognition. conducted experiments confirm applicability of the proposed face reid algorithm that is combining the concepts of face detection, face recognition and passive tracking-by-detection in order to achieve robust and efficient face reid. the system is envisioned as a compact and modular extensions of the existing video production equipment. presented results are based on test implementation that achieves between 18-25 fps on consumer type notebook. ablation experiments also confirmed that the proposed algorithm brings relative gain in the reduction of number of false identities in the range of 73%-93%. we hope that the presented work and shared code implementation will stimulate further interest in development of similar, application specific video analysis tools, and lower the entry barrier for production of high-quality multi-modal datasets in the future.",,2025-05-04,2025-05-08,"['branko brkljač', 'vladimir kalušev', 'branislav popović', 'milan sečujski']"
2505.02064,"rtv-bench: benchmarking mllm continuous perception, understanding and   reasoning through real-time video",cs.cv,"multimodal large language models (mllms) increasingly excel at perception, understanding, and reasoning. however, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. to bridge this gap, we introduce rtv-bench, a fine-grained benchmark for mllm real-time video analysis. rtv-bench uses three key principles: (1) multi-timestamp question answering (mtqa), where answers evolve with scene changes; (2) hierarchical question structure, combining basic and advanced queries; and (3) multi-dimensional evaluation, assessing the ability of continuous perception, understanding, and reasoning. rtv-bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality qa pairs. we evaluated leading mllms, including proprietary (gpt-4o, gemini 2.0), open-source offline (qwen2.5-vl, videollama3), and open-source real-time (vita-1.5, internlm-xcomposer2.5-omnilive) models. experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost rtv-bench performance, sometimes causing slight decreases. this underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with mllms. our benchmark toolkit is available at: https://github.com/ljungang/rtv-bench.",,2025-05-04,2025-05-05,"['shuhang xun', 'sicheng tao', 'jungang li', 'yibo shi', 'zhixin lin', 'zhanhui zhu', 'yibo yan', 'hanqian li', 'linghao zhang', 'shikang wang', 'yixin liu', 'hanbo zhang', 'ying ma', 'xuming hu']"
2505.02071,hierarchical compact clustering attention (coca) for unsupervised   object-centric learning,cs.cv cs.lg,"we propose the compact clustering attention (coca) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. coca is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as coca-net. at its core, coca utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. thanks to this strategy, coca-net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. additionally, coca-net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. we demonstrate coca-net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics.",,2025-05-04,,"['can küçüksözen', 'yücel yemez']"
2505.02075,benchmarking feature upsampling methods for vision foundation models   using interactive segmentation,cs.cv cs.ai cs.lg,"vision foundation models (vfms) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. as vfms' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. however, vfms typically produce low-resolution features, limiting their direct applicability in this context. one way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines vfm features resolution. to assess the effectiveness of this approach, we investigate interactive segmentation (is) as a novel benchmark for evaluating feature upsampling methods on vfms. due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, is creates a challenging environment that demands comprehensive visual scene understanding. our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves vfm features quality. the code is released at https://github.com/havrylovv/isegprobe",,2025-05-04,,"['volodymyr havrylov', 'haiwen huang', 'dan zhang', 'andreas geiger']"
2505.02079,handocc: nerf-based hand rendering with occupancy networks,cs.cv,"we propose handocc, a novel framework for hand rendering based upon occupancy. popular rendering methods such as nerf are often combined with parametric meshes to provide deformable hand models. however, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. the simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. it also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. this paper presents a pipeline for meshless 3d rendering, which we apply to the hands. by providing only a 3d skeleton, the desired appearance is extracted via a convolutional model. we do this by exploiting a nerf renderer conditioned upon an occupancy-based representation. the approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. on the benchmark interhand2.6m dataset, we achieved state-of-the-art results.",,2025-05-04,,"['maksym ivashechkin', 'oscar mendez', 'richard bowden']"
2505.02094,skillmimic-v2: learning robust and generalizable interaction skills from   sparse and noisy demonstrations,cs.lg cs.cv,"we address a fundamental challenge in reinforcement learning from interaction demonstration (rlid): demonstration noise and coverage limitations. while existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. building upon this insight, we present two data augmentation techniques: a stitched trajectory graph (stg) that discovers potential transitions between demonstration skills, and a state transition field (stf) that establishes unique connections for arbitrary states within the demonstration neighborhood. to enable effective rlid with augmented data, we develop an adaptive trajectory sampling (ats) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.",,2025-05-04,,"['runyi yu', 'yinhuai wang', 'qihan zhao', 'hok wai tsui', 'jingbo wang', 'ping tan', 'qifeng chen']"
2505.02108,signsplat: rendering sign language via gaussian splatting,cs.cv,"state-of-the-art approaches for conditional human body rendering via gaussian splatting typically focus on simple body motions captured from many views. this is often in the context of dancing or walking. however, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. the problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. the solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. we focus on how to achieve this, constraining mesh parameters to build an accurate gaussian splatting framework from few views capable of modelling subtle human motion. we leverage regularization techniques on the gaussian parameters to mitigate overfitting and rendering artifacts. additionally, we propose a new adaptive control method to densify gaussians and prune splat points on the mesh surface. to demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. on benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",,2025-05-04,,"['maksym ivashechkin', 'oscar mendez', 'richard bowden']"
2505.02109,unaligned rgb guided hyperspectral image super-resolution with   spatial-spectral concordance,cs.cv,"hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. the recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution hsis, presenting it as a favorable method. however, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. in this paper, we introduce a spatial-spectral concordance hyperspectral super-resolution (ssc-hsr) framework for unaligned reference rgb guided hsi sr to address the issues of inaccurate alignment and poor interactivity of the previous approaches. specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a two-stage image alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. to enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a feature aggregation module and an attention fusion module. in the feature aggregation module, we introduce an iterative deformable feature aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations.",,2025-05-04,,"['yingkai zhang', 'zeqiang lai', 'tao zhang', 'ying fu', 'chenghu zhou']"
2505.02134,hillie: human-in-the-loop training for low-light image enhancement,cs.cv,"developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (llie). in this paper, we propose a human-in-the-loop llie training framework that improves the visual quality of unsupervised llie model outputs through iterative training stages, named hillie. at each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. subsequently, we employ a tailored image quality assessment (iqa) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. with only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the iqa model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing llie results. extensive experiments demonstrate that our approach significantly improves unsupervised llie model performance in terms of both quantitative and qualitative performance. the code and collected ranking dataset will be available at https://github.com/labshuhanggu/hillie.",,2025-05-04,,"['xiaorui zhao', 'xinyue zhou', 'peibei cao', 'junyu lou', 'shuhang gu']"
2505.02147,local herb identification using transfer learning: a cnn-powered mobile   application for nepalese flora,cs.lg cs.cv,"herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as nepal. this study introduces a novel deep learning approach for classifying 60 different herb species using convolutional neural networks (cnns) and transfer learning techniques. using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. our research employed multiple model architectures, including densenet121, 50-layer residual network (resnet50), 16-layer visual geometry group network (vgg16), inceptionv3, efficientnetv2, and vision transformer (vit), with densenet121 ultimately demonstrating superior performance. data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. this work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.",,2025-05-04,,"['prajwal thapa', 'mridul sharma', 'jinu nyachhyon', 'yagya raj pandeya']"
2505.02148,spotting the unexpected (stu): a 3d lidar dataset for anomaly   segmentation in autonomous driving,cs.cv,"to operate safely, autonomous vehicles (avs) need to detect and handle unexpected objects or anomalies on the road. while significant research exists for anomaly detection and segmentation in 2d, research progress in 3d is underexplored. existing datasets lack high-quality multimodal data that are typically found in avs. this paper presents a novel dataset for anomaly segmentation in driving scenarios. to the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3d semantic labeling, incorporating both lidar and camera data, as well as sequential information to enable anomaly detection across various ranges. this capability is critical for the safe navigation of autonomous vehicles. we adapted and evaluated several baseline models for 3d segmentation, highlighting the challenges of 3d anomaly detection in driving environments. our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches.",,2025-05-04,,"['alexey nekrasov', 'malcolm burdorf', 'stewart worrall', 'bastian leibe', 'julie stephany berrio perez']"
2505.02159,"small clips, big gains: learning long-range refocused temporal   information for video super-resolution",cs.cv,"video super-resolution (vsr) can achieve better performance compared to single image super-resolution by additionally leveraging temporal information. in particular, the recurrent-based vsr model exploits long-range temporal information during inference and achieves superior detail restoration. however, effectively learning these long-term dependencies within long videos remains a key challenge. to address this, we propose lrti-vsr, a novel training framework for recurrent vsr that efficiently leverages long-range refocused temporal information. our framework includes a generic training strategy that utilizes temporal propagation features from long video clips while training on shorter video clips. additionally, we introduce a refocused intra&inter-frame transformer block which allows the vsr model to selectively prioritize useful temporal information through its attention module while further improving inter-frame information utilization in the ffn module. we evaluate lrti-vsr on both cnn and transformer-based vsr architectures, conducting extensive ablation studies to validate the contribution of each component. experiments on long-video test sets demonstrate that lrti-vsr achieves state-of-the-art performance while maintaining training and computational efficiency.",,2025-05-04,,"['xingyu zhou', 'wei long', 'jingbo lu', 'shiyin jiang', 'weiyi you', 'haifeng wu', 'shuhang gu']"
2505.02161,focus what matters: matchability-based reweighting for local feature   matching,cs.cv,"since the rise of transformers, many semi-dense matching methods have adopted attention mechanisms to extract feature descriptors. however, the attention weights, which capture dependencies between pixels or keypoints, are often learned from scratch. this approach can introduce redundancy and noisy interactions from irrelevant regions, as it treats all pixels or keypoints equally. drawing inspiration from keypoint selection processes, we propose to first classify all pixels into two categories: matchable and non-matchable. matchable pixels are expected to receive higher attention weights, while non-matchable ones are down-weighted. in this work, we propose a novel attention reweighting mechanism that simultaneously incorporates a learnable bias term into the attention logits and applies a matchability-informed rescaling to the input value features. the bias term, injected prior to the softmax operation, selectively adjusts attention scores based on the confidence of query-key interactions. concurrently, the feature rescaling acts post-attention by modulating the influence of each value vector in the final output. this dual design allows the attention mechanism to dynamically adjust both its internal weighting scheme and the magnitude of its output representations. extensive experiments conducted on three benchmark datasets validate the effectiveness of our method, consistently outperforming existing state-of-the-art approaches.",,2025-05-04,,['dongyue li']
2505.02175,sparsplat: fast multi-view reconstruction with generalizable 2d gaussian   splatting,cs.cv,"recovering 3d information from scenes via multi-view stereo reconstruction (mvs) and novel view synthesis (nvs) is inherently challenging, particularly in scenarios involving sparse-view setups. the advent of 3d gaussian splatting (3dgs) enabled real-time, photorealistic nvs. following this, 2d gaussian splatting (2dgs) leveraged perspective accurate 2d gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3d scene reconstruction while maintaining real-time performance. recent approaches have tackled the problem of sparse real-time nvs using 3dgs within a generalizable, mvs-based learning framework to regress 3d gaussian parameters. our work extends this line of research by addressing the challenge of generalizable sparse 3d reconstruction and nvs jointly, and manages to perform successfully at both tasks. we propose an mvs-based learning pipeline that regresses 2dgs surface element parameters in a feed-forward fashion to perform 3d shape reconstruction and nvs from sparse-view images. we further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. the resulting model attains the state-of-the-art results on the dtu sparse 3d reconstruction benchmark in terms of chamfer distance to ground-truth, as-well as state-of-the-art nvs. it also demonstrates strong generalization on the blendedmvs and tanks and temples datasets. we note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",,2025-05-04,,"['shubhendu jena', 'shishir reddy vutukur', 'adnane boukhayma']"
2505.02176,saliency-guided training for fingerprint presentation attack detection,cs.cv,"saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (pad) tasks. this paper presents its first application to fingerprint pad. we conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated ""pseudosaliency,"" including minutiae-based, image quality-based, and autoencoder-based saliency maps. evaluating on the 2021 fingerprint liveness detection competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. our findings demonstrate the effectiveness of saliency-guided training for fingerprint pad in both limited and large data contexts, and we present a configuration capable of earning the first place on the livdet-2021 benchmark. our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint pad. all collected saliency data and trained models are released with the paper to support reproducible research.",,2025-05-04,,"['samuel webster', 'adam czajka']"
2505.02178,sparfels: fast reconstruction from sparse unposed imagery,cs.cv,"we present a method for sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade gpu. while few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. differently, we propose an efficient and simple pipeline harnessing a single recent 3d foundation model. we leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2d gaussian splatting (2dgs) model, and image correspondences to guide camera optimization midst 2dgs training. key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. reducing this moment in training leads to more accurate shape reconstructions. we demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",,2025-05-04,,"['shubhendu jena', 'amine ouasfi', 'mae younes', 'adnane boukhayma']"
2505.02179,prodisc-vad: an efficient system for weakly-supervised anomaly detection   in video surveillance applications,cs.cv,"weakly-supervised video anomaly detection (ws-vad) using multiple instance learning (mil) suffers from label ambiguity, hindering discriminative feature learning. we propose prodisc-vad, an efficient framework tackling this via two synergistic components. the prototype interaction layer (pil) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. the pseudo-instance discriminative enhancement (pide) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). prodisc-vad achieves strong aucs (97.98% shanghaitech, 87.12% ucf-crime) using only 0.4m parameters, over 800x fewer than recent vit-based methods like vadclip, demonstrating exceptional efficiency alongside state-of-the-art performance. code is available at https://github.com/modadundun/prodisc-vad.",,2025-05-04,,"['tao zhu', 'qi yu', 'xinru dong', 'shiyu li', 'yue liu', 'jinlong jiang', 'lei shu']"
2505.02182,robust ai-generated face detection with imbalanced data,cs.cv,"deepfakes, created using advanced ai techniques such as variational autoencoder and generative adversarial networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. current deepfake detection techniques have evolved from cnn-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like clip, which capture global anomalies and improve cross-domain generalization. despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. to address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. the code is available at https://github.com/purdue-m2/sp_cup.",,2025-05-04,,"['yamini sri krubha', 'aryana hou', 'braden vester', 'web walker', 'xin wang', 'li lin', 'shu hu']"
2505.02192,dualreal: adaptive joint training for lossless identity-motion fusion in   video customization,cs.cv cs.ai,"customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. however, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. to address this, we introduce dualreal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. specifically, dualreal is composed of two units: (1) dual-aware adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) stageblender controller leverages the denoising stages and diffusion transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. we constructed a more comprehensive benchmark than existing methods. the experimental results show that dualreal improves clip-i and dino-i metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.",,2025-05-04,,"['wenchuan wang', 'mengqi huang', 'yijing tu', 'zhendong mao']"
2505.02211,csasn: a multitask attention-based framework for heterogeneous thyroid   carcinoma classification in ultrasound images,eess.iv cs.cv,"heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. to address this issue, we propose a novel multitask learning framework, channel-spatial attention synergy network (csasn), which integrates a dual-branch feature extractor - combining efficientnet for local spatial encoding and vit for global semantic modeling, with a cascaded channel-spatial attention refinement module. a residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as ftc and mtc carcinomas. experimental results show that csasn outperforms existing single-stream cnn or transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. this framework provides a promising strategy for ai-assisted thyroid cancer diagnosis.",,2025-05-04,,"['peiqi li', 'yincheng gao', 'renxing li', 'haojie yang', 'yunyun liu', 'boji liu', 'jiahui ni', 'ying zhang', 'yulu wu', 'xiaowei fang', 'lehang guo', 'liping sun', 'jiangang chen']"
2505.02236,improving physical object state representation in text-to-image   generative systems,cs.cv cs.ai,"current text-to-image generative models struggle to accurately represent object states (e.g., ""a table without a bottle,"" ""an empty tumbler""). in this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. next, we fine-tune several open-source text-to-image models on this synthetic data. we evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using gpt4o-mini, and achieve an average absolute improvement of 8+% across four models on the public genai-bench dataset. we also curate a collection of 200 prompts with a specific focus on common objects in various physical states. we demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. we release all evaluation prompts and code.",,2025-05-04,,"['tianle chen', 'chaitanya chakka', 'deepti ghadiyaram']"
2505.02242,quantizing diffusion models from a sampling-aware perspective,cs.cv,"diffusion models have recently emerged as the dominant approach in visual generation tasks. however, the lengthy denoising chains and the computationally intensive noise estimation networks hinder their applicability in low-latency and resource-limited environments. previous research has endeavored to address these limitations in a decoupled manner, utilizing either advanced samplers or efficient model quantization techniques. in this study, we uncover that quantization-induced noise disrupts directional estimation at each sampling step, further distorting the precise directional estimations of higher-order samplers when solving the sampling equations through discretized numerical methods, thereby altering the optimal sampling trajectory. to attain dual acceleration with high fidelity, we propose a sampling-aware quantization strategy, wherein a mixed-order trajectory alignment technique is devised to impose a more stringent constraint on the error bounds at each sampling step, facilitating a more linear probability flow. extensive experiments on sparse-step fast sampling across multiple datasets demonstrate that our approach preserves the rapid convergence characteristics of high-speed samplers while maintaining superior generation quality. code will be made publicly available soon.",,2025-05-04,,"['qian zeng', 'jie song', 'yuanyu wan', 'huiqiong wang', 'mingli song']"
2505.02246,cricket: a self-powered chirping pixel,cs.cv,"we present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. our sensor, called cricket, harvests energy from incident light. it is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. the carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. we have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. we have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. we show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. we also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.",10.1145/3658196,2025-05-04,,"['shree k. nayar', 'jeremy klotz', 'nikhil nanda', 'mikhail fridberg']"
2505.02278,compositional image-text matching and retrieval by grounding entities,cs.cv,"vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current vision-language models. while with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. however, a notable weakness of pretrained models like clip, is their inability to perform entity grounding and compositional image and text matching~\cite{jiang2024comclip, yang2023amc, rajabi2023groundedvsr, learninglocalizecvpr24}. in this work we propose a novel learning-free zero-shot augmentation of clip embeddings that has favorable compositional properties. we compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % the final embedding is obtained by computing a weighted combination of the sub-image embeddings. the resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\% improvement in image-text matching accuracy on the visual genome and svo probes datasets~\cite{krishna2017visualgenome, svo}. notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the flickr30k and ms-coco retrieval benchmarks~\cite{flickr30ke, mscoco}, improving the state-of-the-art recall@1 by 12\% and 0.4\%, respectively. our code is available at https://github.com/madhukarreddyvongala/groundingclip.",,2025-05-04,,"['madhukar reddy vongala', 'saurabh srivastava', 'jana košecká']"
2505.02287,continuous normalizing flows for uncertainty-aware human pose estimation,cs.cv,"human pose estimation (hpe) is increasingly important for applications like virtual reality and motion analysis, yet current methods struggle with balancing accuracy, computational efficiency, and reliable uncertainty quantification (uq). traditional regression-based methods assume fixed distributions, which might lead to poor uq. heatmap-based methods effectively model the output distribution using likelihood heatmaps, however, they demand significant resources. to address this, we propose continuous flow residual estimation (cfre), an integration of continuous normalizing flows (cnfs) into regression-based models, which allows for dynamic distribution adaptation. through extensive experiments, we show that cfre leads to better accuracy and uncertainty quantification with retained computational efficiency on both 2d and 3d human pose estimation tasks.",,2025-05-04,,"['shipeng liu', 'ziliang xiong', 'bastian wandt', 'per-erik forssén']"
2505.02304,generative sign-description prompts with multi-positive contrastive   learning for sign language recognition,cs.cl cs.cv,"sign language recognition (slr) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. to the best of our knowledge, this is the first work to integrate generative large language models (llms) into slr tasks. we propose a novel generative sign-description prompts multi-positive contrastive learning (gsp-mc) method that leverages retrieval-augmented generation (rag) with domain-specific llms, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. the gsp-mc method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. our approach combines global and part-level losses, optimizing kl divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. experiments demonstrate state-of-the-art performance against existing methods on the chinese slr500 (reaching 97.1%) and turkish autsl datasets (97.07% accuracy). the method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.",,2025-05-04,,"['siyu liang', 'yunan li', 'wentian xin', 'huizhou chen', 'xujie liu', 'kang liu', 'qiguang miao']"
2505.02325,teda: boosting vision-lanuage models for zero-shot 3d object retrieval   via testing-time distribution alignment,cs.cv,"learning discriminative 3d representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3d applications. existing well-established methods often struggle to attain this goal due to insufficient 3d training data from broader concepts. meanwhile, pre-trained large vision-language models (e.g., clip) have shown remarkable zero-shot generalization capabilities. yet, they are limited in extracting suitable 3d representations due to substantial gaps between their 2d training and 3d testing distributions. to address these challenges, we propose testing-time distribution alignment (teda), a novel framework that adapts a pretrained 2d vision-language model clip for unknown 3d object retrieval at test time. to our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3d feature learning. teda projects 3d objects into multi-view images, extracts features using clip, and refines 3d query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. additionally, teda integrates textual descriptions generated by a multimodal language model (internvl) to enhance 3d object understanding, leveraging clip's aligned feature space to fuse visual and textual cues. extensive experiments on four open-set 3d object retrieval benchmarks demonstrate that teda greatly outperforms state-of-the-art methods, even those requiring extensive training. we also experimented with depth maps on objaverse-lvis, further validating its effectiveness. code is available at https://github.com/wangzhichuan123/teda.",,2025-05-04,,"['zhichuan wang', 'yang zhou', 'jinhai xiang', 'yulong wang', 'xinwei he']"
2505.02331,vaemo: efficient representation learning for visual-audio emotion with   knowledge injection,cs.cv cs.sd,"audiovisual emotion recognition (aver) aims to infer human emotions from nonverbal visual-audio (va) cues, offering modality-complementary and language-agnostic advantages. however, aver remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. recent self-supervised aver approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. to address these issues, we propose vaemo, an efficient two-stage framework for emotion-centric joint va representation learning with external knowledge injection. in stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric va corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. in stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of va samples; these rich textual semantics are then injected by aligning their corresponding embeddings with va representations through dual-path contrastive learning, further bridging the emotion gap. extensive experiments on multiple downstream aver benchmarks show that vaemo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable va emotion representations.",,2025-05-04,,"['hao cheng', 'zhiwei zhao', 'yichao he', 'zhenzhen hu', 'jia li', 'meng wang', 'richang hong']"
2505.02335,6d pose estimation on spoons and hands,cs.cv,"accurate dietary monitoring is essential for promoting healthier eating habits. a key area of research is how people interact and consume food using utensils and hands. by tracking their position and orientation, it is possible to estimate the volume of food being consumed, or monitor eating behaviours, highly useful insights into nutritional intake that can be more reliable than popular methods such as self-reporting. hence, this paper implements a system that analyzes stationary video feed of people eating, using 6d pose estimation to track hand and spoon movements to capture spatial position and orientation. in doing so, we examine the performance of two state-of-the-art (sota) video object segmentation (vos) models, both quantitatively and qualitatively, and identify main sources of error within the system.",,2025-05-04,,"['kevin tan', 'fan yang', 'yuhao chen']"
2505.02364,quaternion infrared visible image fusion,cs.cv,"visible images provide rich details and color information only under well-lighted conditions while infrared images effectively highlight thermal targets under challenging conditions such as low visibility and adverse weather. infrared-visible image fusion aims to integrate complementary information from infrared and visible images to generate a high-quality fused image. existing methods exhibit critical limitations such as neglecting color structure information in visible images and performance degradation when processing low-quality color-visible inputs. to address these issues, we propose a quaternion infrared-visible image fusion (qivif) framework to generate high-quality fused images completely in the quaternion domain. qivif proposes a quaternion low-visibility feature learning model to adaptively extract salient thermal targets and fine-grained texture details from input infrared and visible images respectively under diverse degraded conditions. qivif then develops a quaternion adaptive unsharp masking method to adaptively improve high-frequency feature enhancement with balanced illumination. qivif further proposes a quaternion hierarchical bayesian fusion model to integrate infrared saliency and enhanced visible details to obtain high-quality fused images. extensive experiments across diverse datasets demonstrate that our qivif surpasses state-of-the-art methods under challenging low-visibility conditions.",,2025-05-05,,"['weihua yang', 'yicong zhou']"
2505.02365,quaternion multi-focus color image fusion,cs.cv,"multi-focus color image fusion refers to integrating multiple partially focused color images to create a single all-in-focus color image. however, existing methods struggle with complex real-world scenarios due to limitations in handling color information and intricate textures. to address these challenges, this paper proposes a quaternion multi-focus color image fusion framework to perform high-quality color image fusion completely in the quaternion domain. this framework introduces 1) a quaternion sparse decomposition model to jointly learn fine-scale image details and structure information of color images in an iterative fashion for high-precision focus detection, 2) a quaternion base-detail fusion strategy to individually fuse base-scale and detail-scale results across multiple color images for preserving structure and detail information, and 3) a quaternion structural similarity refinement strategy to adaptively select optimal patches from initial fusion results and obtain the final fused result for preserving fine details and ensuring spatially consistent outputs. extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods.",,2025-05-05,,"['weihua yang', 'yicong zhou']"
2505.02369,sharpness-aware minimization with z-score gradient filtering for neural   networks,cs.lg cs.ai cs.cv cs.it cs.ne math.it,"sharpness-aware minimization (sam) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. we introduce zsharp, a refined sharpness-aware optimization method that incorporates layer-wise z-score normalization followed by percentile-based filtering. this process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. zsharp retains the standard two-phase sam structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. we evaluate zsharp on cifar-10, cifar-100, and tiny-imagenet using a range of models including resnet, vgg, and vision transformers. across all architectures and datasets, zsharp consistently achieves higher test accuracy compared to sam, asam, and friendly-sam. these results indicate that z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.",,2025-05-05,2025-05-07,['juyoung yun']
2505.02370,superedit: rectifying and facilitating supervision for instruction-based   image editing,cs.cv cs.ai cs.lg,"due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (vlms) but fail to resolve this fundamental issue. in this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. this includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. based on these prior attributes, we define a unified guide for vlms to rectify editing instructions. however, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. to this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. our method does not require the vlm modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. compared with previous sota smartedit, we achieve 9.19% improvements on the real-edit benchmark with 30x less training data and 13x smaller model size.",,2025-05-05,,"['ming li', 'xin gu', 'fan chen', 'xiaoying xing', 'longyin wen', 'chen chen', 'sijie zhu']"
2505.02385,an arbitrary-modal fusion network for volumetric cranial nerves tract   segmentation,eess.iv cs.cv,"the segmentation of cranial nerves (cns) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual cns. multimodal cns tract segmentation networks, e.g., cntseg, which combine structural magnetic resonance imaging (mri) and diffusion mri, have achieved promising segmentation performance. however, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. in this work, we propose a novel arbitrary-modal fusion network for volumetric cns tract segmentation, called cntseg-v2, which trains one model to handle different combinations of available modalities. instead of directly combining all the modalities, we select t1-weighted (t1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. our model encompasses an arbitrary-modal collaboration module (acm) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of t1w images. meanwhile, we construct a deep distance-guided multi-stage (ddm) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. we evaluate our cntseg-v2 on the human connectome project (hcp) dataset and the clinical multi-shell diffusion mri (mdm) dataset. extensive experimental results show that our cntseg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods.",,2025-05-05,,"['lei xie', 'huajun zhou', 'junxiong huang', 'jiahao huang', 'qingrun zeng', 'jianzhong he', 'jiawei zhang', 'baohua fan', 'mingchu li', 'guoqiang xie', 'hao chen', 'yuanjing feng']"
2505.02388,metascenes: towards automated replica creation for real-world 3d scans,cs.cv cs.ai cs.lg cs.ro,"embodied ai (eai) research requires high-quality, diverse 3d scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. achieving these quality standards, however, necessitates the precise replication of real-world object diversity. existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. to scalably produce realistic and interactive 3d scenes, we first present metascenes, a large-scale, simulatable 3d scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. then, we introduce scan2sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3d scenes. we further propose two benchmarks to evaluate metascenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (vln) to validate cross-domain transfer. results confirm metascene's potential to enhance eai by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for eai research. project website: https://meta-scenes.github.io/.",,2025-05-05,,"['huangyue yu', 'baoxiong jia', 'yixin chen', 'yandan yang', 'puhao li', 'rongpeng su', 'jiaxin li', 'qing li', 'wei liang', 'song-chun zhu', 'tengyu liu', 'siyuan huang']"
2505.02393,uncertainty-weighted image-event multimodal fusion for video anomaly   detection,cs.cv,"most existing video anomaly detectors rely solely on rgb frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. to address this limitation, we propose image-event fusion for video anomaly detection (ief-vad), a framework that synthesizes event representations directly from rgb videos and fuses them with image features through a principled, uncertainty-aware process. the system (i) models heavy-tailed sensor noise with a student`s-t likelihood, deriving value-level inverse-variance weights via a laplace approximation; (ii) applies kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. without any dedicated event sensor or frame-level labels, ief-vad sets a new state of the art across multiple real-world anomaly detection benchmarks. these findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in rgb frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. code and models are available at https://github.com/eavnjeong/ief-vad.",,2025-05-05,2025-05-08,"['sungheon jeong', 'jihong park', 'mohsen imani']"
2505.02396,diagnostic uncertainty in pneumonia detection using cnn mobilenetv2 and   cnn from scratch,eess.iv cs.ai cs.cv,"pneumonia diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. this uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest x-rays, and the presence of co-existing respiratory conditions. this research proposes one of the supervised learning methods, cnn. using mobilenetv2 as the pre-trained one with resnet101v2 architecture and using keras api as the built from scratch model, for identifying lung diseases especially pneumonia. the datasets used in this research were obtained from the website through kaggle. the result shows that by implementing cnn mobilenetv2 and cnn from scratch the result is promising. while validating data, mobilenetv2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. nonetheless, mobilenetv2 is more stable. although it takes more time to train each epoch. meanwhile, after the 10th epoch, the scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. with these results, resnet101v2 offers stability, and the scratch model offers high accuracy.",,2025-05-05,,"['kennard norbert sudiardjo', 'islam nur alam', 'wilson wijaya', 'lili ayu wulandhari']"
2505.02405,estimating commonsense scene composition on belief scene graphs,cs.ro cs.cv,"this work establishes the concept of commonsense scene composition, with a focus on extending belief scene graphs by estimating the spatial distribution of unseen objects. specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. the proposed framework includes two variants of a correlation information (ceci) model for learning probability distributions: (i) a baseline approach based on a graph convolutional network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on large language models (llms). furthermore, this article provides a detailed description of the dataset generation process for such tasks. finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.",,2025-05-05,,"['mario a. v. saucedo', 'vignesh kottayam viswanathan', 'christoforos kanellakis', 'george nikolakopoulos']"
2505.02406,token coordinated prompt attention is needed for visual prompting,cs.cv,"visual prompting techniques are widely used to efficiently fine-tune pretrained vision transformers (vit) by learning a small set of shared prompts for all tokens. however, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of vit. this often leads to indistinguishable and biased prompt-extracted features, hindering performance. to address this issue, we propose a plug-and-play token coordinated prompt attention (tcpa) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. firstly, recognizing the distinct functions of cls and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into cls prompts and image prompts, which interact exclusively with cls tokens and image tokens through attention mechanisms. this enhances their respective discriminative abilities. furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. this enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. extensive experiments across various benchmarks demonstrate that tcpa significantly enhances the diversity and discriminative power of the extracted features. the code is available at https://github.com/zhoujiahuan1991/icml2025-tcpa.",,2025-05-05,2025-05-06,"['zichen liu', 'xu zou', 'gang hua', 'jiahuan zhou']"
2505.02448,recent advances in out-of-distribution detection with clip-like models:   a survey,cs.cv,"out-of-distribution detection (ood) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (id) data during testing. recent advances in ai, particularly vision-language models (vlms) like clip, have revolutionized ood detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. this shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of id images, adhering to a unimodal paradigm. to better align with clip's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. specifically, we categorize existing methods based on how visual and textual information of ood data is utilized within image + text modalities, and further divide them into four groups: ood images (i.e., outliers) seen or unseen, and ood texts (i.e., learnable vectors or class names) known or unknown, across two training strategies (i.e., train-free or training-required). more importantly, we discuss open problems in clip-like ood detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.",,2025-05-05,,"['chaohua li', 'enhao zhang', 'chuanxing geng', 'songcan chen']"
2505.02467,timing is everything: finding the optimal fusion points in multimodal   medical imaging,cs.cv cs.ai,"multimodal deep learning harnesses diverse imaging modalities, such as mri sequences, to enhance diagnostic accuracy in medical imaging. a key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. we propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. at each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. this process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. the approach is validated on two multimodal mri datasets, each addressing different classification tasks. our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. these architectures demonstrated superior accuracy, f-score, and specificity while maintaining competitive or improved auc values. furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. by systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. it provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical ai applications.",,2025-05-05,,"['valerio guarrasi', 'klara mogensen', 'sara tassinari', 'sara qvarlander', 'paolo soda']"
2505.02471,ming-lite-uni: advancements in unified architecture for natural   multimodal interaction,cs.cv,"we introduce ming-lite-uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. specifically, this project provides an open-source implementation of the integrated metaqueries and m2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. by leveraging a fixed mllm and a learnable diffusion model, ming-lite-uni enables native multimodal ar models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. our experimental results demonstrate the strong performance of ming-lite-uni and illustrate the impressive fluid nature of its interactive process. all code and model weights are open-sourced to foster further exploration within the community. notably, this work aligns with concurrent multimodal ai milestones - such as chatgpt-4o with native image generation updated in march 25, 2025 - underscoring the broader significance of unified models like ming-lite-uni on the path toward agi. ming-lite-uni is in alpha stage and will soon be further refined.",,2025-05-05,2025-05-07,"['inclusion ai', 'biao gong', 'cheng zou', 'dandan zheng', 'hu yu', 'jingdong chen', 'jianxin sun', 'junbo zhao', 'jun zhou', 'kaixiang ji', 'lixiang ru', 'libin wang', 'qingpei guo', 'rui liu', 'weilong chai', 'xinyu xiao', 'ziyuan huang']"
2505.02476,point cloud recombination: systematic real data augmentation using   robotic targets for lidar perception validation,cs.ro cs.cv eess.iv,"the validation of lidar-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. in contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. however, these methods do not consider validation and remain limited in controllability because they rely on empirical data. we solve these limitations by proposing point cloud recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3d meshes. using the ouster os1-128 rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. we show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. by providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.",,2025-05-05,,"['hubert padusinski', 'christian steinhauser', 'christian scherl', 'julian gaal', 'jacob langner']"
2505.02481,finger pose estimation for under-screen fingerprint sensor,cs.cv,"two-dimensional pose estimation plays a crucial role in fingerprint recognition by facilitating global alignment and reduce pose-induced variations. however, existing methods are still unsatisfactory when handling with large angle or small area inputs. these limitations are particularly pronounced on fingerprints captured by under-screen fingerprint sensors in smartphones. in this paper, we present a novel dual-modal input based network for under-screen fingerprint pose estimation. our approach effectively integrates two distinct yet complementary modalities: texture details extracted from ridge patches through the under-screen fingerprint sensor, and rough contours derived from capacitive images obtained via the touch screen. this collaborative integration endows our network with more comprehensive and discriminative information, substantially improving the accuracy and stability of pose estimation. a decoupled probability distribution prediction task is designed, instead of the traditional supervised forms of numerical regression or heatmap voting, to facilitate the training process. additionally, we incorporate a mixture of experts (moe) based feature fusion mechanism and a relationship driven cross-domain knowledge transfer strategy to further strengthen feature extraction and fusion capabilities. extensive experiments are conducted on several public datasets and two private datasets. the results indicate that our method is significantly superior to previous state-of-the-art (sota) methods and remarkably boosts the recognition ability of fingerprint recognition algorithms. our code is available at https://github.com/xiongjunguan/draco.",,2025-05-05,,"['xiongjun guan', 'zhiyu pan', 'jianjiang feng', 'jie zhou']"
2505.02501,corr2distrib: making ambiguous correspondences an ally to predict   reliable 6d pose distributions,cs.cv cs.ai cs.lg cs.ro,"we introduce corr2distrib, the first correspondence-based method which estimates a 6d camera pose distribution from an rgb image, explaining the observations. indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. while a few recent methods tackle this problem, they do not rely on local correspondences which, according to the bop challenge, are currently the most effective way to estimate a single 6dof pose solution. using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of pnp. with corr2distrib, we turn these ambiguities into an advantage to recover all valid poses. corr2distrib first learns a symmetry-aware representation for each 3d point on the object's surface, characterized by a descriptor and a local frame. this representation enables the generation of 3dof rotation hypotheses from single 2d-3d correspondences. next, we refine these hypotheses into a 6dof pose distribution using pnp and pose scoring. our experimental evaluations on complex non-synthetic scenes show that corr2distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an rgb image, demonstrating the potential of correspondences-based approaches.",,2025-05-05,,"['asma brazi', 'boris meden', 'fabrice mayran de chamisso', 'steve bourgeois', 'vincent lepetit']"
2505.02527,text to image generation and editing: a survey,cs.cv,"text-to-image generation (t2i) refers to the text-guided generation of high-quality images. in the past few years, t2i has attracted widespread attention and numerous works have emerged. in this survey, we comprehensively review 141 works conducted from 2021 to 2024. first, we introduce four foundation model architectures of t2i (autoregression, non-autoregression, gan and diffusion) and the commonly used key technologies (autoencoder, attention and classifier-free guidance). secondly, we systematically compare the methods of these studies in two directions, t2i generation and t2i editing, including the encoders and the key technologies they use. in addition, we also compare the performance of these researches side by side in terms of datasets, evaluation metrics, training resources, and inference speed. in addition to the four foundation models, we survey other works on t2i, such as energy-based models and recent mamba and multimodality. we also investigate the potential social impact of t2i and provide some solutions. finally, we propose unique insights of improving the performance of t2i models and possible future development directions. in summary, this survey is the first systematic and comprehensive overview of t2i, aiming to provide a valuable guide for future researchers and stimulate continued progress in this field.",,2025-05-05,,"['pengfei yang', 'ngai-man cheung', 'xinda ma']"
2505.02529,robsurv: vector quantization-based multi-modal learning for robust   cancer survival prediction,eess.iv cs.cv,"cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. current approaches struggle to extract consistent features from heterogeneous ct and pet images, limiting their clinical applicability. we address these challenges by introducing robsurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. the key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. this dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via transformer-based processing. in extensive evaluations across three diverse datasets (hecktor, h\&n1, and nsclc radiogenomics), robsurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\% compared to 8-12\% in baseline methods. these results, combined with strong generalization across different cancer types and imaging protocols, establish robsurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.",,2025-05-05,,"['aiman farooq', 'azad singh', 'deepak mishra', 'santanu chaudhury']"
2505.02549,robust duality learning for unsupervised visible-infrared person   re-identification,cs.cv cs.mm,"unsupervised visible-infrared person re-identification (uvi-reid) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. in practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. to address this, we introduce a new learning paradigm that explicitly considers pseudo-label noise (pln), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. to this end, we propose a novel robust duality learning framework (rode) for uvi-reid to mitigate the effects of noisy pseudo-labels. first, to combat noise overfitting, a robust adaptive learning mechanism (ral) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. second, to alleviate error accumulation-where the model reinforces its own mistakes-rode employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. however, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. to resolve this, we introduce cluster consistency matching (ccm), which aligns clusters across models and modalities by measuring cross-cluster similarity. extensive experiments on three benchmarks demonstrate the effectiveness of rode.",10.1109/tifs.2025.3536613,2025-05-05,2025-05-06,"['yongxiang li', 'yuan sun', 'yang qin', 'dezhong peng', 'xi peng', 'peng hu']"
2505.02593,delta: dense depth from events and lidar using transformer's attention,cs.cv,"event cameras and lidars provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. to this day, few works have explored the combination of these two modalities. in this article, we propose a novel neural-network-based method for fusing event and lidar data in order to estimate dense depth maps. our architecture, delta, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and lidar data. following a thorough evaluation, we demonstrate that delta sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous sota.",,2025-05-05,,"['vincent brebion', 'julien moreau', 'franck davoine']"
2505.02626,"detect, classify, act: categorizing industrial anomalies with   multi-modal large language models",cs.cv,"recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. however, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. to address this gap, we propose velm, a novel llm-based pipeline for anomaly classification. given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. if an anomaly is detected, the llm then classifies its type. a key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. to address this limitation, we introduce mvtec-ac and visa-ac, refined versions of the widely used mvtec-ad and visa datasets, which include accurate anomaly class labels for rigorous evaluation. our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on mvtec-ad, exceeding the prior baselines by 5%, and 84% on mvtec-ac, demonstrating the effectiveness of velm in understanding and categorizing anomalies. we hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.",,2025-05-05,,"['sassan mokhtar', 'arian mousakhan', 'silvio galesso', 'jawad tayyub', 'thomas brox']"
2505.02628,deepsparse: a foundation model for sparse-view cbct reconstruction,eess.iv cs.cv,"cone-beam computed tomography (cbct) is a critical 3d imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. sparse-view reconstruction reduces radiation by using fewer x-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. to overcome these limitations, we propose deepsparse, the first foundation model for sparse-view cbct reconstruction, featuring dice (dual-dimensional cross-scale embedding), a novel network that integrates multi-view 2d features and multi-scale 3d features. additionally, we introduce the hyvip (hybrid view sampling pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. extensive experiments and ablation studies demonstrate that our proposed deepsparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient cbct imaging.",,2025-05-05,,"['yiqun lin', 'hualiang wang', 'jixiang chen', 'jiewen yang', 'jiarong guo', 'xiaomeng li']"
2505.02648,mccd: multi-agent collaboration-based compositional diffusion for   complex text-to-image generation,cs.cv,"diffusion models have shown excellent performance in text-to-image generation. nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. therefore, we propose a multi-agent collaboration-based compositional diffusion (mccd) for text-to-image generation for complex scenes. specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing mllms to extract various scene elements effectively. in addition, hierarchical compositional diffusion utilizes a gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. comprehensive experiments demonstrate that our mccd significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.",,2025-05-05,2025-05-06,"['mingcheng li', 'xiaolu hou', 'ziyang liu', 'dingkang yang', 'ziyun qian', 'jiawei chen', 'jinjie wei', 'yue jiang', 'qingyao xu', 'lihua zhang']"
2505.02654,sim2real in endoscopy segmentation with a novel structure aware image   translation,cs.cv,"automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. however, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. while ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. the main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. our approach produces realistic images in different endoscopy scenarios. we demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. in particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. we run experiments both on a novel simulated dataset for fold segmentation, and real data from the endomapper (em) dataset. all our new generated data and new em metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation.",10.1007/978-3-031-73281-2_9,2025-05-05,,"['clara tomasini', 'luis riazuelo', 'ana c. murillo']"
2505.02664,grasp the graph (gtg) 2.0: ensemble of gnns for high-precision grasp   pose detection in clutter,cs.ro cs.cv cs.lg,"grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. this paper introduces grasp the graph 2.0 (gtg 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of graph neural networks for efficient geometric reasoning from point cloud data. building on the success of gtg 1.0, which demonstrated the potential of graph neural networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-dof grasping, gtg 2.0 employs a conventional grasp pose generator to efficiently produce 7-dof grasp candidates. candidates are assessed with an ensemble graph neural network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). this improved representation boosts grasp detection performance over previous methods using the same generator. gtg 2.0 shows up to a 35% improvement in average precision on the graspnet-1billion benchmark compared to hypothesis-and-test and graph neural network-based methods, ranking it among the top three frameworks. experiments with a 3-dof delta parallel robot and kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.",,2025-05-05,,"['ali rashidi moghadam', 'sayedmohammadreza rastegari', 'mehdi tale masouleh', 'ahmad kalhor']"
2505.02677,multimodal deep learning for stroke prediction and detection using   retinal imaging and clinical data,eess.iv cs.cv,"stroke is a major public health problem, affecting millions worldwide. deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. however, existing methods rely on costly medical imaging modalities, such as computed tomography. recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. we propose a multimodal deep neural network that processes optical coherence tomography (oct) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. we pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. the experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\% auroc improvement as compared to the unimodal image-only baseline, and $8$\% improvement compared to an existing state-of-the-art foundation model. in conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.",,2025-05-05,,"['saeed shurrab', 'aadim nepal', 'terrence j. lee-st. john', 'nicola g. ghazi', 'bartlomiej piechowski-jozwiak', 'farah e. shamout']"
2505.02690,dance of fireworks: an interactive broadcast gymnastics training system   based on pose estimation,cs.cv,"this study introduces dance of fireworks, an interactive system designed to combat sedentary health risks by enhancing engagement in radio calisthenics. leveraging mobile device cameras and lightweight pose estimation (posenet/tensorflow lite), the system extracts body keypoints, computes joint angles, and compares them with standardized motions to deliver real-time corrective feedback. to incentivize participation, it dynamically maps users' movements (such as joint angles and velocity) to customizable fireworks animations, rewarding improved accuracy with richer visual effects. experiments involving 136 participants demonstrated a significant reduction in average joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four sessions, with 93.4 percent of users affirming its exercise-promoting efficacy and 85.4 percent praising its entertainment value. the system operates without predefined motion templates or specialised hardware, enabling seamless integration into office environments. future enhancements will focus on improving pose recognition accuracy, reducing latency, and adding features such as multiplayer interaction and music synchronisation. this work presents a cost-effective, engaging solution to promote physical activity in sedentary populations.",,2025-05-05,,"['haotian chen', 'ziyu liu', 'xi cheng', 'chuangqi li']"
2505.02703,structure causal models and llms integration in medical visual question   answering,cs.cv,"medical visual question answering (medvqa) aims to answer medical questions according to medical images. however, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. such cross-modal bias makes it challenging to infer medically meaningful answers. in this work, we propose a causal inference framework for the medvqa task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (qa) session. we are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. during optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. in addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. extensive experiments on three medvqa datasets demonstrate that 1) our method significantly improves the accuracy of medvqa, and 2) our method achieves true causal correlations in the face of complex medical data.",10.1109/tmi.2025.3564320,2025-05-05,,"['zibo xu', 'qiang li', 'weizhi nie', 'weijie wang', 'anan liu']"
2505.02704,vgld: visually-guided linguistic disambiguation for monocular depth   scale recovery,cs.cv,"we propose a robust method for monocular depth scale recovery. monocular depth estimation can be divided into two main directions: (1) relative depth estimation, which provides normalized or inverse depth without scale information, and (2) metric depth estimation, which involves recovering depth with absolute scale. to obtain absolute scale information for practical downstream tasks, utilizing textual information to recover the scale of a relative depth map is a highly promising approach. however, since a single image can have multiple descriptions from different perspectives or with varying styles, it has been shown that different textual descriptions can significantly affect the scale recovery process. to address this issue, our method, vgld, stabilizes the influence of textual information by incorporating high-level semantic information from the corresponding image alongside the textual description. this approach resolves textual ambiguities and robustly outputs a set of linear transformation parameters (scalars) that can be globally applied to the relative depth map, ultimately generating depth predictions with metric-scale accuracy. we validate our method across several popular relative depth models(midas, depthanything), using both indoor scenes (nyuv2) and outdoor scenes (kitti). our results demonstrate that vgld functions as a universal alignment module when trained on multiple datasets, achieving strong performance even in zero-shot scenarios. code is available at: https://github.com/pakinwu/vgld.",,2025-05-05,2025-05-05,"['bojin wu', 'jing chen']"
2505.02705,multi-view learning with context-guided receptance for image denoising,eess.iv cs.cv,"image denoising is essential in low-level vision applications such as photography and automated driving. existing methods struggle with distinguishing complex noise patterns in real-world scenes and consume significant computational resources due to reliance on transformer-based models. in this work, the context-guided receptance weighted key-value (\m) model is proposed, combining enhanced multi-view feature integration with efficient sequence modeling. our approach introduces the context-guided token shift (cts) paradigm, which effectively captures local spatial dependencies and enhance the model's ability to model real-world noise distributions. additionally, the frequency mix (fmix) module extracting frequency-domain features is designed to isolate noise in high-frequency spectra, and is integrated with spatial representations through a multi-view learning process. to improve computational efficiency, the bidirectional wkv (biwkv) mechanism is adopted, enabling full pixel-sequence interaction with linear complexity while overcoming the causal selection constraints. the model is validated on multiple real-world image denoising datasets, outperforming the existing state-of-the-art methods quantitatively and reducing inference time up to 40\%. qualitative results further demonstrate the ability of our model to restore fine details in various scenes.",,2025-05-05,,"['binghong chen', 'tingting chai', 'wei jiang', 'yuanrong xu', 'guanglu zhou', 'xiangqian wu']"
2505.02720,a rate-quality model for learned video coding,cs.cv,"learned video coding (lvc) has recently achieved superior coding performance. in this paper, we model the rate-quality (r-q) relationship for learned video coding by a parametric function. we learn a neural network, termed rqnet, to characterize the relationship between the bitrate and quality level according to video content and coding context. the predicted (r,q) results are further integrated with those from previously coded frames using the least-squares method to determine the parameters of our r-q model on-the-fly. compared to the conventional approaches, our method accurately estimates the r-q relationship, enabling the online adaptation of model parameters to enhance both flexibility and precision. experimental results show that our r-q model achieves significantly smaller bitrate deviations than the baseline method on commonly used datasets with minimal additional complexity.",,2025-05-05,,"['sang nguyenquang', 'cheng-wei chen', 'xiem hoangvan', 'wen-hsiao peng']"
2505.02746,using knowledge graphs to harvest datasets for efficient clip model   training,cs.cv cs.cl cs.ir cs.lg,"training high-quality clip models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest clip models do not cover well -- and drives up training costs. this poses challenges for scientific research that needs fine-grained control over the training procedure of clip models. in this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust clip model can be trained from scratch with considerably less data. specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10m images. moreover, we introduce entitynet, a dataset comprising 33m images paired with 46m text descriptions, which enables the training of a generic clip model in significantly reduced time.",,2025-05-05,,"['simon ging', 'sebastian walter', 'jelena bratulić', 'johannes dienert', 'hannah bast', 'thomas brox']"
2505.02751,platelet enumeration in dense aggregates,eess.iv cs.cv,"identifying and counting blood components such as red blood cells, various types of white blood cells, and platelets is a critical task for healthcare practitioners. deep learning approaches, particularly convolutional neural networks (cnns) using supervised learning strategies, have shown considerable success for such tasks. however, cnn based architectures such as u-net, often struggles to accurately identify platelets due to their sizes and high variability of features. to address these challenges, researchers have commonly employed strategies such as class weighted loss functions, which have demonstrated some success. however, this does not address the more significant challenge of platelet variability in size and tendency to form aggregates and associations with other blood components. in this study, we explored an alternative approach by investigating the role of convolutional kernels in mitigating these issues. we also assigned separate classes to singular platelets and platelet aggregates and performed semantic segmentation using various u-net architectures for identifying platelets. we then evaluated and compared two common methods (pixel area method and connected component analysis) for counting platelets and proposed an alternative approach specialized for single platelets and platelet aggregates. our experiments provided results that showed significant improvements in the identification of platelets, highlighting the importance of optimizing convolutional operations and class designations. we show that the common practice of pixel area-based counting often over estimate platelet counts, whereas the proposed method presented in this work offers significant improvements. we discuss in detail about these methods from segmentation masks.",,2025-05-05,,"['h. martin gillis', 'yogeshwar shendye', 'paul hollensen', 'alan fine', 'thomas trappenberg']"
2505.02753,advancing generalizable tumor segmentation with anomaly-aware   open-vocabulary attention maps and frozen foundation diffusion models,cs.cv,"we explore generalizable tumor segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. in this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named diffugts. diffugts creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. to further improve and refine anomaly segmentation masks, diffugts leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. codes are available at https://github.com/yankai96/diffugts.",,2025-05-05,,"['yankai jiang', 'peng zhang', 'donglin yang', 'yuan tian', 'hai lin', 'xiaosong wang']"
2505.02779,unsupervised deep learning-based keypoint localization estimating   descriptor matching performance,cs.cv,"retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.   in this work, we present a novel unsupervised registration pipeline that entirely eliminates the need for labeled data. our approach is based on the principle that locations with distinctive descriptors constitute reliable keypoints. this fully inverts the conventional state-of-the-art approach, conditioning the detector on the descriptor rather than the opposite.   first, we propose an innovative descriptor learning method that operates without keypoint detection or any labels, generating descriptors for arbitrary locations in retinal images. next, we introduce a novel, label-free keypoint detector network which works by estimating descriptor performance directly from the input image.   we validate our method through a comprehensive evaluation on four hold-out datasets, demonstrating that our unsupervised descriptor outperforms state-of-the-art supervised descriptors and that our unsupervised detector significantly outperforms existing unsupervised detection methods. finally, our full registration pipeline achieves performance comparable to the leading supervised methods, while not employing any labeled data. additionally, the label-free nature and design of our method enable direct adaptation to other domains and modalities.",,2025-05-05,,"['david rivas-villar', 'álvaro s. hervella', 'josé rouco', 'jorge novo']"
2505.02784,advances in automated fetal brain mri segmentation and biometry:   insights from the feta 2024 challenge,cs.cv,"accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. the feta challenge 2024 advanced automated fetal brain mri analysis by introducing biometry prediction as a new task alongside tissue segmentation. for the first time, our diverse multi-centric test set included data from a new low-field (0.55t) mri dataset. evaluation metrics were also expanded to include the topology-specific euler characteristic difference (ed). sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. however, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. the ed metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. overall, feta 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain mri, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable ai tools.",,2025-05-05,2025-05-08,"['vladyslav zalevskyi', 'thomas sanchez', 'misha kaandorp', 'margaux roulet', 'diego fajardo-rojas', 'liu li', 'jana hutter', 'hongwei bran li', 'matthew barkovich', 'hui ji', 'luca wilhelmi', 'aline dändliker', 'céline steger', 'mériam koob', 'yvan gomez', 'anton jakovčić', 'melita klaić', 'ana adžić', 'pavel marković', 'gracia grabarić', 'milan rados', 'jordina aviles verdera', 'gregor kasprian', 'gregor dovjak', 'raphael gaubert-rachmühl', 'maurice aschwanden', 'qi zeng', 'davood karimi', 'denis peruzzo', 'tommaso ciceri', 'giorgio longari', 'rachika e. hamadache', 'amina bouzid', 'xavier lladó', 'simone chiarella', 'gerard martí-juan', 'miguel ángel gonzález ballester', 'marco castellaro', 'marco pinamonti', 'valentina visani', 'robin cremese', 'keïn sam', 'fleur gaudfernau', 'param ahir', 'mehul parikh', 'maximilian zenk', 'michael baumgartner', 'klaus maier-hein', 'li tianhong', 'yang hong', 'zhao longfei', 'domen preloznik', 'žiga špiclin', 'jae won choi', 'muyang li', 'jia fu', 'guotai wang', 'jingwen jiang', 'lyuyang tong', 'bo du', 'andrea gondova', 'sungmin you', 'kiho im', 'abdul qayyum', 'moona mazher', 'steven a niederer', 'andras jakab', 'roxane licandro', 'kelly payette', 'meritxell bach cuadra']"
2505.02787,unsupervised training of keypoint-agnostic descriptors for flexible   retinal image registration,cs.cv,"current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. this enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference.   to validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. additionally, it demonstrates accurate performance regardless of the keypoint detector used. thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain.",,2025-05-05,,"['david rivas-villar', 'álvaro s. hervella', 'josé rouco', 'jorge novo']"
2505.02797,dpnet: dynamic pooling network for tiny object detection,cs.cv,"in unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. resizing images is a common strategy to improve detection accuracy, particularly for small objects. however, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. this paper proposes a dynamic pooling network (dpnet) for tiny object detection to mitigate these issues. dpnet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. thus, we achieve input-aware downsampling. we also design an adaptive normalization module (anm) to make a unified detector compatible with different dfs. a guidance loss supervises the predictor's training. dpnet dynamically allocates computing resources to trade off between detection accuracy and efficiency. experiments on the tinycoco and tinyperson datasets show that dpnet can save over 35% and 25% gflops, respectively, while maintaining comparable detection performance. the code will be made publicly available.",10.1109/jiot.2025.3559921,2025-05-05,,"['luqi gong', 'haotian chen', 'yikun chen', 'tianliang yao', 'chao li', 'shuai zhao', 'guangjie han']"
2505.02815,database-agnostic gait enrollment using settransformers,cs.cv,"gait recognition has emerged as a powerful tool for unobtrusive and long-range identity analysis, with growing relevance in surveillance and monitoring applications. although recent advances in deep learning and large-scale datasets have enabled highly accurate recognition under closed-set conditions, real-world deployment demands open-set gait enrollment, which means determining whether a new gait sample corresponds to a known identity or represents a previously unseen individual. in this work, we introduce a transformer-based framework for open-set gait enrollment that is both dataset-agnostic and recognition-architecture-agnostic. our method leverages a settransformer to make enrollment decisions based on the embedding of a probe sample and a context set drawn from the gallery, without requiring task-specific thresholds or retraining for new environments. by decoupling enrollment from the main recognition pipeline, our model is generalized across different datasets, gallery sizes, and identity distributions. we propose an evaluation protocol that uses existing datasets in different ratios of identities and walks per identity. we instantiate our method using skeleton-based gait representations and evaluate it on two benchmark datasets (casia-b and psymo), using embeddings from three state-of-the-art recognition models (gaitgraph, gaitformer, and gaitpt). we show that our method is flexible, is able to accurately perform enrollment in different scenarios, and scales better with data compared to traditional approaches. we will make the code and dataset scenarios publicly available.",,2025-05-05,,"['nicoleta basoc', 'adrian cosma', 'andy cǎtrunǎ', 'emilian rǎdoi']"
2505.02823,musar: exploring multi-subject customization from single-subject dataset   via attention routing,cs.cv,"current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. to bridge these gaps, we propose musar - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. firstly, to break the data limitation, we introduce debiased diptych learning. it constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch lora. secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. this design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. comprehensive experiments demonstrate that our musar outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.",,2025-05-05,,"['zinan guo', 'pengze zhang', 'yanze wu', 'chong mou', 'songtao zhao', 'qian he']"
2505.02824,towards dataset copyright evasion attack against personalized   text-to-image diffusion models,cs.cv cs.ai cs.cr,"text-to-image (t2i) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. however, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. to combat this, dataset ownership verification (dov) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. these watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. despite the promise of dov for t2i diffusion models, its robustness against copyright evasion attacks (cea) remains unexplored. in this paper, we explore how attackers can bypass these mechanisms through cea, allowing models to circumvent watermarks even when trained on watermarked datasets. we propose the first copyright evasion attack (i.e., ceat2i) specifically designed to undermine dov in t2i diffusion models. concretely, our ceat2i comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. a key insight driving our approach is that t2i models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. leveraging this, ceat2i can reliably detect the watermarked samples. then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. finally, we adopt a closed-form concept erasure method to remove the injected watermark. extensive experiments show that our ceat2i effectively evades dov mechanisms while preserving model performance.",,2025-05-05,,"['kuofeng gao', 'yufei zhu', 'yiming li', 'jiawang bai', 'yong yang', 'zhifeng li', 'shu-tao xia']"
2505.02825,towards application-specific evaluation of vision models: case studies   in ecology and biology,cs.cv,"computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. however, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. we argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. to support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3d posture estimator. we show that even models with strong machine learning performance (e.g., 87% map) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows.",,2025-05-05,2025-05-06,"['alex hoi hang chan', 'otto brookes', 'urs waldmann', 'hemal naik', 'iain d. couzin', 'majid mirmehdi', 'noël adiko houa', 'emmanuelle normand', 'christophe boesch', 'lukas boesch', 'mimi arandjelovic', 'hjalmar kühl', 'tilo burghardt', 'fumihiro kano']"
2505.02830,aor: anatomical ontology-guided reasoning for medical large multimodal   model in chest x-ray interpretation,cs.cv cs.cl,"chest x-rays (cxrs) are the most frequently performed imaging examinations in clinical settings. recent advancements in large multimodal models (lmms) have enabled automated cxr interpretation, enhancing diagnostic accuracy and efficiency. however, despite their strong visual understanding, current medical lmms (mlmms) still face two major challenges: (1) insufficient region-level understanding and interaction, and (2) limited accuracy and interpretability due to single-step reasoning. in this paper, we empower mlmms with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. specifically, we first propose an anatomical ontology-guided reasoning (aor) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. next, under the guidance of expert physicians, we develop aor-instruction, a large instruction dataset for mlmms training. our experiments demonstrate aor's superior performance in both vqa and report generation tasks.",,2025-05-05,,"['qingqiu li', 'zihang cui', 'seongsu bae', 'jilan xu', 'runtian yuan', 'yuejie zhang', 'rui feng', 'quanli shen', 'xiaobo zhang', 'junjun he', 'shujun wang']"
2505.02833,twist: teleoperated whole-body imitation system,cs.ro cs.cv cs.lg,"teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. we present the teleoperated whole-body imitation system (twist), a system for humanoid teleoperation through whole-body motion imitation. we first generate reference motion clips by retargeting human motion capture data to the humanoid robot. we then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (rl+bc). through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (mocap) data improves tracking accuracy. twist enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. our project website: https://humanoid-teleop.github.io",,2025-05-05,,"['yanjie ze', 'zixuan chen', 'joão pedro araújo', 'zi-ang cao', 'xue bin peng', 'jiajun wu', 'c. karen liu']"
2505.02836,scenethesis: a language and vision agentic framework for 3d scene   generation,cs.cv,"synthesizing interactive 3d scenes from text is essential for gaming, virtual reality, and embodied ai. however, existing methods face several challenges. learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. while large language models (llms) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that llms lack. to this end, we introduce scenethesis, a training-free agentic framework that integrates llm-based scene planning with vision-guided layout refinement. given a text prompt, scenethesis first employs an llm to draft a coarse layout. a vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. finally, a judge module verifies spatial coherence. comprehensive experiments show that scenethesis generates diverse, realistic, and physically plausible 3d interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied ai research.",,2025-05-05,,"['lu ling', 'chen-hsuan lin', 'tsung-yi lin', 'yifan ding', 'yu zeng', 'yichen sheng', 'yunhao ge', 'ming-yu liu', 'aniket bera', 'zhaoshuo li']"
2505.02843,physical foundations for trustworthy medical imaging: a review for   artificial intelligence researchers,eess.iv cs.ai cs.cv physics.med-ph,"artificial intelligence in medical imaging has seen unprecedented growth in the last years, due to rapid advances in deep learning and computing resources. applications cover the full range of existing medical imaging modalities, with unique characteristics driven by the physics of each technique. yet, artificial intelligence professionals entering the field, and even experienced developers, often lack a comprehensive understanding of the physical principles underlying medical image acquisition, which hinders their ability to fully leverage its potential. the integration of physics knowledge into artificial intelligence algorithms enhances their trustworthiness and robustness in medical imaging, especially in scenarios with limited data availability. in this work, we review the fundamentals of physics in medical images and their impact on the latest advances in artificial intelligence, particularly, in generative models and reconstruction algorithms. finally, we explore the integration of physics knowledge into physics-inspired machine learning models, which leverage physics-based constraints to enhance the learning of medical imaging features.",,2025-04-28,,"['miriam cobo', 'david corral fontecha', 'wilson silva', 'lara lloret iglesias']"
2505.02845,floating car observers in intelligent transportation systems: detection   modeling and temporal insights,physics.soc-ph cs.cv cs.sy eess.sy,"floating car observers (fcos) extend traditional floating car data (fcd) by integrating onboard sensors to detect and localize other traffic participants, providing richer and more detailed traffic data. in this work, we explore various modeling approaches for fco detections within microscopic traffic simulations to evaluate their potential for intelligent transportation system (its) applications. these approaches range from 2d raytracing to high-fidelity co-simulations that emulate real-world sensors and integrate 3d object detection algorithms to closely replicate fco detections. additionally, we introduce a neural network-based emulation technique that effectively approximates the results of high-fidelity co-simulations. this approach captures the unique characteristics of fco detections while offering a fast and scalable solution for modeling. using this emulation method, we investigate the impact of fco data in a digital twin of a traffic network modeled in sumo. results demonstrate that even at a 20% penetration rate, fcos using lidar-based detections can identify 65% of vehicles across various intersections and traffic demand scenarios. further potential emerges when temporal insights are integrated, enabling the recovery of previously detected but currently unseen vehicles. by employing data-driven methods, we recover over 80% of these vehicles with minimal positional deviations. these findings underscore the potential of fcos for its, particularly in enhancing traffic state estimation and monitoring under varying penetration rates and traffic conditions.",,2025-04-29,,"['jeremias gerner', 'klaus bogenberger', 'stefanie schmidtner']"
2505.02867,resanything: attribute prompting for arbitrary referring segmentation,cs.cv,"we present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (res), targeting input expressions that are more general than what prior works were designed to handle. specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object/part function, design, style, material, etc. our model, coined resanything, leverages chain-of-thoughts (cot) reasoning, where the key idea is attribute prompting. we generate detailed descriptions of object/part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (llm), where the proposals are produced by a foundational image segmentation model. our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. as the first zero-shot and llm-based res method, resanything achieves clearly superior performance among zero-shot methods on traditional res benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. finally, we contribute a new benchmark dataset to offer ~3k carefully curated res instances to assess part-level, arbitrary res solutions.",,2025-05-03,,"['ruiqi wang', 'hao zhang']"
2505.02877,a wireless collaborated inference acceleration framework for plant   disease recognition,cs.lg cs.ai cs.cv,"plant disease is a critical factor affecting agricultural production. traditional manual recognition methods face significant drawbacks, including low accuracy, high costs, and inefficiency. deep learning techniques have demonstrated significant benefits in identifying plant diseases, but they still face challenges such as inference delays and high energy consumption. deep learning algorithms are difficult to run on resource-limited embedded devices. offloading these models to cloud servers is confronted with the restriction of communication bandwidth, and all of these factors will influence the inference's efficiency. we propose a collaborative inference framework for recognizing plant diseases between edge devices and cloud servers to enhance inference speed. the dnn model for plant disease recognition is pruned through deep reinforcement learning to improve the inference speed and reduce energy consumption. then the optimal split point is determined by a greedy strategy to achieve the best collaborated inference acceleration. finally, the system for collaborative inference acceleration in plant disease recognition has been implemented using gradio to facilitate friendly human-machine interaction. experiments indicate that the proposed collaborative inference framework significantly increases inference speed while maintaining acceptable recognition accuracy, offering a novel solution for rapidly diagnosing and preventing plant diseases.",,2025-05-04,,"['hele zhu', 'xinyi huang', 'haojia gao', 'mengfei jiang', 'haohua que', 'lei mu']"
2505.02949,gone with the bits: revealing racial bias in low-rate neural compression   for facial images,cs.cv,"neural compression methods are gaining popularity due to their superior rate-distortion performance over traditional methods, even at extremely low bitrates below 0.1 bpp. as deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. in this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. using this framework, we investigate racial bias in neural compression algorithms by analyzing nine popular models and their variants. through this investigation, we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. we then examine the relationship between bias and realism in the decoded images and demonstrate a trade-off across models. finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy. we additionally show the bias can be attributed to compression model bias and classification model bias. we believe that this work is a first step towards evaluating and eliminating bias in neural image compression models.",,2025-05-05,,"['tian qiu', 'arjun nichani', 'rasta tadayontahmasebi', 'haewon jeong']"
2505.02966,generating narrated lecture videos from slides with synchronized   highlights,cs.cv cs.ai,"turning static slides into engaging video lectures takes considerable time and effort, requiring presenters to record explanations and visually guide their audience through the material. we introduce an end-to-end system designed to automate this process entirely. given a slide deck, this system synthesizes a video lecture featuring ai-generated narration synchronized precisely with dynamic visual highlights. these highlights automatically draw attention to the specific concept being discussed, much like an effective presenter would. the core technical contribution is a novel highlight alignment module. this module accurately maps spoken phrases to locations on a given slide using diverse strategies (e.g., levenshtein distance, llm-based semantic analysis) at selectable granularities (line or word level) and utilizes timestamp-providing text-to-speech (tts) for timing synchronization. we demonstrate the system's effectiveness through a technical evaluation using a manually annotated slide dataset with 1000 samples, finding that llm-based alignment achieves high location accuracy (f1 > 92%), significantly outperforming simpler methods, especially on complex, math-heavy content. furthermore, the calculated generation cost averages under $1 per hour of video, offering potential savings of two orders of magnitude compared to conservative estimates of manual production costs. this combination of high accuracy and extremely low cost positions this approach as a practical and scalable tool for transforming static slides into effective, visually-guided video lectures.",,2025-05-05,,['alexander holmberg']
2505.02971,adversarial robustness analysis of vision-language models in medical   image segmentation,cs.cv,"adversarial attacks have been fairly explored for computer vision and vision-language models. however, the avenue of adversarial attack for the vision language segmentation models (vlsms) is still under-explored, especially for medical image analysis.   thus, we have investigated the robustness of vlsms against adversarial attacks for 2d medical images with different modalities with radiology, photography, and endoscopy. the main idea of this project was to assess the robustness of the fine-tuned vlsms specially in the medical domain setting to address the high risk scenario.   first, we have fine-tuned pre-trained vlsms for medical image segmentation with adapters.   then, we have employed adversarial attacks -- projected gradient descent (pgd) and fast gradient sign method (fgsm) -- on that fine-tuned model to determine its robustness against adversaries.   we have reported models' performance decline to analyze the adversaries' impact.   the results exhibit significant drops in the dsc and iou scores after the introduction of these adversaries. furthermore, we also explored universal perturbation but were not able to find for the medical images.   \footnote{https://github.com/anjilab/secure-private-ai}",,2025-05-05,,"['anjila budathoki', 'manish dhakal']"
2505.02980,completing spatial transcriptomics data for gene expression prediction   benchmarking,cs.cv,"spatial transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. among the various spatial transcriptomics techniques available, visium has emerged as the most widely adopted. however, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. to address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. to bridge this gap, we introduce spared, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. we further propose spackle, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. finally, we establish the spared benchmark, evaluating eight state-of-the-art prediction models on both raw and spackle-completed data, demonstrating spackle substantially improves the results across all the gene expression prediction models. altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on spatial transcriptomics.",,2025-05-05,,"['daniela ruiz', 'paula cardenas', 'leonardo manrique', 'daniela vega', 'gabriel mejia', 'pablo arbelaez']"
2505.03007,ntire 2025 challenge on ugc video enhancement: methods and results,cs.cv,"this paper presents an overview of the ntire 2025 challenge on ugc video enhancement. the challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. the goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. given the widespread use of ugc on short-form video platforms, this task holds substantial practical importance. the evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. the challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. the outcomes may provide insights into the state-of-the-art in ugc video enhancement and highlight emerging trends and effective strategies in this evolving research area. all data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/ntire25_ugc_video_enhancement.",,2025-05-05,,"['nikolay safonov', 'alexey bryncev', 'andrey moskalenko', 'dmitry kulikov', 'dmitry vatolin', 'radu timofte', 'haibo lei', 'qifan gao', 'qing luo', 'yaqing li', 'jie song', 'shaozhe hao', 'meisong zheng', 'jingyi xu', 'chengbin wu', 'jiahui liu', 'ying chen', 'xin deng', 'mai xu', 'peipei liang', 'jie ma', 'junjie jin', 'yingxue pang', 'fangzhou luo', 'kai chen', 'shijie zhao', 'mingyang wu', 'renjie li', 'yushen zuo', 'shengyun zhong', 'zhengzhong tu']"
2505.03012,gif: generative inspiration for face recognition at scale,cs.cv,"aiming to reduce the computational cost of softmax in massive label space of face recognition (fr) benchmarks, recent studies estimate the output using a subset of identities. although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. a shared characteristic among available fr methods is the employment of atomic scalar labels during training. consequently, the input to label matching is through a dot product between the feature vector of the input and the softmax centroids. inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. then, we train an fr backbone to predict the code for each input instead of its scalar label. as a result, the associated computational cost becomes logarithmic w.r.t. number of identities. we demonstrate the benefits of the proposed method by conducting experiments. in particular, our method outperforms its competitors by 1.52%, and 0.6% at tar@far$=1e-4$ on ijb-b and ijb-c, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. see code at https://github.com/msed-ebrahimi/gif",,2025-05-05,,"['saeed ebrahimi', 'sahar rahimi', 'ali dabouei', 'srinjoy das', 'jeremy m. dawson', 'nasser m. nasrabadi']"
2505.03018,lesion-aware generative artificial intelligence for virtual   contrast-enhanced mammography in breast cancer,cs.cv cs.ai,"contrast-enhanced spectral mammography (cesm) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. it acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. while cesm offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. to address these limitations, we propose seg-cyclegan, a generative deep learning framework for virtual contrast enhancement in cesm. the model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. building upon the standard cyclegan architecture, seg-cyclegan introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. experiments on the cesm@ucbm dataset demonstrate that seg-cyclegan outperforms the baseline in terms of psnr and ssim, while maintaining competitive mse and vif. qualitative evaluations further confirm improved lesion fidelity in the generated images. these results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free cesm alternatives.",,2025-05-05,,"['aurora rofena', 'arianna manchia', 'claudia lucia piccolo', 'bruno beomonte zobel', 'paolo soda', 'valerio guarrasi']"
2505.03037,dual prompting for diverse count-level pet denoising,eess.iv cs.cv physics.med-ph,"the to-be-denoised positron emission tomography (pet) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. in this work, we resort to the recently flourished prompt learning to achieve generalizable pet denoising with different count levels. specifically, we propose dual prompts to guide the pet denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential pet denoising knowledge. then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. the prompts are able to dynamically guide the noise-conditioned denoising process. therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. we evaluated on 1940 low-count pet 3d volumes with uniformly randomly selected 13-22\% fractions of events from 97 $^{18}$f-mk6240 tau pet studies. it shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.",,2025-05-05,,"['xiaofeng liu', 'yongsong huang', 'thibault marin', 'samira vafay eslahi', 'tiss amal', 'yanis chemli', 'keith johnson', 'georges el fakhri', 'jinsong ouyang']"
2505.03039,an explainable anomaly detection framework for monitoring depression and   anxiety using consumer wearable devices,cs.cv stat.ap,"continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. in this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. leveraging data from 2,023 participants with defined healthy baselines, our lstm autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). the model achieved an adjusted f1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (f1 = 0.84) and for more pronounced symptom changes (>=10-point increases, f1 = 0.85). model interpretability was supported by shap-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings.",,2025-05-05,,"['yuezhou zhang', 'amos a. folarin', 'callum stewart', 'heet sankesara', 'yatharth ranjan', 'pauline conde', 'akash roy choudhury', 'shaoxiong sun', 'zulqarnain rashid', 'richard j. b. dobson']"
2505.03046,sim2real transfer for vision-based grasp verification,cs.ro cs.cv,"the verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. in this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. our method employs a two-stage architecture; first yolo-based object detection model to detect and locate the robot's gripper and then a resnet-based classifier determines the presence of an object. to address the limitations of real-world data capture, we introduce hsr-graspsynth, a synthetic dataset designed to simulate diverse grasping scenarios. furthermore, we explore the use of visual question answering capabilities as a zero-shot baseline to which we compare our model. experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. code and datasets are publicly available at https://github.com/pauamargant/hsr-graspsynth .",,2025-05-05,,"['pau amargant', 'peter hönig', 'markus vincze']"
2505.03097,not all parameters matter: masking diffusion models for enhancing   generation ability,cs.cv,"the diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., resnet or gans) which captures or generates the image semantic information at different layers. this difference inspires us to explore the time-wise diffusion models. we initially investigate the key contributions of the u-net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. capitalizing on this discovery, we propose a simple yet effective method-termed ``maskunet''- that enhances generation quality with negligible parameter numbers. our method fully leverages timestep- and sample-dependent effective u-net parameters. to optimize maskunet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. in zero-shot inference on the coco dataset, maskunet achieves the best fid score and further demonstrates its effectiveness in downstream task evaluations. project page: https://gudaochangsheng.github.io/maskunet-page/",,2025-05-05,,"['lei wang', 'senmao li', 'fei yang', 'jianye wang', 'ziheng zhang', 'yuhan liu', 'yaxing wang', 'jian yang']"
2505.03114,path and bone-contour regularized unpaired mri-to-ct translation,cs.cv,"accurate mri-to-ct translation promises the integration of complementary imaging information without the need for additional imaging sessions. given the practical challenges associated with acquiring paired mri and ct scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the mri-to-ct translation. current unpaired mri-to-ct translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on ct but less distinguishable on mri, such as bone structures. this limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. to address this challenge, we propose a path- and bone-contour regularized approach for unpaired mri-to-ct translation. in our method, mri and ct images are projected to a shared latent space, where the mri-to-ct mapping is modeled as a continuous flow governed by neural ordinary differential equations. the optimal mapping is obtained by minimizing the transition path length of the flow. to enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from mri and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired mri-to-ct translation approaches, achieving lower overall error rates. moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. our code is available at: https://github.com/kennysyp/pabot.",,2025-05-05,,"['teng zhou', 'jax luo', 'yuping sun', 'yiheng tan', 'shun yao', 'nazim haouchine', 'scott raymond']"
2505.03116,timetracker: event-based continuous point tracking for video frame   interpolation with non-linear motion,cs.cv,"video frame interpolation (vfi) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. a hurdle for event-based vfi is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. unfortunately, motion errors often degrade the vfi quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. in this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. in light of this, we propose a novel continuous point tracking-based vfi framework, named timetracker. specifically, we first design a scene-aware region segmentation (sars) module to divide the scene into similar patches. then, a continuous trajectory guided motion estimation (ctme) module is proposed to track the continuous motion trajectory of each patch through events. finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. moreover, we collect a real-world dataset that features fast non-linear motion. extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.",,2025-05-05,,"['haoyue liu', 'jinghan xu', 'yi chang', 'hanyu zhou', 'haozhi zhao', 'lin wang', 'luxin yan']"
2505.03123,stg: spatiotemporal graph neural network with fusion and spatiotemporal   decoupling learning for prognostic prediction of colorectal cancer liver   metastasis,eess.iv cs.cv cs.mm,"we propose a multimodal spatiotemporal graph neural network (stg) framework to predict colorectal cancer liver metastasis (crlm) progression. current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. our stg framework combines preoperative ct imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. the framework uses graphsage to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. a lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. the model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. experimental results on the mskcc crlm dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. the innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.",,2025-05-05,,"['yiran zhu', 'wei yang', 'yan su', 'zesheng li', 'chengchang pan', 'honggang qi']"
2505.03132,vislix: an xai framework for validating vision models with slice   discovery and analysis,cs.cv cs.ai cs.hc,"real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. the evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. first, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. to overcome these limitations and better support the machine learning operations lifecycle, we introduce vislix, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. we evaluate vislix with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.",,2025-05-05,,"['xinyuan yan', 'xiwei xuan', 'jorge piazentin ono', 'jiajing guo', 'vikram mohanty', 'shekar arvind kumar', 'liang gou', 'bei wang', 'liu ren']"
2505.03134,enhancing glass defect detection with diffusion models: addressing   imbalanced datasets in manufacturing quality control,cs.cv cs.lg,"visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. this paper presents a novel approach using denoising diffusion probabilistic models (ddpms) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. the methodology significantly enhances image classification performance of standard cnn architectures (resnet50v2, efficientnetb0, and mobilenetv2) in detecting anomalies by increasing the minority class representation. experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. the most dramatic improvement was observed in resnet50v2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. this work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.",,2025-05-05,,"['sajjad rezvani boroujeni', 'hossein abedi', 'tom bush']"
2505.03149,motion-compensated cardiac mri using low-rank diffeomorphic flow (dmoco),cs.cv cs.ai,"we introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3d cardiac magnetic resonance imaging (mri). we express the image volume corresponding to each specific motion phase as the deformation of a single static image template. the main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. the diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. the velocity field at different phases is represented using a low-rank model. the static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. the more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3d cine mri.",,2025-05-05,2025-05-07,"['joseph kettelkamp', 'ludovica romanin', 'sarv priya', 'mathews jacob']"
2505.03153,robust fairness vision-language learning for medical image analysis,cs.cv,"the advent of vision-language models (vlms) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. however, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. in this paper, we introduce a framework for ensuring robustness and fairness of vlm models. this framework modifies the loss function at training by identifying and adjusting faulty image-text pairs through a dynamic bad pair mining algorithm and also utilizing sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled auc.",,2025-05-05,,"['sparsh bansal', 'mingyang wu', 'xin wang', 'shu hu']"
2505.03154,stablemotion: training motion cleanup models with unpaired corrupted   data,cs.cv cs.ai cs.gr,"motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. in this work, we present stablemotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. the core component of our method is the introduction of motion quality indicators, which can be easily annotated through manual labeling or heuristic algorithms and enable training of quality-aware motion generation models on raw motion data with mixed quality. at test time, the model can be prompted to generate high-quality motions using the quality indicators. our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. we demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying stablemotion to soccermocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. the trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. see https://youtu.be/3y7mmah02b4 for more results.",,2025-05-06,,"['yuxuan mu', 'hung yu ling', 'yi shi', 'ismael baira ojeda', 'pengcheng xi', 'chang shu', 'fabio zinno', 'xue bin peng']"
2505.03173,ravu: retrieval augmented video understanding with compositional   reasoning over graph,cs.cv cs.ai,"comprehending long videos remains a significant challenge for large multi-modal models (lmms). current lmms struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. to address this limitation, we propose ravu (retrieval augmented video understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. we construct a graph representation of the video, capturing both spatial and temporal relationships between entities. this graph serves as a long-term memory, allowing us to track objects and their actions across time. to answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other sota methods and baselines on two major video qa datasets, next-qa and egoschema.",,2025-05-06,,"['sameer malik', 'moyuru yamada', 'ayush singh', 'dishank aggarwal']"
2505.03174,automated data curation using gps & nlp to generate instruction-action   pairs for autonomous vehicle vision-language navigation datasets,cs.ro cs.cv cs.lg,"instruction-action (ia) data pairs are valuable for training robotic systems, especially autonomous vehicles (avs), but having humans manually annotate this data is costly and time-inefficient. this paper explores the potential of using mobile application global positioning system (gps) references and natural language processing (nlp) to automatically generate large volumes of ia commands and responses without having a human generate or retroactively tag the data. in our pilot data collection, by driving to various destinations and collecting voice instructions from gps applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. we provide details on our completely automated data collection prototype system, advlat-engine. we characterize collected gps voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. through research and exploration into the automation of ia data pairs using gps references, the potential to increase the speed and volume at which high-quality ia datasets are created, while minimizing cost, can pave the way for robust vision-language-action (vla) models to serve tasks in vision-language navigation (vln) and human-interactive autonomous systems.",,2025-05-06,,"['guillermo roque', 'erika maquiling', 'jose giovanni tapia lopez', 'ross greer']"
2505.03184,interactive instance annotation with siamese networks,cs.cv,"annotating instance masks is time-consuming and labor-intensive. a promising solution is to predict contours using a deep learning model and then allow users to refine them. however, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. in this paper, we propose siamanno, a framework inspired by the use of siamese networks in object tracking. siamanno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. trained on one dataset and tested on another without fine-tuning, siamanno achieves state-of-the-art (sota) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. we also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. to our knowledge, siamanno is the first model to explore siamese architecture for instance annotation.",,2025-05-06,,"['xiang xu', 'ruotong li', 'mengjun yi', 'baile xu', 'furao shen', 'jian zhao']"
2505.03203,pico: enhancing text-image alignment with improved noise selection and   precise mask control in diffusion models,cs.cv,"advanced diffusion models have made notable progress in text-to-image compositional generation. however, it is still a challenge for existing models to achieve text-image alignment when confronted with complex text prompts. in this work, we highlight two factors that affect this alignment: the quality of the randomly initialized noise and the reliability of the generated controlling mask. we then propose pico (pick-and-control), a novel training-free approach with two key components to tackle these two factors. first, we develop a noise selection module to assess the quality of the random noise and determine whether the noise is suitable for the target text. a fast sampling strategy is utilized to ensure efficiency in the noise selection stage. second, we introduce a referring mask module to generate pixel-level masks and to precisely modulate the cross-attention maps. the referring mask is applied to the standard diffusion process to guide the reasonable interaction between text and image features. extensive experiments have been conducted to verify the effectiveness of pico in liberating users from the tedious process of random generation and in enhancing the text-image alignment for diverse text descriptions.",,2025-05-06,,"['chang xie', 'chenyi zhuang', 'pan gao']"
2505.03204,dcs-st for classification of breast cancer histopathology images with   limited annotations,cs.cv cs.ai,"deep learning methods have shown promise in classifying breast cancer histopathology images, but their performance often declines with limited annotated data, a critical challenge in medical imaging due to the high cost and expertise required for annotations.",,2025-05-06,2025-05-07,"['liu suxing', 'byungwon min']"
2505.03220,dual-domain masked image modeling: a self-supervised pretraining   strategy using spatial and frequency domain masking for hyperspectral data,cs.cv,"hyperspectral images (hsis) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. however, the scarcity of labeled hsi data limits the full potential of deep learning, especially for transformer-based architectures that require large-scale training. to address this constraint, we propose spatial-frequency masked image modeling (sfmim), a self-supervised pretraining strategy for hyperspectral data that utilizes the large portion of unlabeled data. our method introduces a novel dual-domain masking mechanism that operates in both spatial and frequency domains. the input hsi cube is initially divided into non-overlapping patches along the spatial dimension, with each patch comprising the entire spectrum of its corresponding spatial location. in spatial masking, we randomly mask selected patches and train the model to reconstruct the masked inputs using the visible patches. concurrently, in frequency masking, we remove portions of the frequency components of the input spectra and predict the missing frequencies. by learning to reconstruct these masked components, the transformer-based encoder captures higher-order spectral-spatial correlations. we evaluate our approach on three publicly available hsi classification benchmarks and demonstrate that it achieves state-of-the-art performance. notably, our model shows rapid convergence during fine-tuning, highlighting the efficiency of our pretraining strategy.",,2025-05-06,,"['shaheer mohamed', 'tharindu fernando', 'sridha sridharan', 'peyman moghadam', 'clinton fookes']"
2505.03242,seeing the abstract: translating the abstract language for vision   language models,cs.cv cs.ai,"natural language goes beyond dryly describing visual content. it contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. yet, current research in vision language models (vlms) has not shed light on abstract-oriented language. our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. by analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. however, a critical challenge emerges: current general-purpose or fashion-specific vlms are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. we propose a training-free and model-agnostic method, abstract-to-concrete translator (act), to shift abstract representations towards well-represented concrete ones in the vlm latent space, using pre-trained models and existing multimodal databases. on the text-to-image retrieval task, despite being training-free, act outperforms the fine-tuned vlms in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. moreover, the improvement introduced by act is consistent with various vlms, making it a plug-and-play solution.",,2025-05-06,,"['davide talon', 'federico girella', 'ziyue liu', 'marco cristani', 'yiming wang']"
2505.03254,prom: prioritize reduction of multiplications over lower bit-widths for   efficient cnns,cs.cv cs.lg,"convolutional neural networks (cnns) are crucial for computer vision tasks on resource-constrained devices. quantization effectively compresses these models, reducing storage size and energy cost. however, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. by applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. to this end, we introduce prom, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. applying prom to mobilenetv2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on imagenet. our method advances the pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on imagenet. prom addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.",,2025-05-06,,"['lukas meiner', 'jens mehnert', 'alexandru paul condurache']"
2505.03261,diffvqa: video quality assessment using diffusion feature extractor,cs.cv eess.iv,"video quality assessment (vqa) aims to evaluate video quality based on perceptual distortions and human preferences. despite the promising performance of existing methods using convolutional neural networks (cnns) and vision transformers (vits), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. this challenge is exacerbated by the limited scale and diversity of available datasets. to address this limitation, we introduce a novel vqa framework, diffvqa, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. our framework adapts these models to reconstruct identical input frames through a control module. the adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. to enhance the model's ability to handle long-term temporal dynamics, a parallel mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. experiments across multiple datasets demonstrate diffvqa's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. these results confirm that leveraging a diffusion model as a feature extractor can offer enhanced vqa performance compared to cnn and vit backbones.",,2025-05-06,,"['wei-ting chen', 'yu-jiet vong', 'yi-tsung lee', 'sy-yen kuo', 'qiang gao', 'sizhuo ma', 'jian wang']"
2505.03284,occcylindrical: multi-modal fusion with cylindrical representation for   3d semantic occupancy prediction,cs.cv cs.ro,"the safe operation of autonomous vehicles (avs) is highly dependent on their understanding of the surroundings. for this, the task of 3d semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. recent perception models have used multisensor fusion to perform this task. however, existing multisensor fusion-based approaches focus mainly on using sensor information in the cartesian coordinate system. this ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. in this paper, we propose occcylindrical that merges and refines the different modality features under cylindrical coordinates. our method preserves more fine-grained geometry detail that leads to better performance. extensive experiments conducted on the nuscenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. the code will be available at: https://github.com/danielming123/occcylindrical",,2025-05-06,,"['zhenxing ming', 'julie stephany berrio', 'mao shan', 'yaoqi huang', 'hongyu lyu', 'nguyen hoang khoi tran', 'tzu-yun tseng', 'stewart worrall']"
2505.03286,base-detail feature learning framework for visible-infrared person   re-identification,cs.cv,"visible-infrared person re-identification (vireid) provides a solution for reid tasks in 24-hour scenarios; however, significant challenges persist in achieving satisfactory performance due to the substantial discrepancies between visible (vis) and infrared (ir) modalities. existing methods inadequately leverage information from different modalities, primarily focusing on digging distinguishing features from modality-shared information while neglecting modality-specific details. to fully utilize differentiated minutiae, we propose a base-detail feature learning framework (bdlf) that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information. specifically, the proposed bdlf mines detail and base features through a lossless detail feature extraction module and a complementary base embedding generation mechanism, respectively, supported by a novel correlation restriction method that ensures the features gained by bdlf enrich both detail and base knowledge across vis and ir features. comprehensive experiments conducted on the sysu-mm01, regdb, and llcm datasets validate the effectiveness of bdlf.",,2025-05-06,,"['zhihao gong', 'lian wu', 'yong xu']"
2505.03299,towards efficient benchmarking of foundation models in remote sensing: a   capabilities encoding approach,cs.cv cs.ai,"foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. in the field of earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. however, none has consistently outperformed the others across all available downstream tasks. to facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. this method is based on what we call ""capabilities encoding."" the utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. codes are available at https://github.com/pierreadorni/capabilities-encoding.",,2025-05-06,,"['pierre adorni', 'minh-tan pham', 'stéphane may', 'sébastien lefèvre']"
2505.03300,3d can be explored in 2d: pseudo-label generation for lidar point clouds   using sensor-intensity-based 2d semantic segmentation,cs.cv,"semantic segmentation of 3d lidar point clouds, essential for autonomous driving and infrastructure management, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. we introduce a new 3d semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2d segmentation methods, avoiding the need for direct 3d annotation or reliance on additional modalities such as camera images at inference time. our approach generates 2d views from lidar scans colored by sensor intensity and applies 2d semantic segmentation to these views using a camera-domain pretrained model. the segmented 2d outputs are then back-projected onto the 3d points, with a simple voting-based estimator that merges the labels associated to each 3d point. our main contribution is a global pipeline for 3d semantic segmentation requiring no prior 3d annotation and not other modality for inference, which can be used for pseudo-label generation. we conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the unsupervised domain adaptation task.",10.1109/iv55156.2024.10588443,2025-05-06,,"['andrew caunes', 'thierry chateau', 'vincent frémont']"
2505.03303,comparative analysis of lightweight deep learning models for   memory-constrained devices,cs.cv cs.ai,"this paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. five state-of-the-art architectures - mobilenetv3 small, resnet18, squeezenet, efficientnetv2-s, and shufflenetv2 - are benchmarked across three diverse datasets: cifar-10, cifar-100, and tiny imagenet. the models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (flops), and model size. additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on mobilenetv3 small. our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like tiny imagenet. efficientnetv2 consistently achieves the highest accuracy, while mobilenetv3 offers the best balance between accuracy and efficiency, and squeezenet excels in inference speed and compactness. this study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. by addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.",,2025-05-06,,['tasnim shahriar']
2505.03310,3d gaussian splatting data compression with mixture of priors,cs.cv,"3d gaussian splatting (3dgs) data compression is crucial for enabling efficient storage and transmission in 3d scene modeling. however, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. in this work, we propose a novel mixture of priors (mop) strategy to simultaneously address these two challenges. specifically, inspired by the mixture-of-experts (moe) paradigm, our mop approach processes hyperprior information through multiple lightweight mlps to generate diverse prior features, which are subsequently integrated into the mop feature via a gating mechanism. to enhance lossless compression, the resulting mop feature is utilized as a hyperprior to improve conditional entropy modeling. meanwhile, for lossy compression, we employ the mop feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided coarse-to-fine quantization (c2fq) strategy with a predefined quantization step value. specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the mop feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. extensive experiments demonstrate that our proposed 3dgs data compression framework achieves state-of-the-art performance across multiple benchmarks, including mip-nerf360, bungeenerf, deepblending, and tank&temples.",,2025-05-06,,"['lei liu', 'zhenghao chen', 'dong xu']"
2505.03318,unified multimodal chain-of-thought reward model through reinforcement   fine-tuning,cs.cv,"recent advances in multimodal reward models (rms) have shown significant promise in delivering reward signals to align vision models with human preferences. however, current rms are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. we posit that incorporating explicit long chains of thought (cot) into the reward reasoning process can significantly strengthen their reliability and robustness. furthermore, we believe that once rms internalize cot reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. to this end, this paper proposes unifiedreward-think, the first unified multimodal cot-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) we first use a small amount of image generation preference data to distill the reasoning process of gpt-4o, which is then used for the model's cold start to learn the format and structure of cot reasoning. (2) subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. during this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for group relative policy optimization (grpo) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. extensive experiments across various vision reward tasks demonstrate the superiority of our model.",,2025-05-06,,"['yibin wang', 'zhimin li', 'yuhang zang', 'chunyu wang', 'qinglin lu', 'cheng jin', 'jiaqi wang']"
2505.03319,sd-vsum: a method and dataset for script-driven video summarization,cs.cv cs.ai cs.mm,"in this work, we introduce the task of script-driven video summarization, which aims to produce a summary of the full-length video by selecting the parts that are most relevant to a user-provided script outlining the visual content of the desired summary. following, we extend a recently-introduced large-scale dataset for generic video summarization (videoxum) by producing natural language descriptions of the different human-annotated summaries that are available per video. in this way we make it compatible with the introduced task, since the available triplets of ``video, summary and summary description'' can be used for training a method that is able to produce different summaries for a given video, driven by the provided script about the content of each summary. finally, we develop a new network architecture for script-driven video summarization (sd-vsum), that relies on the use of a cross-modal attention mechanism for aligning and fusing information from the visual and text modalities. our experimental evaluations demonstrate the advanced performance of sd-vsum against state-of-the-art approaches for query-driven and generic (unimodal and multimodal) summarization from the literature, and document its capacity to produce video summaries that are adapted to each user's needs about their content.",,2025-05-06,,"['manolis mylonas', 'evlampios apostolidis', 'vasileios mezaris']"
2505.03327,very high-resolution forest mapping with tandem-x insar data and   self-supervised learning,cs.cv cs.ai cs.lg eess.iv,"deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with tandem-x interferometric sar data. such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. in this work, our aim is to exploit the high-resolution capabilities of the tandem-x mission to map forests at 6 m. the goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. to cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. a 1 m resolution forest/non-forest reference map over pennsylvania, usa, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. we select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the amazon rainforest, where only very few labeled data at high resolution are available. in this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with tandem-x data.",,2025-05-06,,"['josé-luis bueso-bello', 'benjamin chauvel', 'daniel carcereri', 'philipp posovszky', 'pietro milillo', 'jennifer ruiz', 'juan-carlos fernández-diaz', 'carolina gonzález', 'michele martone', 'ronny hänsch', 'paola rizzoli']"
2505.03329,flux-text: a simple and advanced diffusion transformer baseline for   scene text editing,cs.cv,"the task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. recent works based on latent diffusion models (ldm) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-latin ones (\eg, chinese), which have complex glyph structures. to address these issues, we present flux-text, a simple and advanced multilingual scene text editing framework based on flux-fill. specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. to retain the original generative capabilities of flux-fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. owning to the lightweight design, flux-text is trained only with $100k$ training examples compared to current popular methods trained with 2.9m ones. with no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity.",,2025-05-06,,"['rui lan', 'yancheng bai', 'xu duan', 'mingxing li', 'lei sun', 'xiangxiang chu']"
2505.03334,from word to sentence: a large-scale multi-instance dataset for open-set   aerial detection,cs.cv cs.db,"in recent years, language-guided open-world aerial object detection has gained significant attention due to its better alignment with real-world application needs. however, due to limited datasets, most existing language-guided methods primarily focus on vocabulary, which fails to meet the demands of more fine-grained open-world detection. to address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. centered around an open-source large vision-language model and integrating image-operation-based preprocessing with bert-based postprocessing, we present the os-w2s label engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called multi-instance open-set aerial dataset (mi-oad), addressing the limitations of current remote sensing grounding data and enabling effective open-set aerial detection. specifically, mi-oad contains 163,023 images and 2 million image-caption pairs, approximately 40 times larger than comparable datasets. we also employ state-of-the-art open-set methods from the natural image domain, trained on our proposed dataset, to validate the model's open-set detection capabilities. for instance, when trained on our dataset, grounding dino achieves improvements of 29.5 ap_{50} and 33.7 recall@10 for sentence inputs under zero-shot transfer conditions. both the dataset and the label engine will be released publicly.",,2025-05-06,,"['guoting wei', 'yu liu', 'xia yuan', 'xizhe xue', 'linlin guo', 'yifan yang', 'chunxia zhao', 'zongwen bai', 'haokui zhang', 'rong xiao']"
2505.03350,a vision-language model for focal liver lesion classification,cs.cv,"accurate classification of focal liver lesions is crucial for diagnosis and treatment in hepatology. however, traditional supervised deep learning models depend on large-scale annotated datasets, which are often limited in medical imaging. recently, vision-language models (vlms) such as contrastive language-image pre-training model (clip) has been applied to image classifications. compared to the conventional convolutional neural network (cnn), which classifiers image based on visual information only, vlm leverages multimodal learning with text and images, allowing it to learn effectively even with a limited amount of labeled data. inspired by clip, we pro-pose a liver-vlm, a model specifically designed for focal liver lesions (flls) classification. first, liver-vlm incorporates class information into the text encoder without introducing additional inference overhead. second, by calculating the pairwise cosine similarities between image and text embeddings and optimizing the model with a cross-entropy loss, liver-vlm ef-fectively aligns image features with class-level text features. experimental results on mpct-flls dataset demonstrate that the liver-vlm model out-performs both the standard clip and medclip models in terms of accuracy and area under the curve (auc). further analysis shows that using a lightweight resnet18 backbone enhances classification performance, particularly under data-constrained conditions.",,2025-05-06,,"['song jian', 'hu yuchang', 'wang hui', 'chen yen-wei']"
2505.03351,guava: generalizable upper body 3d gaussian avatar,cs.cv,"reconstructing a high-quality, animatable 3d human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3d human avatar reconstruction typically requires multi-view or monocular videos and training on individual ids, which is both complex and time-consuming. furthermore, limited by smplx's expressiveness, these methods often focus on body motion but struggle with facial expressions. to address these challenges, we first introduce an expressive human model (ehm) to enhance facial expression capabilities and develop an accurate tracking method. based on this template model, we propose guava, the first framework for fast animatable upper-body 3d gaussian avatar reconstruction. we leverage inverse texture mapping and projection sampling techniques to infer ubody (upper-body) gaussians from a single image. the rendered images are refined through a neural refiner. experimental results demonstrate that guava significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.",,2025-05-06,,"['dongbin zhang', 'yunfei liu', 'lijian lin', 'ye zhu', 'yang li', 'minghan qin', 'yu li', 'haoqian wang']"
2505.03361,interpretable zero-shot learning with infinite class concepts,cs.cv,"zero-shot learning (zsl) aims to recognize unseen classes by aligning images with intermediate class semantics, like human-annotated concepts or class definitions. an emerging alternative leverages large-scale language models (llms) to automatically generate class documents. however, these methods often face challenges with transparency in the classification process and may suffer from the notorious hallucination problem in llms, resulting in non-visual class semantics. this paper redefines class semantics in zsl with a focus on transferability and discriminability, introducing a novel framework called zero-shot learning with infinite class concepts (infzsl). our approach leverages the powerful capabilities of llms to dynamically generate an unlimited array of phrase-level class concepts. to address the hallucination challenge, we introduce an entropy-based scoring process that incorporates a ``goodness"" concept selection mechanism, ensuring that only the most transferable and discriminative concepts are selected. our infzsl framework not only demonstrates significant improvements on three popular benchmark datasets but also generates highly interpretable, image-grounded concepts. code will be released upon acceptance.",,2025-05-06,,"['zihan ye', 'shreyank n gowda', 'shiming chen', 'yaochu jin', 'kaizhu huang', 'xiaobo jin']"
2505.03362,3d surface reconstruction with enhanced high-frequency details,cs.cv,"neural implicit 3d reconstruction can reproduce shapes without 3d supervision, and it learns the 3d scene through volume rendering methods and neural implicit representations. current neural surface reconstruction methods tend to randomly sample the entire image, making it difficult to learn high-frequency details on the surface, and thus the reconstruction results tend to be too smooth. we designed a method (freneus) based on high-frequency information to solve the problem of insufficient surface detail. specifically, freneus uses pixel gradient changes to easily acquire high-frequency regions in an image and uses the obtained high-frequency information to guide surface detail reconstruction. high-frequency information is first used to guide the dynamic sampling of rays, applying different sampling strategies according to variations in high-frequency regions. to further enhance the focus on surface details, we have designed a high-frequency weighting method that constrains the representation of high-frequency details during the reconstruction process. qualitative and quantitative results show that our method can reconstruct fine surface details and obtain better surface reconstruction quality compared to existing methods. in addition, our method is more applicable and can be generalized to any neus-based work.",,2025-05-06,,"['shikun zhang', 'yiqun wang', 'cunjian chen', 'yong li', 'qiuhong ke']"
2505.03374,reducing annotation burden in physical activity research using   vision-language models,cs.cv,"introduction: data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. one common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. methods: we compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in oxfordshire, united kingdom and sichuan, china, respectively, using the autographer (omg life, defunct) wearable camera. results: we found that the best open-source vision-language model (vlm) and fine-tuned discriminative model (dm) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the oxfordshire study; median f1-scores: vlm = 0.89 (0.84, 0.92), dm = 0.91 (0.86, 0.95). performance declined for light (vlm = 0.60 (0.56,0.67), dm = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (vlm = 0.66 (0.53, 0.85); dm = 0.72 (0.58, 0.84)). when applied to the external sichuan study, performance fell across all intensity categories, with median cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the vlm, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the dm. conclusion: freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden.",,2025-05-06,,"['abram schonfeldt', 'benjamin maylor', 'xiaofang chen', 'ronald clark', 'aiden doherty']"
2505.03380,reinforced correlation between vision and language for precise medical   ai assistant,cs.cv cs.ai eess.iv,"medical ai assistants support doctors in disease diagnosis, medical image analysis, and report generation. however, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. we propose rcmed, a full-stack ai assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. a self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. this correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. trained on 20 million image-mask-description triplets, rcmed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. it achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. rcmed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. this work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric ai healthcare.",,2025-05-06,,"['haonan wang', 'jiaji mao', 'lehan wang', 'qixiang zhang', 'marawan elbatel', 'yi qin', 'huijun hu', 'baoxun li', 'wenhui deng', 'weifeng qin', 'hongrui li', 'jialin liang', 'jun shen', 'xiaomeng li']"
2505.03383,attention-aggregated attack for boosting the transferability of facial   adversarial examples,cs.cv,"adversarial examples have revealed the vulnerability of deep learning models and raised serious concerns about information security. the transfer-based attack is a hot topic in black-box attacks that are practical to real-world scenarios where the training datasets, parameters, and structure of the target model are unknown to the attacker. however, few methods consider the particularity of class-specific deep models for fine-grained vision tasks, such as face recognition (fr), giving rise to unsatisfactory attacking performance. in this work, we first investigate what in a face exactly contributes to the embedding learning of fr models and find that both decisive and auxiliary facial features are specific to each fr model, which is quite different from the biological mechanism of human visual system. accordingly we then propose a novel attack method named attention-aggregated attack (aaa) to enhance the transferability of adversarial examples against fr, which is inspired by the attention divergence and aims to destroy the facial features that are critical for the decision-making of other fr models by imitating their attentions on the clean face images. extensive experiments conducted on various fr models validate the superiority and robust effectiveness of the proposed method over existing methods.",,2025-05-06,,"['jian-wei li', 'wen-ze shao']"
2505.03394,eopose : exemplar-based object reposing using generalized pose   correspondences,cs.cv,"reposing objects in images has a myriad of applications, especially for e-commerce where several variants of product images need to be produced quickly. in this work, we leverage the recent advances in unsupervised keypoint correspondence detection between different object images of the same class to propose an end-to-end framework for generic object reposing. our method, eopose, takes a target pose-guidance image as input and uses its keypoint correspondence with the source object image to warp and re-render the latter into the target pose using a novel three-step approach. unlike generative approaches, our method also preserves the fine-grained details of the object such as its exact colors, textures, and brand marks. we also prepare a new dataset of paired objects based on the objaverse dataset to train and test our network. eopose produces high-quality reposing output as evidenced by different image quality metrics (psnr, ssim and fid). besides a description of the method and the dataset, the paper also includes detailed ablation and user studies to indicate the efficacy of the proposed method",,2025-05-06,,"['sarthak mehrotra', 'rishabh jain', 'mayur hemani', 'balaji krishnamurthy', 'mausoom sarkar']"
2505.03401,ddatr: dynamic difference-aware temporal residual network for   longitudinal radiology report generation,cs.cv cs.ai,"radiology report generation (rrg) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. longitudinal radiology report generation (lrrg) extends rrg by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. existing lrrg approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. however, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in lrrg. to address this, we develop a novel dynamic difference-aware temporal residual network (ddatr). in ddatr, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. the dynamic feature alignment module (dfam) is designed to align prior features across modalities for the integrity of prior clinical information. prompted by the enriched prior features, the dynamic difference-aware module (ddam) captures favorable difference information by identifying relationships across exams. furthermore, our ddatr employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both rrg and lrrg tasks.",,2025-05-06,,"['shanshan song', 'hui tang', 'honglong yang', 'xiaomeng li']"
2505.03412,cxr-ad: component x-ray image dataset for industrial anomaly detection,cs.cv,"internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. however, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available x-ray datasets targeting internal defects in components. to address this gap, we construct the first publicly accessible component x-ray anomaly detection (cxr-ad) dataset, comprising real-world x-ray images. the dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. we systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in x-ray imaging, and (3) significant variations in defect scales and morphologies. to evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). experimental results demonstrate a 29.78% average performance degradation on cxr-ad compared to mvtec ad, highlighting the limitations of current algorithms in handling internal defect detection tasks. to the best of our knowledge, cxr-ad represents the first publicly available x-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.",,2025-05-06,,"['haoyu bai', 'jie wang', 'gaomin li', 'xuan li', 'xiaohu zhang', 'xia yang']"
2505.03422,liftfeat: 3d geometry-aware local feature matching,cs.cv cs.ro,"robust and efficient local feature matching plays a crucial role in applications such as slam and visual localization for robotics. despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. in this paper, we propose a new lightweight network called \textit{liftfeat}, which lifts the robustness of raw descriptor by aggregating 3d geometric feature. specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3d geometric feature in terms of predicted surface normal. we then design a 3d geometry-aware feature lifting module to fuse surface normal feature with raw 2d descriptor feature. integrating such 3d geometric feature enhances the discriminative ability of 2d feature description in extreme conditions. extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our liftfeat outperforms some lightweight state-of-the-art methods. code will be released at : https://github.com/lyp-deeplearning/liftfeat.",,2025-05-06,,"['yepeng liu', 'wenpeng lai', 'zhou zhao', 'yuxuan xiong', 'jinchi zhu', 'jun cheng', 'yongchao xu']"
2505.03426,phenotype-guided generative model for high-fidelity cardiac mri   synthesis: advancing pretraining and clinical applications,cs.cv cs.ai,"cardiac magnetic resonance (cmr) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. however, the limited availability of large-scale, high-quality cmr datasets poses a major challenge to the effective application of artificial intelligence (ai) in this domain. even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of ai models on downstream tasks. in this study, we present cardiac phenotype-guided cmr generation (cpgg), a novel approach for generating diverse cmr data that covers a wide spectrum of cardiac health status. the cpgg framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from cmr data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity cmr cine sequences that capture both structural and functional features of the heart in a fine-grained manner. we synthesized a massive amount of cmr to expand the pretraining data. experimental results show that cpgg generates high-quality synthetic cmr data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. these gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. code is availabel at https://anonymous.4open.science/r/cpgg.",,2025-05-06,,"['ziyu li', 'yujian hu', 'zhengyao ding', 'yiheng mao', 'haitao li', 'fan yi', 'hongkun zhang', 'zhengxing huang']"
2505.03431,a fusion-guided inception network for hyperspectral image   super-resolution,cs.cv,"the fusion of low-spatial-resolution hyperspectral images (hsis) with high-spatial-resolution conventional images (e.g., panchromatic or rgb) has played a significant role in recent advancements in hsi super-resolution. however, this fusion process relies on the availability of precise alignment between image pairs, which is often challenging in real-world scenarios. to mitigate this limitation, we propose a single-image super-resolution model called the fusion-guided inception network (fgin). specifically, we first employ a spectral-spatial fusion module to effectively integrate spectral and spatial information at an early stage. next, an inception-like hierarchical feature extraction strategy is used to capture multiscale spatial dependencies, followed by a dedicated multi-scale fusion block. to further enhance reconstruction quality, we incorporate an optimized upsampling module that combines bilinear interpolation with depthwise separable convolutions. experimental evaluations on two publicly available hyperspectral datasets demonstrate the competitive performance of our method.",,2025-05-06,,"['usman muhammad', 'jorma laaksonen']"
2505.03435,robustness in ai-generated detection: enhancing resistance to   adversarial attacks,cs.cv,"the rapid advancement of generative image technology has introduced significant security concerns, particularly in the domain of face generation detection. this paper investigates the vulnerabilities of current ai-generated face detection systems. our study reveals that while existing detection methods often achieve high accuracy under standard conditions, they exhibit limited robustness against adversarial attacks. to address these challenges, we propose an approach that integrates adversarial training to mitigate the impact of adversarial examples. furthermore, we utilize diffusion inversion and reconstruction to further enhance detection robustness. experimental results demonstrate that minor adversarial perturbations can easily bypass existing detection systems, but our method significantly improves the robustness of these systems. additionally, we provide an in-depth analysis of adversarial and benign examples, offering insights into the intrinsic characteristics of ai-generated content. all associated code will be made publicly available in a dedicated repository to facilitate further research and verification.",,2025-05-06,,"['sun haoxuan', 'hong yan', 'zhan jiahui', 'chen haoxing', 'lan jun', 'zhu huijia', 'wang weiqiang', 'zhang liqing', 'zhang jianfu']"
2505.03445,polar coordinate-based 2d pose prior with neural distance field,cs.cv,"human pose capture is essential for sports analysis, enabling precise evaluation of athletes' movements. while deep learning-based human pose estimation (hpe) models from rgb videos have achieved impressive performance on public datasets, their effectiveness in real-world sports scenarios is often hindered by motion blur, occlusions, and domain shifts across different pose representations. fine-tuning these models can partially alleviate such challenges but typically requires large-scale annotated data and still struggles to generalize across diverse sports environments. to address these limitations, we propose a 2d pose prior-guided refinement approach based on neural distance fields (ndf). unlike existing approaches that rely solely on angular representations of human poses, we introduce a polar coordinate-based representation that explicitly incorporates joint connection lengths, enabling a more accurate correction of erroneous pose estimations. additionally, we define a novel non-geodesic distance metric that separates angular and radial discrepancies, which we demonstrate is better suited for polar representations than traditional geodesic distances. to mitigate data scarcity, we develop a gradient-based batch-projection augmentation strategy, which synthesizes realistic pose samples through iterative refinement. our method is evaluated on a long jump dataset, demonstrating its ability to improve 2d pose estimation across multiple pose representations, making it robust across different domains. experimental results show that our approach enhances pose plausibility while requiring only limited training data. code is available at: https://github.com/qgan2019/polar-ndf.",,2025-05-06,,"['qi gan', 'sao mai nguyen', 'eric fenaux', 'stephan clémençon', 'mounîm el yacoubi']"
2505.03463,nonperiodic dynamic ct reconstruction using backward-warping inr with   regularization of diffeomorphism (bird),cs.cv physics.med-ph,"dynamic computed tomography (ct) reconstruction faces significant challenges in addressing motion artifacts, particularly for nonperiodic rapid movements such as cardiac imaging with fast heart rates. traditional methods struggle with the extreme limited-angle problems inherent in nonperiodic cases. deep learning methods have improved performance but face generalization challenges. recent implicit neural representation (inr) techniques show promise through self-supervised deep learning, but have critical limitations: computational inefficiency due to forward-warping modeling, difficulty balancing dvf complexity with anatomical plausibility, and challenges in preserving fine details without additional patient-specific pre-scans. this paper presents a novel inr-based framework, bird, for nonperiodic dynamic ct reconstruction. it addresses these challenges through four key contributions: (1) backward-warping deformation that enables direct computation of each dynamic voxel with significantly reduced computational cost, (2) diffeomorphism-based dvf regularization that ensures anatomically plausible deformations while maintaining representational capacity, (3) motion-compensated analytical reconstruction that enhances fine details without requiring additional pre-scans, and (4) dimensional-reduction design for efficient 4d coordinate encoding. through various simulations and practical studies, including digital and physical phantoms and retrospective patient data, we demonstrate the effectiveness of our approach for nonperiodic dynamic ct reconstruction with enhanced details and reduced motion artifacts. the proposed framework enables more accurate dynamic ct reconstruction with potential clinical applications, such as one-beat cardiac reconstruction, cinematic image sequences for functional imaging, and motion artifact reduction in conventional ct scans.",,2025-05-06,,"['muge du', 'zhuozhao zheng', 'wenying wang', 'guotao quan', 'wuliang shi', 'le shen', 'li zhang', 'liang li', 'yinong liu', 'yuxiang xing']"
2505.03470,blending 3d geometry and machine learning for multi-view stereopsis,cs.cv cs.ai cs.cg cs.lg,"traditional multi-view stereo (mvs) methods primarily depend on photometric and geometric consistency constraints. in contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3d geometry, applying explicit geometric consistency (gc) checks only as a post-processing step, with no impact on the learning process itself. in this work, we introduce gc mvsnet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see fig. 1). this integrated gc check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other mvs methods. furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. extensive experiments demonstrate that our approach achieves a new state of the art on the dtu and blendedmvs datasets and secures second place on the tanks and temples benchmark. to our knowledge, gc mvsnet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. our code is available.",,2025-05-06,,"['vibhas vats', 'md. alimoor reza', 'david crandall', 'soon-heung jung']"
2505.03494,upmad-net: a brain tumor segmentation network with uncertainty guidance   and adaptive multimodal feature fusion,cs.cv,"background: brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. objective: we propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. methods: the proposed method utilizes a multi-scale feature fusion (msff) module and adaptive attention mechanisms (aam) to extract multi-scale features and capture global contextual information. to enhance the model's robustness in low-confidence regions, the monte carlo dropout (mc dropout) strategy is employed for uncertainty estimation. results: extensive experiments demonstrate that the proposed method achieves superior performance on brain tumor segmentation (brats) datasets, significantly outperforming various state-of-the-art methods. on the brats2021 dataset, the test dice scores are 89.18% for enhancing tumor (et) segmentation, 93.67% for whole tumor (wt) segmentation, and 91.23% for tumor core (tc) segmentation. on the brats2019 validation set, the validation dice scores are 87.43%, 90.92%, and 90.40% for et, wt, and tc segmentation, respectively. ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. conclusion: this study proposed a novel 3d brain tumor segmentation network based on the u-net architecture. by incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. the code for the proposed method is available at https://github.com/chenzhao2023/upmad_net_brainseg.",,2025-05-06,,"['zhanyuan jia', 'ni yao', 'danyang sun', 'chuang han', 'yanting li', 'jiaofen nan', 'fubao zhu', 'chen zhao', 'weihua zhou']"
2505.03498,mri motion correction via efficient residual-guided denoising diffusion   probabilistic models,cs.cv physics.med-ph,"purpose: motion artifacts in magnetic resonance imaging (mri) significantly degrade image quality and impair quantitative analysis. conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. this study introduces res-mocodiff, an efficient denoising diffusion probabilistic model tailored for mri motion artifact correction. methods: res-mocodiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. a u-net backbone enhanced with swin-transformer blocks conventional attention layers, improving adaptability across resolutions. training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. res-mocodiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. comparative analyses were conducted against established methods, including cyclegan, pix2pix, and mt-ddpm using quantitative metrics such as peak signal-to-noise ratio (psnr), structural similarity index measure (ssim), and normalized mean squared error (nmse). results: the proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. res-mocodiff consistently achieved the highest ssim and the lowest nmse values, with a psnr of up to 41.91+-2.94 db for minor distortions. notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.",,2025-05-06,,"['mojtaba safari', 'shansong wang', 'qiang li', 'zach eidex', 'richard l. j. qiu', 'chih-wei chang', 'hui mao', 'xiaofeng yang']"
2505.03507,modality-guided dynamic graph fusion and temporal diffusion for   self-supervised rgb-t tracking,cs.cv,"to reduce the reliance on large-scale annotations, self-supervised rgb-t tracking approaches have garnered significant attention. however, the omission of the object region by erroneous pseudo-label or the introduction of background noise affects the efficiency of modality fusion, while pseudo-label noise triggered by similar object noise can further affect the tracking performance. in this paper, we propose gdstrack, a novel approach that introduces dynamic graph fusion and temporal diffusion to address the above challenges in self-supervised rgb-t tracking. gdstrack dynamically fuses the modalities of neighboring frames, treats them as distractor noise, and leverages the denoising capability of a generative model. specifically, by constructing an adjacency matrix via an adjacency matrix generator (amg), the proposed modality-guided dynamic graph fusion (mdgf) module uses a dynamic adjacency matrix to guide graph attention, focusing on and fusing the object's coherent regions. temporal graph-informed diffusion (tgid) models mdgf features from neighboring frames as interference, and thus improving robustness against similar-object noise. extensive experiments conducted on four public rgb-t tracking datasets demonstrate that gdstrack outperforms the existing state-of-the-art methods. the source code is available at https://github.com/lishenglana/gdstrack.",,2025-05-06,,"['shenglan li', 'rui yao', 'yong zhou', 'hancheng zhu', 'kunyang sun', 'bing liu', 'zhiwen shao', 'jiaqi zhao']"
2505.03510,from neurons to computation: biological reservoir computing for pattern   recognition,cs.ne cs.ai cs.cv,"in this paper, we introduce a novel paradigm for reservoir computing (rc) that leverages a pool of cultured biological neurons as the reservoir substrate, creating a biological reservoir computing (brc). this system operates similarly to an echo state network (esn), with the key distinction that the neural activity is generated by a network of cultured neurons, rather than being modeled by traditional artificial computational units. the neuronal activity is recorded using a multi-electrode array (mea), which enables high-throughput recording of neural signals. in our approach, inputs are introduced into the network through a subset of the mea electrodes, while the remaining electrodes capture the resulting neural activity. this generates a nonlinear mapping of the input data to a high-dimensional biological feature space, where distinguishing between data becomes more efficient and straightforward, allowing a simple linear classifier to perform pattern recognition tasks effectively. to evaluate the performance of our proposed system, we present an experimental study that includes various input patterns, such as positional codes, bars with different orientations, and a digit recognition task. the results demonstrate the feasibility of using biological neural networks to perform tasks traditionally handled by artificial neural networks, paving the way for further exploration of biologically-inspired computing systems, with potential applications in neuromorphic engineering and bio-hybrid computing.",,2025-05-06,,"['ludovico iannello', 'luca ciampi', 'gabriele lagani', 'fabrizio tonelli', 'eleonora crocco', 'lucio maria calcagnile', 'angelo di garbo', 'federico cremisi', 'giuseppe amato']"
2505.03522,optimization of module transferability in single image super-resolution:   universality assessment and cycle residual blocks,cs.cv cs.ai,"deep learning has substantially advanced the single image super-resolution (sisr). however, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. in this paper, we introduce the concept of ""universality"" and its associated definitions which extend the traditional notion of ""generalization"" to encompass the modules' ease of transferability, thus revealing the relationships between module universality and model generalizability. then we propose the universality assessment equation (uae), a metric for quantifying how readily a given module could be transplanted across models. guided by the uae results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, cycle residual block (crb) and depth-wise cycle residual block (dcrb). through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, extreme-industrial imagery and on-device deployments, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a psnr enhancement of up to 0.83db or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity.",,2025-05-06,,"['haotong cheng', 'zhiqi zhang', 'hao li', 'xinshang zhang']"
2505.03528,coop-wd: cooperative perception with weighting and denoising for robust   v2v communication,cs.cv,"cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (v2v) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. existing works have explored the effects of v2v communication impairments on perception precision, but they lack generalization to different levels of impairments. in this work, we propose a joint weighting and denoising framework, coop-wd, to enhance cooperative perception subject to v2v channel impairments. in this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. an efficient variant model, coop-wd-eco, is proposed to selectively deactivate denoising to reduce processing overhead. rician fading, non-stationarity, and time-varying distortion are considered. simulation results demonstrate that the proposed coop-wd outperforms conventional benchmarks in all types of channels. qualitative analysis with visual examples further proves the superiority of our proposed method. the proposed coop-wd-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.",,2025-05-06,,"['chenguang liu', 'jianjun chen', 'yunfei chen', 'yubei he', 'zhuangkun wei', 'hongjian sun', 'haiyan lu', 'qi hao']"
2505.03538,rail: region-aware instructive learning for semi-supervised tooth   segmentation in cbct,cs.cv,"semi-supervised learning has become a compelling approach for 3d tooth segmentation from cbct scans, where labeled data is minimal. however, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. to address these problems, we propose region-aware instructive learning (rail), a dual-group dual-student, semi-supervised framework. each group contains two student models guided by a shared teacher network. by alternating training between the two groups, rail promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. specifically, rail introduces two instructive mechanisms. disagreement-focused supervision (dfs) controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. in the unsupervised phase, confidence-aware learning (cal) modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. this helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. extensive experiments on four cbct tooth segmentation datasets show that rail surpasses state-of-the-art methods under limited annotation. our code will be available at https://github.com/tournesol-saturday/rail.",,2025-05-06,,"['chuyu zhao', 'hao huang', 'jiashuo guo', 'ziyu shen', 'zhongwei zhou', 'jie liu', 'zekuan yu']"
2505.03539,panoramic out-of-distribution segmentation,cs.cv cs.ro eess.iv,"panoramic imaging enables capturing 360{\deg} images with an ultra-wide field-of-view (fov) for dense omnidirectional perception. however, current panoramic semantic segmentation methods fail to identify outliers, and pinhole out-of-distribution segmentation (oos) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. to address these issues, we introduce a new task, panoramic out-of-distribution segmentation (panoos), achieving oos for panoramas. furthermore, we propose the first solution, pos, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. specifically, pos integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of clip. the proposed prompt-based restoration attention (pra) optimizes semantic decoding by prompt guidance and self-adaptive correction, while bilevel prompt distribution learning (bpdl) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. besides, to compensate for the scarcity of panoos datasets, we establish two benchmarks: denseoos, which features diverse outliers in complex environments, and quadoos, captured by a quadruped robot with a panoramic annular lens system. extensive experiments demonstrate superior performance of pos, with auprc improving by 34.25% and fpr95 decreasing by 21.42% on denseoos, outperforming state-of-the-art pinhole-oos methods. moreover, pos achieves leading closed-set segmentation capabilities. code and datasets will be available at https://github.com/mengfeid/panoos.",,2025-05-06,,"['mengfei duan', 'kailun yang', 'yuheng zhang', 'yihong cao', 'fei teng', 'kai luo', 'jiaming zhang', 'zhiyong li', 'shutao li']"
2505.03554,read my ears! horse ear movement detection for equine affective state   assessment,cs.cv,"the equine facial action coding system (equifacs) enables the systematic annotation of facial movements through distinct action units (aus). it serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. however, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial aus is both time-consuming and costly. to address this challenge, automated annotation systems are essential for leveraging existing datasets and improving affective states detection tools. in this work, we study different methods for specific ear au detection and localization from horse videos. we leverage past works on deep learning-based video feature extraction combined with recurrent neural networks for the video classification task, as well as a classic optical flow based approach. we achieve 87.5% classification accuracy of ear movement presence on a public horse video dataset, demonstrating the potential of our approach. we discuss future directions to develop these systems, with the aim of bridging the gap between automated au detection and practical applications in equine welfare and veterinary diagnostics. our code will be made publicly available at https://github.com/jmalves5/read-my-ears.",,2025-05-06,,"['joão alves', 'pia haubro andersen', 'rikke gade']"
2505.03557,generating synthetic data via augmentations for improved facial   resemblance in dreambooth and instantid,cs.cv cs.ai,"the personalization of stable diffusion for generating professional portraits from amateur photographs is a burgeoning area, with applications in various downstream contexts. this paper investigates the impact of augmentations on improving facial resemblance when using two prominent personalization techniques: dreambooth and instantid. through a series of experiments with diverse subject datasets, we assessed the effectiveness of various augmentation strategies on the generated headshots' fidelity to the original subject. we introduce facedistance, a wrapper around facenet, to rank the generations based on facial similarity, which aided in our assessment. ultimately, this research provides insights into the role of augmentations in enhancing facial resemblance in sdxl-generated portraits, informing strategies for their effective deployment in downstream applications.",,2025-05-06,,"['koray ulusan', 'benjamin kiefer']"
2505.03562,real-time person image synthesis using a flow matching model,cs.cv cs.ai,"pose-guided person image synthesis (pgpis) generates realistic person images conditioned on a target pose and a source image. this task plays a key role in various real-world applications, such as sign language video generation, ar/vr, gaming, and live streaming. in these scenarios, real-time pgpis is critical for providing immediate visual feedback and maintaining user immersion.however, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-fidelity images from diverse and dynamic human poses. recent diffusion-based methods have shown impressive image quality in pgpis, but their slow sampling speeds hinder deployment in time-sensitive applications. this latency is particularly problematic in tasks like generating sign language videos during live broadcasts, where rapid image updates are required. therefore, developing a fast and reliable pgpis model is a crucial step toward enabling real-time interactive systems. to address this challenge, we propose a generative model based on flow matching (fm). our approach enables faster, more stable, and more efficient training and sampling. furthermore, the proposed model supports conditional generation and can operate in latent space, making it especially suitable for real-time pgpis applications where both speed and quality are critical. we evaluate our proposed method, real-time person image synthesis using a flow matching model (rpfm), on the widely used deepfashion dataset for pgpis tasks. our results show that rpfm achieves near-real-time sampling speeds while maintaining performance comparable to the state-of-the-art models. our methodology trades off a slight, acceptable decrease in generated-image accuracy for over a twofold increase in generation speed, thereby ensuring real-time performance.",,2025-05-06,,"['jiwoo jeong', 'kirok kim', 'wooju kim', 'nam-joon kim']"
2505.03567,uncertainty-aware prototype semantic decoupling for text-based person   search in full images,cs.cv,"text-based pedestrian search (tbps) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. however, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. to address this, we propose upd-tbps, a novel framework comprising three modules: multi-granularity uncertainty estimation (mue), prototype-based uncertainty decoupling (pud), and cross-modal re-identification (reid). mue conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. pud leverages visual context decoupling and prototype mining to extract features of the target pedestrian described in the query. it separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. reid evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. experiments on cuhk-sysu-tbps and prw-tbps datasets validate the effectiveness of our framework.",,2025-05-06,2025-05-06,"['zengli luo', 'canlong zhang', 'xiaochun lu', 'zhixin li', 'zhiwen wang']"
2505.03569,corner cases: how size and position of objects challenge   imagenet-trained models,cs.cv,"backgrounds in images play a major role in contributing to spurious correlations among different data points. owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. in this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. to better illustrate our findings, we propose a synthetic dataset derived from imagenet1k, hard-spurious-imagenet, which contains images with various backgrounds, object positions, and object sizes. by evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (roi) to image ratio is small and the object is far from the center of the image. moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change.",,2025-05-06,,"['mishal fatima', 'steffen jung', 'margret keuper']"
2505.03575,supervised and unsupervised textile classification via near-infrared   hyperspectral imaging and deep learning,cs.cv physics.app-ph,"recycling textile fibers is critical to reducing the environmental impact of the textile industry. hyperspectral near-infrared (nir) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. in this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. we show that optimized convolutional neural networks (cnns) and autoencoder networks achieve robust generalization under varying conditions. these results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.",10.5445/ksp/1000178356,2025-05-06,,"['maria kainz', 'johannes k. krondorfer', 'malte jaschik', 'maria jernej', 'harald ganster']"
2505.03581,dygenc: encoding a sequence of textual scene graphs to reason and answer   questions in dynamic scenes,cs.cv,"the analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. current approaches predominantly utilize visual models. however, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. to address this issue we introduce dygenc - a novel method for encoding a dynamic graph. this method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. the purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. extended evaluations on the star and agqa datasets indicate that dygenc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. we hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. code is available at github.com/linukc/dygenc.",,2025-05-06,,"['sergey linok', 'vadim semenov', 'anastasia trunova', 'oleg bulichev', 'dmitry yudin']"
2505.03597,fixed-length dense fingerprint representation,cs.cv,"fixed-length fingerprint representations, which map each fingerprint to a compact and fixed-size feature vector, are computationally efficient and well-suited for large-scale matching. however, designing a robust representation that effectively handles diverse fingerprint modalities, pose variations, and noise interference remains a significant challenge. in this work, we propose a fixed-length dense descriptor of fingerprints, and introduce flare-a fingerprint matching framework that integrates the fixed-length dense descriptor with pose-based alignment and robust enhancement. this fixed-length representation employs a three-dimensional dense descriptor to effectively capture spatial relationships among fingerprint ridge structures, enabling robust and locally discriminative representations. to ensure consistency within this dense feature space, flare incorporates pose-based alignment using complementary estimation methods, along with dual enhancement strategies that refine ridge clarity while preserving the original fingerprint modality. the proposed dense descriptor supports fixed-length representation while maintaining spatial correspondence, enabling fast and accurate similarity computation. extensive experiments demonstrate that flare achieves superior performance across rolled, plain, latent, and contactless fingerprints, significantly outperforming existing methods in cross-modality and low-quality scenarios. further analysis validates the effectiveness of the dense descriptor design, as well as the impact of alignment and enhancement modules on the accuracy of dense descriptor matching. experimental results highlight the effectiveness and generalizability of flare as a unified and scalable solution for robust fingerprint representation and matching. the implementation and code will be publicly available at https://github.com/yu-yy/flare.",,2025-05-06,,"['zhiyu pan', 'xiongjun guan', 'yongjie duan', 'jianjiang feng', 'jie zhou']"
2505.03599,from pixels to polygons: a survey of deep learning approaches for   medical image-to-mesh reconstruction,cs.cv,"deep learning-based medical image-to-mesh reconstruction has rapidly evolved, enabling the transformation of medical imaging data into three-dimensional mesh models that are critical in computational medicine and in silico trials for advancing our understanding of disease mechanisms, and diagnostic and therapeutic techniques in modern medicine. this survey systematically categorizes existing approaches into four main categories: template models, statistical models, generative models, and implicit models. each category is analysed in detail, examining their methodological foundations, strengths, limitations, and applicability to different anatomical structures and imaging modalities. we provide an extensive evaluation of these methods across various anatomical applications, from cardiac imaging to neurological studies, supported by quantitative comparisons using standard metrics. additionally, we compile and analyze major public datasets available for medical mesh reconstruction tasks and discuss commonly used evaluation metrics and loss functions. the survey identifies current challenges in the field, including requirements for topological correctness, geometric accuracy, and multi-modality integration. finally, we present promising future research directions in this domain. this systematic review aims to serve as a comprehensive reference for researchers and practitioners in medical image analysis and computational medicine.",,2025-05-06,,"['fengming lin', 'arezoo zakeri', 'yidan xue', 'michael macraild', 'haoran dou', 'zherui zhou', 'ziwei zou', 'ali sarrami-foroushani', 'jinming duan', 'alejandro f. frangi']"
2505.03610,learning knowledge-based prompts for robust 3d mask presentation attack   detection,cs.cv,"3d mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3d mask attacks. while most existing methods utilize multimodal features or remote photoplethysmography (rppg) signals to distinguish between real faces and 3d masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. detection-related text descriptions offer concise, universal information and are cost-effective to obtain. however, the potential of vision-language multimodal features for 3d mask presentation attack detection remains unexplored. in this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3d mask presentation attack detection. specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. during training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.",,2025-05-06,,"['fangling jiang', 'qi li', 'bing liu', 'weining wang', 'caifeng shan', 'zhenan sun', 'ming-hsuan yang']"
2505.03611,learning unknown spoof prompts for generalized face anti-spoofing using   only real face images,cs.cv,"face anti-spoofing is a critical technology for ensuring the security of face recognition systems. however, its ability to generalize across diverse scenarios remains a significant challenge. in this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. to address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model's ability to generalize to unseen target domains. specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. this framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images.",,2025-05-06,,"['fangling jiang', 'qi li', 'weining wang', 'wei shen', 'bing liu', 'zhenan sun']"
2505.03621,physllm: harnessing large language models for cross-modal remote   physiological sensing,cs.cv,"remote photoplethysmography (rppg) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. large language models (llms) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rppg signals due to their text-centric design. to bridge this gap, we introduce physllm, a collaborative optimization framework that synergizes llms with domain-specific rppg components. specifically, the text prototype guidance (tpg) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into llm-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. besides, a novel dual-domain stationary (dds) algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. finally, rppg task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. evaluation on four benchmark datasets, physllm achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.",,2025-05-06,,"['yiping xie', 'bo zhao', 'mingtong dai', 'jian-ping zhou', 'yue sun', 'tao tan', 'weicheng xie', 'linlin shen', 'zitong yu']"
2505.03623,bounding box-guided diffusion for synthesizing industrial images and   segmentation map,cs.cv,"synthetic dataset generation in computer vision, particularly for industrial applications, is still underexplored. industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. to address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. we introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. the code is publicly available at https://github.com/covisionlab/diffusion_labeling.",,2025-05-06,,"['alessandro simoni', 'francesco pelosin']"
2505.03631,breaking annotation barriers: generalized video quality assessment via   ranking-based self-supervision,cs.cv,"video quality assessment (vqa) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. while recent supervised vqa models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. to bridge this gap, we introduce a self-supervised learning framework for vqa to learn quality assessment capabilities from large-scale, unlabeled web videos. our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (lmm) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing vqa models and relative quality ranking based on synthetic distortion simulations. furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. by training on a dataset $10\times$ larger than the existing vqa benchmarks, our model: (1) achieves zero-shot performance on in-domain vqa benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (ood) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. extensive experimental results validate the effectiveness of our self-supervised approach in training generalized vqa models. the datasets and code will be publicly released to facilitate future research.",,2025-05-06,2025-05-07,"['linhan cao', 'wei sun', 'kaiwei zhang', 'yicong peng', 'guangtao zhai', 'xiongkuo min']"
2505.03638,towards smart point-and-shoot photography,cs.cv,"hundreds of millions of people routinely take photos using their smartphones as point and shoot (pas) cameras, yet very few would have the photography skills to compose a good shot of a scene. while traditional pas cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. in this paper, we present a first of its kind smart point and shoot (spas) system to help users to take good photos. our spas proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. we first constructed a large dataset containing 320k images with camera pose information from 4000 scenes. we then developed an innovative clip-based composition quality assessment (ccqa) model to assign pseudo labels to these images. the ccqa introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. and finally we have developed a camera pose adjustment model (cpam) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. the two tasks of cpam make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the cpam in an end-to-end manner. we will present extensive results to demonstrate the performances of our spas system using publicly available image composition datasets.",,2025-05-06,,"['jiawan li', 'fei zhou', 'zhipeng zhong', 'jiongzhi lin', 'guoping qiu']"
2505.03646,alma: aggregated lipschitz maximization attack on auto-encoders,cs.lg cs.ai cs.cv,"despite the extensive use of deep autoencoders (aes) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. ae robustness is characterized by the lipschitz bounds of its components. existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in aes. in the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. to address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local lipschitz bounds by enhancing loss gradient information propagation during attack optimization. we demonstrate through extensive experiments on state-of-the-art aes that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. as a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.",,2025-05-06,,"['chethan krishnamurthy ramanaik', 'arjun roy', 'eirini ntoutsi']"
2505.03662,revolutionizing brain tumor imaging: generating synthetic 3d fa maps   from t1-weighted mri using cyclegan models,cs.cv cs.ai,"fractional anisotropy (fa) and directionally encoded colour (dec) maps are essential for evaluating white matter integrity and structural connectivity in neuroimaging. however, the spatial misalignment between fa maps and tractography atlases hinders their effective integration into predictive models. to address this issue, we propose a cyclegan based approach for generating fa maps directly from t1-weighted mri scans, representing the first application of this technique to both healthy and tumour-affected tissues. our model, trained on unpaired data, produces high fidelity maps, which have been rigorously evaluated using structural similarity index (ssim) and peak signal-to-noise ratio (psnr), demonstrating particularly robust performance in tumour regions. radiological assessments further underscore the model's potential to enhance clinical workflows by providing an ai-driven alternative that reduces the necessity for additional scans.",,2025-05-06,,"['xin du', 'francesca m. cozzi', 'rajesh jena']"
2505.03667,distribution-conditional generation: from class distribution to creative   generation,cs.cv,"text-to-image (t2i) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose distribution-conditional generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. building on this, we propose distok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. distok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. to enforce distributional consistency, latent vectors sampled from a gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. the resulting tokens are added to the concept pool for subsequent composition. extensive experiments demonstrate that distok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores.",,2025-05-06,,"['fu feng', 'yucheng xie', 'xu yang', 'jing wang', 'xin geng']"
2505.03679,caraffusion: improving 2d semantic segmentation with camera-radar point   cloud fusion and zero-shot image inpainting,cs.cv,"segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. in contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. therefore, a promising approach is to fuse information from both sensors. in this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. we leverage radar point features to create pseudo-masks using the segment-anything model, treating the projected radar points as point prompts. additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. our method improves the camera-only segmentation baseline by 2.63% in miou and enhances our camera-radar fusion architecture by 1.48% in miou on the waterscenes dataset. this demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.",,2025-05-06,,"['huawei sun', 'bora kunter sahin', 'georg stettinger', 'maximilian bernhard', 'matthias schubert', 'robert wille']"
2505.03692,matching distance and geometric distribution aided learning multiview   point cloud registration,cs.cv cs.ro,"multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. this paper concentrates on pose graph construction and motion synchronization within multiview registration. previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. to identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. for motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. the source code is available at https://github.com/shi-qi-li/mdgd.",10.1109/lra.2024.3455783,2025-05-06,,"['shiqi li', 'jihua zhu', 'yifan xie', 'naiwen hu', 'di wang']"
2505.03703,fill the gap: quantifying and reducing the modality gap in image-text   representation learning,cs.cv cs.lg,"vision-language models (vlms) allow to embed texts and images in a shared representation space. however, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. while this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. we therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. our code is available at the url provided in the paper's abstract.",,2025-05-06,,"['françois role', 'sébastien meyer', 'victor amblard']"
2505.03715,disarm++: beyond scanner-free harmonization,cs.cv,"harmonization of t1-weighted mr images across different scanners is crucial for ensuring consistency in neuroimaging studies. this study introduces a novel approach to direct image harmonization, moving beyond feature standardization to ensure that extracted features remain inherently reliable for downstream analysis. our method enables image transfer in two ways: (1) mapping images to a scanner-free space for uniform appearance across all scanners, and (2) transforming images into the domain of a specific scanner used in model training, embedding its unique characteristics. our approach presents strong generalization capability, even for unseen scanners not included in the training phase. we validated our method using mr images from diverse cohorts, including healthy controls, traveling subjects, and individuals with alzheimer's disease (ad). the model's effectiveness is tested in multiple applications, such as brain age prediction (r2 = 0.60 \pm 0.05), biomarker extraction, ad classification (test accuracy = 0.86 \pm 0.03), and diagnosis prediction (auc = 0.95). in all cases, our harmonization technique outperforms state-of-the-art methods, showing improvements in both reliability and predictive accuracy. moreover, our approach eliminates the need for extensive preprocessing steps, such as skull-stripping, which can introduce errors by misclassifying brain and non-brain structures. this makes our method particularly suitable for applications that require full-head analysis, including research on head trauma and cranial deformities. additionally, our harmonization model does not require retraining for new datasets, allowing smooth integration into various neuroimaging workflows. by ensuring scanner-invariant image quality, our approach provides a robust and efficient solution for improving neuroimaging studies across diverse settings. the code is available at this link.",,2025-05-06,,"['luca caldera', 'lara cavinato', 'alessio cirone', 'isabella cama', 'sara garbarino', 'raffaele lodi', 'fabrizio tagliavini', 'anna nigri', 'silvia de francesco', 'andrea cappozzo', 'michele piana', 'francesca ieva']"
2505.03730,flexiact: towards flexible action control in heterogeneous scenarios,cs.cv cs.ai cs.mm,"action customization involves generating videos where the subject performs actions dictated by input control signals. current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. to overcome these limitations, we propose flexiact, which transfers actions from a reference video to an arbitrary target image. unlike existing methods, flexiact allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. achieving this requires precise action control, spatial structure adaptation, and consistency preservation. to this end, we introduce refadapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. so we propose fae (frequency-aware action extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. we release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/flexiact/",,2025-05-06,,"['shiyi zhang', 'junhao zhuang', 'zhaoyang zhang', 'ying shan', 'yansong tang']"
2505.03735,multi-agent system for comprehensive soccer understanding,cs.cv,"recent advancements in ai-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. to bridge this gap, we propose a comprehensive framework for holistic soccer understanding. specifically, we make the following contributions in this paper: (i) we construct soccerwiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present soccerbench, the largest and most comprehensive soccer-specific benchmark, featuring around 10k standardized multimodal (text, image, video) multi-choice qa pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce socceragent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from soccerwiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art mllms on soccerbench, highlighting the superiority of our proposed agentic system. all data and code are publicly available at: https://jyrao.github.io/socceragent/.",,2025-05-06,,"['jiayuan rao', 'zifeng li', 'haoning wu', 'ya zhang', 'yanfeng wang', 'weidi xie']"
2505.03757,on the residual-based neural network for unmodeled distortions in   coordinate transformation,physics.geo-ph cs.cv cs.lg stat.ap stat.ml,"coordinate transformation models often fail to account for nonlinear and spatially dependent distortions, leading to significant residual errors in geospatial applications. here we propose a residual-based neural correction strategy, in which a neural network learns to model only the systematic distortions left by an initial geometric transformation. by focusing solely on residual patterns, the proposed method reduces model complexity and improves performance, particularly in scenarios with sparse or structured control point configurations. we evaluate the method using both simulated datasets with varying distortion intensities and sampling strategies, as well as under the real-world image georeferencing tasks. compared with direct neural network coordinate converter and classical transformation models, the residual-based neural correction delivers more accurate and stable results under challenging conditions, while maintaining comparable performance in ideal cases. these findings demonstrate the effectiveness of residual modelling as a lightweight and robust alternative for improving coordinate transformation accuracy.",,2025-04-19,,"['vinicius francisco rofatto', 'luiz felipe rodrigues de almeida', 'marcelo tomio matsuoka', 'ivandro klein', 'mauricio roberto veronez', 'luiz gonzaga da silveira junior']"
2505.03788,calibrating uncertainty quantification of multi-modal llms using   grounding,cs.cl cs.ai cs.cv,"we introduce a novel approach for calibrating uncertainty quantification (uq) tailored for multi-modal large language models (llms). existing state-of-the-art uq methods rely on consistency among multiple responses generated by the llm on an input query under diverse settings. however, these approaches often report higher confidence in scenarios where the llm is consistently incorrect. this leads to a poorly calibrated confidence with respect to accuracy. to address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. specifically, we ground the textual responses to the visual inputs. the confidence from the grounding model is used to calibrate the overall confidence. given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. we evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (slake) and visual question answering (vqav2), considering multi-modal models such as llava-med and llava. the experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.",,2025-04-30,,"['trilok padhi', 'ramneet kaur', 'adam d. cobb', 'manoj acharya', 'anirban roy', 'colin samplawski', 'brian matejek', 'alexander m. berenbeim', 'nathaniel d. bastian', 'susmit jha']"
2505.03800,design description of wisdom computing persperctive,cs.ai cs.cv cs.hc,"this course design aims to develop and research a handwriting matrix recognition and step-by-step visual calculation process display system, addressing the issue of abstract formulas and complex calculation steps that students find difficult to understand when learning mathematics. by integrating artificial intelligence with visualization animation technology, the system enhances precise recognition of handwritten matrix content through the introduction of mamba backbone networks, completes digital extraction and matrix reconstruction using the yolo model, and simultaneously combines coordattention coordinate attention mechanisms to improve the accurate grasp of character spatial positions. the calculation process is demonstrated frame by frame through the manim animation engine, vividly showcasing each mathematical calculation step, helping students intuitively understand the intrinsic logic of mathematical operations. through dynamically generating animation processes for different computational tasks, the system exhibits high modularity and flexibility, capable of generating various mathematical operation examples in real-time according to student needs. by innovating human-computer interaction methods, it brings mathematical calculation processes to life, helping students bridge the gap between knowledge and understanding on a deeper level, ultimately achieving a learning experience where ""every step is understood."" the system's scalability and interactivity make it an intuitive, user-friendly, and efficient auxiliary tool in education.",,2025-05-02,,['tianyi yu']
2505.03807,facilitating video story interaction with multi-agent collaborative   system,cs.hc cs.ai cs.cv cs.ma,"video story interaction enables viewers to engage with and explore narrative content for personalized experiences. however, existing methods are limited to user selection, specially designed narratives, and lack customization. to address this, we propose an interactive system based on user intent. our system uses a vision language model (vlm) to enable machines to understand video stories, combining retrieval-augmented generation (rag) and a multi-agent system (mas) to create evolving characters and scene experiences. it includes three stages: 1) video story processing, utilizing vlm and prior knowledge to simulate human understanding of stories across three modalities. 2) multi-space chat, creating growth-oriented characters through mas interactions based on user queries and story stages. 3) scene customization, expanding and visualizing various story scenes mentioned in dialogue. applied to the harry potter series, our study shows the system effectively portrays emergent character social behavior and growth, enhancing the interactive experience in the video story world.",,2025-05-02,,"['yiwen zhang', 'jianing hao', 'zhan wang', 'hongling sheng', 'wei zeng']"
2505.03808,"ai-driven multi-source data fusion for algal bloom severity   classification in small inland water bodies: leveraging sentinel-2, dem, and   noaa climate data",cs.lg cs.cv physics.ao-ph,"harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. this research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. key data sources include copernicus sentinel-2 optical imagery, the copernicus digital elevation model (dem), and noaa's high-resolution rapid refresh (hrrr) climate data, all efficiently retrieved using platforms like google earth engine (gee) and microsoft planetary computer (mpc). the nir and two swir bands from sentinel-2, the altitude from the elevation model, the temperature and wind from noaa as well as the longitude and latitude were the most important features. the approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. while the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. the method leverages high-resolution satellite imagery and ai-driven analysis to monitor algal blooms dynamically, and although initially developed for a nasa competition in the u.s., it shows potential for global application. the complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and ai to address critical environmental challenges (https://github.com/ioannisnasios/harmfulalgalbloomdetection).",,2025-05-02,,['ioannis nasios']
2505.03821,beyond recognition: evaluating visual perspective taking in vision   language models,cs.cv cs.ai,"we investigate the ability of vision language models (vlms) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. by systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. our evaluation of several state-of-the-art models, including gpt-4-turbo, gpt-4o, llama-3.2-11b-vision-instruct, and variants of claude sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future vlm development.",,2025-05-02,,"['gracjan góral', 'alicja ziarko', 'piotr miłoś', 'michał nauman', 'maciej wołczyk', 'michał kosiński']"
2505.03826,in-situ and non-contact etch depth prediction in plasma etching via   machine learning (ann & bnn) and digital image colorimetry,cs.cv cs.ai,"precise monitoring of etch depth and the thickness of insulating materials, such as silicon dioxide and silicon nitride, is critical to ensuring device performance and yield in semiconductor manufacturing. while conventional ex-situ analysis methods are accurate, they are constrained by time delays and contamination risks. to address these limitations, this study proposes a non-contact, in-situ etch depth prediction framework based on machine learning (ml) techniques. two scenarios are explored. in the first scenario, an artificial neural network (ann) is trained to predict average etch depth from process parameters, achieving a significantly lower mean squared error (mse) compared to a linear baseline model. the approach is then extended to incorporate variability from repeated measurements using a bayesian neural network (bnn) to capture both aleatoric and epistemic uncertainty. coverage analysis confirms the bnn's capability to provide reliable uncertainty estimates. in the second scenario, we demonstrate the feasibility of using rgb data from digital image colorimetry (dic) as input for etch depth prediction, achieving strong performance even in the absence of explicit process parameters. these results suggest that the integration of dic and ml offers a viable, cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to enhanced process stability, and manufacturing efficiency.",,2025-05-03,,"['minji kang', 'seongho kim', 'eunseo go', 'donghyeon paek', 'geon lim', 'muyoung kim', 'soyeun kim', 'sung kyu jang', 'min sup choi', 'woo seok kang', 'jaehyun kim', 'jaekwang kim', 'hyeong-u kim']"
2505.03829,videollm benchmarks and evaluation: a survey,cs.cv cs.ai,"the rapid development of large language models (llms) has catalyzed significant advancements in video understanding technologies. this survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for video large language models (videollms). we examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. the paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. we highlight the performance trends of state-of-the-art videollms across these benchmarks and identify key challenges in current evaluation frameworks. additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. this survey aims to equip researchers with a structured understanding of how to effectively evaluate videollms and identify promising avenues for advancing the field of video understanding with large language models.",,2025-05-03,,['yogesh kumar']
2505.03832,video forgery detection for surveillance cameras: a review,cs.cv cs.ai,"the widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. surveillance footage plays a crucial role in security, law enforcement, and judicial processes. however, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. this paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. the findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.",,2025-05-04,,"['noor b. tayfor', 'tarik a. rashid', 'shko m. qader', 'bryar a. hassan', 'mohammed h. abdalla', 'jafar majidpour', 'aram m. ahmed', 'hussein m. ali', 'aso m. aladdin', 'abdulhady a. abdullah', 'ahmed s. shamsaldin', 'haval m. sidqi', 'abdulrahman salih', 'zaher m. yaseen', 'azad a. ameen', 'janmenjoy nayak', 'mahmood yashar hamza']"
2505.03833,pointexplainer: towards transparent parkinson's disease diagnosis,cs.cv cs.ai cs.lg,"deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of parkinson's disease. however, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. in this paper, we propose pointexplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. specifically, pointexplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3d point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. we also introduce consistency measures to further address the issue of faithfulness in explanations. extensive experiments on two benchmark datasets and a newly constructed dataset show that pointexplainer can provide intuitive explanations with no diagnostic performance degradation. the source code is available at https://github.com/chaoxuewang/pointexplainer.",,2025-05-04,,"['xuechao wang', 'sven nomm', 'junqing huang', 'kadri medijainen', 'aaro toomela', 'michael ruzhansky']"
2505.03836,obd-finder: explainable coarse-to-fine text-centric oracle bone   duplicates discovery,cs.ir cs.ai cs.cv,"oracle bone inscription (obi) is the earliest systematic writing system in china, while the identification of oracle bone (ob) duplicates is a fundamental issue in obi research. in this work, we design a progressive ob duplicate discovery framework that combines unsupervised low-level keypoints matching with high-level text-centric content-based matching to refine and rank the candidate ob duplicates with semantic awareness and interpretability. we compare our approach with state-of-the-art content-based image retrieval and image matching methods, showing that our approach yields comparable recall performance and the highest simplified mean reciprocal rank scores for both top-5 and top-15 retrieval results, and with significantly accelerated computation efficiency. we have discovered over 60 pairs of new ob duplicates in real-world deployment, which were missed by obi researchers for decades. the models, video illustration and demonstration of this work are available at: https://github.com/cszhanglmu/obd-finder/.",,2025-05-04,,"['chongsheng zhang', 'shuwen wu', 'yingqi chen', 'matthias aßenmacher', 'christian heumann', 'yi men', 'gaojuan fan', 'joão gama']"
2505.03837,explainable face recognition via improved localization,cs.cv cs.ai,"biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. face is the most common form of biometric modality that has proven effective. deep learning-based face recognition systems are now commonly used across different domains. however, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. this is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. this paper addresses this problem by applying an efficient method for explainable face recognition systems. we use a class activation mapping (cam)-based discriminative localization (very narrow/specific localization) technique called scaled directed divergence (sdd) to visually explain the results of deep learning-based face recognition systems. we perform fine localization of the face features relevant to the deep learning model for its prediction/decision. our experiments show that the sdd class activation map (cam) highlights the relevant face features very specifically compared to the traditional cam and very accurately. the provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.",,2025-05-04,,"['rashik shadman', 'daqing hou', 'faraz hussain', 'm g sarwar murshed']"
2505.03838,intellicardiac: an intelligent platform for cardiac image segmentation   and classification,eess.iv cs.ai cs.cv,"precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. we introduce intellicardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4d cardiac images and disease classification, utilizing an ai model trained on the publicly accessible acdc dataset. the system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. the system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patient's cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. intellicardiac combines a deep learning-based segmentation model with a two-step classification pipeline. the segmentation module gains an overall accuracy of 92.6%. the classification module, trained on characteristics taken from segmented heart structures, achieves 98% accuracy in five categories. these results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. intellicardiac, which supports real-time visualization, workflow integration, and ai-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis.",,2025-05-05,2025-05-07,"['ting yu tsai', 'an yu', 'meghana spurthi maadugundu', 'ishrat jahan mohima', 'umme habiba barsha', 'mei-hwa f. chen', 'balakrishnan prabhakaran', 'ming-ching chang']"
2505.03845,a deep learning approach for depressive symptoms assessment in   parkinson's disease patients using facial videos,eess.iv cs.ai cs.cv cs.lg,"parkinson's disease (pd) is a neurodegenerative disorder, manifesting with motor and non-motor symptoms. depressive symptoms are prevalent in pd, affecting up to 45% of patients. they are often underdiagnosed due to overlapping motor features, such as hypomimia. this study explores deep learning (dl) models-vivit, video swin tiny, and 3d cnn-lstm with attention layers-to assess the presence and severity of depressive symptoms, as detected by the geriatric depression scale (gds), in pd patients through facial video analysis. the same parameters were assessed in a secondary analysis taking into account whether patients were one hour after (on-medication state) or 12 hours without (off-medication state) dopaminergic medication. using a dataset of 1,875 videos from 178 patients, the video swin tiny model achieved the highest performance, with up to 94% accuracy and 93.7% f1-score in binary classification (presence of absence of depressive symptoms), and 87.1% accuracy with an 85.4% f1-score in multiclass tasks (absence or mild or severe depressive symptoms).",,2025-05-05,,"['ioannis kyprakis', 'vasileios skaramagkas', 'iro boura', 'georgios karamanis', 'dimitrios i. fotiadis', 'zinovia kefalopoulou', 'cleanthe spanaki', 'manolis tsiknakis']"
2505.03846,game: learning multimodal interactions via graph structures for   personality trait estimation,cs.cv cs.ai,"apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. in this paper, we propose game, a graph-augmented multimodal encoder designed to robustly model and fuse multi-source features for automatic personality prediction. for the visual stream, we construct a facial graph and introduce a dual-branch geo two-stream network, which combines graph convolutional networks (gcns) and convolutional neural net-works (cnns) with attention mechanisms to capture both structural and appearance-based facial cues. complementing this, global context and iden-tity features are extracted using pretrained resnet18 and vggface back-bones. to capture temporal dynamics, frame-level features are processed by a bigru enhanced with temporal attention modules. meanwhile, audio representations are derived from the vggish network, and linguistic se-mantics are captured via the xlm-roberta transformer. to achieve effective multimodal integration, we propose a channel attention-based fusion module, followed by a multi-layer perceptron (mlp) regression head for predicting personality traits. extensive experiments show that game con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.",,2025-05-05,,"['kangsheng wang', 'yuhang li', 'chengwei ye', 'yufei lin', 'huanzhen zhang', 'bohan hu', 'linuo xu', 'shuyan liu']"
2505.03848,advanced clustering framework for semiconductor image analytics   integrating deep tda with self-supervised and transfer learning techniques,cs.cv cs.ai cs.et cs.lg,"semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. this paper introduces an advanced clustering framework that integrates deep topological data analysis (tda) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. tda captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. this study highlights the transformative potential of combining tda, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.",,2025-05-05,,"['janhavi giri', 'attila lengyel', 'don kent', 'edward kibardin']"
2505.03856,an active inference model of covert and overt visual attention,cs.cv cs.ai q-bio.nc,"the ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. this paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. the model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. to test the effectiveness of the model, we analyze its behavior in the posner cueing task and a simple target focus task using two-dimensional(2d) visual data. reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. the results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.",,2025-05-06,,"['tin mišić', 'karlo koledić', 'fabio bonsignorio', 'ivan petrović', 'ivan marković']"
2505.03859,deepfakes on demand: the rise of accessible non-consensual deepfake   image generators,cs.cy cs.ai cs.cv,"advances in multimodal machine learning have made text-to-image (t2i) models increasingly accessible and popular. however, t2i models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. this paper presents an empirical study exploring the accessibility of deepfake model variants online. through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, hugging face and civitai, we demonstrate a huge rise in easily accessible deepfake models. almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on civitai. these deepfake models have been downloaded almost 15 million times since november 2022, with the models targeting a range of individuals from global celebrities to instagram users with under 10,000 followers. both stable diffusion and flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (ncii). deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (lora), requiring as few as 20 images, 24gb vram, and 15 minutes of time, making this process widely accessible via consumer-grade computers. despite these models violating the terms of service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and ncii.",,2025-05-06,,"['will hawkins', 'chris russell', 'brent mittelstadt']"
2505.03896,novel extraction of discriminative fine-grained feature to improve   retinal vessel segmentation,cs.cv cs.ai,"retinal vessel segmentation is a vital early detection method for several severe ocular diseases. despite significant progress in retinal vessel segmentation with the advancement of neural networks, there are still challenges to overcome. specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. to address these issues, we propose a novel attention u-shaped kolmogorov-arnold network named attukan along with a novel label-guided pixel-wise contrastive loss for retinal vessel segmentation. specifically, we implement attention gates into kolmogorov-arnold networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of kan blocks. additionally, we also design a novel label-guided pixel-wise contrastive loss to supervise our proposed attukan to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. experiments are conducted across four public datasets including drive, stare, chase_db1, hrf and our private dataset. attukan achieves f1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with miou scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. quantitative and qualitative results show that our attukan achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. our code will be available at https://github.com/stevezs315/attukan.",,2025-05-06,,"['shuang zeng', 'chee hong lee', 'micky c nnamdi', 'wenqi shi', 'j ben tamo', 'lei zhu', 'hangzhou he', 'xinliang zhang', 'qian chen', 'may d. wang', 'yanye lu', 'qiushi ren']"
2505.03912,"openhelix: a short survey, empirical analysis, and open-source   dual-system vla model for robotic manipulation",cs.ro cs.cv,"dual-system vla (vision-language-action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. to address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. ultimately, it will provide a low-cost open-source model for further exploration. of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. project page: https://openhelix-robot.github.io/.",,2025-05-06,,"['can cui', 'pengxiang ding', 'wenxuan song', 'shuanghao bai', 'xinyang tong', 'zirui ge', 'runze suo', 'wanqi zhou', 'yang liu', 'bofang jia', 'han zhao', 'siteng huang', 'donglin wang']"
2505.03974,deep learning framework for infrastructure maintenance: crack detection   and high-resolution imaging of infrastructure surfaces,cs.cv cs.ai eess.iv,"recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. however, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. a few studies used super-resolution techniques to address the problem of low-resolution images. nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. in order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (cnn) and efficient sub-pixel convolutional neural network (espcnn). cnn accurately classified both the classes. espcnn, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from cnn. the espcnn outperformed bicubic interpolation in all the evaluation metrics for super-resolution. based on the performance metrics, the combination of cnn and espcnn was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. the visual inspection showed that epscnn is able to capture crack propagation, complex geometry of even minor cracks. the proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.",,2025-05-06,,"['nikhil m. pawar', 'jorge a. prozzi', 'feng hong', 'surya sarat chandra congress']"
2505.03991,"action spotting and precise event detection in sports: datasets,   methods, and challenges",cs.cv,"video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. recent advancements in deep learning, particularly convolutional neural networks (cnns) and transformers, have significantly improved accuracy and efficiency in temporal action localization (tal), action spotting (as), and precise event spotting (pes). this survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. we thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. this survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.",,2025-05-06,,"['hao xu', 'arbind agrahari baniya', 'sam well', 'mohamed reda bouadjenek', 'richard dazeley', 'sunil aryal']"
2505.04003,prototype-based information compensation network for multi-source remote   sensing data classification,eess.iv cs.cv,"multi-source remote sensing data joint classification aims to provide accuracy and reliability of land cover classification by leveraging the complementary information from multiple data sources. existing methods confront two challenges: inter-frequency multi-source feature coupling and inconsistency of complementary information exploration. to solve these issues, we present a prototype-based information compensation network (picnet) for land cover classification based on hsi and sar/lidar data. specifically, we first design a frequency interaction module to enhance the inter-frequency coupling in multi-source feature extraction. the multi-source features are first decoupled into high- and low-frequency components. then, these features are recoupled to achieve efficient inter-frequency communication. afterward, we design a prototype-based information compensation module to model the global multi-source complementary information. two sets of learnable modality prototypes are introduced to represent the global modality information of multi-source data. subsequently, cross-modal feature integration and alignment are achieved through cross-attention computation between the modality-specific prototype vectors and the raw feature representations. extensive experiments on three public datasets demonstrate the significant superiority of our picnet over state-of-the-art methods. the codes are available at https://github.com/oucailab/picnet.",,2025-05-06,,"['feng gao', 'sheng liu', 'chuanzheng gong', 'xiaowei zhou', 'jiayi wang', 'junyu dong', 'qian du']"
2505.04006,the eye as a window to systemic health: a survey of retinal imaging from   classical techniques to oculomics,eess.iv cs.cv,"the unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. the retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. the advancement in imaging technology leveraging artificial intelligence has seized this opportunity to bridge the gap between the eye and human health. this track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. the new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. in this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of ai-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. we also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.",,2025-05-06,,"['n/a inamullah', 'imran razzak', 'shoaib jameel']"
2505.04050,terrafusion: joint generation of terrain geometry and texture using   latent diffusion models,cs.gr cs.cv,"3d terrain models are essential in fields such as video game development and film production. since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. however, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. in this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. first, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.",,2025-05-06,,"['kazuki higo', 'toshiki kanai', 'yuki endo', 'yoshihiro kanamori']"
2505.04052,person-in-situ: scene-consistent human image insertion with   occlusion-aware pose control,cs.gr cs.cv,"compositing human figures into scene images has broad applications in areas such as entertainment and advertising. however, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer. moreover, they offer limited control over the inserted person's pose. to address these challenges, we propose two methods. both allow explicit pose control via a 3d body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks. the first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly. the second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision. quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses.",,2025-05-06,,"['shun masuda', 'yuki endo', 'yoshihiro kanamori']"
2505.04055,foodtrack: estimating handheld food portions with egocentric video,cs.cv,"accurately tracking food consumption is crucial for nutrition and health monitoring. traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. we propose the foodtrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. foodtrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. we achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.",,2025-05-06,,"['ervin wang', 'yuhao chen']"
2505.04087,seva: leveraging single-step ensemble of vicinal augmentations for   test-time adaptation,cs.cv,"test-time adaptation (tta) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. while existing tta methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. in this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. to address this limitation, we propose a novel tta approach named single-step ensemble of vicinal augmentations (seva), which can take advantage of data augmentations without increasing the computational burden. specifically, instead of explicitly utilizing the augmentation strategy to generate new data, seva develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of tta. the comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of seva. the code will be publicly available.",,2025-05-06,,"['zixuan hu', 'yichun hu', 'ling-yu duan']"
2505.04095,scalable aerial gnss localization for marine robots,cs.ro cs.cv,"accurate localization is crucial for water robotics, yet traditional onboard global navigation satellite system (gnss) approaches are difficult or ineffective due to signal reflection on the water's surface and its high cost of aquatic gnss receivers. existing approaches, such as inertial navigation, doppler velocity loggers (dvl), slam, and acoustic-based methods, face challenges like error accumulation and high computational complexity. therefore, a more efficient and scalable solution remains necessary. this paper proposes an alternative approach that leverages an aerial drone equipped with gnss localization to track and localize a marine robot once it is near the surface of the water. our results show that this novel adaptation enables accurate single and multi-robot marine robot localization.",,2025-05-06,,"['shuo wen', 'edwin meriaux', 'mariana sosa guzmán', 'charlotte morissette', 'chloe si', 'bobak baghi', 'gregory dudek']"
2505.04097,3d brain mri classification for alzheimer diagnosis using cnn with data   augmentation,eess.iv cs.cv cs.lg,"a three-dimensional convolutional neural network was developed to classify t1-weighted brain mri scans as healthy or alzheimer. the network comprises 3d convolution, pooling, batch normalization, dense relu layers, and a sigmoid output. using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the roc curve of 0.961, an improvement of approximately 0.027 over resizing alone. sensitivity and specificity both exceeded 0.90. these results align with prior work reporting up to 0.10 gain via synthetic augmentation. the findings demonstrate the effectiveness of simple augmentation for 3d mri classification and motivate future exploration of advanced augmentation methods and architectures such as 3d u-net and vision transformers.",,2025-05-06,,"['thien nhan vo', 'bac nam ho', 'thanh xuan truong']"
2505.04109,one2any: one-reference 6d pose estimation for any object,cs.cv,"6d object pose estimation remains challenging for many applications due to dependencies on complete 3d models, multi-view images, or training limited to specific object categories. these requirements make generalization to novel objects difficult for which neither 3d models nor multi-view images may be available. to address this, we propose a novel method one2any that estimates the relative 6-degrees of freedom (dof) object pose using only a single reference-single query rgb-d image, without prior knowledge of its 3d model, multi-view data, or category constraints. we treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive reference object pose embedding (rope) that encodes an object shape, orientation, and texture from a single reference view. using this embedding, a u-net-based pose decoding module produces reference object coordinate (roc) for new views, enabling fast and accurate pose estimation. this simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or cad inputs, at a fraction of compute.",,2025-05-06,,"['mengya liu', 'siyuan li', 'ajad chhatkuli', 'prune truong', 'luc van gool', 'federico tombari']"
2505.04147,"r^3-vqa: ""read the room"" by video social reasoning",cs.cv cs.ai,"""read the room"" is a significant social reasoning capability in human daily life. humans can infer others' mental states from subtle social cues. previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. in this paper, we contribute a valuable, high-quality, and comprehensive video dataset named r^3-vqa with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. moreover, we include human-annotated and model-generated qas. our task r^3-vqa includes three aspects: social event understanding, mental state estimation, and social causal reasoning. as a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (lvlms). comprehensive experiments show that (i) lvlms are still far from human-level consistent social reasoning in complex social scenarios; (ii) theory of mind (tom) prompting can help lvlms perform better on social reasoning tasks. we provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.",,2025-05-07,,"['lixing niu', 'jiapeng li', 'xingping yu', 'shu wang', 'ruining feng', 'bo wu', 'ping wei', 'yisen wang', 'lifeng fan']"
2505.04150,learning from similarity proportion loss for classifying skeletal muscle   recovery stages,cs.cv cs.lg,"evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. the conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. there is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. given the limited availability of fully labeled data, a possible approach is learning from label proportions (llp), a weakly supervised learning method using class label proportions. however, current llp methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. to address these issues, we propose ordinal scale learning from similarity proportion (oslsp), which uses a similarity proportion loss derived from two bag combinations. oslsp can update the feature extractor by using class proportion attention to the ordinal scale of the class. our model with oslsp outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.",,2025-05-07,2025-05-08,"['yu yamaoka', 'weng ian chan', 'shigeto seno', 'soichiro fukada', 'hideo matsuda']"
2505.04173,diffpattern-flex: efficient layout pattern generation via discrete   diffusion,cs.lg cs.cv,"recent advancements in layout pattern generation have been dominated by deep generative models. however, relying solely on neural networks for legality guarantees raises concerns in many practical applications. in this paper, we present \tool{diffpattern}-flex, a novel approach designed to generate reliable layout patterns efficiently. \tool{diffpattern}-flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. to ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. experimental results across various benchmarks demonstrate that \tool{diffpattern}-flex significantly outperforms existing methods and excels at producing reliable layout patterns.",,2025-05-07,,"['zixiao wang', 'wenqian zhao', 'yunheng shen', 'yang bai', 'guojin chen', 'farzan farnia', 'bei yu']"
2505.04175,dota: deformable optimized transformer architecture for end-to-end text   recognition with retrieval-augmented generation,cs.cv cs.ai,"text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. this paper introduces a novel end-to-end framework that combines resnet and vision transformer backbones with advanced methodologies, including deformable convolutions, retrieval-augmented generation, and conditional random fields (crf). these innovations collectively enhance feature representation and improve optical character recognition (ocr) performance. specifically, the framework substitutes standard convolution layers in the third and fourth blocks with deformable convolutions, leverages adaptive dropout for regularization, and incorporates crf for more refined sequence modeling. extensive experiments conducted on six benchmark datasets ic13, ic15, svt, iiit5k, svtp, and cute80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on ic13, 58.26% on ic15, 88.10% on svt, 74.13% on iiit5k, 82.17% on svtp, and 66.67% on cute80, resulting in an average accuracy of 77.77%. these results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.",,2025-05-07,,"['naphat nithisopa', 'teerapong panboonyuen']"
2505.04185,s3d: sketch-driven 3d model generation,cs.cv cs.ai,"generating high-quality 3d models from 2d sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. in this paper, we present s3d, a novel framework that converts simple hand-drawn sketches into detailed 3d models. our method utilizes a u-net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3d representation that can be rendered from novel views. to ensure robust consistency between the sketch domain and the 3d output, we introduce a novel style-alignment loss that aligns the u-net bottleneck features with the initial encoder outputs of the 3d generation module, significantly enhancing reconstruction fidelity. to further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. this streamlined framework demonstrates the effectiveness of s3d in generating high-quality 3d models from sketch inputs. the source code for this project is publicly available at https://github.com/hailsong/s3d.",,2025-05-07,,"['hail song', 'wonsik shin', 'naeun lee', 'soomin chung', 'nojun kwak', 'woontack woo']"
2505.04192,videopath-llava: pathology diagnostic reasoning through video   instruction tuning,cs.cv cs.ai cs.cl,"we present videopath-llava, the first large multimodal model (lmm) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. by generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, videopath-llava bridges visual narratives with diagnostic reasoning.   central to our approach is the videopath-instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on youtube. although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. to overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. videopath-llava establishes a new benchmark in pathology video analysis and offers a promising foundation for future ai systems that support clinical decision-making through integrated visual and diagnostic reasoning. our code, data, and model are publicly available at https://github.com/trinhvg/videopath-llava.",,2025-05-07,,"['trinh t. l. vuong', 'jin tae kwak']"
2505.04201,stola: self-adaptive touch-language framework with tactile commonsense   reasoning in open-ended scenarios,cs.cv,"this paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. we identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. to overcome these challenges, we introduce stola, a self-adaptive touch-language framework. stola utilizes mixture of experts (moe) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. experiments show stola exhibits competitive performance compared to existing models on the physiclear benchmark and self-constructed datasets, proving the effectiveness of the mixture of experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.",,2025-05-07,,"['ning cheng', 'jinan xu', 'jialing chen', 'wenjuan han']"
2505.04214,cm1 -- a dataset for evaluating few-shot information extraction with   large vision language models,cs.cv,"the automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. a reliable extraction is a prerequisite for the mass digitization efforts of many archives. large vision language models (lvlm) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. in this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of lvlms. the cm1 documents are a historic collection of forms with handwritten entries created in europe to administer the care and maintenance program after world war two. the dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. we provide baseline results for two different lvlms and compare performances to an established full-page extraction model. while the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered lvlms benefit from their size and heavy pretraining and outperform the classical approach.",,2025-05-07,,"['fabian wolf', 'oliver tüselmann', 'arthur matei', 'lukas hennies', 'christoph rass', 'gernot a. fink']"
2505.04228,low resolution next best view for robot packing,cs.ro cs.cv,"automating the packing of objects with robots is a key challenge in industrial automation, where efficient object perception plays a fundamental role. this paper focuses on scenarios where precise 3d reconstruction is not required, prioritizing cost-effective and scalable solutions. the proposed low-resolution next best view (lr-nbv) algorithm leverages a utility function that balances pose redundancy and acquisition density, ensuring efficient object reconstruction. experimental validation demonstrates that lr-nbv consistently outperforms standard nbv approaches, achieving comparable accuracy with significantly fewer poses. this method proves highly suitable for applications requiring efficiency, scalability, and adaptability without relying on high-precision sensing.",,2025-05-07,,"['giuseppe fabio preziosa', 'chiara castellano', 'andrea maria zanchettin', 'marco faroni', 'paolo rocco']"
2505.04262,bridging geometry-coherent text-to-3d generation with multi-view   diffusion priors and gaussian splatting,cs.cv,"score distillation sampling (sds) leverages pretrained 2d diffusion models to advance text-to-3d generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3d content. in this work, we propose coupled score distillation (csd), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3d generation while enabling the stable and direct optimization of 3d gaussian splatting. specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3d assets. additionally, we propose a framework that directly optimizes 3d gaussian splatting (3d-gs) with random initialization to generate geometrically consistent 3d content. we further employ a deformable tetrahedral grid, initialized from 3d-gs and refined through csd, to produce high-quality, refined meshes. quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",,2025-05-07,,"['feng yang', 'wenliang qian', 'wangmeng zuo', 'hui li']"
2505.04270,object-shot enhanced grounding network for egocentric video,cs.cv cs.ai,"egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. to address these limitations, we propose osgnet, an object-shot enhanced grounding network for egocentric video. specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. experiments conducted on three datasets demonstrate that osgnet achieves state-of-the-art performance, validating the effectiveness of our approach. our code can be found at https://github.com/yisen-feng/osgnet.",,2025-05-07,,"['yisen feng', 'haoyu zhang', 'meng liu', 'weili guan', 'liqiang nie']"
2505.04276,hdifftg: a lightweight hybrid diffusion-transformer-gcn architecture for   3d human pose estimation,cs.cv cs.mm,"we propose hdifftg, a novel 3d human pose estimation (3dhpe) method that integrates transformer, graph convolutional network (gcn), and diffusion model into a unified framework. hdifftg leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. the transformer captures global spatiotemporal dependencies, the gcn models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. this integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. evaluation results on the human3.6m and mpi-inf-3dhp datasets demonstrate that hdifftg achieves state-of-the-art (sota) performance on the mpi-inf-3dhp dataset while excelling in both accuracy and computational efficiency. additionally, the model exhibits exceptional robustness in noisy and occluded environments. source codes and models are available at https://github.com/circejie/hdifftg",,2025-05-07,,"['yajie fu', 'chaorui huang', 'junwei li', 'hui kong', 'yibin tian', 'huakang li', 'zhiyuan zhang']"
2505.04281,ts-diff: two-stage diffusion model for low-light raw image enhancement,cs.cv eess.iv,"this paper presents a novel two-stage diffusion model (ts-diff) for enhancing extremely low-light raw images. in the pre-training stage, ts-diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. camera feature integration (cfi) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. during the aligning stage, cfis are averaged to create a target-specific cfi$^t$, which is fine-tuned using a small amount of real raw data to adapt to the noise characteristics of specific cameras. a structural reparameterization technique further simplifies cfi$^t$ for efficient deployment. to address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. additionally, a novel dataset, qid, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. experimental results demonstrate that ts-diff achieves state-of-the-art performance on multiple datasets, including qid, sid, and eld, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. these findings highlight the robustness and versatility of ts-diff, making it a practical solution for low-light imaging applications. source codes and models are available at https://github.com/circcclek/ts-diff",,2025-05-07,,"['yi li', 'zhiyuan zhang', 'jiangnan xia', 'jianghan cheng', 'qilong wu', 'junwei li', 'yibin tian', 'hui kong']"
2505.04306,mode: mixture of diffusion experts for any occluded face recognition,cs.cv,"with the continuous impact of epidemics, people have become accustomed to wearing masks. however, most current occluded face recognition (ofr) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. in this paper, we propose an identity-gated mixture of diffusion experts (mode) for ofr. each diffusion-based generative expert estimates one possible complete image for occluded faces. considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. to ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. moreover, our mode is a plug-and-play module for most existing face recognition models. extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.",,2025-05-07,,"['qiannan fan', 'zhuoyang li', 'jitong li', 'chenyang cao']"
2505.04320,multi-turn consistent image editing,cs.cv,"many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. however, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. as a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. to address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. our approach leverages flow matching for accurate image inversion and a dual-objective linear quadratic regulators (lqr) for stable sampling, effectively mitigating error accumulation. additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.",,2025-05-07,,"['zijun zhou', 'yingying deng', 'xiangyu he', 'weiming dong', 'fan tang']"
2505.04347,countdiffusion: text-to-image synthesis with training-free   counting-guidance diffusion,cs.cv,"stable diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. in this paper, we propose countdiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. countdiffusion consists of two stages. in the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. in the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. the proposed countdiffusion can be plugged into any diffusion-based text-to-image (t2i) generation models without further training. experiment results demonstrate the superiority of our proposed countdiffusion, which improves the accurate object quantity generation ability of t2i models by a large margin.",,2025-05-07,,"['yanyu li', 'pencheng wan', 'liang han', 'yaowei wang', 'liqiang nie', 'min zhang']"
2505.04369,wdmamba: when wavelet degradation prior meets vision mamba for image   dehazing,cs.cv,"in this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. exploiting this insight, we propose a novel dehazing framework, wdmamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. this coarse-to-fine strategy enables wdmamba to effectively capture features specific to each stage of the dehazing process, resulting in high-quality restored images. specifically, in the low-frequency restoration stage, we integrate mamba blocks to reconstruct global structures with linear complexity, efficiently removing overall haze and producing a coarse restored image. thereafter, the detail enhancement stage reinstates fine-grained information that may have been overlooked during the previous phase, culminating in the final dehazed output. furthermore, to enhance detail retention and achieve more natural dehazing, we introduce a self-guided contrastive regularization during network training. by utilizing the coarse restored output as a hard negative example, our model learns more discriminative representations, substantially boosting the overall dehazing performance. extensive evaluations on public dehazing benchmarks demonstrate that our method surpasses state-of-the-art approaches both qualitatively and quantitatively. code is available at https://github.com/sunj000/wdmamba.",,2025-05-07,,"['jie sun', 'heng liu', 'yongzhen wang', 'xiao-ping zhang', 'mingqiang wei']"
2505.04375,"balancing accuracy, calibration, and efficiency in active learning with   vision transformers under label noise",cs.cv cs.ai cs.lg,"fine-tuning pre-trained convolutional neural networks on imagenet for downstream tasks is well-established. still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. we explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (base and large with 16x16 and 32x32 patch sizes) and three swin transformer configurations (tiny, small, and base) on cifar10 and cifar100 datasets, under varying label noise rates. our findings show that larger vit models (vitl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while swin transformers exhibit weaker robustness across all noise levels. we find that smaller patch sizes do not always lead to better performance, as vitl16 performs consistently worse than vitl32 while incurring a higher computational cost. we also find that information-based active learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. we hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.",,2025-05-07,,"[""moseli mots'oehli"", 'hope mogale', 'kyungim baek']"
2505.04376,label-efficient single photon images classification via active learning,eess.iv cs.cv,"single-photon lidar achieves high-precision 3d imaging in extreme environments through quantum-level photon detection technology. current research primarily focuses on reconstructing 3d scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. this paper presents the first active learning framework for single-photon image classification. the core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. by identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. on real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. this illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.",,2025-05-07,,"['zili zhang', 'ziting wen', 'yiheng qiang', 'hongzhou dong', 'wenle dong', 'xinyang li', 'xiaofan wang', 'xiaoqiang ren']"
2505.04380,tetrahedron-net for medical image registration,eess.iv cs.cv cs.ir,"medical image registration plays a vital role in medical image processing. extracting expressive representations for medical images is crucial for improving the registration quality. one common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. the de facto structure, u-net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. in this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. the new decoder is designed to interact with both the original encoder and decoder. in this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. the new architecture is concise yet generalized, with only one encoder and two decoders forming a ``tetrahedron'' structure, thereby dubbed tetrahedron-net. three instantiations of tetrahedron-net are further constructed regarding the different structures of the appended decoder. our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. finally, such a ``tetrahedron'' design can also be easily integrated into popular u-net-like architectures including voxelmorph, vit-v-net, and transmorph, leading to consistent performance gains.",,2025-05-07,,"['jinhai xiang', 'shuai guo', 'qianru han', 'dantong shi', 'xinwei he', 'xiang bai']"
2505.04384,data: multi-disentanglement based contrastive learning for open-world   semi-supervised deepfake attribution,cs.cv,"deepfake attribution (dfa) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. however, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. to address these issues, in this paper we propose an innovative multi-disentanglement based contrastive learning framework, data, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (oss-dfa) task. specifically, since all generation techniques can be abstracted into a similar architecture, data defines the concept of 'orthonormal deepfake basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. additionally, to enhance the standardization and discrimination of features, data uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. extensive experimental evaluations show that data achieves state-of-the-art performance on the oss-dfa benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.",,2025-05-07,,"['ming-hui liu', 'xiao-qian liu', 'xin luo', 'xin-shun xu']"
2505.04387,geometry-aware texture generation for 3d head modeling with   artist-driven control,cs.gr cs.cv,"creating realistic 3d head assets for virtual characters that match a precise artistic vision remains labor-intensive. we present a novel framework that streamlines this process by providing artists with intuitive control over generated 3d heads. our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. the framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. experiments demonstrate that our method produces diverse results with clean geometries. we showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. this integrated approach aims to streamline the artistic workflow in virtual character creation.",,2025-05-07,,"['amin fadaeinejad', 'abdallah dib', 'luiz gustavo hafemann', 'emeline got', 'trevor anderson', 'amaury depierre', 'nikolaus f. troje', 'marcus a. brubaker', 'marc-andré carbonneau']"
2505.04392,predicting road surface anomalies by visual tracking of a preceding   vehicle,cs.cv,"a novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. the method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. the method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. a challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. the method is effective and performs in real time on standard consumer hardware.",,2025-05-07,,"['petr jahoda', 'jan cech']"
2505.04394,swinlip: an efficient visual speech encoder for lip reading using swin   transformer,cs.cv cs.sd eess.as,"this paper presents an efficient visual speech encoder for lip reading. while most recent lip reading studies have been based on the resnet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). to overcome the limitations of convolutional neural network (cnn)-based models, we apply the hierarchical structure and window self-attention of the swin transformer to lip reading. we configure a new lightweight scale of the swin transformer suitable for processing lip reading data and present the swinlip visual speech encoder, which efficiently reduces computational load by integrating modified convolution-augmented transformer (conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. through extensive experiments, we have validated that our swinlip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. in particular, our swinlip demonstrated robust performance in both english lrw and mandarin lrw-1000 datasets and achieved state-of-the-art performance on the mandarin lrw-1000 dataset with less computation compared to the existing state-of-the-art model.",10.1016/j.neucom.2025.130289,2025-05-07,,"['young-hu park', 'rae-hong park', 'hyung-min park']"
2505.04397,deep residual learning with product units,cs.cv cs.ai cs.lg eess.iv,"we propose a deep product-unit residual neural network (pure) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. pure replaces conventional convolutional layers with 2d product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. we validate pure on three benchmark datasets. on galaxy10 decals, pure34 achieves the highest test accuracy of 84.89%, surpassing the much deeper resnet152, while converging nearly five times faster and demonstrating strong robustness to poisson noise. on imagenet, pure architectures outperform standard resnet models at similar depths, with pure34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper resnet variants (resnet50, resnet101) while utilizing significantly fewer parameters and computational resources. on cifar-10, pure consistently outperforms resnet variants across varying depths, with pure272 reaching 95.01% test accuracy, comparable to resnet1001 but at less than half the model size. these results demonstrate that pure achieves a favorable balance between accuracy, efficiency, and robustness. compared to traditional residual networks, pure not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.",,2025-05-07,,"['ziyuan li', 'uwe jaekel', 'babette dellen']"
2505.04408,mfseg: efficient multi-frame 3d semantic segmentation,cs.cv,"we propose mfseg, an efficient multi-frame 3d semantic segmentation framework. by aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, mfseg reduces computational overhead while maintaining high accuracy. moreover, by employing a lightweight mlp-based point decoder, our method eliminates the need to upsample redundant points from past frames. experiments on the nuscenes and waymo datasets show that mfseg outperforms existing methods, demonstrating its effectiveness and efficiency.",,2025-05-07,,"['chengjie huang', 'krzysztof czarnecki']"
2505.04410,declip: decoupled learning for open-vocabulary dense perception,cs.cv,"dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. while vision-language models (vlms) like clip have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. in this work, we present our observation that clip's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. to address this issue, we propose declip, a novel framework that enhances clip by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. the ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as dino. extensive experiments demonstrate that declip significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/declip}.",,2025-05-07,,"['junjie wang', 'bin chen', 'yulin li', 'bin kang', 'yichi chen', 'zhuotao tian']"
2505.04424,rlministyler: light-weight rl style agent for arbitrary sequential   neural style generation,cs.cv,"arbitrary style transfer aims to apply the style of any given artistic image to another content image. still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer rlministyler. this framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. through a series of experiments across image various resolutions, we have validated the advantages of rlministyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. codes are available at https://github.com/fengxiaoming520/rlministyler.",,2025-05-07,,"['jing hu', 'chengming feng', 'shu hu', 'ming-ching chang', 'xin li', 'xi wu', 'xin wang']"
2505.04460,learning real facial concepts for independent deepfake detection,cs.cv,"deepfake detection models often struggle with generalization to unseen datasets, manifesting as misclassifying real instances as fake in target domains. this is primarily due to an overreliance on forgery artifacts and a limited understanding of real faces. to address this challenge, we propose a novel approach realid to enhance generalization by learning a comprehensive concept of real faces while assessing the probabilities of belonging to the real and fake classes independently. realid comprises two key modules: the real concept capture module (realc2) and the independent dual-decision classifier (idc). with the assistance of a multireal memory, realc2 maintains various prototypes for real faces, allowing the model to capture a comprehensive concept of real class. meanwhile, idc redefines the classification strategy by making independent decisions based on the concept of the real class and the presence of forgery artifacts. through the combined effect of the above modules, the influence of forgery-irrelevant patterns is alleviated, and extensive experiments on five widely used datasets demonstrate that realid significantly outperforms existing state-of-the-art methods, achieving a 1.74% improvement in average accuracy.",,2025-05-07,,"['ming-hui liu', 'harry cheng', 'tianyi wang', 'xin luo', 'xin-shun xu']"
2505.04481,cad-llama: leveraging large language models for computer-aided design   parametric 3d model generation,cs.cv,"recently, large language models (llms) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. this study investigates the generation of parametric sequences for computer-aided design (cad) models using llms. this endeavor represents an initial step towards creating parametric 3d shapes with llms, as cad model parameters directly correlate with shapes in three-dimensional space. despite the formidable generative capacities of llms, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3d structures. to address this, we present cad-llama, a framework designed to enhance pretrained llms for generating parametric 3d cad models. specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3d cad command sequences into structured parametric cad code (spcc), incorporating hierarchical semantic descriptions. furthermore, we propose an adaptive pretraining approach utilizing spcc, followed by an instruction tuning process aligned with cad-specific guidelines. this methodology aims to equip llms with the spatial knowledge inherent in parametric sequences. experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing llm baselines.",,2025-05-07,,"['jiahao li', 'weijian ma', 'xueyang li', 'yunzhong lou', 'guichun zhou', 'xiangdong zhou']"
2505.04485,fa-kpconv: introducing euclidean symmetries to kpconv via frame   averaging,cs.cv,"we present frame-averaging kernel-point convolution (fa-kpconv), a neural network architecture built on top of the well-known kpconv, a widely adopted backbone for 3d point cloud analysis. even though invariance and/or equivariance to euclidean transformations are required for many common tasks, kpconv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. using frame averaging, we allow to flexibly customize point cloud neural networks built with kpconv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. by simply wrapping around an existing kpconv-based network, fa-kpconv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. we showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.",,2025-05-07,2025-05-08,"['ali alawieh', 'alexandru p. condurache']"
2505.04488,"""i can see forever!"": evaluating real-time videollms for assisting   individuals with visual impairments",cs.cv cs.ai cs.hc cs.mm,"the visually impaired population, especially the severely visually impaired, is currently large in scale, and daily activities pose significant challenges for them. although many studies use large language and vision-language models to assist the blind, most focus on static content and fail to meet real-time perception needs in dynamic and complex environments, such as daily activities. to provide them with more effective intelligent assistance, it is imperative to incorporate advanced visual understanding technologies. although real-time vision and speech interaction videollms demonstrate strong real-time visual understanding, no prior work has systematically evaluated their effectiveness in assisting visually impaired individuals. in this work, we conduct the first such evaluation. first, we construct a benchmark dataset (visassistdaily), covering three categories of assistive tasks for visually impaired individuals: basic skills, home life tasks, and social life tasks. the results show that gpt-4o achieves the highest task success rate. next, we conduct a user study to evaluate the models in both closed-world and open-world scenarios, further exploring the practical challenges of applying videollms in assistive contexts. one key issue we identify is the difficulty current models face in perceiving potential hazards in dynamic environments. to address this, we build an environment-awareness dataset named safevid and introduce a polling mechanism that enables the model to proactively detect environmental risks. we hope this work provides valuable insights and inspiration for future research in this field.",,2025-05-07,,"['ziyi zhang', 'zhen sun', 'zongmin zhang', 'zifan peng', 'yuemeng zhao', 'zichun wang', 'zeren luo', 'ruiting zuo', 'xinlei he']"
2505.04497,defining and quantifying creative behavior in popular image generators,cs.cv cs.ai,"creativity of generative ai models has been a subject of scientific debate in the last years, without a conclusive answer. in this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable ai model for a given task. we evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.",,2025-05-07,2025-05-08,"['aditi ramaswamy', 'hana chockler', 'melane navaratnarajah']"
2505.04502,leveraging simultaneous usage of edge gpu hardware engines for video   face detection and recognition,cs.cv cs.ar eess.iv,"video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. this paper aims to maximize the simultaneous usage of hardware engines available in edge gpus nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. this also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via gbps ethernet network. this constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. in addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. the results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent nvidia edge orin gpu, higher throughput, and a slight saving of power consumption of around 300 mw, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. the performance gets even higher by considering several video streams simultaneously. further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor rt framework for the face recognition task was lower. thus, the paper suggests some hardware improvements to the existing edge gpu processors to enhance their performance even higher.",,2025-05-07,,"['asma baobaid', 'mahmoud meribout']"
2505.04512,hunyuancustom: a multimodal-driven architecture for customized video   generation,cs.cv,"customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. in this paper, we propose hunyuancustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. built upon hunyuanvideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on llava for enhanced multi-modal understanding, along with an image id enhancement module that leverages temporal concatenation to reinforce identity features across frames. to enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an audionet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. extensive experiments on single- and multi-subject scenarios demonstrate that hunyuancustom significantly outperforms state-of-the-art open- and closed-source methods in terms of id consistency, realism, and text-video alignment. moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. all the code and models are available at https://hunyuancustom.github.io.",,2025-05-07,2025-05-08,"['teng hu', 'zhentao yu', 'zhengguang zhou', 'sen liang', 'yuan zhou', 'qin lin', 'qinglin lu']"
2505.04522,text2ct: towards 3d ct volume generation from free-text descriptions   using diffusion model,eess.iv cs.cv,"generating 3d ct volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. in this paper, we introduce text2ct, a novel approach for synthesizing 3d ct volumes from textual descriptions using the diffusion model. unlike previous methods that rely on fixed-format text input, text2ct employs a novel prompt formulation that enables generation from diverse, free-text descriptions. the proposed framework encodes medical text into latent representations and decodes them into high-resolution 3d ct scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3d framework. our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.",,2025-05-07,,"['pengfei guo', 'can zhao', 'dong yang', 'yufan he', 'vishwesh nath', 'ziyue xu', 'pedro r. a. s. bassi', 'zongwei zhou', 'benjamin d. simon', 'stephanie anne harmon', 'baris turkbey', 'daguang xu']"
2505.04524,edge-gpu based face tracking for face detection and recognition   acceleration,cs.cv cs.ar cs.lg eess.iv,"cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. however, despite their high performance, which could be reached using specialized edge or cloud ai hardware accelerators, there is still room for improvement in throughput and power consumption. this paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge gpus, namely nvidia jetson agx orin. first, it leverages the simultaneous usage of all its hardware engines to improve processing time. this offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the cpu or, to a higher extent, to the gpu core. additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. the results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the orin gpu and tracker integration into the pipeline yield an impressive throughput of 290 fps (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. additionally, a substantial saving of power consumption of around 800 mw was achieved when compared to running the task on the cpu/gpu engines only and without integrating a tracker into the orin gpu\'92s pipeline. this hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.",,2025-05-07,,"['asma baobaid', 'mahmoud meribout']"
2505.04526,dfvo: learning darkness-free visible and infrared image disentanglement   and fusion all at once,cs.cv cs.ai,"visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. however, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. to this end, a darkness-free network is proposed to handle visible and infrared image disentanglement and fusion all at once (dfvo), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. specifically, we construct a latent-common feature extractor (lcfe) to obtain latent features for the cascaded tasks strategy. firstly, a details-extraction module (dem) is devised to acquire high-frequency semantic information. secondly, we design a hyper cross-attention module (hcam) to extract low-frequency information and preserve texture features from source images. finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. particularly, dfvo can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the llvip dataset with 63.258 db psnr and 0.724 cc, providing more effective information for high-level vision tasks. our code is publicly accessible at https://github.com/davin-qi530/dfvo.",,2025-05-07,,"['qi zhou', 'yukai shi', 'xiaojun yang', 'xiaoyu xian', 'lunjia liao', 'ruimao zhang', 'liang lin']"
2505.04529,raft: robust augmentation of features for image segmentation,cs.cv,"image segmentation is a powerful computer vision technique for scene understanding. however, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. however, deep neural networks trained on synthetic data often face the syn2real problem, leading to poor performance in real-world deployments.   to mitigate the aforementioned gap in image segmentation, we propose raft, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. to validate raft, we perform experiments on the synthetic-to-real ""synthia->cityscapes"" and ""gtav->cityscapes"" benchmarks. we managed to surpass the previous state of the art, halo. synthia->cityscapes experiences an improvement in miou* upon domain adaptation of 2.1%/79.9%, and gtav->cityscapes experiences a 0.4%/78.2% improvement in miou. furthermore, we test our approach on the real-to-real benchmark of ""cityscapes->acdc"", and again surpass halo, with a gain in miou upon adaptation of 1.3%/73.2%. finally, we examine the effect of the allocated annotation budget and various components of raft upon the final transfer miou.",,2025-05-07,,"['edward humes', 'xiaomin lin', 'uttej kallakuri', 'tinoosh mohsenin']"
2505.04540,registration of 3d point sets using exponential-based similarity matrix,cs.cv,"point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3d point sets captured from varying viewpoints using depth sensors such as lidar or structured light. in modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. however, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. these challenges can lead to misalignments and, consequently, to inaccurate or distorted 3d reconstructions. in this work, we address both these limitations by proposing a robust modification to the classic iterative closest point (icp) algorithm. our method, termed exponential similarity matrix icp (esm-icp), integrates a gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. this matrix facilitates improved estimation of both rotational and translational components during alignment. we demonstrate the robustness of esm-icp in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-gaussian noise. our results show that esm-icp outperforms traditional geometric registration techniques as well as several recent learning-based methods. to encourage reproducibility and community engagement, our full implementation is made publicly available on github. https://github.com/aralab-unr/esm_icp",,2025-05-07,,"['ashutosh singandhupe', 'sanket lokhande', 'hung manh la']"
2505.04575,componential prompt-knowledge alignment for domain incremental learning,cs.cv cs.lg,"domain incremental learning (dil) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. this arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.to address this, we propose componential prompt-knowledge alignment (ka-prompt), a novel prompt-based dil method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. ka-prompt operates in two phases: (1) initial componential structure configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) online alignment preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. extensive experiments on dil benchmarks demonstrate the effectiveness of our ka-prompt. our source code is available at https://github.com/zhoujiahuan1991/icml2025-ka-prompt",,2025-05-07,,"['kunlun xu', 'xu zou', 'gang hua', 'jiahuan zhou']"
2505.04586,active sampling for mri-based sequential decision making,eess.iv cs.cv cs.lg,"despite the superior diagnostic capability of magnetic resonance imaging (mri), its use as a point-of-care (poc) device remains limited by high cost and complexity. to enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. such work shows that single diagnostic decisions can be made, but if we aspire to see mri as a true poc, multiple and sequential decisions are necessary while minimizing the number of samples acquired. we present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. our approach during inference actively adapts to sequential decisions to optimally sample. to achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. we evaluate our approach in two sequential knee pathology assessment tasks: acl sprain detection and cartilage thickness loss assessment. our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. our approach paves the way for the future of mri as a comprehensive and affordable poc device. our code is publicly available at https://github.com/vios-s/mri_sequential_active_sampling",,2025-05-07,,"['yuning du', 'jingshuai liu', 'rohan dharmakumar', 'sotirios a. tsaftaris']"
2505.04590,tetweave: isosurface extraction using on-the-fly delaunay tetrahedral   grids for gradient-based mesh optimization,cs.gr cs.cv,"we introduce tetweave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for marching tetrahedra and a novel directional signed distance at each point. tetweave constructs tetrahedral grids on-the-fly via delaunay triangulation, enabling increased flexibility compared to predefined grids. the extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. the flexibility of tetweave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. this leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. consequently, tetweave exhibits near-linear memory scaling relative to the vertex count of the output mesh - a substantial improvement over predefined grids. we demonstrate the applicability of tetweave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3d reconstruction, mesh compression and geometric texture generation.",10.1145/3730851,2025-05-07,2025-05-08,"['alexandre binninger', 'ruben wiersma', 'philipp herholz', 'olga sorkine-hornung']"
2505.04596,dynamic network flow optimization for task scheduling in ptz camera   surveillance systems,math.oc cs.cv cs.sy eess.sy,"this paper presents a novel approach for optimizing the scheduling and control of pan-tilt-zoom (ptz) cameras in dynamic surveillance environments. the proposed method integrates kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. by assigning kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. this prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. to further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. in addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. by adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.",,2025-05-07,,"['mohammad merati', 'david castañón']"
2505.04601,"openvision: a fully-open, cost-effective family of advanced vision   encoders for multimodal learning",cs.cv,"openai's clip, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. although recent alternatives such as siglip have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. this paper fills this gap with openvision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of openai's clip when integrated into multimodal frameworks like llava. openvision builds on existing works -- e.g., clips for training framework and recap-datacomp-1b for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. by releasing vision encoders spanning from 5.9m to 632.1m parameters, openvision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.",,2025-05-07,,"['xianhang li', 'yanqing liu', 'haoqin tu', 'hongru zhu', 'cihang xie']"
2505.04616,"person recognition at altitude and range: fusion of face, body shape and   gait",cs.cv,"we address the problem of whole-body person recognition in unconstrained environments. this problem arises in surveillance scenarios such as those in the iarpa biometric recognition and identification at altitude and range (briar) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). to this end, we propose farsight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. farsight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. these components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. extensive experiments on the briar dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of farsight. compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (tar@0.1% far), a 17.8% increase in closed-set identification (rank-20), and a 34.3% reduction in open-set identification errors (fnir@1% fpir). furthermore, farsight was evaluated in the 2025 nist rte face in video evaluation (five), which conducts standardized face recognition testing on the briar dataset. these results establish farsight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.",,2025-05-07,,"['feng liu', 'nicholas chimitt', 'lanqing guo', 'jitesh jain', 'aditya kane', 'minchul kim', 'wes robbins', 'yiyang su', 'dingqiang ye', 'xingguang zhang', 'jie zhu', 'siddharth satyakam', 'christopher perry', 'stanley h. chan', 'arun ross', 'humphrey shi', 'zhangyang wang', 'anil jain', 'xiaoming liu']"
2505.04619,merging and disentangling views in visual reinforcement learning for   robotic manipulation,cs.lg cs.cv cs.ro,"vision is well-known for its use in manipulation, especially using visual servoing. to make it robust, multiple cameras are needed to expand the field of view. that is computationally challenging. merging multiple views and using q-learning allows the design of more effective representations and optimization of sample efficiency. such a solution might be expensive to deploy. to mitigate this, we introduce a merge and disentanglement (mad) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. we demonstrate the efficiency and robustness of our approach using meta-world and maniskill3. for project website and code, see https://aalmuzairee.github.io/mad",,2025-05-07,,"['abdulaziz almuzairee', 'rohan patil', 'dwait bhatt', 'henrik i. christensen']"
2505.04620,on path to multimodal generalist: general-level and general-bench,cs.cv,"the multimodal large language model (mllm) is currently experiencing rapid growth, driven by the advanced capabilities of llms. unlike earlier specialists, existing mllms are evolving towards a multimodal generalist paradigm. initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. while many benchmarks exist to assess mllms, a critical question arises: can we simply assume that higher performance across tasks indicates a stronger mllm capability, bringing us closer to human-level ai? we argue that the answer is not as straightforward as it seems. this project introduces general-level, an evaluation framework that defines 5-scale levels of mllm performance and generality, offering a methodology to compare mllms and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards agi. at the core of the framework is the concept of synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. to support this evaluation, we present general-bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. the evaluation results that involve over 100 existing state-of-the-art mllms uncover the capability rankings of generalists, highlighting the challenges in reaching genuine ai. we expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of agi. project page: https://generalist.top/",,2025-05-07,,"['hao fei', 'yuan zhou', 'juncheng li', 'xiangtai li', 'qingshan xu', 'bobo li', 'shengqiong wu', 'yaoting wang', 'junbao zhou', 'jiahao meng', 'qingyu shi', 'zhiyuan zhou', 'liangtao shi', 'minghe gao', 'daoan zhang', 'zhiqi ge', 'weiming wu', 'siliang tang', 'kaihang pan', 'yaobo ye', 'haobo yuan', 'tao zhang', 'tianjie ju', 'zixiang meng', 'shilin xu', 'liyu jia', 'wentao hu', 'meng luo', 'jiebo luo', 'tat-seng chua', 'shuicheng yan', 'hanwang zhang']"
2505.04622,primitiveanything: human-crafted 3d primitive assembly generation with   auto-regressive transformer,cs.gr cs.cv,"shape primitive abstraction, which decomposes complex 3d shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. while recent advances in 3d content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. we present primitiveanything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. primitiveanything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. the proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. through extensive experiments, we demonstrate that primitiveanything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. it benefits various 3d applications and shows potential for enabling primitive-based user-generated content (ugc) in games. project page: https://primitiveanything.github.io",,2025-05-07,,"['jingwen ye', 'yuze he', 'yanning zhou', 'yiqin zhu', 'kaiwen xiao', 'yong-jin liu', 'wei yang', 'xiao han']"
2505.04623,echoink-r1: exploring audio-visual reasoning in multimodal llms via   reinforcement learning,eess.as cs.ai cs.cv cs.mm cs.sd,"multimodal large language models (mllms) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. we introduce echoink-r1, a reinforcement learning framework that enhances such reasoning in mllms. built upon the qwen2.5-omni-7b foundation and optimized with group relative policy optimization (grpo), echoink-r1 tackles multiple-choice question answering over synchronized audio-image pairs. to enable this, we curate avqa-r1-6k, a dataset pairing such audio-image inputs with multiple-choice questions derived from omniinstruct-v1. echoink-r1-7b achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. beyond accuracy, echoink-r1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. these results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in mllms. echoink-r1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. code and data are publicly released to facilitate further research.",,2025-05-07,,"['zhenghao xing', 'xiaowei hu', 'chi-wing fu', 'wenhai wang', 'jifeng dai', 'pheng-ann heng']"
2505.04647,channelexplorer: exploring class separability through activation channel   visualization,cs.gr cs.cv cs.lg,"deep neural networks (dnns) achieve state-of-the-art performance in many vision tasks, yet understanding their internal behavior remains challenging, particularly how different layers and activation channels contribute to class separability. we introduce channelexplorer, an interactive visual analytics tool for analyzing image-based outputs across model layers, emphasizing data-driven insights over architecture analysis for exploring class separability. channelexplorer summarizes activations across layers and visualizes them using three primary coordinated views: a scatterplot view to reveal inter- and intra-class confusion, a jaccard similarity view to quantify activation overlap, and a heatmap view to inspect activation channel patterns. our technique supports diverse model architectures, including cnns, gans, resnet and stable diffusion models. we demonstrate the capabilities of channelexplorer through four use-case scenarios: (1) generating class hierarchy in imagenet, (2) finding mislabeled images, (3) identifying activation channel contributions, and(4) locating latent states' position in stable diffusion model. finally, we evaluate the tool with expert users.",,2025-05-06,,"['md rahat-uz- zaman', 'bei wang', 'paul rosen']"
2505.04652,rethinking boundary detection in deep learning-based medical image   segmentation,eess.iv cs.cv,"medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. while current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. in this study, we propose a novel network architecture named cto, which combines convolutional neural networks (cnns), vision transformer (vit) models, and explicit edge detection operators to tackle this challenge. cto surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. specifically, cto adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream cnn stream for capturing local features and an auxiliary stitchvit stream for integrating long-range dependencies. furthermore, to enhance the model's ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. we validate the performance of cto through extensive experiments conducted on seven challenging medical image segmentation datasets, namely isic 2016, ph2, isic 2018, conic, lits17, and btcv. our experimental results unequivocally demonstrate that cto achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. the codes have been released at: https://github.com/xiaofang007/cto.",,2025-05-06,,"['yi lin', 'dong zhang', 'xiao fang', 'yufan chen', 'kwang-ting cheng', 'hao chen']"
2505.04653,advancing conversational diagnostic ai with multimodal reasoning,cs.cl cs.ai cs.cv cs.lg,"large language models (llms) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of llms to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. here we advance the conversational diagnosis and management performance of the articulate medical intelligence explorer (amie) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. leveraging gemini 2.0 flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. we compared amie to primary care physicians (pcps) in a randomized, blinded, osce-style study of chat-based consultations with patient actors. we constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ecgs, and pdfs of clinical documents across diverse conditions and demographics. our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. specialist evaluation showed amie to be superior to pcps on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). the results show clear progress in multimodal conversational diagnostic ai, but real-world translation needs further research.",,2025-05-06,,"['khaled saab', 'jan freyberg', 'chunjong park', 'tim strother', 'yong cheng', 'wei-hung weng', 'david g. t. barrett', 'david stutz', 'nenad tomasev', 'anil palepu', 'valentin liévin', 'yash sharma', 'roma ruparel', 'abdullah ahmed', 'elahe vedadi', 'kimberly kanada', 'cian hughes', 'yun liu', 'geoff brown', 'yang gao', 'sean li', 's. sara mahdavi', 'james manyika', 'katherine chou', 'yossi matias', 'avinatan hassidim', 'dale r. webster', 'pushmeet kohli', 's. m. ali eslami', 'joëlle barral', 'adam rodman', 'vivek natarajan', 'mike schaekermann', 'tao tu', 'alan karthikesalingam', 'ryutaro tanno']"
2505.04660,ai-generated fall data: assessing llms and diffusion model for wearable   fall detection,cs.cl cs.cv,"training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. to address this, we explore the potential of large language models (llms) for generating synthetic fall data. this study evaluates text-to-motion (t2m, sato, parco) and text-to-text models (gpt4o, gpt4, gemini) in simulating realistic fall scenarios. we generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a long short-term memory (lstm) model. additionally, we compare llm-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with llm-generated data performing best in low-frequency settings (e.g., 20hz) while showing instability in high-frequency datasets (e.g., 200hz). while text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. an ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. these findings provide insights into optimizing synthetic data generation for fall detection models.",,2025-05-06,,"['sana alamgeer', 'yasine souissi', 'anne h. h. ngu']"
2505.04664,advancing 3d medical image segmentation: unleashing the potential of   planarian neural networks in artificial intelligence,eess.iv cs.ai cs.cv,"our study presents pnn-unet as a method for constructing deep neural networks that replicate the planarian neural network (pnn) structure in the context of 3d medical image data. planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organism's neurological system. accordingly, pnn-unet comprises a deep-unet and a wide-unet as the nerve cords, with a densely connected autoencoder performing the role of the brain. this distinct architecture offers advantages over both monolithic (unet) and modular networks (ensemble-unet). our outcomes on a 3d mri hippocampus dataset, with and without data augmentation, demonstrate that pnn-unet outperforms the baseline unet and several other unet variants in image segmentation.",,2025-05-06,,"['ziyuan huang', 'kevin huggins', 'srikar bellur']"
2505.04672,histo-miner: deep learning based tissue features extraction pipeline   from h&e whole slide images of cutaneous squamous cell carcinoma,cs.cv q-bio.qm,"recent advancements in digital pathology have enabled comprehensive analysis of whole-slide images (wsi) from tissue samples, leveraging high-resolution microscopy and computational capabilities. despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. here we propose histo-miner, a deep learning-based pipeline for analysis of skin wsis and generate two datasets with labeled nuclei and tumor regions. we develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cscc), a frequent non-melanoma skin cancer. utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented wsis respectively, both from cscc patients, histo-miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. performance of trained models positively compares to state of the art with multi-class panoptic quality (mpq) of 0.569 for nucleus segmentation, macro-averaged f1 of 0.832 for nucleus classification and mean intersection over union (miou) of 0.884 for tumor region segmentation. from these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. here, we use histo-miner to predict cscc patient response to immunotherapy based on pre-treatment wsis from 45 patients. histo-miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. this highlights the applicability of histo-miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.",,2025-05-07,,"['lucas sancéré', 'carina lorenz', 'doris helbig', 'oana-diana persa', 'sonja dengler', 'alexander kreuter', 'martim laimer', 'anne fröhlich', 'jennifer landsberg', 'johannes brägelmann', 'katarzyna bozek']"
2505.04713,comparison of visual trackers for biomechanical analysis of running,cs.cv cs.lg,"human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. these developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.   this work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. the proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. the experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. we propose a post-processing module for outlier detection and fusion prediction in the joint angles.   the experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. when integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. the experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. however, there is still room for improvement in applications where high accuracy is required.",,2025-05-07,,"['luis f. gomez', 'gonzalo garrido-lopez', 'julian fierrez', 'aythami morales', 'ruben tolosana', 'javier rueda', 'enrique navarro']"
2505.04718,lay-your-scene: natural scene layout generation with diffusion   transformers,cs.cv cs.lg,"we present lay-your-scene (shorthand layousyn), a novel text-to-layout generation pipeline for natural scenes. prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. in this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion transformer architecture trained in an open-vocabulary manner for conditional layout generation. extensive experiments demonstrate that layousyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. additionally, we present two applications of layousyn. first, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. second, we present a pipeline for adding objects to images, demonstrating the potential of layousyn in image editing applications.",,2025-05-07,,"['divyansh srivastava', 'xiang zhang', 'he wen', 'chenru wen', 'zhuowen tu']"
2505.04720,false promises in medical imaging ai? assessing validity of   outperformance claims,cs.cv,"performance comparisons are fundamental in medical imaging artificial intelligence (ai) research, often driving claims of superiority based on relative improvements in common performance metrics. however, such claims frequently rely solely on empirical mean performance. in this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. we quantify the probability of false claims based on a bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. according to our results, the majority (>80%) of papers claims outperformance when introducing a new method. our analysis further revealed a high probability (>5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. these findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging ai are frequently unsubstantiated, posing a risk of misdirecting future research efforts.",,2025-05-07,,"['evangelia christodoulou', 'annika reinke', 'pascaline andrè', 'patrick godau', 'piotr kalinowski', 'rola houhou', 'selen erkan', 'carole h. sudre', 'ninon burgos', 'sofiène boutaj', 'sophie loizillon', 'maëlys solal', 'veronika cheplygina', 'charles heitz', 'michal kozubek', 'michela antonelli', 'nicola rieke', 'antoine gilson', 'leon d. mayer', 'minu d. tizabi', 'm. jorge cardoso', 'amber simpson', 'annette kopp-schneider', 'gaël varoquaux', 'olivier colliot', 'lena maier-hein']"
2505.04740,hyb-kan vit: hybrid kolmogorov-arnold networks augmented vision   transformer,cs.cv,"this study addresses the inherent limitations of multi-layer perceptrons (mlps) in vision transformers (vits) by introducing hybrid kolmogorov-arnold network (kan)-vit (hyb-kan vit), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the vit architecture and integration of edge detection capabilities of wavelet functions. we propose two key modules: efficient-kan (eff-kan), which replaces mlp layers with spline functions and wavelet-kan (wav-kan), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. these modules are systematically integrated in vit encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. experiments on imagenet-1k (image recognition), coco (object detection and instance segmentation), and ade20k (semantic segmentation) demonstrate state-of-the-art performance with hyb-kan vit. ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. the framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures.",,2025-05-07,,"['sainath dey', 'mitul goswami', 'jashika sethi', 'prasant kumar pattnaik']"
2505.04758,lightweight rgb-d salient object detection from a speed-accuracy   tradeoff perspective,cs.cv,"current rgb-d methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. to balance the efficiency and performance, we propose a speed-accuracy tradeoff network (satnet) for lightweight rgb-d sod from three fundamental perspectives: depth quality, modality fusion, and feature representation. concerning depth quality, we introduce the depth anything model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. for modality fusion, we propose a decoupled attention module (dam) to explore the consistency within and between modalities. here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. for feature representation, we develop a dual information representation module (dirm) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. dirm models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. finally, we design a dual feature aggregation module (dfam) in the decoder to aggregate texture and saliency features. extensive experiments on five public rgb-d sod datasets indicate that the proposed satnet excels state-of-the-art (sota) cnn-based heavyweight models and achieves a lightweight framework with 5.2 m parameters and 415 fps.",,2025-05-07,,"['songsong duan', 'xi yang', 'nannan wang', 'xinbo gao']"
2505.04769,"vision-language-action models: concepts, progress, applications and   challenges",cs.cv,"vision-language-action (vla) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. this foundational review presents a comprehensive synthesis of recent advancements in vision-language-action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. we begin by establishing the conceptual foundations of vla systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (vlms), action planners, and hierarchical controllers. our methodology adopts a rigorous literature review framework, covering over 80 vla models published in the past three years. key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. we explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. the review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. drawing from the state-of-the-art, we propose targeted solutions including agentic ai adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. in our forward-looking discussion, we outline a future roadmap where vla models, vlms, and agentic ai converge to power socially aligned, adaptive, and general-purpose embodied agents. this work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >vision-language-action, agentic ai, ai agents, vision-language models",,2025-05-07,,"['ranjan sapkota', 'yang cao', 'konstantinos i. roumeliotis', 'manoj karkee']"
2505.04793,detreidx: a stress-test dataset for real-world uav-based person   recognition,cs.cv,"person reidentification (reid) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. this paper introduces detreidx, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to reid under real-world conditions. detreidx is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. more important, as a key novelty, detreidx subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person reid. plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, reid, and action recognition. in order to provide empirical evidence of detreidx usefulness, we considered the specific tasks of human detection and reid, where sota methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in rank-1 reid) when exposed to detreidxs conditions. the dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/detreidx/",,2025-05-07,,"['kailash a. hambarde', 'nzakiese mbongo', 'pavan kumar mp', 'satish mekewad', 'carolina fernandes', 'gökhan silahtaroğlu', 'alice nithya', 'pawan wasnik', 'md. rashidunnabi', 'pranita samale', 'hugo proença']"
2505.04813,wir3d: visually-informed and geometry-aware 3d shape abstraction,cs.gr cs.cv,"we present wir3d, a technique for abstracting 3d shapes through a sparse set of visually meaningful curves in 3d. we optimize the parameters of bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. we leverage the intermediate activations of a pre-trained foundation model (clip) to guide our optimization process. we divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. our second phase supervision is spatially guided by a novel localized keypoint loss. this spatial guidance enables user control over abstracted features. we ensure fidelity to the original surface through a neural sdf loss, which allows the curves to be used as intuitive deformation handles. we successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.",,2025-05-07,,"['richard liu', 'daniel fu', 'noah tan', 'itai lang', 'rana hanocka']"
2505.04835,are synthetic corruptions a reliable proxy for real-world corruptions?,cs.cv,"deep learning (dl) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. collecting diverse real-world data for testing the robustness of dl models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. however, are synthetic corruptions a reliable proxy for real-world corruptions? to answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. we further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. open-source code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation",,2025-05-07,,"['shashank agnihotri', 'david schader', 'nico sharei', 'mehmet ege kaçar', 'margret keuper']"
2505.04836,integrated image reconstruction and target recognition based on deep   learning technique,eess.sp cs.cv,"computational microwave imaging (cmi) has gained attention as an alternative technique for conventional microwave imaging techniques, addressing their limitations such as hardware-intensive physical layer and slow data collection acquisition speed to name a few. despite these advantages, cmi still encounters notable computational bottlenecks, especially during the image reconstruction stage. in this setting, both image recovery and object classification present significant processing demands. to address these challenges, our previous work introduced classigan, which is a generative deep learning model designed to simultaneously reconstruct images and classify targets using only back-scattered signals. in this study, we build upon that framework by incorporating attention gate modules into classigan. these modules are intended to refine feature extraction and improve the identification of relevant information. by dynamically focusing on important features and suppressing irrelevant ones, the attention mechanism enhances the overall model performance. the proposed architecture, named att-classigan, significantly reduces the reconstruction time compared to traditional cmi approaches. furthermore, it outperforms current advanced methods, delivering improved normalized mean squared error (nmse), higher structural similarity index (ssim), and better classification outcomes for the reconstructed targets.",,2025-05-07,,"['cien zhang', 'jiaming zhang', 'jiajun he', 'okan yurduseven']"
2505.04838,seeing cells clearly: evaluating machine vision strategies for microglia   centroid detection in 3d images,cs.cv,"microglia are important cells in the brain, and their shape can tell us a lot about brain health. in this project, i test three different tools for finding the center points of microglia in 3d microscope images. the tools include ilastik, 3d morph, and omnipose. i look at how well each one finds the cells and how their results compare. my findings show that each tool sees the cells in its own way, and this can affect the kind of information we get from the images.",,2025-05-07,,['youjia zhang']
2505.04850,orxe: orchestrating experts for dynamically configurable efficiency,cs.cv,"this paper presents orxe, a modular and adaptable framework for achieving real-time configurable efficiency in ai models. by leveraging a collection of pre-trained experts with diverse computational costs and performance levels, orxe dynamically adjusts inference pathways based on the complexity of input samples. unlike conventional approaches that require complex metamodel training, orxe achieves high efficiency and flexibility without complicating the development process. the proposed system utilizes a confidence-based gating mechanism to allocate appropriate computational resources for each input. orxe also supports adjustments to the preference between inference cost and prediction performance across a wide range during runtime. we implemented a training-free orxe system for image classification tasks, evaluating its efficiency and accuracy across various devices. the results demonstrate that orxe achieves superior performance compared to individual experts and other dynamic models in most cases. this approach can be extended to other applications, providing a scalable solution for diverse real-world deployment scenarios.",,2025-05-07,,"['qingyuan wang', 'guoxin wang', 'barry cardiff', 'deepu john']"
2505.04851,craft: cultural russian-oriented dataset adaptation for focused   text-to-image generation,cs.ai cs.cl cs.cv cs.cy cs.lg,"despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. this is due to the content of existing large training datasets collected on the internet, which are predominantly based on western european or american popular culture. meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. in an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. we propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the russian one. we explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the kandinsky 3.1 text-to-image model. human evaluation results demonstrate an increase in the level of awareness of russian culture in the model.",10.1134/s1064562424602324,2025-05-07,,"['viacheslav vasilev', 'vladimir arkhipkin', 'julia agafonova', 'tatiana nikulina', 'evelina mironova', 'alisa shichanina', 'nikolai gerasimenko', 'mikhail shoytov', 'denis dimitrov']"
2505.04860,d-coda: diffusion for coordinated dual-arm data augmentation,cs.ro cs.ai cs.cv cs.lg,"learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. however, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. while prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. in this work, we propose diffusion for coordinated dual-arm data augmentation (d-coda), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. it employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. we evaluate d-coda on 5 simulated and 3 real-world tasks. our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. our project website is at: https://dcodaaug.github.io/d-coda/.",,2025-05-07,,"['i-chun arthur liu', 'jason chen', 'gaurav sukhatme', 'daniel seita']"
2505.04861,mix-qsam: mixed-precision quantization of the segment anything model,cs.cv,"the segment anything model (sam) is a popular vision foundation model; however, its high computational and memory demands make deployment on resource-constrained devices challenging. while post-training quantization (ptq) is a practical approach for reducing computational overhead, existing ptq methods rely on fixed bit-width quantization, leading to suboptimal accuracy and efficiency. to address this limitation, we propose mix-qsam, a mixed-precision ptq framework for sam. first, we introduce a layer-wise importance score, derived using kullback-leibler (kl) divergence, to quantify each layer's contribution to the model's output. second, we introduce cross-layer synergy, a novel metric based on causal mutual information, to capture dependencies between adjacent layers. this ensures that highly interdependent layers maintain similar bit-widths, preventing abrupt precision mismatches that degrade feature propagation and numerical stability. using these metrics, we formulate an integer quadratic programming (iqp) problem to determine optimal bit-width allocation under model size and bit-operation constraints, assigning higher precision to critical layers while minimizing bit-width in less influential layers. experimental results demonstrate that mix-qsam consistently outperforms existing ptq methods on instance segmentation and object detection tasks, achieving up to 20% higher average precision under 6-bit and 4-bit mixed-precision settings, while maintaining computational efficiency.",,2025-05-07,,"['navin ranjan', 'andreas savakis']"
2505.04864,auto-regressive transformation for image alignment,cs.cv cs.ai,"existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. we thus propose auto-regressive transformation (art), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. by incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. extensive experiments across diverse datasets demonstrate that art significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.",,2025-05-07,,"['kanggeon lee', 'soochahn lee', 'kyoung mu lee']"
2505.04888,cross-branch orthogonality for improved generalization in face deepfake   detection,cs.cv cs.ai,"remarkable advancements in generative ai technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. in particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. this is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. to combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. a novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. comprehensive experiments on three public benchmarks: faceforensics++, celeb-df, and the deepfake detection challenge (dfdc) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the celeb-df dataset and 7% on the dfdc dataset in a cross-dataset evaluation setting.",,2025-05-07,,"['tharindu fernando', 'clinton fookes', 'sridha sridharan', 'simon denman']"
2505.04899,owt: a foundational organ-wise tokenization framework for medical   imaging,cs.cv,"recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. these issues are especially critical in medical imaging. to address these limitations, we propose an organ-wise tokenization (owt) framework with a token group-based reconstruction (tgr) training paradigm. unlike conventional approaches that produce holistic features, owt explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. experiments on ct and mri datasets demonstrate the effectiveness of owt in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. these findings underscore the potential of owt as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.",,2025-05-07,,"['sifan song', 'siyeop yoon', 'pengfei jin', 'sekeun kim', 'matthew tivnan', 'yujin oh', 'runqi meng', 'ling chen', 'zhiliang lyu', 'dufan wu', 'ning guo', 'xiang li', 'quanzheng li']"
2505.04905,pro2sam: mask prompt to sam with grid points for weakly supervised   object localization,cs.cv,"weakly supervised object localization (wsol), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. current studies focus on the class activation map (cam) of cnn and the self-attention map of transformer to identify the region of objects. however, both cam and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of wsol. to address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in segment anything model (sam) to boost the activation of integral object regions. further, to alleviate the semantic ambiguity issue accrued in single point prompt-based sam, we propose an innovative mask prompt to sam (pro2sam) network with grid points for wsol task. first, we devise a global token transformer (gtformer) to generate a coarse-grained foreground map as a flexible mask prompt, where the gtformer jointly embeds patch tokens and novel global tokens to learn foreground semantics. secondly, we deliver grid points as dense prompts into sam to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to sam, where the mask with the highest score is viewed as the final localization map. experiments show that the proposed pro2sam achieves state-of-the-art performance on both cub-200-2011 and ilsvrc, with 84.03\% and 66.85\% top-1 loc, respectively.",,2025-05-07,,"['xi yang', 'songsong duan', 'nannan wang', 'xinbo gao']"
2505.04911,spatialprompting: keyframe-driven zero-shot spatial reasoning with   off-the-shelf multimodal large language models,cs.cv cs.ai cs.cl,"this study introduces spatialprompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3d) environments. unlike existing methods that rely on expensive 3d-specific fine-tuning with specialized 3d inputs such as point clouds or voxel-based features, spatialprompting employs a keyframe-driven prompt generation strategy. this framework uses metrics such as vision-language similarity, mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3d structures. the proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as scanqa and sqa3d, across several metrics. the proposed method effectively eliminates the need for specialized 3d inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.",,2025-05-07,,"['shun taguchi', 'hideki deguchi', 'takumi hamazaki', 'hiroyuki sakai']"
2505.04913,advanced 3d imaging approach to tsv/tgv metrology and inspection using   only optical microscopy,eess.iv cs.cv physics.optics,"this paper introduces an innovative approach to silicon and glass via inspection, which combines hybrid field microscopy with photometric stereo. conventional optical microscopy techniques are generally limited to superficial inspections and struggle to effectively visualize the internal structures of silicon and glass vias. by utilizing various lighting conditions for 3d reconstruction, the proposed method surpasses these limitations. by integrating photometric stereo to the traditional optical microscopy, the proposed method not only enhances the capability to detect micro-scale defects but also provides a detailed visualization of depth and edge abnormality, which are typically not visible with conventional optical microscopy inspection. the experimental results demonstrated that the proposed method effectively captures intricate surface details and internal structures. quantitative comparisons between the reconstructed models and actual measurements present the capability of the proposed method to significantly improve silicon and glass via inspection process. as a result, the proposed method achieves enhanced cost-effectiveness while maintaining high accuracy and repeatability, suggesting substantial advancements in silicon and glass via inspection techniques",,2025-05-07,,['gugeong sung']
2505.04915,glyphmastero: a glyph encoder for high-fidelity scene text editing,cs.cv,"scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. while diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. these methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like chinese. in such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. we present glyphmastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. our key insight is that existing methods, despite using pretrained ocr models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. to address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. meanwhile, our model implements a feature pyramid network to fuse the multi-scale ocr backbone features at the global-level. through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region fr\'echet inception distance by 53.28\%.",,2025-05-07,,"['tong wang', 'ting liu', 'xiaochao qu', 'chengjing wu', 'luoqi liu', 'xiaolin hu']"
2505.04917,a simple detector with frame dynamics is a strong tracker,cs.cv,"infrared object tracking plays a crucial role in anti-unmanned aerial vehicle (anti-uav) applications. existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. to address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. our method is based on object detection and achieves significant improvements through two key innovations. first, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared uav tracking scenarios. notably, we achieve state-of-the-art performance in the 4th anti-uav challenge, securing 1st place in track 1 and 2nd place in track 2.",,2025-05-07,,"['chenxu peng', 'chenxu wang', 'minrui zou', 'danyang li', 'zhengpeng yang', 'yimian dai', 'ming-ming cheng', 'xiang li']"
2505.04921,"perception, reason, think, and plan: a survey on large multimodal   reasoning models",cs.cv cs.cl,"reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. in artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. large multimodal reasoning models (lmrms) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. as research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. while instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. to address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. first, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. next, we examine recent approaches that unify reasoning into multimodal llms, with advances such as multimodal chain-of-thought (mcot) and multimodal reinforcement learning enabling richer and more structured reasoning chains. finally, drawing on empirical insights from challenging benchmarks and experimental cases of openai o3 and o4-mini, we discuss the conceptual direction of native large multimodal reasoning models (n-lmrms), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.",,2025-05-07,,"['yunxin li', 'zhenyu liu', 'zitao li', 'xuanyu zhang', 'zhenran xu', 'xinyu chen', 'haoyuan shi', 'shenyuan jiang', 'xintong wang', 'jifang wang', 'shouzheng huang', 'xinping zhao', 'borui jiang', 'lanqing hong', 'longyue wang', 'zhuotao tian', 'baoxing huai', 'wenhan luo', 'weihua luo', 'zheng zhang', 'baotian hu', 'min zhang']"
2505.04922,canny2palm: realistic and controllable palmprint generation for   large-scale pre-training,cs.cv,"palmprint recognition is a secure and privacy-friendly method of biometric identification. one of the major challenges to improve palmprint recognition accuracy is the scarcity of palmprint data. recently, a popular line of research revolves around the synthesis of virtual palmprints for large-scale pre-training purposes. in this paper, we propose a novel synthesis method named canny2palm that extracts palm textures with canny edge detector and uses them to condition a pix2pix network for realistic palmprint generation. by re-assembling palmprint textures from different identities, we are able to create new identities by seeding the generator with new assemblies. canny2palm not only synthesizes realistic data following the distribution of real palmprints but also enables controllable diversity to generate large-scale new identities. on open-set palmprint recognition benchmarks, models pre-trained with canny2palm synthetic data outperform the state-of-the-art with up to 7.2% higher identification accuracy. moreover, the performance of models pre-trained with canny2palm continues to improve given 10,000 synthetic ids while those with existing methods already saturate, demonstrating the potential of our method for large-scale pre-training.",,2025-05-07,,"['xingzeng lan', 'xing duan', 'chen chen', 'weiyu lin', 'bo wang']"
2505.04941,building-guided pseudo-label learning for cross-modal building damage   mapping,cs.cv,"accurate building damage assessment using bi-temporal multi-modal remote sensing images is essential for effective disaster response and recovery planning. this study proposes a novel building-guided pseudo-label learning framework to address the challenges of mapping building damage from pre-disaster optical and post-disaster sar images. first, we train a series of building extraction models using pre-disaster optical images and building labels. to enhance building segmentation, we employ multi-model fusion and test-time augmentation strategies to generate pseudo-probabilities, followed by a low-uncertainty pseudo-label training method for further refinement. next, a change detection model is trained on bi-temporal cross-modal images and damaged building labels. to improve damage classification accuracy, we introduce a building-guided low-uncertainty pseudo-label refinement strategy, which leverages building priors from the previous step to guide pseudo-label generation for damaged buildings, reducing uncertainty and enhancing reliability. experimental results on the 2025 ieee grss data fusion contest dataset demonstrate the effectiveness of our approach, which achieved the highest miou score (54.28%) and secured first place in the competition.",,2025-05-08,,"['jiepan li', 'he huang', 'yu sheng', 'yujun guo', 'wei he']"
2505.04946,t2vtextbench: a human evaluation benchmark for textual control in video   generation models,cs.cv cs.ai cs.cl cs.lg,"thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. however, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. in this work, we introduce t2vtextbench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. we evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. these results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.",,2025-05-08,,"['xuyang guo', 'jiayan huo', 'zhenmei shi', 'zhao song', 'jiahao zhang', 'jiale zhao']"
2505.04959,more-3dgsmr: motion-resolved reconstruction framework for free-breathing   pulmonary mri based on 3d gaussian representation,eess.iv cs.cv,"this study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (mri), utilizing a three-dimensional gaussian representation (3dgs). the proposed method leverages 3dgs to address the challenges of motion-resolved 3d isotropic pulmonary mri reconstruction by enabling data smoothing between voxels for continuous spatial representation. pulmonary mri data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. a 3dgs framework is then applied to reconstruct a reference image volume from the first motion state. subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (dvfs), which are used to generate the remaining motion states through spatial transformation of the reference volume. the proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. the experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary mr images. compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. the proposed unsupervised 3dgs-based reconstruction method enables accurate motion-resolved pulmonary mri with isotropic spatial resolution. its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary mr imaging.",,2025-05-08,,"['tengya peng', 'ruyi zha', 'qing zou']"
2505.04961,add: physics-based motion imitation with adversarial differential   discriminators,cs.gr cs.ai cs.cv cs.ro,"multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. the performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. these limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. to bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. the proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. we demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. results are best visualized through https://youtu.be/rz8byce9e2w.",,2025-05-08,,"['ziyu zhang', 'sergey bashkirov', 'dun yang', 'michael taylor', 'xue bin peng']"
2505.04962,an efficient method for accurate pose estimation and error correction of   cuboidal objects,cs.cv cs.ro,"the proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. this paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. however, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. this paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.",,2025-05-08,,"['utsav rai', 'hardik mehta', 'vismay vakharia', 'aditya choudhary', 'amit parmar', 'rolif lima', 'kaushik das']"
2505.04964,cag-vlm: fine-tuning of a large-scale model to recognize angiographic   images for next-generation diagnostic systems,cs.cv,"coronary angiography (cag) is the gold-standard imaging modality for evaluating coronary artery disease, but its interpretation and subsequent treatment planning rely heavily on expert cardiologists. to enable ai-based decision support, we introduce a two-stage, physician-curated pipeline and a bilingual (japanese/english) cag image-report dataset. first, we sample 14,686 frames from 539 exams and annotate them for key-frame detection and left/right laterality; a convnext-base cnn trained on this data achieves 0.96 f1 on laterality classification, even on low-contrast frames. second, we apply the cnn to 243 independent exams, extract 1,114 key frames, and pair each with its pre-procedure report and expert-validated diagnostic and treatment summary, yielding a parallel corpus. we then fine-tune three open-source vlms (paligemma2, gemma3, and conceptclip-enhanced gemma3) via lora and evaluate them using vlscore and cardiologist review. although paligemma2 w/lora attains the highest vlscore, gemma3 w/lora achieves the top clinician rating (mean 7.20/10); we designate this best-performing model as cag-vlm. these results demonstrate that specialized, fine-tuned vlms can effectively assist cardiologists in generating clinical reports and treatment recommendations from cag images.",,2025-05-08,,"['yuto nakamura', 'satoshi kodera', 'haruki settai', 'hiroki shinohara', 'masatsugu tamura', 'tomohiro noguchi', 'tatsuki furusawa', 'ryo takizawa', 'tempei kabayama', 'norihiko takeda']"
2505.04965,densegrounding: improving dense language-vision semantics for   ego-centric 3d visual grounding,cs.cv,"enabling intelligent agents to comprehend and interact with 3d environments through natural language is crucial for advancing robotics and human-computer interaction. a fundamental task in this field is ego-centric 3d visual grounding, where agents locate target objects in real-world 3d spaces based on verbal descriptions. however, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. we propose densegrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. for visual features, we introduce the hierarchical scene semantic enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. for text descriptions, we propose a language semantic enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. extensive experiments show that densegrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the sota in egocentric 3d visual grounding. our method also achieves 1st place and receives the innovation award in the cvpr 2024 autonomous grand challenge multi-view 3d visual grounding track, validating its effectiveness and robustness.",,2025-05-08,,"['henry zheng', 'hao shi', 'qihang peng', 'yong xien chng', 'rui huang', 'yepeng weng', 'zhongchao shi', 'gao huang']"
2505.04969,general transform: a unified framework for adaptive transform to enhance   representations,cs.lg cs.cl cs.cv,"discrete transforms, such as the discrete fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. however, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. in this work, we propose general transform (gt), an adaptive transform-based representation designed for machine learning applications. unlike conventional transforms, gt learns data-driven mapping tailored to the dataset and task of interest. here, we demonstrate that models incorporating gt outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.",,2025-05-08,,"['gekko budiutama', 'shunsuke daimon', 'hirofumi nishi', 'yu-ichiro matsushita']"
2505.04972,ai and vision based autonomous navigation of nano-drones in   partially-known environments,cs.ro cs.ai cs.cv cs.lg cs.ni,"the miniaturisation of sensors and processors, the advancements in connected edge intelligence, and the exponential interest in artificial intelligence are boosting the affirmation of autonomous nano-size drones in the internet of robotic things ecosystem. however, achieving safe autonomous navigation and high-level tasks such as exploration and surveillance with these tiny platforms is extremely challenging due to their limited resources. this work focuses on enabling the safe and autonomous flight of a pocket-size, 30-gram platform called crazyflie 2.1 in a partially known environment. we propose a novel ai-aided, vision-based reactive planning method for obstacle avoidance under the ambit of integrated sensing, computing and communication paradigm. we deal with the constraints of the nano-drone by splitting the navigation task into two parts: a deep learning-based object detector runs on the edge (external hardware) while the planning algorithm is executed onboard. the results show the ability to command the drone at $\sim8$ frames-per-second and a model performance reaching a coco mean-average-precision of $60.8$. field experiments demonstrate the feasibility of the solution with the drone flying at a top speed of $1$ m/s while steering away from an obstacle placed in an unknown position and reaching the target destination. the outcome highlights the compatibility of the communication delay and the model performance with the requirements of the real-time navigation task. we provide a feasible alternative to a fully onboard implementation that can be extended to autonomous exploration with nano-drones.",,2025-05-08,,"['mattia sartori', 'chetna singhal', 'neelabhro roy', 'davide brunelli', 'james gross']"
2505.04974,realign: bilingual text-to-motion generation via step-aware   reward-guided alignment,cs.cv,"bilingual text-to-motion generation, which synthesizes 3d human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. however, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. to address these challenges, we propose bihumanml3d, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. furthermore, we propose a bilingual motion diffusion model (bimd), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. building upon this, we propose reward-guided sampling alignment (realign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. this reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. project page: https://wengwanjiang.github.io/realign-page/.",,2025-05-08,,"['wanjiang weng', 'xiaofeng tan', 'hongsong wang', 'pan zhou']"
2505.04996,inter-diffusion generation model of speakers and listeners for effective   communication,cs.gr cs.cv cs.sd eess.as,"full-body gestures play a pivotal role in natural interactions and are crucial for achieving effective communication. nevertheless, most existing studies primarily focus on the gesture generation of speakers, overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them. this paper innovatively proposes an inter-diffusion generation model of speakers and listeners for effective communication. for the first time, we integrate the full-body gestures of listeners into the generation framework. by devising a novel inter-diffusion mechanism, this model can accurately capture the complex interaction patterns between speakers and listeners during communication. in the model construction process, based on the advanced diffusion model architecture, we innovatively introduce interaction conditions and the gan model to increase the denoising step size. as a result, when generating gesture sequences, the model can not only dynamically generate based on the speaker's speech information but also respond in realtime to the listener's feedback, enabling synergistic interaction between the two. abundant experimental results demonstrate that compared with the current state-of-the-art gesture generation methods, the model we proposed has achieved remarkable improvements in the naturalness, coherence, and speech-gesture synchronization of the generated gestures. in the subjective evaluation experiments, users highly praised the generated interaction scenarios, believing that they are closer to real life human communication situations. objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators, providing more powerful support for effective communication.",,2025-05-08,,"['jinhe huang', 'yongkang cheng', 'yuming hang', 'gaoge han', 'jinewei li', 'jing zhang', 'xingjian gu']"
2505.05001,stabstitch++: unsupervised online video stitching with spatiotemporal   bidirectional warps,cs.cv cs.ai,"we retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. to address this issue, we propose stabstitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. first, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. compared with stabstitch that sacrifices alignment for stabilization, stabstitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. to establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. experiments exhibit that stabstitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.",,2025-05-08,,"['lang nie', 'chunyu lin', 'kang liao', 'yun zhang', 'shuaicheng liu', 'yao zhao']"
2505.05004,automated thoracolumbar stump rib detection and analysis in a large ct   cohort,cs.cv cs.lg,"thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. while some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. to this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (dice score 0.997 vs. 0.779, p-value < 0.01). in addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. when analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. we show that with partially visible ribs, these features can achieve an f1-score of 0.84 in differentiating stump ribs from regular ones. we publish the model weights and masks for public use.",,2025-05-08,,"['hendrik möller', 'hanna schön', 'alina dima', 'benjamin keinert-weth', 'robert graf', 'matan atad', 'johannes paetzold', 'friederike jungmann', 'rickmer braren', 'florian kofler', 'bjoern menze', 'daniel rueckert', 'jan s. kirschke']"
2505.05008,adaptive contextual embedding for robust far-view borehole detection,cs.cv,"in controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. however, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. to address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., yolo) by explicitly leveraging consistent embedding representations derived through exponential moving average (ema)-based statistical updates.   our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. the pervasive use of ema in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline yolo-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios.",,2025-05-08,,"['xuesong liu', 'tianyu hao', 'emmett j. ientilucci']"
2505.05023,split matching for inductive zero-shot semantic segmentation,cs.cv,"zero-shot semantic segmentation (zss) aims to segment categories that are not annotated during training. while fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. as an alternative to fully supervised approaches, query-based segmentation has shown great latent in zss, as it enables object localization without relying on explicit labels. however, conventional hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of zss. to address this issue, we propose split matching (sm), a novel assignment strategy that decouples hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. to discover unseen candidates, we cluster clip dense features to generate pseudo masks and extract region-level embeddings using cls tokens. matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. additionally, we introduce a multi-scale feature enhancement (mfe) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. sm is the first to introduce decoupled hungarian matching under the inductive zss setting, and achieves state-of-the-art performance on two standard benchmarks.",,2025-05-08,,"['jialei chen', 'xu zheng', 'dongyue li', 'chong yi', 'seigo ito', 'danda pani paudel', 'luc van gool', 'hiroshi murase', 'daisuke deguchi']"
2505.05040,image-text relation prediction for multilingual tweets,cs.cl cs.ai cs.cv,"various social networks have been allowing media uploads for over a decade now. still, it has not always been clear what is their relation with the posted text or even if there is any at all. in this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from twitter posts in latvian along with their manual translations into english. we compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.",,2025-05-08,,"['matīss rikters', 'edison marrese-taylor']"
2505.05041,adnp-15: an open-source histopathological dataset for neuritic plaque   segmentation in human brain whole slide images with frequency domain image   enhancement for stain normalization,eess.iv cs.cv,"alzheimer's disease (ad) is a neurodegenerative disorder characterized by amyloid-beta plaques and tau neurofibrillary tangles, which serve as key histopathological features. the identification and segmentation of these lesions are crucial for understanding ad progression but remain challenging due to the lack of large-scale annotated datasets and the impact of staining variations on automated image analysis. deep learning has emerged as a powerful tool for pathology image segmentation; however, model performance is significantly influenced by variations in staining characteristics, necessitating effective stain normalization and enhancement techniques. in this study, we address these challenges by introducing an open-source dataset (adnp-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of dystrophic tau-positive neurites) in human brain whole slide images. we establish a comprehensive benchmark by evaluating five widely adopted deep learning models across four stain normalization techniques, providing deeper insights into their influence on neuritic plaque segmentation. additionally, we propose a novel image enhancement method that improves segmentation accuracy, particularly in complex tissue structures, by enhancing structural details and mitigating staining inconsistencies. our experimental results demonstrate that this enhancement strategy significantly boosts model generalization and segmentation accuracy. all datasets and code are open-source, ensuring transparency and reproducibility while enabling further advancements in the field.",,2025-05-08,,"['chenxi zhao', 'jianqiang li', 'qing zhao', 'jing bai', 'susana boluda', 'benoit delatour', 'lev stimmer', 'daniel racoceanu', 'gabriel jimenez', 'guanghui fu']"
2505.05043,xtrace: a facial expressive behaviour analysis tool for continuous   affect recognition,cs.cv,"recognising expressive behaviours in face videos is a long-standing challenge in affective computing. despite significant advancements in recent years, it still remains a challenge to build a robust and reliable system for naturalistic and in-the-wild facial expressive behaviour analysis in real time. this paper addresses two key challenges in building such a system: (1). the paucity of large-scale labelled facial affect video datasets with extensive coverage of the 2d emotion space, and (2). the difficulty of extracting facial video features that are discriminative, interpretable, robust, and computationally efficient. toward addressing these challenges, we introduce xtrace, a robust tool for facial expressive behaviour analysis and predicting continuous values of dimensional emotions, namely valence and arousal, from in-the-wild face videos.   to address challenge (1), our affect recognition model is trained on the largest facial affect video data set, containing ~450k videos that cover most emotion zones in the dimensional emotion space, making xtrace highly versatile in analysing a wide spectrum of naturalistic expressive behaviours. to address challenge (2), xtrace uses facial affect descriptors that are not only explainable, but can also achieve a high degree of accuracy and robustness with low computational complexity. the key components of xtrace are benchmarked against three existing tools: mediapipe, openface, and augsburg affect toolbox. on an in-the-wild validation set composed of 50k videos, xtrace achieves 0.86 mean ccc and 0.13 mean absolute error values. we present a detailed error analysis of affect predictions from xtrace, illustrating (a). its ability to recognise emotions with high accuracy across most bins in the 2d emotion space, (b). its robustness to non-frontal head pose angles, and (c). a strong correlation between its uncertainty estimates and its accuracy.",,2025-05-08,,"['mani kumar tellamekala', 'shashank jaiswal', 'thomas smith', 'timur alamev', 'gary mckeown', 'anthony brown', 'michel valstar']"
2505.05054,direct image classification from fourier ptychographic microscopy   measurements without reconstruction,eess.iv cs.ai cs.cv,"the computational imaging technique of fourier ptychographic microscopy (fpm) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. however, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. therefore, in this paper, we investigate the idea of classifying the image content in the fpm measurements directly without performing a reconstruction step first. we show that convolutional neural networks (cnn) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.",,2025-05-08,,"['navya sonal agarwal', 'jan philipp schneider', 'kanchana vaishnavi gandikota', 'syed muhammad kazim', 'john meshreki', 'ivo ihrke', 'michael moeller']"
2505.05062,ulfine: unbiased lightweight fine-tuning for foundation-model-assisted   long-tailed semi-supervised learning,cs.cv,"based on the success of large-scale visual foundation models like clip in various downstream tasks, this paper initially attempts to explore their impact on long-tailed semi-supervised learning (ltssl) by employing the foundation model with three strategies: linear probing (lp), lightweight fine-tuning (lft), and full fine-tuning (fft). our analysis presents the following insights: i) compared to ltssl algorithms trained from scratch, fft results in a decline in model performance, whereas lp and lft, although boosting overall model performance, exhibit negligible benefits to tail classes. ii) lp produces numerous false pseudo-labels due to \textit{underlearned} training data, while lft can reduce the number of these false labels but becomes overconfident about them owing to \textit{biased fitting} training data. this exacerbates the pseudo-labeled and classifier biases inherent in ltssl, limiting performance improvement in the tail classes. with these insights, we propose a unbiased lightweight fine-tuning strategy, \textbf{ulfine}, which mitigates the overconfidence via confidence-aware adaptive fitting of textual prototypes and counteracts the pseudo-labeled and classifier biases via complementary fusion of dual logits. extensive experiments demonstrate that ulfine markedly decreases training costs by over ten times and substantially increases prediction accuracies compared to state-of-the-art methods.",,2025-05-08,,"['enhao zhang', 'chaohua li', 'chuanxing geng', 'songcan chen']"
2505.05073,repsnet: a nucleus instance segmentation model based on boundary   regression and structural re-parameterization,eess.iv cs.cv,"pathological diagnosis is the gold standard for tumor diagnosis, and nucleus instance segmentation is a key step in digital pathology analysis and pathological diagnosis. however, the computational efficiency of the model and the treatment of overlapping targets are the major challenges in the studies of this problem. to this end, a neural network model repsnet was designed based on a nucleus boundary regression and a structural re-parameterization scheme for segmenting and classifying the nuclei in h\&e-stained histopathological images. first, repsnet estimates the boundary position information (bpi) of the parent nucleus for each pixel. the bpi estimation incorporates the local information of the pixel and the contextual information of the parent nucleus. then, the nucleus boundary is estimated by aggregating the bpis from a series of pixels using a proposed boundary voting mechanism (bvm), and the instance segmentation results are computed from the estimated nucleus boundary using a connected component analysis procedure. the bvm intrinsically achieves a kind of synergistic belief enhancement among the bpis from various pixels. therefore, different from the methods available in literature that obtain nucleus boundaries based on a direct pixel recognition scheme, repsnet computes its boundary decisions based on some guidances from macroscopic information using an integration mechanism. in addition, repsnet employs a re-parametrizable encoder-decoder structure. this model can not only aggregate features from some receptive fields with various scales which helps segmentation accuracy improvement, but also reduce the parameter amount and computational burdens in the model inference phase through the structural re-parameterization technique. extensive experiments demonstrated the superiorities of repsnet compared to several typical benchmark models.",10.1007/s11263-024-02332-z,2025-05-08,,"['shengchun xiong', 'xiangru li', 'yunpeng zhong', 'wanfen peng']"
2505.05074,visual affordances: enabling robots to understand object functionality,cs.cv cs.ro,"human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. in this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. to address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. to favour transparency, we introduce the affordance sheet, a document to detail the proposed solution, the datasets, and the validation. as the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.",,2025-05-08,,"['tommaso apicella', 'alessio xompero', 'andrea cavallaro']"
2505.05076,the city that never settles: simulation-based lidar dataset for   long-term place recognition under extreme structural changes,cs.ro cs.cv,"large-scale construction and demolition significantly challenge long-term place recognition (pr) by drastically reshaping urban and suburban environments. existing datasets predominantly reflect limited or indoor-focused changes, failing to adequately represent extensive outdoor transformations. to bridge this gap, we introduce the city that never settles (cns) dataset, a simulation-based dataset created using the carla simulator, capturing major structural changes-such as building construction and demolition-across diverse maps and sequences. additionally, we propose tcr_sym, a symmetric version of the original tcr metric, enabling consistent measurement of structural changes irrespective of source-target ordering. quantitative comparisons demonstrate that cns encompasses more extensive transformations than current real-world benchmarks. evaluations of state-of-the-art lidar-based pr methods on cns reveal substantial performance degradation, underscoring the need for robust algorithms capable of handling significant environmental changes. our dataset is available at https://github.com/hyunho111/cns_dataset.",,2025-05-08,,"['hyunho song', 'dongjae lee', 'seunghun oh', 'minwoo jung', 'ayoung kim']"
2505.05088,ssh-net: a self-supervised and hybrid network for noisy image watermark   removal,cs.mm cs.cv eess.iv,"visible watermark removal is challenging due to its inherent complexities and the noise carried within images. existing methods primarily rely on supervised learning approaches that require paired datasets of watermarked and watermark-free images, which are often impractical to obtain in real-world scenarios. to address this challenge, we propose ssh-net, a self-supervised and hybrid network specifically designed for noisy image watermark removal. ssh-net synthesizes reference watermark-free images using the watermark distribution in a self-supervised manner and adopts a dual-network design to address the task. the upper network, focused on the simpler task of noise removal, employs a lightweight cnn-based architecture, while the lower network, designed to handle the more complex task of simultaneously removing watermarks and noise, incorporates transformer blocks to model long-range dependencies and capture intricate image features. to enhance the model's effectiveness, a shared cnn-based feature encoder is introduced before dual networks to extract common features that both networks can leverage. our code will be available at https://github.com/wenyang001/ssh-net.",,2025-05-08,,"['wenyang liu', 'jianjun gao', 'kim-hui yap']"
2505.05089,nonlinear motion-guided and spatio-temporal aware network for   unsupervised event-based optical flow,cs.cv,"event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. however, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. in this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. therefore, we propose e-nmstflow, a novel unsupervised event-based optical flow network focusing on long-time sequences. we propose a spatio-temporal motion feature aware (stmfa) module and an adaptive motion feature enhancement (amfe) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. extensive experiments demonstrate the effectiveness and superiority of our method. remarkably, our method ranks first among unsupervised learning methods on the mvsec and dsec-flow datasets. our project page is available at https://wynelio.github.io/e-nmstflow.",,2025-05-08,,"['zuntao liu', 'hao zhuang', 'junjie jiang', 'yuhang song', 'zheng fang']"
2505.05091,dispbench: benchmarking disparity estimation to synthetic corruptions,cs.cv cs.lg,"deep learning (dl) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. one such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. however, dl-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.   to address this gap, we introduce dispbench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. dispbench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2d common corruptions across multiple datasets and diverse corruption scenarios. we conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. open-source code for dispbench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation",,2025-05-08,,"['shashank agnihotri', 'amaan ansari', 'annika dackermann', 'fabian rösch', 'margret keuper']"
2505.05098,x-driver: explainable autonomous driving with vision-language models,cs.ro cs.cl cs.cv cs.et,"end-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. however, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. in this paper, we introduce x-driver, a unified multi-modal large language models(mllms) framework designed for closed-loop autonomous driving, leveraging chain-of-thought(cot) and autoregressive modeling to enhance perception and decision-making. we validate x-driver across multiple autonomous driving tasks using public benchmarks in carla simulation environment, including bench2drive[6]. our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(sota) while improving the interpretability of driving decisions. these findings underscore the importance of structured reasoning in end-to-end driving and establish x-driver as a strong baseline for future research in closed-loop autonomous driving.",,2025-05-08,,"['wei liu', 'jiyuan zhang', 'binxiong zheng', 'yufeng hu', 'yingzhan lin', 'zengfeng zeng']"
2505.05112,mdaa-diff: ct-guided multi-dose adaptive attention diffusion model for   pet denoising,eess.iv cs.cv,"acquiring high-quality positron emission tomography (pet) images requires administering high-dose radiotracers, which increases radiation exposure risks. generating standard-dose pet (spet) from low-dose pet (lpet) has become a potential solution. however, previous studies have primarily focused on single low-dose pet denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from ct images. in this work, we propose a novel ct-guided multi-dose adaptive attention denoising diffusion model (mdaa-diff) for multi-dose pet denoising. our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. specifically, this approach incorporates a ct-guided high-frequency wavelet attention (hwa) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from ct images. these extracted features are then incorporated into pet imaging through an adaptive weighted fusion mechanism to enhance edge details. additionally, we propose the dose-adaptive attention (daa) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. extensive experiments on 18f-fdg and 68ga-fapi datasets demonstrate that mdaa-diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. our code is publicly available.",,2025-05-08,,"['xiaolong niu', 'zanting ye', 'xu han', 'yanchao huang', 'hao sun', 'hubing wu', 'lijun lu']"
2505.05136,automated vision-based assistance tools in bronchoscopy: stenosis   severity estimation,cs.cv,"purpose: subglottic stenosis refers to the narrowing of the subglottis, the airway between the vocal cords and the trachea. its severity is typically evaluated by estimating the percentage of obstructed airway. this estimation can be obtained from ct data or through visual inspection by experts exploring the region. however, visual inspections are inherently subjective, leading to less consistent and robust diagnoses. no public methods or datasets are currently available for automated evaluation of this condition from bronchoscopy video.   methods: we propose a pipeline for automated subglottic stenosis severity estimation during the bronchoscopy exploration, without requiring the physician to traverse the stenosed region. our approach exploits the physical effect of illumination decline in endoscopy to segment and track the lumen and obtain a 3d model of the airway. this 3d model is obtained from a single frame and is used to measure the airway narrowing.   results: our pipeline is the first to enable automated and robust subglottic stenosis severity measurement using bronchoscopy images. the results show consistency with ground-truth estimations from ct scans and expert estimations, and reliable repeatability across multiple estimations on the same patient. our evaluation is performed on our new subglottic stenosis dataset of real bronchoscopy procedures data.   conclusion: we demonstrate how to automate evaluation of subglottic stenosis severity using only bronchoscopy. our approach can assist with and shorten diagnosis and monitoring procedures, with automated and repeatable estimations and less exploration time, and save radiation exposure to patients as no ct is required. additionally, we release the first public benchmark for subglottic stenosis severity assessment.",,2025-05-08,,"['clara tomasini', 'javier rodriguez-puigvert', 'dinora polanco', 'manuel viñuales', 'luis riazuelo', 'ana cristina murillo']"
2505.05137,research on anomaly detection methods based on diffusion models,cs.lg cs.cv,"anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. in this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (dpms) to effectively identify anomalies in both image and audio data. the proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. to enhance the framework's performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. extensive experiments on benchmark datasets, including mvtec ad and urbansound8k, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. this research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications.",,2025-05-08,,['yi chen']
2505.05163,probabilistic embeddings for frozen vision-language models: uncertainty   quantification with gaussian process latent variable models,cs.cv cs.lg,"vision-language models (vlms) learn joint representations by mapping images and text into a shared latent space. however, recent research highlights that deterministic embeddings from standard vlms often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. existing approaches tackle this by learning probabilistic embeddings during vlm training, which demands large datasets and does not leverage the powerful representations already learned by large-scale vlms like clip. in this paper, we propose grove, a post-hoc approach to obtaining probabilistic embeddings from frozen vlms. grove builds on gaussian process latent variable model (gplvm) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. once trained, the gaussian process model generates uncertainty-aware probabilistic embeddings. evaluation shows that grove achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.",,2025-05-08,,"['aishwarya venkataramanan', 'paul bodesheim', 'joachim denzler']"
2505.05183,panicar: securing the perception of advanced driving assistance systems   against emergency vehicle lighting,cs.cv cs.lg,"the safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). while previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. in this research, we unveil panicar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. this vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. in addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (adass). we assess seven commercial adass (tesla model 3, ""manufacturer c"", hp, pelsee, azdome, imagebon, rexing), four object detectors (yolo, ssd, retinanet, faster r-cnn), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. we also evaluate four sota flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. to mitigate this risk, we propose caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. our evaluation shows that on yolov3 and faster rcnn, caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. in addition, caracetamol is capable of processing frames at a rate of between 30-50 fps, enabling real-time adas car detection.",,2025-05-08,,"['elad feldman', 'jacob shams', 'dudi biton', 'alfred chen', 'shaoyuan xie', 'satoru koda', 'yisroel mirsky', 'asaf shabtai', 'yuval elovici', 'ben nassi']"
2505.05189,biomed-dpt: dual modality prompt tuning for biomedical vision-language   models,cs.cv cs.ai,"prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (vlms) to the biomedical image classification tasks in few shot scenarios. however, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. in this work, we propose biomed-dpt, a knowledge-enhanced dual modality prompt tuning technique. in designing the text prompt, biomed-dpt constructs a dual prompt including the template-driven clinical prompts and the large language model (llm)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. in designing the vision prompt, biomed-dpt introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. biomed-dpt achieves an average classification accuracy of 66.14\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\% in base classes and 75.97\% in novel classes, surpassing the context optimization (coop) method by 6.20\%, 3.78\%, and 8.04\%, respectively. our code are available at \underline{https://github.com/kanyooo/biomed-dpt}.",,2025-05-08,,"['wei peng', 'kang liu', 'jianchen hu', 'meng zhang']"
2505.05195,concept-based unsupervised domain adaptation,cs.lg cs.ai cs.cv,"concept bottleneck models (cbms) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. this assumption often fails under domain shifts, leading to degraded performance and poor generalization. to address these limitations and improve the robustness of cbms, we propose the concept-based unsupervised domain adaptation (cuda) framework. cuda is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling cbms to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (da) with theoretical guarantees, improving interpretability and establishing new benchmarks for da. experiments demonstrate that our approach significantly outperforms the state-of-the-art cbm and da methods on real-world datasets.",,2025-05-08,,"['xinyue xu', 'yueying hu', 'hui tang', 'yi qin', 'lu mi', 'hao wang', 'xiaomeng li']"
2505.05208,improved brain tumor detection in mri: fuzzy sigmoid convolution in deep   learning,eess.iv cs.cv,"early detection and accurate diagnosis are essential to improving patient outcomes. the use of convolutional neural networks (cnns) for tumor detection has shown promise, but existing models often suffer from overparameterization, which limits their performance gains. in this study, fuzzy sigmoid convolution (fsc) is introduced along with two additional modules: top-of-the-funnel and middle-of-the-funnel. the proposed methodology significantly reduces the number of trainable parameters without compromising classification accuracy. a novel convolutional operator is central to this approach, effectively dilating the receptive field while preserving input data integrity. this enables efficient feature map reduction and enhances the model's tumor detection capability. in the fsc-based model, fuzzy sigmoid activation functions are incorporated within convolutional layers to improve feature extraction and classification. the inclusion of fuzzy logic into the architecture improves its adaptability and robustness. extensive experiments on three benchmark datasets demonstrate the superior performance and efficiency of the proposed model. the fsc-based architecture achieved classification accuracies of 99.17%, 99.75%, and 99.89% on three different datasets. the model employs 100 times fewer parameters than large-scale transfer learning architectures, highlighting its computational efficiency and suitability for detecting brain tumors early. this research offers lightweight, high-performance deep-learning models for medical imaging applications.",,2025-05-08,,"['muhammad irfan', 'anum nawaz', 'riku klen', 'abdulhamit subasi', 'tomi westerlund', 'wei chen']"
2505.05209,eam: enhancing anything with diffusion transformers for blind   super-resolution,cs.cv,"utilizing pre-trained text-to-image (t2i) diffusion models to guide blind super-resolution (bsr) has become a predominant approach in the field. while t2i models have traditionally relied on u-net architectures, recent advancements have demonstrated that diffusion transformers (dit) achieve significantly higher performance in this domain. in this work, we introduce enhancing anything model (eam), a novel bsr method that leverages dit and outperforms previous u-net-based approaches. we introduce a novel block, $\psi$-dit, which effectively guides the dit to enhance image restoration. this block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained dit. to fully exploit the prior guidance capabilities of t2i models and enhance their generalization in bsr, we introduce a progressive masked image modeling strategy, which also reduces training costs. additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. this strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of t2i diffusion priors. our experiments demonstrate that eam achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.",,2025-05-08,,"['haizhen xie', 'kunpeng du', 'qiangyu yan', 'sen lu', 'jianhong han', 'hanting chen', 'hailin hu', 'jie hu']"
2505.05212,hqc-nbv: a hybrid quantum-classical view planning approach,cs.cv,"efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. while classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. this study introduces hqc-nbv, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. we propose a specific hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. this work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.",,2025-05-08,,"['xiaotong yu', 'chang wen chen']"
2505.05215,diffusion model quantization: a review,cs.cv,"recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. to facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. this survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. first, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on u-net architectures and diffusion transformers (dit). we then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. from a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. from a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. in conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. the list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage https://github.com/taylorjocelyn/diffusion-model-quantization.",,2025-05-08,,"['qian zeng', 'chenggong hu', 'mingli song', 'jie song']"
2505.05223,multi-objective reinforcement learning for adaptive personalized   autonomous driving,cs.ro cs.cv cs.lg,"human drivers exhibit individual preferences regarding driving style. adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. however, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. we propose a novel approach using multi-objective reinforcement learning (morl) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\unicode{x2013}$without requiring policy retraining. our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the carla simulator. experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.",,2025-05-08,,"['hendrik surmann', 'jorge de heuvel', 'maren bennewitz']"
2505.05229,does clip perceive art the same way we do?,cs.cv cs.mm,"clip has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it ""see"" the same way humans do - especially when interpreting artworks? in this paper, we investigate clip's ability to extract high-level semantic and stylistic information from paintings, including both human-created and ai-generated imagery. we evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. by designing targeted probing tasks and comparing clip's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. our findings reveal both strengths and limitations in clip's visual representations, particularly in relation to aesthetic cues and artistic intent. we further discuss the implications of these insights for using clip as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.",,2025-05-08,,"['andrea asperti', 'leonardo dessì', 'maria chiara tonetti', 'nico wu']"
2505.05240,padriver: towards personalized autonomous driving,cs.cv,"in this paper, we propose padriver, a novel closed-loop framework for personalized autonomous driving (pad). built upon multi-modal large language model (mllm), padriver takes streaming frames and personalized textual prompts as inputs. it autoaggressively performs scene understanding, danger level estimation and action decision. the predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. moreover, we construct a closed-loop benchmark named pad-highway based on highway-env simulator to comprehensively evaluate the decision performance under traffic rules. the dataset contains 250 hours videos with high-quality annotation to facilitate the development of pad behavior analysis. experimental results on the constructed benchmark show that padriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.",,2025-05-08,,"['genghua kou', 'fan jia', 'weixin mao', 'yingfei liu', 'yucheng zhao', 'ziheng zhang', 'osamu yoshie', 'tiancai wang', 'ying li', 'xiangyu zhang']"
2505.05248,white light specular reflection data augmentation for deep learning   polyp detection,eess.iv cs.cv,"colorectal cancer is one of the deadliest cancers today, but it can be prevented through early detection of malignant polyps in the colon, primarily via colonoscopies. while this method has saved many lives, human error remains a significant challenge, as missing a polyp could have fatal consequences for the patient. deep learning (dl) polyp detectors offer a promising solution. however, existing dl polyp detectors often mistake white light reflections from the endoscope for polyps, which can lead to false positives.to address this challenge, in this paper, we propose a novel data augmentation approach that artificially adds more white light reflections to create harder training scenarios. specifically, we first generate a bank of artificial lights using the training dataset. then we find the regions of the training images that we should not add these artificial lights on. finally, we propose a sliding window method to add the artificial light to the areas that fit of the training images, resulting in augmented images. by providing the model with more opportunities to make mistakes, we hypothesize that it will also have more chances to learn from those mistakes, ultimately improving its performance in polyp detection. experimental results demonstrate the effectiveness of our new data augmentation method.",,2025-05-08,,"['jose angel nuñez', 'fabian vazquez', 'diego adame', 'xiaoyan fu', 'pengfei gu', 'bin fu']"
2505.05279,mtl-ue: learning to learn nothing for multi-task learning,cs.lg cs.cr cs.cv,"most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (stl) models with personal data. nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (mtl), targeting generalist and foundation models that can handle multiple tasks simultaneously. despite their growing importance, mtl data and models have been largely neglected while pursuing unlearnable strategies. this paper presents mtl-ue, the first unified framework for generating unlearnable examples for multi-task data and mtl models. instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. in addition, mtl-ue incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. furthermore, mtl-ue is versatile with good supports for dense prediction tasks in mtl. it is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. extensive experiments show that mtl-ue achieves superior attacking performance consistently across 4 mtl datasets, 3 base ue methods, 5 model backbones, and 5 mtl task-weighting strategies.",,2025-05-08,,"['yi yu', 'song xia', 'siyuan yang', 'chenqi kong', 'wenhan yang', 'shijian lu', 'yap-peng tan', 'alex c. kot']"
2505.05288,placeit3d: language-guided object placement in real 3d scenes,cs.cv cs.ai cs.ro,"we introduce the novel task of language-guided object placement in real 3d scenes. our model is given a 3d scene's point cloud, a 3d asset, and a textual prompt broadly describing where the 3d asset should be placed. the task here is to find a valid placement for the 3d asset that respects the prompt. compared with other language-guided localization tasks in 3d scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3d geometric relationships and free space. we inaugurate this task by proposing a new benchmark and evaluation protocol. we also introduce a new dataset for training 3d llms on this task, as well as the first method to serve as a non-trivial baseline. we believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3d llm models.",,2025-05-08,,"['ahmed abdelreheem', 'filippo aleotti', 'jamie watson', 'zawar qureshi', 'abdelrahman eldesokey', 'peter wonka', 'gabriel brostow', 'sara vicente', 'guillermo garcia-hernando']"
2505.05307,pre-mamba: a 4d state space model for ultra-high-frequent event camera   deraining,cs.cv,"event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. in this paper, we propose pre-mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. our framework introduces a 4d event cloud representation that integrates dual temporal scales to preserve high temporal precision, a spatio-temporal decoupling and fusion module (stdf) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a multi-scale state space model (ms3m) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. enhanced by frequency-domain regularization, pre-mamba achieves superior performance (0.95 sr, 0.91 nr, and 0.4s/m events) with only 0.26m parameters on eventrain-27k, a comprehensive dataset with labeled synthetic and real-world sequences. moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.",,2025-05-08,,"['ciyu ruan', 'ruishan guo', 'zihang gong', 'jingao xu', 'wenhan yang', 'xinlei chen']"
2505.05309,augmented deep contexts for spatially embedded video coding,eess.iv cs.cv,"most neural video codecs (nvcs) only employ temporal references to generate temporal-only contexts and latent prior. these temporal-only nvcs fail to handle large motions or emerging objects due to limited contexts and misaligned latent prior. to relieve the limitations, we propose a spatially embedded video codec (sevc), in which the low-resolution video is compressed for spatial references. firstly, our sevc leverages both spatial and temporal references to generate augmented motion vectors and hybrid spatial-temporal contexts. secondly, to address the misalignment issue in latent prior and enrich the prior information, we introduce a spatial-guided latent prior augmented by multiple temporal latent representations. at last, we design a joint spatial-temporal optimization to learn quality-adaptive bit allocation for spatial references, further boosting rate-distortion performance. experimental results show that our sevc effectively alleviates the limitations in handling large motions or emerging objects, and also reduces 11.9% more bitrate than the previous state-of-the-art nvc while providing an additional low-resolution bitstream. our code and model are available at https://github.com/esakak/sevc.",,2025-05-08,,"['yifan bian', 'chuanbo tang', 'li li', 'dong liu']"
2505.05318,"mapping user trust in vision language models: research landscape,   challenges, and prospects",cs.cv cs.ai cs.cy cs.hc cs.ro,"the rapid adoption of vision language models (vlms), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. this survey reviews studies on trust dynamics in user-vlm interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. literature insights and findings from a workshop with prospective vlm users inform preliminary requirements for future vlm trust studies.",,2025-05-08,,"['agnese chiatti', 'sara bernardini', 'lara shibelski godoy piccolo', 'viola schiaffonati', 'matteo matteucci']"
2505.05321,feature-augmented deep networks for multiscale building segmentation in   high-resolution uav and satellite imagery,cs.cv cs.ai,"accurate building segmentation from high-resolution rgb imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. in this study, we present a comprehensive deep learning framework for multiscale building segmentation using rgb aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. we curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including principal component analysis (pca), visible difference vegetation index (vdvi), morphological building index (mbi), and sobel edge filters from rgb channels. these features guide a res-u-net architecture in learning complex spatial patterns more effectively. we also propose training policies incorporating layer freezing, cyclical learning rates, and superconvergence to reduce training time and resource usage. evaluated on a held-out worldview-3 image, our model achieves an overall accuracy of 96.5%, an f1-score of 0.86, and an intersection over union (iou) of 0.80, outperforming existing rgb-based benchmarks. this study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.",,2025-05-08,,"['chintan b. maniyar', 'minakshi kumar', 'gengchen mai']"
2505.05331,aesthetics without semantics,cs.cv q-bio.nc stat.co,"while it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. we address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. the resulting minimum semantic content (msc) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. we next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.",,2025-05-08,,"['c. alejandro parraga', 'olivier penacchio', 'marcos muňoz gonzalez', 'bogdan raducanu', 'xavier otazu']"
2505.05336,progressive inertial poser: progressive real-time kinematic chain   estimation for 3d full-body pose from three imu sensors,cs.cv,"the motion capture system that supports full-body virtual representation is of key significance for virtual reality. compared to vision-based systems, full-body pose estimation from sparse tracking signals is not limited by environmental conditions or recording range. however, previous works either face the challenge of wearing additional sensors on the pelvis and lower-body or rely on external visual sensors to obtain global positions of key joints. to improve the practicality of the technology for virtual reality applications, we estimate full-body poses using only inertial data obtained from three inertial measurement unit (imu) sensors worn on the head and wrists, thereby reducing the complexity of the hardware system. in this work, we propose a method called progressive inertial poser (progip) for human pose estimation, which combines neural network estimation with a human dynamics model, considers the hierarchical structure of the kinematic chain, and employs a multi-stage progressive network estimation with increased depth to reconstruct full-body motion in real time. the encoder combines transformer encoder and bidirectional lstm (te-bilstm) to flexibly capture the temporal dependencies of the inertial sequence, while the decoder based on multi-layer perceptrons (mlps) transforms high-dimensional features and accurately projects them onto skinned multi-person linear (smpl) model parameters. quantitative and qualitative experimental results on multiple public datasets show that our method outperforms state-of-the-art methods with the same inputs, and is comparable to recent works using six imu sensors.",,2025-05-08,,"['zunjie zhu', 'yan zhao', 'yihan hu', 'guoxiang wang', 'hai qiu', 'bolun zheng', 'chenggang yan', 'feng xu']"
2505.05343,hearing and seeing through clip: a framework for self-supervised sound   source localization,cs.cv cs.sd eess.as,"large-scale vision-language models demonstrate strong multimodal alignment and generalization across diverse tasks. among them, clip stands out as one of the most successful approaches. in this work, we extend the application of clip to sound source localization, proposing a self-supervised method operates without explicit text input. we introduce a framework that maps audios into tokens compatible with clip's text encoder, producing audio-driven embeddings. these embeddings are used to generate sounding region masks, from which visual features are extracted and aligned with the audio embeddings through a contrastive audio-visual correspondence objective. our findings show that alignment knowledge of pre-trained multimodal foundation model enables our method to generate more complete and compact localization for sounding objects. we further propose an llm-guided extension that distills object-aware audio-visual scene understanding into the model during training to enhance alignment. extensive experiments across five diverse tasks demonstrate that our method, in all variants, outperforms state-of-the-art approaches and achieves strong generalization in zero-shot settings.",,2025-05-08,,"['sooyoung park', 'arda senocak', 'joon son chung']"
2505.05356,time of the flight of the gaussians: optimizing depth indirectly in   dynamic radiance fields,cs.gr cs.ai cs.cv,"we present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (c-tof) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. quickly achieving high-fidelity dynamic 3d reconstruction from a single viewpoint is a significant challenge in computer vision. in c-tof radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. this problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3d gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. we incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by gaussians. experimental results show that our approach produces accurate reconstructions under constrained c-tof sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf",,2025-05-08,,"['runfeng li', 'mikhail okunev', 'zixuan guo', 'anh ha duong', 'christian richardt', ""matthew o'toole"", 'james tompkin']"
2505.05367,joint super-resolution and segmentation for 1-m impervious surface area   mapping in china's yangtze river economic belt,cs.cv eess.iv,"we propose a novel joint framework by integrating super-resolution and segmentation, called jointseg, which enables the generation of 1-meter isa maps directly from freely available sentinel-2 imagery. jointseg was trained on multimodal cross-resolution inputs, offering a scalable and affordable alternative to traditional approaches. this synergistic design enables gradual resolution enhancement from 10m to 1m while preserving fine-grained spatial textures, and ensures high classification fidelity through effective cross-scale feature fusion. this method has been successfully applied to the yangtze river economic belt (yreb), a region characterized by complex urban-rural patterns and diverse topography. as a result, a comprehensive isa mapping product for 2021, referred to as isa-1, was generated, covering an area of over 2.2 million square kilometers. quantitative comparisons against the 10m esa worldcover and other benchmark products reveal that isa-1 achieves an f1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by 9.5%, and surpassing other isa datasets by 21.43%-61.07%. in densely urbanized areas (e.g., suzhou, nanjing), isa-1 reduces isa overestimation through improved discrimination of green spaces and water bodies. conversely, in mountainous regions (e.g., ganzi, zhaotong), it identifies significantly more isa due to its enhanced ability to detect fragmented anthropogenic features such as rural roads and sparse settlements, demonstrating its robustness across diverse landscapes. moreover, we present biennial isa maps from 2017 to 2023, capturing spatiotemporal urbanization dynamics across representative cities. the results highlight distinct regional growth patterns: rapid expansion in upstream cities, moderate growth in midstream regions, and saturation in downstream metropolitan areas.",,2025-05-08,,"['jie deng', 'danfeng hong', 'chenyu li', 'naoto yokoya']"
2505.05374,ocularage: a comparative study of iris and periocular images for   pediatric age estimation,eess.iv cs.cv,"estimating a child's age from ocular biometric images is challenging due to subtle physiological changes and the limited availability of longitudinal datasets. although most biometric age estimation studies have focused on facial features and adult subjects, pediatric-specific analysis, particularly of the iris and periocular regions, remains relatively unexplored. this study presents a comparative evaluation of iris and periocular images for estimating the ages of children aged between 4 and 16 years. we utilized a longitudinal dataset comprising more than 21,000 near-infrared (nir) images, collected from 288 pediatric subjects over eight years using two different imaging sensors. a multi-task deep learning framework was employed to jointly perform age prediction and age-group classification, enabling a systematic exploration of how different convolutional neural network (cnn) architectures, particularly those adapted for non-square ocular inputs, capture the complex variability inherent in pediatric eye images. the results show that periocular models consistently outperform iris-based models, achieving a mean absolute error (mae) of 1.33 years and an age-group classification accuracy of 83.82%. these results mark the first demonstration that reliable age estimation is feasible from children's ocular images, enabling privacy-preserving age checks in child-centric applications. this work establishes the first longitudinal benchmark for pediatric ocular age estimation, providing a foundation for designing robust, child-focused biometric systems. the developed models proved resilient across different imaging sensors, confirming their potential for real-world deployment. they also achieved inference speeds of less than 10 milliseconds per image on resource-constrained vr headsets, demonstrating their suitability for real-time applications.",,2025-05-08,,"['naveenkumar g venkataswamy', 'poorna ravi', 'stephanie schuckers', 'masudul h. imtiaz']"
2505.05397,pillarmamba: learning local-global context for roadside point cloud via   hybrid state space model,cs.cv,"serving the intelligent transport system (its) and vehicle-to-everything (v2x) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. however, roadside point cloud oriented 3d object detection has not been effectively explored. to some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. the recent emergence of mamba, based on state space model (ssm), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. in this work, we introduce mamba to pillar-based roadside point cloud perception and propose a framework based on cross-stage state-space group (csg), called pillarmamba. it enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. however, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. to address this, we propose the hybrid state-space block (hsb) to obtain the local-global context of roadside point cloud. specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. the proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: dair-v2x-i. the code will be released soon.",,2025-05-08,,"['zhang zhang', 'chao sun', 'chao yue', 'da wen', 'tianze wang', 'jianghao leng']"
2505.05422,toklip: marry visual tokens to clip for multimodal comprehension and   generation,cs.cv cs.ai cs.cl,"pioneering token-based works such as chameleon and emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. in this paper, we introduce toklip, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (vq) tokens and incorporating clip-level semantics while enabling end-to-end multimodal autoregressive training with standard vq tokens. toklip integrates a low-level discrete vq tokenizer with a vit-based token encoder to capture high-level continuous semantics. unlike previous approaches (e.g., vila-u) that discretize high-level features, toklip disentangles training objectives for comprehension and generation, allowing the direct application of advanced vq tokenizers without the need for tailored quantization operations. our empirical results demonstrate that toklip achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive transformers in both comprehension and generation tasks. the code and models are available at https://github.com/tencentarc/toklip.",,2025-05-08,,"['haokun lin', 'teng wang', 'yixiao ge', 'yuying ge', 'zhichao lu', 'ying wei', 'qingfu zhang', 'zhenan sun', 'ying shan']"
2505.05446,adaptive markup language generation for contextually-grounded visual   document understanding,cs.cv cs.cl,"visual document understanding has become essential with the increase of text-rich visual content. this field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. to address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as markdown, json, html, and tikz, to build highly structured document representations and deliver contextually-grounded responses. we introduce two fine-grained structured datasets: docmark-pile, comprising approximately 3.8m pretraining data pairs for document parsing, and docmark-instruct, featuring 624k fine-tuning data annotations for grounded instruction following. extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart mllms across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. our code and models are released at https://github. com/euphoria16/docmark.",,2025-05-08,,"['han xiao', 'yina xie', 'guanxin tan', 'yinghao chen', 'rui hu', 'ke wang', 'aojun zhou', 'hao li', 'hao shao', 'xudong lu', 'peng gao', 'yafei wen', 'xiaoxin chen', 'shuai ren', 'hongsheng li']"
2505.05456,site: towards spatial intelligence thorough evaluation,cs.cv,"spatial intelligence (si) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. we introduce site, a benchmark dataset towards si thorough evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and si factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental si factor. moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied ai task.",,2025-05-08,,"['wenqi wang', 'reuben tan', 'pengyue zhu', 'jianwei yang', 'zhengyuan yang', 'lijuan wang', 'andrey kolobov', 'jianfeng gao', 'boqing gong']"
2505.05467,streambridge: turning your offline video large language model into a   proactive streaming assistant,cs.cv cs.ai cs.cl,"we present streambridge, a simple yet effective framework that seamlessly transforms offline video-llms into streaming-capable models. it addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. specifically, streambridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing video-llms, enabling continuous proactive responses. to further support streambridge, we construct stream-it, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. extensive experiments show that streambridge significantly improves the streaming understanding capabilities of offline video-llms across various tasks, outperforming even proprietary models such as gpt-4o and gemini 1.5 pro. simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.",,2025-05-08,,"['haibo wang', 'bo feng', 'zhengfeng lai', 'mingze xu', 'shiyu li', 'weifeng ge', 'afshin dehghan', 'meng cao', 'ping huang']"
2505.05469,generating physically stable and buildable lego designs from text,cs.cv,"we introduce legogpt, the first approach for generating physically stable lego brick models from text prompts. to achieve this, we construct a large-scale, physically stable dataset of lego designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. to improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. our experiments show that legogpt produces stable, diverse, and aesthetically pleasing lego designs that align closely with the input text prompts. we also develop a text-based lego texturing method to generate colored and textured designs. we show that our designs can be assembled manually by humans and automatically by robotic arms. we also release our new dataset, stabletext2lego, containing over 47,000 lego structures of over 28,000 unique 3d objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/legogpt/.",,2025-05-08,,"['ava pun', 'kangle deng', 'ruixuan liu', 'deva ramanan', 'changliu liu', 'jun-yan zhu']"
2505.05473,diffusionsfm: predicting structure and motion via ray origin and   endpoint diffusion,cs.cv,"current structure-from-motion (sfm) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. in contrast, we propose a data-driven multi-view reasoning approach that directly infers 3d scene geometry and camera poses from multi-view images. our framework, diffusionsfm, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. to address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. we empirically validate diffusionsfm on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.",,2025-05-08,,"['qitao zhao', 'amy lin', 'jeff tan', 'jason y. zhang', 'deva ramanan', 'shubham tulsiani']"
2505.05474,3d scene generation: a survey,cs.cv,"3d scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied ai. early methods based on procedural rules offered scalability but limited diversity. recent advances in deep generative models (e.g., gans, diffusion models) and 3d representations (e.g., nerf, 3d gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. recent advances like diffusion models bridge 3d scene synthesis and photorealism by reframing generation as image or video synthesis problems. this survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3d-based generation, image-based generation, and video-based generation. we analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. we conclude by discussing key challenges in generation capacity, 3d representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. this review organizes recent advances in 3d scene generation and highlights promising directions at the intersection of generative ai, 3d vision, and embodied intelligence. to track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/awesome-3d-scene-generation.",,2025-05-08,,"['beichen wen', 'haozhe xie', 'zhaoxi chen', 'fangzhou hong', 'ziwei liu']"
2505.05475,svad: from single image to 3d avatar via synthetic data generation with   video diffusion and data augmentation,cs.cv,"creating high-quality animatable 3d human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3d information from a single viewpoint. current approaches face a clear limitation: 3d gaussian splatting (3dgs) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. we present svad, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3dgs avatars. comprehensive evaluations demonstrate that svad outperforms state-of-the-art (sota) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3dgs approaches. extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. by effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3dgs, our work establishes a new approach for high-fidelity avatar generation from a single image input.",,2025-05-08,,['yonwoo choi']
